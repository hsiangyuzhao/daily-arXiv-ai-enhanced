<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.LG](#cs.LG) [Total: 35]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: 本文提出了VL4Gaze，首个大规模基准，用于系统评估和训练视觉语言模型（VLMs）在眼神理解方面的能力。该基准包含48.9万个自动生成的问答对，涵盖12.4万张图像，并将眼神理解任务统一为四个互补的VQA任务：（1）注视物体描述，（2）注视方向描述，（3）注视点定位，（4）模糊问题识别。实验表明，即使大型VLMs在无特定监督的情况下也难以可靠推断眼神语义和空间定位；而通过在VL4Gaze上进行训练，则能显著且一致地提升各项任务表现，凸显了针对性多任务监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽在场景级推理中表现良好，但对眼神理解这一关键社交线索仍研究不足，缺乏系统性评估与训练基准，因此亟需一个专门用于探究与提升模型眼神理解能力的基准数据集。

Method: 构建并发布VL4Gaze，一个包含48.9万组自动标注问答对的大规模视觉-语言数据集，涵盖12.4万张图像，设计四种互补的视觉问答任务以统一表征眼神理解任务，并在多种VLMs上进行上下文学习与微调评估。

Result: 大型VLMs在缺乏特定任务监督时难以准确理解眼神语义与空间位置；但在使用VL4Gaze进行训练后，所有任务均实现显著且一致的性能提升，证明了多任务监督对发展眼神理解能力的关键作用。

Conclusion: VL4Gaze是首个专门针对眼神理解的大型基准，其结果表明，通用预训练模型无法自发具备眼神理解能力，必须通过特定任务的多任务监督才能有效提升。该数据集与代码的公开将推动相关领域研究发展。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [2] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 提出首个真实世界、基于相机的空中语义场景补全（SSC）基准数据集OccuFly，覆盖不同季节和高度的多种场景，支持22类语义，采用摄像头模态实现无激光雷达的数据生成，通过3D重建与2D标注投影自动完成标签传递，显著减少人工标注成本，并对当前SOTA方法进行评估，揭示高空视角下的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 现有语义场景补全（SSC）研究主要集中在地面场景如自动驾驶，而空中场景如无人机飞行仍缺乏系统研究；同时，主流依赖激光雷达的数据采集方式受限于无人机的重量、能耗及法规限制，且高空视角下点云稀疏，难以有效支撑高质量的3D感知。因此亟需一种适用于无人机的新型数据采集与标注方案。

Method: 提出基于摄像头的无激光雷达数据生成框架：利用传统3D重建技术生成空中场景点云，并将部分已标注的2D图像掩码提升至三维空间，实现自动化标签转移，大幅降低人工3D标注成本；构建了多季节、多高度（50m, 40m, 30m）、多场景（城市、工业、农村）的真实数据集OccuFly。

Result: 成功构建首个真实世界、基于相机的空中语义场景补全基准数据集OccuFly，涵盖22个语义类别，支持现有研究无缝接入；验证了所提框架在减少人工标注方面的有效性；在该数据集上对当前最先进的方法进行了基准测试，揭示了高空视角下语义理解的独特挑战，为未来空中3D感知研究提供了重要基础。

Conclusion: OccuFly是首个面向无人机场景的相机驱动型语义场景补全基准，其提出的无激光雷达标注框架具有高效性与可扩展性，推动了空中3D感知从理论走向实际应用，为后续研究提供坚实数据与方法基础。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [3] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: NullBUS是一种多模态混合监督框架，通过引入可选提示（nullable prompts）实现图像与文本提示的联合学习，在无文本提示时自动转为仅依赖图像信息，有效提升乳腺超声图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺超声分割方法依赖文本或空间提示，但多数公开数据集缺乏可靠元数据或报告，导致训练受限于小规模多模态子集，影响模型鲁棒性。

Method: 提出可选提示机制，使用可学习的空嵌入和存在掩码处理缺失文本，使模型在有/无文本提示情况下均能有效工作，统一训练于多源数据。

Result: 在三个公开乳腺超声数据集的统一测试中，取得0.8568的平均IoU和0.9103的平均Dice系数，达到当前最佳性能。

Conclusion: NullBUS通过灵活处理提示缺失问题，实现了在混合提示条件下更鲁棒、更高效的乳腺超声分割，具有良好的应用前景。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [4] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: CHAMMI-75 是一个开放获取的异构多通道显微图像数据集，涵盖75个不同的生物学研究，旨在支持可适应不同显微成像通道的细胞形态量化模型。该数据集通过高多样性显微成像模式提升多通道生物成像任务的性能，推动下一代细胞形态学模型的发展。


<details>
  <summary>Details</summary>
Motivation: 当前细胞形态量化模型通常仅针对单一显微成像类型训练，导致模型在跨研究应用时受限于技术规格不匹配或实验条件超出分布范围，因此需要一个多样化、通用性强的数据集来支持更普适的模型开发。

Method: 从公开来源收集并整理75个不同生物学研究的多通道显微图像，构建CHAMMI-75数据集，并利用其多样性进行模型训练以评估其在跨模态任务中的表现。

Result: 使用CHAMMI-75进行训练的模型在多通道生物成像任务中表现出更好的性能，主要得益于其丰富的显微成像模态多样性。

Conclusion: CHAMMI-75为开发通用、通道自适应的细胞形态学模型提供了重要资源，有助于实现跨研究、跨技术的可重用性，推动生物研究中细胞形态分析的智能化发展。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [5] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 本文提出一种自适应视觉预处理方法，根据图像内容动态调整输入分辨率和空间覆盖范围，以减少视觉冗余。该方法无需修改FastVLM架构或重新训练，即可在不牺牲性能的前提下显著提升推理效率。实验表明，该方法使单图像推理时间减少50%以上，生成时间降低，视觉标记数减少超过55%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在高分辨率输入下存在推理延迟高、计算成本大的问题，且传统静态预处理对简单图像造成冗余计算。因此需要一种能根据图像内容自适应调整预处理策略的方法，以提高部署效率。

Method: 结合内容感知图像分析、自适应分辨率选择与内容感知裁剪，动态优化输入图像的分辨率和空间区域，在不改变原模型结构的前提下实现高效预处理。

Result: 在DocVQA子集上的实验显示，该方法使每图像推理时间减少超50%，平均生成时间下降，视觉标记数量减少超过55%，证明其在效率方面的显著优势。

Conclusion: 输入感知的自适应预处理是一种有效且轻量级的提升视觉语言模型部署效率的方法，适用于现有高效架构如FastVLM，具有良好的可复现性与实用性。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [6] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: ALIVE 是一个本地部署的智能视频学习系统，将传统讲座视频转变为实时互动学习体验。它结合语音识别、大语言模型优化、神经头像合成生成讲解视频，并通过语义与时间戳对齐的内容感知检索机制，实现精准知识点定位。用户可暂停视频并以文本或语音提问，系统以文本或虚拟形象实时回答，支持低延迟响应。该系统采用轻量嵌入模型、FAISS检索和分段预加载策略，保障性能。在医学影像课程上的实验表明其具有高准确率、低延迟和良好用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统讲座视频缺乏实时答疑机制，学习者需外部搜索解决困惑；现有交互系统多依赖云端服务、缺乏讲座上下文理解，且难以兼顾隐私与实时性。因此亟需一种本地化、内容感知、高响应的互动学习系统。

Method: ALIVE 采用三阶段架构：(1) 利用ASR转录与LLM优化生成高质量讲座解说内容；(2) 通过融合语义相似度与时间戳对齐的检索机制，精准定位相关知识片段；(3) 支持多模态交互（文本/语音提问），并以虚拟形象实时反馈解释结果。系统通过轻量级嵌入模型、FAISS索引和分段预加载技术保证实时性与本地运行能力。

Result: 在完整医学影像课程上的评估显示，ALIVE 在检索准确率、响应延迟和用户满意度方面表现优异，能有效提供内容相关、即时且沉浸式的个性化学习支持，显著提升录制讲座的教学价值。

Conclusion: ALIVE 展示了多模态AI结合内容感知检索与本地部署在增强录播课教学效果方面的巨大潜力，为构建下一代交互式学习环境提供了可扩展的技术路径。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [7] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 本文针对三维探地雷达（3D GPR）在地下管道检测中多视角特征关联弱、小目标识别精度低及复杂场景鲁棒性不足的问题，提出一种新型3D管道智能检测框架。通过B/C/D-Scan三视图联合分析策略，结合FDTD仿真与实测数据进行交叉验证，构建三视图特征评估方法；提出DCO-YOLO模型，融合DySample、CGLU和OutlookAttention跨维度相关机制，增强小尺度管道边缘特征提取能力；设计3D-DIoU空间特征匹配算法，引入三维几何约束与中心距离惩罚项，实现多视角标注的自动关联；采用三视图融合策略解决单视角检测固有歧义。实验表明，在复杂多管道场景下，该方法准确率、召回率和mAP分别达到96.2%、93.3%和96.7%，较基线模型提升2.0%、2.1%和0.9%。消融实验验证了动态特征增强模块的协同优化效果，Grad-CAM++热力图显示模型显著增强了对管道几何特征的关注能力。研究将深度学习优化策略与3D GPR物理特性融合，为地下管道的智能识别与定位提供了高效可靠的全新技术框架。


<details>
  <summary>Details</summary>
Motivation: 传统3D GPR在地下管道检测中存在多视角特征关联弱、小目标识别精度低、复杂场景下鲁棒性差等问题，限制了其在实际工程中的应用。亟需一种能够有效融合多视角信息、提升小目标检测能力并增强环境适应性的智能检测方法。

Method: 提出基于B/C/D-Scan三视图联合分析的特征评估方法，构建DCO-YOLO框架（集成DySample、CGLU、OutlookAttention），设计3D-DIoU空间匹配算法，并采用三视图融合策略，结合真实城市地下管道数据进行训练与验证。

Result: 在复杂多管道场景下，该方法的准确率、召回率和平均精度均值（mAP）分别达到96.2%、93.3%和96.7%，较基线模型分别提升2.0%、2.1%和0.9%；消融实验验证了各模块协同作用，热力图分析表明模型对管道几何特征关注显著增强。

Conclusion: 本研究将深度学习优化策略与3D GPR物理特性深度融合，构建了一套高效、可靠、可推广的地下管道智能检测新框架，显著提升了复杂环境下小目标识别的准确性与鲁棒性，具有良好的工程应用前景。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [8] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360 是一种针对 360 度视频的端到端框架，通过仅解码用户选择的视口来显著降低内存占用并提升解码速度。它将视口提取集成到解码过程，并引入时空仿射变换模块以实现基于视角和时间的条件解码。在 6K 分辨率视频上，相比 HNeRV，NeRV360 实现了 7 倍内存减少和 2.5 倍解码速度提升，同时在客观指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 高分辨率 360 度视频使用隐式神经表示（NeRV）时面临内存消耗大和解码速度慢的问题，难以支持实时应用。因此需要一种高效且可扩展的解决方案。

Method: NeRV360 采用端到端设计，将视口提取融入解码流程，并引入空间-时间仿射变换模块，根据当前视角和时间动态调整解码策略，实现仅对用户关注区域进行重建。

Result: 在 6K 分辨率视频上，NeRV360 相比 HNeRV 实现了 7 倍内存减少、2.5 倍解码速度提升，并在图像质量方面取得更好客观指标表现。

Conclusion: NeRV360 有效解决了高分辨率 360 度视频中隐式神经表示的内存与速度瓶颈，为实时交互式全景视频应用提供了可行方案。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [9] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

TL;DR: 本文提出一种名为领域表示注入（DRI）的新颖参数高效微调策略，用于跨模态船舶重识别（CMS Re-ID），以解决模态差异问题。通过将优化视角从权重空间转移到特征空间，DRI在保持视觉基础模型（VFM）完全冻结的前提下，利用轻量级可学习偏移编码器提取包含模态与身份属性的领域特定表示，并通过调制器自适应转换后注入中间层，动态调整特征分布以适应下游任务，无需修改预训练权重。实验表明，该方法在仅需极少可训练参数的情况下达到当前最优性能，在HOSS-ReID数据集上分别取得57.9%和60.5%的mAP，代码已公开。


<details>
  <summary>Details</summary>
Motivation: 跨模态船舶重识别面临显著的模态差异挑战，现有方法依赖大规模成对数据进行预训练，且通用的参数高效微调方法在容量有限的模型上表现不佳，因此亟需一种无需大量标注数据、能有效弥合模态差距的新方法。

Method: 提出领域表示注入（DRI）策略，采用冻结的视觉基础模型，设计轻量级偏移编码器提取领域特定表示，结合中间层上下文信息通过调制器自适应转换，并以加性融合方式注入中间层，实现特征分布的动态调整。

Result: 在多个数据集上取得当前最优性能，尤其在HOSS-ReID数据集上分别达到57.9%和60.5%的mAP，仅使用1.54M和7.05M可训练参数，显著优于现有方法。

Conclusion: DRI通过在特征空间中引入可学习的领域表示注入机制，在不破坏预训练知识的前提下，有效缓解了跨模态差异问题，为资源受限场景下的跨模态重识别提供了高效可行的新范式。

Abstract: Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [10] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出首个全面评估视觉语言模型（VLM）在压缩图像上表现的基准，涵盖超过一百万张不同编码器和比特率的压缩图像，并分析性能差距来源。研究发现，信息损失难以克服，但可通过通用适配器缓解泛化差距，显著提升模型在多种压缩图像上的性能，提升达10%-30%。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）的发展和应用需求增长，高效压缩图像输入变得愈发重要。现有VLM主要处理高比特率压缩图像，对低比特率压缩图像的理解能力尚未被系统探索，亟需建立评估基准并提升模型鲁棒性。

Method: 构建涵盖多种图像编码器和任务的百万级压缩图像基准；通过分类分析性能差距来源（信息损失与泛化失败）；设计并验证一个通用的VLM适配器以增强模型在压缩图像上的表现。

Result: 提出的通用适配器可在不同编码方式和比特率下使VLM性能提升10%-30%，证明其有效性与普适性。

Conclusion: 本研究提供的基准和适配方法为理解并改善VLM在压缩图像上的表现提供了关键洞见，有助于缩小模型与实际压缩图像之间的差距。

Abstract: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [11] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

TL;DR: PanoGrounder 是一个通用的 3D 视觉定位框架，通过结合多模态全景表示与预训练的 2D 视觉语言模型（VLMs），实现强大的视觉-语言推理能力。该方法利用带有 3D 语义和几何特征的全景渲染作为 2D 与 3D 之间的中间表示，既可直接输入 VLMs，又保留了长距离物体间关系。采用三阶段流程：根据场景布局选择少量全景视点，用 VLM 在每个视图上定位文本查询，并通过提升融合为单一 3D 边界框。在 ScanRefer 和 Nr3D 上达到当前最佳性能，并展现出对未见 3D 数据集和文本重述的优异泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统监督模型依赖显式 3D 几何信息，但受限于 3D 视觉语言数据集稀缺以及推理能力不足，导致泛化性差。因此需要一种能结合强大视觉-语言理解与 3D 场景推理能力的新方法。

Method: 提出 PanoGrounder 框架，使用带 3D 语义和几何特征的全景渲染作为中间表示；设计三阶段流程：1）基于场景布局选择紧凑的全景视点；2）利用预训练 2D VLM 在各视图上进行文本查询定位；3）通过提升策略融合各视图预测，生成最终 3D 边界框。

Result: 在 ScanRefer 和 Nr3D 基准上取得当前最优结果，且在未见过的 3D 数据集和文本重述任务中表现出显著更强的泛化能力。

Conclusion: PanoGrounder 通过将全景表示与预训练 VLM 结合，有效提升了 3D 视觉定位的泛化性和推理能力，为连接视觉-语言感知与机器人应用提供了有力支持。

Abstract: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [12] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Transductive Visual Programming (TVP) introduces a framework that builds new tools from experience, rather than relying on fixed or speculative toolsets. It accumulates solutions in an Example Library and abstracts recurring patterns into higher-level tools for an evolving Tool Library, enabling better performance and generalization on spatial reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing visual programming methods struggle with spatial reasoning due to reliance on fixed or speculative tool induction, leading to suboptimal programs and poor tool utilization. There is a need for a more adaptive and experience-driven approach to tool creation.

Method: TVP first solves problems using basic tools, collecting experiential solutions into an Example Library. It then abstracts recurring program patterns into reusable higher-level tools, which are added to an evolving Tool Library. This enables the system to tackle new problems with increasingly powerful, self-learned tools.

Result: On Omni3D-Bench, TVP outperforms GPT-4o by 22% and the previous best visual programming system by 11%. Its transductively learned tools are used 5x more frequently and show strong generalization on unseen spatial tasks without modification.

Conclusion: TVP establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents capable of effectively solving complex spatial reasoning challenges.

Abstract: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [13] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

TL;DR: 提出一种协作多智能体推理框架，将语义规划与视觉合成解耦，在单次生成中实现语义和结构一致的可见部分推断。引入自校正验证智能体和多样化假设生成器，提升推理稳定性与多样性。设计新型人类对齐评估指标MAC-Score，实验表明该方法显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有渐进式方法在可视化部分推断任务中存在推理不稳定和误差累积问题，难以保证语义一致性与结构完整性。

Method: 构建协作多智能体框架，分离语义规划与视觉合成；引入自校正验证智能体进行链式思维推理以修正可见区域分割并识别残留遮挡物；设计多样化假设生成器以应对不可见区域的模糊性；开发新评估指标MAC-Score以更准确衡量生成质量。

Result: 在多个数据集上实验结果表明，所提方法在结构完整性和语义一致性方面均显著优于当前最先进的方法，且评估指标与人类判断高度一致。

Conclusion: 所提出的多智能体协同推理框架有效解决了可视化部分推断中的语义与结构一致性难题，为未来研究提供了新的范式与评估标准。

Abstract: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [14] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: 提出一种新的检测范式REM，通过建模真实图像的鲁棒分布而非依赖生成器特定伪影来提升合成图像检测的泛化能力。利用特征级扰动和跨域一致性约束构建真实图像流形边界，在复杂退化条件下表现优异。在RealChain基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法易受生成器特定伪影影响，对真实世界退化（如多轮跨平台分享、后处理）敏感，导致泛化能力差；随着生成模型演进，传统伪影线索失效，亟需更鲁棒的检测机制。

Method: 提出Real-centric Envelope Modeling (REM)，通过自重建中的特征级扰动生成近真实样本，并使用具有跨域一致性的包络估计器学习真实图像流形边界，实现对真实图像分布的稳健建模。

Result: 在八个基准评估中平均性能提升7.5%；在严重退化的RealChain基准上仍保持优异泛化能力，显著优于当前最优方法。

Conclusion: REM为真实世界条件下的合成图像检测提供了坚实基础，有效应对生成模型演进与复杂退化带来的挑战，推动检测技术向更鲁棒、更实用的方向发展。

Abstract: The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>


### [15] [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976)
*Zeqing Song,Zhongmiao Yan,Junyuan Deng,Songpengcheng Xia,Xiang Mu,Jingyi Xu,Qi Wu,Ling Pei*

Main category: cs.CV

TL;DR: 提出XGrid-Mapping，一种结合显式与隐式表示的混合网格框架，利用稀疏网格提供几何先验和结构引导，同时通过隐式密集网格丰富场景表示。采用VDB结构与子地图组织降低计算负载，实现大规模高效增量映射。引入基于蒸馏的重叠对齐策略以减少子地图间不连续性，并结合动态移除模块提升鲁棒性和采样效率。实验表明该方法在映射质量与效率上均优于现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经LiDAR映射方法多依赖密集隐式表示，未充分利用几何结构；而传统体素引导方法难以实现实时性能。因此需要一种既能保留几何信息又具备高效计算能力的新型框架。

Method: 提出XGrid-Mapping框架，融合稀疏网格（提供几何先验）与隐式密集网格（增强表示能力），结合VDB结构与子地图组织以降低计算开销，设计基于蒸馏的重叠对齐策略保证子地图一致性，并引入动态移除模块提升效率与鲁棒性。

Result: 在大规模增量映射任务中，XGrid-Mapping实现了卓越的映射质量，显著提升了计算效率，克服了传统体素引导方法的实时性瓶颈，优于当前最先进的映射方法。

Conclusion: XGrid-Mapping通过显式与隐式表示的协同，有效平衡了精度与效率，在大规模环境中实现了高效、鲁棒且高质量的神经LiDAR增量映射。

Abstract: Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

</details>


### [16] [X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data](https://arxiv.org/abs/2512.20980)
*Xinquan Yang,Jinheng Xie,Yawen Huang,Yuexiang Li,Huimin Huang,Hao Zheng,Xian Wu,Yefeng Zheng,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据合成流程，利用大量正常胸部X光片来增强尾部病变的训练数据。通过预训练扩散模型生成正常X光图像，并用于修复病灶X光片中的头部病变，从而保留尾部类别作为增强数据。同时引入大语言模型知识引导（LKG）模块和渐进式增量学习（PIL）策略以稳定微调过程。在MIMIC和CheXpert公开数据集上的实验表明，该方法显著提升了诊断性能，达到了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在处理胸部X光中长尾肺部异常时，由于罕见病变样本不足，生成能力受限，导致诊断精度不理想。因此需要一种有效方法来增强稀有病变的训练数据。

Method: 构建一个基于大量正常X光片的扩散模型，用于生成正常图像；将该模型用于病灶图像的局部修复（inpainting），以保留尾部病变特征；结合大语言模型知识引导（LKG）与渐进式增量学习（PIL）策略，提升微调稳定性。

Result: 在MIMIC和CheXpert数据集上，所提方法显著优于现有方法，实现了新的性能基准，尤其在罕见病变分类任务中表现突出。

Conclusion: 本研究提出的方法有效缓解了长尾分布下罕见病变数据稀缺的问题，通过扩散模型与知识引导机制协同增强训练数据，显著提升了肺部异常检测的准确性与鲁棒性。

Abstract: Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.

</details>


### [17] [PUFM++: Point Cloud Upsampling via Enhanced Flow Matching](https://arxiv.org/abs/2512.20988)
*Zhi-Song Liu,Chenhang He,Roland Maier,Andreas Rupp*

Main category: cs.CV

TL;DR: PUFM++ is an enhanced flow-matching framework for point cloud upsampling that improves geometric fidelity, robustness to noisy/partial inputs, and consistency with surface-based tasks. It uses a two-stage flow-matching strategy, a data-driven adaptive time scheduler, on-manifold constraints, and a recurrent interface network (RIN) to achieve state-of-the-art performance on both synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for point cloud upsampling struggle with maintaining geometric accuracy under noisy, sparse, or incomplete input conditions, and often fail to align well with downstream surface-based applications. There is a need for more robust and accurate methods that preserve structural integrity while improving reconstruction quality.

Method: PUFM++ introduces a two-stage flow-matching approach: first learning a direct path from sparse to dense point clouds, then refining it using noise-perturbed samples. It employs an adaptive time scheduler for faster and more stable inference, enforces on-manifold constraints to keep generated points on the underlying surface, and integrates a recurrent interface network (RIN) to enhance hierarchical feature interactions.

Result: PUFM++ achieves superior performance in both visual quality and quantitative metrics across multiple benchmarks, including synthetic datasets and real-world scans. It outperforms existing methods in terms of reconstruction accuracy, robustness to input imperfections, and compatibility with downstream surface-based tasks.

Conclusion: PUFM++ establishes a new state of the art in point cloud upsampling by effectively addressing key challenges in geometric fidelity, input robustness, and surface consistency through a combination of advanced flow-matching design, adaptive sampling, on-manifold constraints, and hierarchical feature modeling.

Abstract: Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.

</details>


### [18] [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003)
*Xiangzuo Wu,Chengwei Ren,Jun Zhou,Xiu Li,Yuan Liu*

Main category: cs.CV

TL;DR: 提出了一种前馈式的多视角逆渲染框架，通过跨视角交替注意力机制，在单次前向传播中实现几何、材质和光照的一致性重建。针对真实数据稀缺问题，引入基于一致性的微调策略，利用无标签的真实视频提升模型在野外场景下的泛化能力。实验表明该方法在多视角一致性、材质与法线估计质量以及真实图像泛化方面均达到当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有单视角逆渲染方法忽略跨视角关系，导致结果不一致；而多视角优化方法依赖缓慢的可微渲染和逐场景微调，计算成本高且难以扩展。因此需要一种高效且能保持多视角一致性的新方法。

Method: 设计一个前馈式多视角逆渲染框架，直接从多视角RGB图像序列预测空间变化的反照率、金属度、粗糙度、漫反射阴影和表面法线。通过跨视角交替注意力机制建模视图内长距离光照交互和视图间材质一致性，实现场景级推理。同时提出基于一致性的微调策略，利用未标注的真实世界视频增强模型对真实场景的鲁棒性。

Result: 在基准数据集上的大量实验表明，该方法在多视角一致性、材质与法线估计精度以及真实世界图像泛化性能方面均优于现有方法，达到当前最佳水平。

Conclusion: 所提出的前馈式多视角逆渲染框架结合了高效推理与强一致性，配合一致性微调策略，显著提升了真实场景下的重建质量与泛化能力，为大规模实际应用提供了可行方案。

Abstract: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

</details>


### [19] [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004)
*Jinghan Li,Yang Jin,Hao Jiang,Yadong Mu,Yang Song,Kun Xu*

Main category: cs.CV

TL;DR: NExT-Vid提出了一种新的自回归视觉生成预训练框架，通过掩码下一帧预测联合建模图像和视频。该方法引入了上下文隔离的自回归预测器以解耦语义表示与目标解码，并采用条件流匹配解码器提升生成质量与多样性。在大规模预训练模型上的实验表明，NExT-Vid在下游分类任务中通过注意力探测显著优于以往的生成式预训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成预训练方法多依赖BERT风格的掩码建模，忽视了视频分析中至关重要的时间信息；而现有的自回归视觉预训练方法存在语义定位不准确、生成质量差等问题，限制了其在视觉表示学习中的表现。

Method: 提出NExT-Vid框架，采用掩码下一帧预测进行图像与视频联合建模；引入上下文隔离的自回归预测器以分离语义表示与解码过程；使用条件流匹配解码器提升生成质量与多样性。

Result: 在大规模预训练模型上，NExT-Vid在多个下游视觉分类任务中表现出色，通过注意力探测验证了其强表示能力，优于现有生成式预训练方法。

Conclusion: NExT-Vid通过上下文隔离的流匹配预训练，实现了高质量的视觉表示学习，在图像与视频联合建模方面具有显著优势。

Abstract: Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

</details>


### [20] [Granular-ball Guided Masking: Structure-aware Data Augmentation](https://arxiv.org/abs/2512.21011)
*Shuyin Xia,Fan Chen,Dawei Dai,Meng Yang,Junwei Han,Xinbo Gao,Guoyin Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Granular-ball Guided Masking (GBGM)的结构感知数据增强方法，通过粗到细的分层掩码过程，自适应保留语义丰富且结构重要的区域，同时抑制冗余区域，从而提升模型在有限数据或分布偏移下的鲁棒性。该方法简单、模型无关，可无缝集成至CNN和Vision Transformers中，在多个基准上均实现了分类准确率和掩码图像重建性能的持续提升，验证了其有效性与广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩码的信息丢弃类数据增强方法缺乏结构感知能力，容易丢弃重要语义信息，导致模型在数据有限或分布变化时表现不佳。需要一种能够自适应保留关键结构信息的增强策略以提升模型鲁棒性。

Method: 提出Granular-ball Guided Masking (GBGM)，基于粒球计算（GBC）构建粗到细的分层掩码机制，通过结构感知判断哪些区域应被保留或抑制，实现对语义丰富区域的保护和冗余区域的剔除。

Result: 在多个视觉识别基准上，GBGM显著提升了分类准确率和掩码图像重建性能，证明其在不同架构（CNNs、Vision Transformers）中的有效性与通用性。

Conclusion: GBGM是一种高效、通用且结构感知的数据增强方法，为应对小样本学习与分布外泛化问题提供了新范式，具有良好的实际应用前景。

Abstract: Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.

</details>


### [21] [FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing](https://arxiv.org/abs/2512.21015)
*Mingshu Cai,Yixuan Li,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: FluencyVE 提出了一种简单有效的单次视频编辑方法，通过在基于预训练 Stable Diffusion 模型的视频编辑框架中引入线性时间序列模块 Mamba，替代传统的时序注意力层，实现全局帧级注意力并降低计算开销。同时，采用低秩近似矩阵替换因果注意力中的查询和键权重矩阵，并在训练中使用加权平均技术更新注意力分数，有效保留了文本到图像模型的生成能力，显著减少计算负担。实验表明，该方法在真实世界视频中编辑多种属性、主体和场景方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法依赖于在预训练文本到图像模型基础上添加时序注意力机制，但面临时间不一致性和高计算开销的问题，亟需一种高效且稳定的视频编辑方案。

Method: 引入 Mamba 模块替代传统时序注意力，结合低秩近似矩阵与加权平均策略优化注意力计算，以实现高效、一致的视频编辑。

Result: 在真实世界视频中成功编辑多种属性、主体和位置，表现出良好的生成质量与时间一致性，同时大幅降低计算成本。

Conclusion: FluencyVE 通过引入 Mamba 和轻量化注意力设计，实现了高效、稳定且高质量的视频编辑，为大规模视频生成与编辑提供了可行路径。

Abstract: Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.

</details>


### [22] [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019)
*Rui-qing Sun,Xingshan Yao,Tian Lan,Hui-Yang Zhao,Jia-Ling Shi,Chen-Hao Cui,Zhijing Wu,Chen Yang,Xian-Ling Mao*

Main category: cs.CV

TL;DR: 提出一种新型高效的视频防御框架，用于保护3D场域说话人脸生成（TFG）方法中的个人肖像视频，通过扰动3D信息获取过程，在保持高保真度的同时实现47倍加速，且对缩放操作和先进净化攻击具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3D场域说话人脸生成方法可实时生成高保真个性化说话人脸视频，引发严重的隐私泄露风险，但缺乏有效的防御框架。传统基于图像的防御方法虽能施加2D扰动，但计算成本高、视频质量下降严重，无法有效破坏3D信息。因此亟需一种高效且不损害视频质量的防御机制。

Method: 提出一种针对3D场域TFG的视频防御框架，引入相似性引导的参数共享机制以提升计算效率，并设计多尺度双域注意力模块，联合优化空间与频率域扰动，从而在不破坏视频质量的前提下干扰3D信息的提取过程。

Result: 实验表明，该框架具备强防御能力，在保持高保真度的同时相比最快基线实现47倍加速；对缩放操作和先进净化攻击均表现出鲁棒性；消融实验证明了各设计组件的有效性。

Conclusion: 所提出的视频防御框架能够有效抵御3D场域说话人脸生成技术的滥用，兼顾高效性与高质量输出，为个人肖像视频提供了实用化的隐私保护方案。

Abstract: State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

</details>


### [23] [Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model](https://arxiv.org/abs/2512.21032)
*Mingshu Cai,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出一种基于潜在扩散模型的新型红外转可见光人脸生成方法，通过引入多属性分类器和自注意力Mamba模块，有效提升生成图像质量与身份特征保留能力，在两个基准数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部识别模型多在可见光数据集上训练，导致在夜间红外图像上性能显著下降。早期基于特征的方法效果不佳，生成式方法虽能将红外图像转换为可见光图像，但存在模态差异和特征损失问题。

Method: 提出基于潜在扩散模型的红外到可见光图像生成框架，结合多属性分类器以提取关键面部属性，减少特征丢失；引入自注意力Mamba模块以增强跨模态特征的全局建模能力并提升推理速度。

Result: 在两个公开基准数据集上的实验表明，该方法在图像质量和身份特征保留方面均优于现有方法，达到当前最优水平。

Conclusion: 所提出的模型有效解决了红外人脸识别中的域偏移与特征损失问题，实现了高质量、高保真的可见光图像生成，为异构人脸识别提供了新范式。

Abstract: Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.

</details>


### [24] [A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography](https://arxiv.org/abs/2512.21040)
*Jaehong Lee,You Chan No,YoungWoo Kim,Duksu Kim*

Main category: cs.CV

TL;DR: 本文提出KOREATECH-CGH数据集，包含6,000对RGB-D图像与复数全息图，覆盖256×256至2048×2048分辨率及广深度范围。引入幅度投影后处理技术，在保持相位不变的前提下优化全息波场幅度，显著提升重建质量，PSNR达27.01 dB，SSIM达0.87，优于现有方法。该数据集可有效支持下一代机器学习全息系统训练与评估。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习全息技术受限于高质量、大规模全息数据集的缺乏，亟需构建兼具高分辨率与大深度范围的数据资源以推动发展。

Method: 构建KOREATECH-CGH数据集，涵盖多分辨率与深度范围的RGB-D图像与对应复数全息图；提出幅度投影技术，通过替换不同深度层的全息波场幅度分量来提升重建质量。

Result: 幅度投影使全息重建达到27.01 dB PSNR和0.87 SSIM，相比先进方法提升2.03 dB和0.04 SSIM；数据集在全息生成与超分辨率任务中验证了其有效性。

Conclusion: KOREATECH-CGH是首个支持宽深度范围与高分辨率的公开全息数据集，结合幅度投影技术，显著提升全息重建质量，为未来机器学习驱动的全息系统提供重要基础。

Abstract: Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.

</details>


### [25] [Matrix Completion Via Reweighted Logarithmic Norm Minimization](https://arxiv.org/abs/2512.21050)
*Zhijie Wang,Liangtian He,Qinghua Zhang,Jifei Miao,Liang-Jian Deng,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新的重加权对数范数作为低秩矩阵补全的非凸替代，通过ADMM高效求解，在图像修复任务中表现出优于现有方法的视觉质量和定量指标。


<details>
  <summary>Details</summary>
Motivation: 核范数作为秩函数的凸近似存在奇异值过度压缩的问题，导致次优解，因此需要更精确的非凸替代来提升低秩矩阵补全性能。

Method: 提出重加权对数范数作为非凸替代，并采用交替方向乘子法（ADMM）求解优化问题。

Result: 在图像修复任务上，所提方法在视觉质量与定量指标上均优于现有先进方法。

Conclusion: 所提出的重加权对数范数能够更有效地逼近秩函数，结合ADMM求解，显著提升了低秩矩阵补全的性能。

Abstract: Low-rank matrix completion (LRMC) has demonstrated remarkable success in a wide range of applications. To address the NP-hard nature of the rank minimization problem, the nuclear norm is commonly used as a convex and computationally tractable surrogate for the rank function. However, this approach often yields suboptimal solutions due to the excessive shrinkage of singular values. In this letter, we propose a novel reweighted logarithmic norm as a more effective nonconvex surrogate, which provides a closer approximation than many existing alternatives. We efficiently solve the resulting optimization problem by employing the alternating direction method of multipliers (ADMM). Experimental results on image inpainting demonstrate that the proposed method achieves superior performance compared to state-of-the-art LRMC approaches, both in terms of visual quality and quantitative metrics.

</details>


### [26] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于事件相机的光学流引导6自由度（6DoF）物体姿态跟踪方法，通过2D-3D混合特征提取策略精确表征物体运动，利用最大事件关联概率搜索角点光流，并通过光流引导建立角点与边缘的相关性，最终通过最小化角点与边缘间距离实现姿态的迭代优化，在模拟和真实事件数据上均优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统摄像头在物体姿态跟踪中面临运动模糊、传感器噪声、部分遮挡和光照变化等挑战；事件相机具有高动态范围和低延迟优势，具备解决这些问题的潜力，因此需要开发针对事件相机的高效姿态跟踪方法。

Method: 采用2D-3D混合特征提取策略检测事件和物体模型中的角点与边缘；在时空窗口内最大化事件关联概率以搜索角点的光流；利用光流建立角点与边缘之间的相关性；通过最小化角点与边缘间的距离，迭代优化6DoF物体姿态实现连续跟踪。

Result: 在模拟和真实事件数据上的实验结果表明，该方法在精度和鲁棒性方面均优于现有的基于事件的先进方法。

Conclusion: 所提出的光学流引导6DoF物体姿态跟踪方法充分利用事件相机的优势，有效提升了复杂环境下的姿态跟踪性能，为未来多媒体应用中的实时精准跟踪提供了可行方案。

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [27] [DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors](https://arxiv.org/abs/2512.21054)
*Kaustubh Kundu,Hrishav Bakul Barua,Lucy Robertson-Bell,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: DexAvatar提出了一种从单目手语视频中重建生物力学准确的精细手部动作和身体运动的新框架，利用学习到的3D手部和身体先验知识，在缺乏高质量3D数据的情况下实现了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法依赖大量精确的2D和3D人体姿态数据，但当前大多数手语数据集为视频形式，仅提供自动重建的2D关键点，缺乏准确的3D信息；同时，现有3D人体姿态估计方法在自遮挡、噪声和运动模糊下表现不佳，导致重建质量差。

Method: DexAvatar通过引入基于学习的3D手部和身体先验，从野外单目手语视频中重建高精度的手部细微动作与身体运动，有效缓解了自遮挡、噪声和运动模糊问题。

Result: 在SGNify动作捕捉数据集上，DexAvatar相比最先进方法在身体和手部姿态估计上提升了35.11%的性能，成为该任务唯一基准上的领先模型。

Conclusion: DexAvatar成功解决了手语视频中3D姿态重建的挑战，为数据驱动的手语生成提供了更高质量的输入，推动了该领域的技术进步。

Abstract: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.

</details>


### [28] [Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control](https://arxiv.org/abs/2512.21058)
*Minghao Han,YiChen Liu,Yizhou Liu,Zizhi Chen,Jingqun Tang,Xuecheng Wu,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: UniPath提出了一种语义驱动的病理图像生成框架，通过多流控制机制实现精细的语义控制，解决了数据稀缺、语义不一致和生成精度不足的问题。研究构建了265万规模的图像-文本语料库，并引入诊断语义令牌与原型库以增强生成质量，实验表明其在路径学图像生成上达到领先性能（如Patho-FID 80.9，较第二名提升51%），并支持接近真实图像的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 当前计算病理学中的理解与生成任务发展分离：理解模型已具备诊断级能力，而生成模型仍局限于像素模拟。主要瓶颈在于缺乏高质量图像-文本数据集、缺乏精确的细粒度语义控制，以及术语异构性导致文本条件不可靠。

Method: 提出UniPath框架，采用多流控制机制：原始文本流、高层语义流（利用冻结的病理多模态大模型提取鲁棒诊断语义令牌并扩展为诊断感知属性包）、原型流（通过原型库实现组件级形态控制）。同时构建265万图像-文本语料库及6.8万高质量标注子集，用于缓解数据稀缺问题。

Result: UniPath在病理图像生成任务中表现卓越，路径学FID达80.9（比第二名提升51%），细粒度语义控制接近真实图像水平（98.7%），且评估体系涵盖四层病理专用指标，验证其有效性。

Conclusion: UniPath通过融合先进诊断理解能力与可控生成机制，显著推动了病理图像生成的发展；所构建的数据集、代码与预训练模型将公开共享，促进领域研究进展。

Abstract: In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.

</details>


### [29] [Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition](https://arxiv.org/abs/2512.21064)
*Hongsong Wang,Heng Fei,Bingxuan Dai,Jie Gui*

Main category: cs.CV

TL;DR: 本文提出了一种名为“分解与组合”的自监督多模态骨架动作表征学习框架，旨在解决多模态动作理解中效率与性能难以兼顾的问题。通过分解策略将融合特征还原为独立的单模态特征并进行对齐，通过组合策略利用单模态特征作为自监督信号来增强多模态表征学习。在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II数据集上的实验表明，该方法在计算成本与模型性能之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖于简单的后期融合，导致计算开销大；而早期融合虽高效但性能不佳。因此亟需一种既能保持高效又能提升性能的多模态融合机制。

Method: 提出分解与组合双策略：分解策略将多模态融合特征拆分为单模态特征并与其真实单模态对应项对齐；组合策略则利用多个单模态特征作为自监督信号，指导多模态表示的学习。

Result: 在多个标准数据集上均取得优异性能，同时显著降低计算成本，实现了效率与效果的平衡。

Conclusion: 所提出的分解与组合框架有效解决了多模态动作理解中效率与性能之间的矛盾，为高效且高性能的多模态学习提供了新思路。

Abstract: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.

</details>


### [30] [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078)
*Tianchen Deng,Xun Chen,Ziming Li,Hongming Shen,Danwei Wang,Javier Civera,Hesheng Wang*

Main category: cs.CV

TL;DR: UniPR-3D is the first VPR architecture that effectively integrates multiple views using a VGGT backbone with dedicated 2D and 3D feature aggregators. It leverages both 3D tokens and intermediate 2D tokens, combines single- and multi-frame aggregation, and employs variable-length sequence retrieval to improve generalization. The method achieves state-of-the-art performance on VPR benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing VPR methods primarily focus on single-image retrieval and struggle to generalize across diverse environments. Multi-view integration offers advantages but remains underexplored, motivating the need for a robust, generalizable multi-view VPR framework.

Method: UniPR-3D uses a VGGT backbone to encode multi-view 3D representations. It introduces specialized aggregation modules for 2D and 3D features, fuses them into a unified descriptor, and incorporates both single- and multi-frame aggregation with variable-length sequence retrieval.

Result: UniPR-3D achieves superior performance over both single- and multi-view baselines on standard VPR benchmarks, demonstrating strong generalization and effectiveness of geometry-grounded tokens in place recognition.

Conclusion: UniPR-3D establishes a new state of the art in visual place recognition by effectively leveraging multi-view information through a novel fusion of 2D and 3D tokens, enabling better generalization and robustness across diverse environments.

Abstract: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

</details>


### [31] [Hierarchical Modeling Approach to Fast and Accurate Table Recognition](https://arxiv.org/abs/2512.21083)
*Takaya Kawakatsu*

Main category: cs.CV

TL;DR: 本文提出一种新型多任务模型，利用非因果注意力捕捉完整表格结构，并采用并行推理算法加速单元格内容识别。在两个大型公开数据集上，该方法在视觉和统计上均表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽然在表格识别任务中表现良好，但其有效性缺乏充分解释，且推理时间较长，难以满足高效信息提取的需求。

Method: 采用非因果注意力机制以全局捕获表格结构，并设计并行推理算法以加快单元格内容识别速度。

Result: 在两个大型公开数据集上，该方法在准确性和推理效率方面均优于现有方法，验证了其有效性和优越性。

Conclusion: 所提出的多任务模型结合非因果注意力与并行推理，在保持高精度的同时显著提升推理速度，为文档知识提取提供了高效解决方案。

Abstract: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.

</details>


### [32] [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094)
*Zhe Cao,Tao Wang,Jiaming Wang,Yanghai Wang,Yuanxing Zhang,Jialu Chen,Miao Deng,Jiahao Wang,Yubin Guo,Chenxi Liao,Yize Zhang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出T2AV-Compass，一个统一的T2AV生成系统评估基准，包含500个多样且复杂的提示，通过分类驱动的构建流程确保语义丰富性和物理合理性。该基准引入双层评估框架，结合信号级客观指标与基于多模态大模型的主观评分，全面评估视频质量、音频质量、跨模态对齐、指令遵循和感知真实性。对11个代表性T2AV系统的评估表明，当前最强模型在真实感和跨模态一致性方面仍远低于人类水平，暴露出音频真实感、精细同步和指令遵循等方面的显著缺陷，凸显T2AV-Compass作为挑战性诊断测试平台的价值。


<details>
  <summary>Details</summary>
Motivation: 现有T2AV生成系统的评估方法分散，依赖单一模态指标或局限性较强的基准，难以全面衡量跨模态对齐、指令遵循和复杂提示下的感知真实性。

Method: 提出T2AV-Compass基准，采用分类驱动的提示生成管道构建500个复杂、多样、语义丰富的提示，并设计双层评估框架：包括信号级客观指标（视频/音频质量、跨模态对齐）和基于多模态大模型作为裁判的主观评估（指令遵循、现实感）。

Result: 对11个代表性T2AV系统进行评估发现，即使最先进的模型在音频真实感、细粒度同步、指令遵循和整体一致性方面仍存在明显不足，远未达到人类水平的真实感与协调性。

Conclusion: T2AV-Compass是一个具有挑战性和诊断性的综合评估基准，能有效揭示当前T2AV系统的关键缺陷，为未来模型的改进提供明确方向。

Abstract: Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

</details>


### [33] [UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters](https://arxiv.org/abs/2512.21095)
*Yongkun Du,Zhineng Chen,Yazhen Xie,Weikang Baiand Hao Feng,Wei Shi,Yuchen Su,Can Huang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: UniRec-0.1B 是一个仅 0.1B 参数的轻量级统一识别模型，可实现文本与公式在字符、词、行、段落和文档等多个层级上的识别。研究构建了包含 4000 万样本的 UniRec40M 大规模数据集，并针对结构多样性与语义纠缠两大挑战，提出分层监督训练和语义解耦分词器。实验表明，该模型在多个中英文跨领域基准上超越通用视觉语言模型和主流文档解析模型，同时实现 2–9 倍加速，兼具高效性与强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽能统一识别文本与公式，但参数量大、计算开销高，难以在资源受限场景中应用。亟需一种轻量级、高效且具备多层级理解能力的统一识别模型。

Method: 构建 UniRec40M 大规模混合数据集；设计分层监督训练策略以增强对多层级结构的理解；引入语义解耦分词器，分离文本与公式表示，缓解语义纠缠问题。

Result: UniRec-0.1B 在多个中英文文档基准上表现优于主流 VLMs 和专业文档解析模型，推理速度提升 2–9 倍，验证了其高效性与有效性。

Conclusion: UniRec-0.1B 成功实现了轻量化与高性能的统一文本与公式识别，为实际部署提供可行方案，推动文档智能解析系统的普及与应用。

Abstract: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.

</details>


### [34] [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104)
*Chao Gong,Dong Li,Yingwei Pan,Jingjing Chen,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: FreeInpaint是一种无需调参的即插即用方法，通过在推理阶段直接优化扩散潜变量，提升图像修复结果与文本提示的一致性及视觉合理性。它采用先验引导的噪声优化策略，聚焦有效修复区域，并设计了专门针对修复任务的复合引导目标，在每一步优化中间潜变量，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像修复方法虽借助预训练文本到图像扩散模型生成逼真结果，但在保持文本提示一致性与视觉合理性方面仍存在挑战，尤其难以同时兼顾两者。因此需要一种更高效、无需额外训练的方法来提升修复质量。

Method: 提出FreeInpaint，通过在推理阶段动态优化扩散模型的初始噪声和中间潜变量；引入先验引导的噪声优化以聚焦修复区域，并设计复合引导目标指导去噪过程，实现对生成内容的精准控制。

Result: 在多种扩散模型和评估指标下，FreeInpaint展现出优异的性能与鲁棒性，显著提升修复结果与文本提示的一致性及视觉质量，且无需微调或重新训练。

Conclusion: FreeInpaint是一种高效、通用且无需调参的图像修复方法，能够有效提升文本引导修复中提示对齐与视觉合理性的平衡，具有广泛的应用潜力。

Abstract: Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

</details>


### [35] [MarineEval: Assessing the Marine Intelligence of Vision-Language Models](https://arxiv.org/abs/2512.21126)
*YuK-Kwan Wong,Tuan-An To,Jipeng Zhang,Ziqiang Zheng,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文构建了首个大规模海洋视觉语言模型（VLM）数据集和基准测试MarineEval，包含2000个基于图像的问答对，涵盖7个任务维度和20个能力维度，旨在评估现有VLM在海洋领域专家级问题上的表现。实验表明，当前VLM在应对专业海洋问题时表现不佳，存在显著提升空间，研究呼吁未来工作关注该领域的特定需求。


<details>
  <summary>Details</summary>
Motivation: 探究现有视觉语言模型（VLMs）是否具备作为海洋领域专家的能力，准确回答需要高度专业知识的海洋相关问题，并揭示其在处理特定领域挑战时的局限性。

Method: 构建首个大规模海洋领域VLM数据集MarineEval，包含2000个图像-文本问答对，覆盖7个任务维度和20个能力维度；通过海洋领域专家验证数据质量与领域适配性；在该基准上全面评测17个现有VLM模型的表现。

Result: 实验结果显示，现有VLM在回答海洋领域特定问题时表现不佳，无法有效理解或推理复杂海洋知识，说明其在专业领域应用中仍存在明显短板，需进一步优化。

Conclusion: MarineEval为评估VLM在专业领域的能力提供了重要基准，揭示了当前模型在海洋领域知识理解上的不足，呼吁未来研究关注领域适应性与知识增强技术，以推动VLM向真实专家水平演进。

Abstract: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/

</details>


### [36] [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135)
*Gaoren Lin,Huangxuan Zhao,Yuan Xiong,Lefei Zhang,Bo Du,Wentao Zhu*

Main category: cs.CV

TL;DR: TGC-Net 是一种基于 CLIP 的参数高效、任务特定的医学图像分割框架，通过引入语义-结构协同编码器（SSE）、领域增强文本编码器（DATE）和视觉-语言校准模块（VLCM），有效解决了 CLIP 在医学影像中细粒度结构保留不足、复杂临床描述建模差及领域语义错位的问题，在多个胸部X光和胸腔CT数据集上实现了最先进的分割性能，且参数量显著减少。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的医学分割方法依赖于未对齐的图像与文本编码器，需要复杂的交互模块进行多模态融合；而直接应用CLIP于医学影像存在三个主要问题：细粒度解剖结构保留不足、复杂临床描述建模能力弱、领域语义不匹配。

Method: 提出TGC-Net框架，包含：1）语义-结构协同编码器（SSE），在CLIP ViT基础上增加CNN分支以实现多尺度结构细化；2）领域增强文本编码器（DATE），注入大语言模型生成的医学知识；3）视觉-语言校准模块（VLCM），在统一特征空间中优化跨模态对应关系。

Result: 在五个不同数据集（涵盖胸部X光和胸腔CT）上的实验表明，TGC-Net达到当前最优性能，尤其在具有挑战性的基准测试中显著提升Dice系数，同时大幅减少可训练参数数量。

Conclusion: TGC-Net通过参数高效的多模态适配策略，成功克服了CLIP在医学图像分割中的局限性，为结合临床报告进行精准分割提供了高效且强大的解决方案。

Abstract: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

</details>


### [37] [ORCA: Object Recognition and Comprehension for Archiving Marine Species](https://arxiv.org/abs/2512.21150)
*Yuk-Kwan Wong,Haixin Liang,Zeyu Ma,Yiwei Chen,Ziqiang Zheng,Rinaldi Gotama,Pascal Sebastian,Lauren D. Sparks,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: ORCA 是一个针对海洋研究的多模态基准，包含 14,647 张图像、478 种物种、42,217 个边界框标注和 22,321 个专家验证的实例描述。该数据集通过细粒度的视觉与文本标注，捕捉了不同海洋物种的形态特征。研究评估了 18 个前沿模型在目标检测（闭集与开放词汇）、实例描述生成和视觉定位三个任务上的表现，揭示了物种多样性、形态重叠和领域特殊性等挑战，推动海洋视觉理解的研究进展。


<details>
  <summary>Details</summary>
Motivation: 海洋视觉理解对监测和保护海洋生态系统至关重要，但受限于训练数据不足以及缺乏将领域特定挑战与计算机视觉任务系统对齐的任务范式，导致模型应用受限。

Method: 构建 ORCA 多模态基准数据集，涵盖大量海洋物种的图像与精细标注，并在三个任务上评估多种先进模型的表现，以揭示关键挑战并推动方法发展。

Result: 评估结果表明，海洋理解面临物种多样性高、形态相似性强及领域需求复杂等挑战，凸显了现有模型的局限性，同时证实 ORCA 作为全面基准的有效性。

Conclusion: ORCA 为海洋领域研究建立了系统性的多模态基准，有助于推动海洋视觉理解的理论与技术进步。

Abstract: Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.

</details>


### [38] [Towards Arbitrary Motion Completing via Hierarchical Continuous Representation](https://arxiv.org/abs/2512.21183)
*Chenghao Xu,Guangtao Lyu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文首次探索了人体运动序列的连续表示，能够以任意帧率对输入运动序列进行插值、补间和外推。提出了一种基于隐式神经表示（INRs）的分层隐式表示框架NAME，通过多时序尺度的层次化时间编码机制捕捉复杂的时序模式，并引入基于傅里叶变换的参数化激活函数增强模型表达能力，显著提升了复杂运动行为的建模精度。在多个基准数据集上的实验验证了方法的有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统视频帧率受限于离散采样，难以实现平滑的运动生成与任意帧率下的精确控制。为解决这一问题，本文旨在构建一种连续的运动表示方法，支持任意帧率下的插值、补间和外推，从而提升运动序列的平滑性与时间一致性。

Method: 提出名为NAME的分层隐式神经表示框架，采用多时序尺度的层次化时间编码提取运动特征，并结合基于傅里叶变换的参数化激活函数的MLP解码器，实现高精度的连续运动建模。

Result: 在多个基准数据集上，该方法在运动插值、补间和外推任务中均表现出优异性能，验证了其在任意帧率下生成高质量连续运动序列的能力。

Conclusion: NAME框架成功实现了对人体运动序列的连续表示，具备强大的插值、补间与外推能力，为高帧率运动生成与动画编辑提供了新的技术路径。

Abstract: Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.

</details>


### [39] [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185)
*Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan*

Main category: cs.CV

TL;DR: UltraShape 1.0 是一个可扩展的3D扩散框架，用于高保真3D几何生成。采用两阶段生成流程：先生成粗略全局结构，再进行细化以产生高质量细节。通过创新的封闭处理方法和高质量数据过滤，改进了公开3D数据集的几何质量。利用基于体素的精炼机制，在固定空间位置上实现局部几何细节合成，结合RoPE编码的位置锚点，使模型聚焦于局部细节生成。模型仅在公开数据集上训练，仍达到优异几何质量。评估表明其性能优于或媲美现有开源方法。代码与模型将公开。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在几何保真度和细节还原方面存在局限，尤其在缺乏高质量训练数据的情况下难以生成精细且结构完整的3D形状。因此，亟需一种高效、可扩展的生成框架，能够在有限资源下提升生成质量，并支持细粒度几何优化。

Method: 提出两阶段生成流程：首先生成粗略全局结构，然后通过基于体素的精炼机制在固定空间位置上进行细节优化；采用新的封闭处理方法和数据过滤策略，修复孔洞、增强薄结构并保留细节；利用RoPE编码提供显式位置锚点，引导扩散模型在受限结构化空间中专注局部细节合成。

Result: UltraShape 1.0 在不依赖私有数据的前提下，实现了高保真3D几何生成，显著提升了公开数据集的几何质量；在多项评估中表现优于或媲美现有开源方法；生成结果具有丰富的细节和良好的拓扑完整性。

Conclusion: UltraShape 1.0 成功构建了一个高效、可扩展的3D生成框架，通过创新的数据处理与精炼机制，显著提升了3D几何生成的质量与可控性。该工作为开放研究提供了坚实基础，未来有望推动3D内容生成的广泛应用。

Abstract: In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

</details>


### [40] [VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs](https://arxiv.org/abs/2512.21194)
*Brigitta Malagurski Törtei,Yasser Dahou,Ngoc Dung Huynh,Wamiq Reyaz Para,Phúc H. Lê Khac,Ankit Singh,Sofian Chaybouti,Sanath Narayan*

Main category: cs.CV

TL;DR: VisRes Bench 是一个用于研究自然场景下视觉推理能力的基准，通过三个层次的复杂性测试模型在感知、规则推理和组合推理方面的能力。结果显示，最先进的视觉语言模型在细微扰动下表现接近随机，表明其抽象能力有限，主要依赖模式识别而非真正的视觉推理。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉问答和图像描述等任务上表现优异，但其是否真正具备视觉推理能力仍不明确，尤其是否过度依赖语言先验。因此需要一个无上下文语言监督的基准来评估真实视觉推理能力。

Method: 引入 VisRes Bench 基准，包含三个层级的任务：Level 1 测试感知补全与全局图像匹配在模糊、纹理变化、遮挡、旋转等扰动下的表现；Level 2 测试单一属性（如颜色、数量、方向）的规则推理；Level 3 考察多属性组合推理。所有任务基于超过19,000张受控图像进行评估。

Result: 在细微感知扰动下，最先进的视觉语言模型性能接近随机，表明其在抽象推理方面存在明显局限，主要依赖表面模式而非深层视觉理解。

Conclusion: VisRes 提供了一个统一框架，有助于推动多模态研究中抽象视觉推理的发展，为未来模型设计提供重要参考。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.

</details>


### [41] [Human Motion Estimation with Everyday Wearables](https://arxiv.org/abs/2512.21209)
*Siqi Zhu,Yixuan Li,Junfu Li,Qi Wu,Zan Wang,Haozhe Ma,Wei Liang*

Main category: cs.CV

TL;DR: EveryWear提出一种基于日常可穿戴设备（智能手机、智能手表、耳机、智能眼镜）的轻量级人体动作捕捉方法，无需校准，利用多模态教师-学生框架融合第一人称视觉与惯性信号，在真实世界数据上训练以消除仿真到现实的差距，显著提升动作估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于体表设备的人体动作估计方法存在可穿戴性差、硬件成本高和校准繁琐等问题，限制了其在日常生活中的应用。

Method: 采用多模态教师-学生框架，结合第一人称摄像头的视觉信息与消费级设备的惯性信号，在真实世界数据上直接训练模型，避免使用合成数据。

Result: 实验表明，该方法在全身体运动估计任务中优于基线模型，验证了其在实际应用中的有效性。

Conclusion: EveryWear提供了一种无需校准、轻量且实用的全身体运动捕捉方案，适用于日常场景，推动了可穿戴式动作捕捉技术的发展。

Abstract: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.

</details>


### [42] [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218)
*Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig*

Main category: cs.CV

TL;DR: 本文提出了一种无任务依赖的机制，使大型多模态模型（LMMs）能够在没有显式监督的情况下发现并使用视觉推理标记。这些标记全局关注并以任务自适应的方式重新编码图像，从而在不依赖人工设计的视觉抽象的前提下提取相关视觉信息。该方法在多种以视觉为中心的任务上表现优于直接微调，并实现了最先进的性能，同时具备跨任务泛化能力，尤其适用于中间抽象难以定义的任务。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型仍以文本为核心推理模态，难以有效处理主要依赖视觉的推理任务。现有方法虽通过辅助图像、深度图或图像裁剪来监督中间视觉步骤，但引入了限制性先验、高标注成本且泛化能力差，因此亟需一种无需显式监督、可自适应提取视觉信息的方法。

Method: 提出一种任务无关的机制，训练LMMs自动发现和利用视觉推理令牌。这些令牌对图像进行全局注意力并以任务自适应方式重新编码，实现无需手工设计的视觉信息提取。

Result: 在多种视觉主导型任务中，该方法优于直接微调，达到当前最优性能；在中间抽象难以明确指定的任务中表现尤为突出，并成功推广至多任务指令微调场景。

Conclusion: 所提出的视觉推理令牌机制为大模型提供了无需显式监督的视觉推理能力，显著提升了模型在复杂视觉任务中的表现与泛化性，是迈向真正多模态智能的重要一步。

Abstract: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>


### [43] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的两阶段图像-文本检索管道，利用以事件为中心的实体提取来引入真实场景描述中的时间与上下文信息。第一阶段使用基于BM25的显著实体进行高效候选过滤，第二阶段采用BEiT-3模型捕捉深层多模态语义并重新排序结果。在OpenEvents v1基准上，该方法达到0.559的平均精度，显著优于现有基线，验证了事件引导过滤与长文本视觉语言建模结合的有效性。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的图像-文本检索面临查询模糊、语言多样性以及可扩展性等挑战，现有方法难以有效处理复杂上下文和时间信息，因此需要一种既能高效又能准确捕捉语义的检索机制。

Method: 提出一个两阶段检索框架：第一阶段通过事件中心的实体提取，利用BM25算法基于关键实体进行快速候选过滤；第二阶段采用BEiT-3模型对候选结果进行深度多模态语义理解与重排序，以提升检索精度。

Result: 在OpenEvents v1数据集上，该方法实现了0.559的平均精度，显著超越已有基线方法，证明了其在复杂真实场景下的有效性与高效性。

Conclusion: 结合事件引导的过滤与长文本视觉语言建模，能够有效提升图像-文本检索在复杂现实场景中的准确性与效率，为实际应用提供了可行方案。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [44] [SegMo: Segment-aligned Text to 3D Human Motion Generation](https://arxiv.org/abs/2512.21237)
*Bowen Dang,Lin Wu,Xiaohang Yang,Zheng Yuan,Zhixiang Chen*

Main category: cs.CV

TL;DR: SegMo提出一种细粒度文本-动作对齐的3D人体动作生成框架，通过分解文本和动作为语义一致的片段，并利用对比学习实现精确对齐，在HumanML3D数据集上取得0.553的TOP 1得分，优于基线方法，并可应用于动作定位与文本检索等任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在序列层面对齐文本与动作，忽略了模态内部的语义结构。文本描述和动作序列均可自然分解为更小且语义连贯的片段，这些片段可作为原子对齐单元以实现更细粒度的对应关系。

Method: 提出SegMo框架，包含三个模块：(1) 文本片段提取，将复杂文本描述分解为时序有序的短语，每个代表一个简单动作；(2) 动作片段提取，将完整动作序列划分为对应的动作片段；(3) 细粒度文本-动作对齐，通过对比学习实现文本与动作片段的精准匹配。

Result: 在两个常用数据集上实验表明，SegMo显著优于强基线模型，在HumanML3D测试集上达到0.553的TOP 1得分；同时，由于学习到的共享嵌入空间，该方法还可用于动作定位和动作到文本检索等检索类任务。

Conclusion: SegMo通过引入语义片段级别的对齐机制，实现了更精细的文本-动作对应，提升了3D人体动作生成的质量与应用灵活性。

Abstract: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.

</details>


### [45] [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252)
*Jiawei Liu,Junqiao Li,Jiangfan Deng,Gen Li,Siyu Zhou,Zetao Fang,Shanshan Lao,Zengde Deng,Jianing Zhu,Tingting Ma,Jiayi Li,Yunqiu Wang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 本文提出DreaMontage框架，实现基于任意帧引导的长时序、无缝、富有表现力的一镜到底视频生成。通过轻量级中间条件机制、高质量数据集与视觉表达微调、分段自回归推理策略，解决了现有方法在视觉连贯性与计算效率上的不足，显著提升生成质量与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法依赖简单片段拼接，难以保证视觉平滑与时间连贯性；一镜到底视频虽具美学价值，但实际制作成本高、约束多，亟需高效且高质量的虚拟生成方案。

Method: 1. 在DiT架构中引入轻量级中间条件机制，结合自适应调优策略实现任意帧控制；2. 构建高质量数据集并进行视觉表达监督微调（SFT），采用定制化直接偏好优化（DPO）提升运动合理性与过渡流畅性；3. 设计分段自回归（SAR）推理策略，实现内存高效的长序列生成。

Result: 实验表明，DreaMontage能生成视觉惊艳、连贯性强的一镜到底视频，在保持计算效率的同时显著提升生成成功率与用户体验，成功将碎片化素材转化为连贯的电影级视觉体验。

Conclusion: DreaMontage为一镜到底视频生成提供了高效、灵活且高质量的解决方案，突破了传统方法在可控性、表达力与可扩展性方面的瓶颈，推动了视频生成技术向更真实、更具艺术性的方向发展。

Abstract: The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

</details>


### [46] [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264)
*Changwei Wu,Yifei Chen,Yuxin Du,Mingxuan Liu,Jinying Zong,Beining Wu,Jie Dong,Feiwei Qin,Yunkang Cao,Qiyuan Tian*

Main category: cs.CV

TL;DR: 本文提出一种统一的任意模态异常检测框架（Any-Modality AD），可在任意磁共振成像（MRI）模态缺失情况下实现鲁棒的异常检测与定位。通过双路径DINOv2编码器与特征分布对齐机制，模型能有效处理不完整模态输入，并在训练中通过随机模态掩码和间接特征补全实现对所有模态组合的自适应，无需重新训练。引入内在正常原型（INPs）提取器与引导解码器，确保仅重建正常解剖结构，增强异常差异的凸显。在BraTS2018、MU-Glioma-Post和Pretreat-MetsToBrain-Masks数据集上，该方法在7种模态组合下均优于现有先进基线，展现出卓越的泛化能力，为真实临床环境中多模态医学异常检测提供了可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 现有单类或多类异常检测模型通常依赖固定模态配置，需重复训练，且难以泛化到未见的模态组合，限制了其在真实临床场景中的应用。由于真实医疗流程中常存在标注异常病例稀缺及关键影像模态缺失的问题，亟需一种能够适应任意模态条件的通用异常检测方法。

Method: 提出基于双路径DINOv2编码器的统一框架，结合特征分布对齐机制以对齐不完整模态特征与完整模态表示；设计内在正常原型（INPs）提取器与INP引导解码器，仅重建正常解剖模式并放大异常偏差；采用随机模态掩码和间接特征补全策略进行训练，使模型具备对所有模态配置的自适应能力而无需再训练。

Result: 在三个公开脑部MRI数据集（BraTS2018、MU-Glioma-Post、Pretreat-MetsToBrain-Masks）上，针对7种不同模态组合的实验表明，该方法在异常检测与定位任务中持续超越当前最先进的工业与医学异常检测基线，显著提升模型在真实、不完整模态条件下的泛化性能。

Conclusion: 本研究建立了一种适用于真实临床环境下不完整模态条件的可扩展多模态医学异常检测新范式，为实际部署提供了强有力的技术支持，具有良好的临床应用前景。

Abstract: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

</details>


### [47] [GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation](https://arxiv.org/abs/2512.21276)
*Snehal Singh Tomar,Alexandros Graikos,Arjun Krishna,Dimitris Samaras,Klaus Mueller*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像序列生成方法，通过先在低分辨率下生成粗略序列，再单独对每个帧进行超分辨率处理，以提升生成质量与效率。该方法基于Diffusion Transformer（DiT）的自注意力机制，无需修改架构即可将2D图像生成器扩展为3D序列生成器，并实现跨域泛化能力。相比现有最先进方法，本方法在合成质量、序列一致性、推理速度（至少快两倍）和数据使用效率方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的图像序列生成方法通常将图像序列视为大张量，但这种方式存在效率低下和瓶颈问题。作者旨在探索更有效的序列建模方式，以克服现有方法在生成质量、一致性、效率和泛化能力方面的局限性。

Method: 将图像序列生成分解为两个阶段：首先在低分辨率下生成粗略序列（使用子采样帧训练），利用DiT的自注意力机制捕捉帧间相关性；然后独立地对每一帧进行超分辨率重建，添加高分辨率细节。整个过程不改变原始架构，仅通过训练策略实现3D序列生成能力。

Result: 所提方法在多个数据集上均优于现有最先进模型，在图像合成质量、序列连贯性、推理速度（至少快两倍）、长序列生成能力和训练数据利用率方面表现优异；同时具备良好的跨领域泛化能力，无需额外先验或监督。

Conclusion: 该方法通过分阶段生成与超分辨率重建，有效提升了图像序列生成的质量与效率，且无需复杂架构调整，具有广泛适用性和优越性能，是当前图像序列生成领域的有效改进方案。

Abstract: Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

</details>


### [48] [Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction](https://arxiv.org/abs/2512.21287)
*Suren Bandara*

Main category: cs.CV

TL;DR: 本文提出一种多尺度信号处理方法，用于从表格掩码中检测表格外边。通过将行和列的过渡建模为一维信号，并使用方差递增的高斯卷积结合统计阈值化来抑制噪声并保留稳定结构边缘，最终将检测到的信号峰值映射回图像坐标以获得精确的分段边界。该方法在低分辨率或噪声图像下表现鲁棒，且在PubLayNet-1M基准上将CASA指标从67%提升至76%。


<details>
  <summary>Details</summary>
Motivation: 现有基于transformer的方法在噪声输入下适应性差，而传统掩码边缘检测方法存在对噪声敏感、分辨率损失或计算成本高等问题，因此需要一种更鲁棒、高效的表格边缘检测方法。

Method: 将行/列过渡建模为一维信号，采用高斯卷积（方差逐步增加）进行平滑处理，结合统计阈值化抑制噪声，再通过峰值检测定位边缘，并映射回图像坐标获取准确分段边界。

Result: 在PubLayNet-1M数据集上，使用TableNet与PyTesseract OCR时，列边缘检测使Cell-Aware Segmentation Accuracy（CASA）从67%提升至76%，且方法对分辨率变化具有鲁棒性，输出适合下游分析的结构化表格数据。

Conclusion: 所提出的多尺度信号处理方法有效提升了表格边缘检测的准确性与鲁棒性，尤其适用于低质量或不完整表格图像，为文档图像分析中的结构化数据提取提供了高效解决方案。

Abstract: Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.

</details>


### [49] [TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning](https://arxiv.org/abs/2512.21331)
*Varun Belagali,Saarthak Kapse,Pierre Marza,Srijan Das,Zilinghan Li,Sofiène Boutaj,Pushpak Pati,Srikar Yellapragada,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Prateek Prasanna,Stergios Christodoulidis Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: TICON 是一种基于 Transformer 的瓦片表示上下文化模型，能够为计算病理学中的任何应用生成丰富、上下文相关的嵌入。它通过单一共享编码器，在遮蔽建模目标下预训练，统一并上下文化来自不同瓦片级基础模型的表示，显著提升多个任务的表现，并在瓦片级和切片级基准测试中达到新的最先进水平。此外，仅使用11,000张全切片图像预训练的聚合器就超越了使用高达35万张图像预训练的现有最先进切片级基础模型。


<details>
  <summary>Details</summary>
Motivation: 标准的基于瓦片编码器的流程在提取瓦片嵌入时剥离了其上下文信息，无法建模对局部和全局任务至关重要的丰富切片级信息。同时，不同的瓦片编码器在不同下游任务上表现各异，因此需要一个统一模型来上下文化来自任意瓦片级基础模型的嵌入。

Method: TICON 采用单一共享编码器，通过遮蔽建模目标进行预训练，以统一并上下文化来自多种瓦片级病理学基础模型的表示。该模型可整合任意瓦片级基础模型的输出，并生成具有丰富上下文信息的嵌入。

Result: TICON-contextualized 嵌入在多个任务上显著提升了性能，在 HEST-Bench、THUNDER、CATCH 等瓦片级基准以及 Patho-Bench 等切片级基准上均达到了新的最先进水平。基于 TICON 预训练的聚合器仅用 11,000 张全切片图像即超越了使用多达 350,000 张图像预训练的当前最先进模型。

Conclusion: TICON 成功实现了瓦片级嵌入的统一与上下文化，为计算病理学提供了强大的通用表示框架，显著提升了多任务性能，并以极低的数据成本实现了高效的切片级基础模型构建。

Abstract: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.

</details>


### [50] [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333)
*Avilasha Mandal,Chaoning Zhang,Fachrina Dewi Puspitasari,Xudong Wang,Jiaquan Zhang,Caiyan Qin,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文提出一种基于文本引导的视觉令牌剪枝框架，用于提升SAM2模型在视频对象分割中的推理效率。该方法在视觉编码后、时间传播前对冗余视觉令牌进行选择性剪枝，通过轻量级路由机制结合局部视觉上下文、语义相关性（来自目标中心的文本描述）和不确定性提示，保留最具信息量的令牌，从而显著降低计算与内存开销。实验表明，该方法在保持高分割精度的同时，实现最高42.50%的推理加速和37.41%的GPU内存减少，为实时及资源受限场景下的视觉模型部署提供了有效路径。


<details>
  <summary>Details</summary>
Motivation: 现有SAM2模型在视频对象分割中存在高计算与内存开销问题，主要源于对所有视觉令牌进行无差别的时间传播，导致二次方级别的内存注意力开销，限制了其实际部署能力。因此亟需一种高效的方法在不修改原有架构的前提下降低冗余计算。

Method: 提出一种文本引导的后编码器令牌剪枝框架。该方法在图像编码后、时间推理模块前，利用轻量级路由机制对视觉令牌进行评分，综合考虑局部视觉上下文、基于文本描述的语义相关性以及边界模糊区域的不确定性提示，仅保留最相关信息的令牌以供后续处理。

Result: 在多个挑战性视频分割基准上，所提方法实现了最高42.50%的推理速度提升和37.41%的GPU内存占用降低，同时保持了与原始SAM2相当的J和F指标性能，验证了其在效率与精度间的良好平衡。

Conclusion: 通过早期视觉令牌的选择性剪枝，可有效提升基于Transformer的视频分割系统在实时性和资源受限场景下的可扩展性，该方法无需修改原模型架构，具有良好的实用价值和推广潜力。

Abstract: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

</details>


### [51] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: Streamo 是一个实时流式视频大模型，作为通用交互式助手，能够执行多种任务如实时叙述、动作理解、事件字幕生成、时间事件定位和时间敏感问答。研究构建了 Streamo-Instruct-465K 数据集以支持多任务训练，并通过端到端训练实现对连续视频流的强时序推理与响应能力，在多个基准测试中表现出色，推动了离线视频感知模型向实时多模态智能助手的演进。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频模型功能局限，仅专注于问答或字幕生成，缺乏对实时流式视频的全面理解与交互能力。为实现通用、实时、多任务的视频理解，需要一种能处理连续视频流并支持多样化交互任务的统一模型。

Method: 构建大规模指令跟随数据集 Streamo-Instruct-465K，涵盖多样化的时序上下文和多任务监督信号；采用简化的端到端训练流程，使模型在统一框架下学习多种流式视频任务。

Result: Streamo 在多项流式视频任务上表现优异，具备强大的时序推理能力、快速响应能力和跨任务泛化能力，显著缩小了离线视频模型与实时多模态助手之间的差距。

Conclusion: Streamo 实现了对连续视频流的统一、智能理解，是迈向通用实时视频交互助手的重要一步，具有广泛的应用前景。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


### [52] [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/abs/2512.21338)
*Haonan Qiu,Shikun Liu,Zijian Zhou,Zhaochong An,Weiming Ren,Zhiheng Liu,Jonas Schult,Sen He,Shoufa Chen,Yuren Cong,Tao Xiang,Ziwei Liu,Juan-Manuel Perez-Rua*

Main category: cs.CV

TL;DR: HiStream 是一种高效的自回归框架，通过空间、时间和时间步三方面的压缩策略，显著提升高分辨率视频生成的效率。该方法在保持高质量的同时，相比基线模型提速高达107.5倍，使高分辨率视频生成更具实用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率视频生成受限于扩散模型的二次复杂度，导致推理计算成本过高，难以实际应用。因此需要一种高效的方法来降低计算开销并提升生成速度。

Method: 提出 HiStream 框架，包含三项优化：(i) 空间压缩——先在低分辨率下去噪，再利用缓存特征进行高分辨率细化；(ii) 时间压缩——采用分块处理结合固定大小锚点缓存，保证推理速度稳定；(iii) 时间步压缩——对后续缓存条件下的块减少去噪步骤。

Result: 在 1080p 基准测试中，主模型 HiStream（i+ii）实现最佳视觉质量，去噪速度比 Wan2.1 基线快 76.2 倍，且质量损失可忽略；而更快的 HiStream+（i+ii+iii）达到 107.5 倍加速，实现了速度与质量的良好平衡。

Conclusion: HiStream 及其变体 HiStream+ 有效解决了高分辨率视频生成中的计算瓶颈问题，大幅提升了生成效率，为实际应用提供了可行方案。

Abstract: High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [53] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: 本文提出一种基于稀疏自编码器（SAEs）的新方法，用于自动发现大语言模型（LLMs）评估中的两类“缺口”：模型自身的能力短板（模型缺口）和评估基准的覆盖不均衡（基准缺口）。该方法通过提取模型内部概念激活并计算加权性能分数，实现基于模型内部表征的评估，从而在不同基准间进行比较。应用到两个开源模型和十个基准后，发现模型在反对讨好行为（如礼貌拒绝或设定边界）及安全相关概念上表现较差，且多数基准过度强调服从、权威或指令遵循，忽略了应有核心概念。该方法为评估提供了概念级分解，补充了传统聚合指标，揭示了模型得分原因并指导基准改进。代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估依赖标准化基准，但其聚合指标可能掩盖模型在特定子领域的能力短板（模型缺口）以及基准本身在概念覆盖上的不平衡（基准缺口），亟需一种能深入分析这些缺陷的方法。

Method: 使用稀疏自编码器（SAEs）提取模型内部概念激活，结合显著性加权性能评分，对基准数据进行分析，实现基于模型内部表示的评估，并识别出模型与基准层面的潜在缺口。

Result: 发现模型在非讨好性行为（如拒绝请求、设定边界）和安全相关概念上表现不佳；同时，多个基准存在过度强调服从、权威、指令遵循等问题，缺乏应有核心概念覆盖。该方法无需人工标注即可自动识别这些缺口，验证了其有效性。

Conclusion: 所提方法提供了一种基于模型内部表征的评估框架，能够实现概念级的性能分解，不仅补充了传统聚合指标，还揭示了模型表现背后的原因及基准设计的改进方向。

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [54] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: SA-DiffuSeq introduces sparse attention into diffusion models for long text generation, reducing computational cost and memory usage while maintaining quality. It uses a soft absorbing state to stabilize diffusion and improve sampling efficiency, outperforming current methods in speed and scalability, especially for long sequences.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for long text generation are computationally expensive and memory-intensive as sequence length grows. There is a need for more scalable approaches that maintain semantic coherence and generation quality.

Method: SA-DiffuSeq integrates sparse attention into the diffusion process, selectively allocating attention to reduce complexity. It employs a soft absorbing state designed for sparse attention dynamics to stabilize training and accelerate reconstruction.

Result: SA-DiffuSeq achieves superior training efficiency and faster sampling speed compared to state-of-the-art diffusion models, with notable improvements on long sequences. It excels in modeling long-range dependencies and is suitable for applications like scientific writing, code generation, and dialogue.

Conclusion: Incorporating structured sparsity into diffusion models is a promising path toward efficient and expressive long-form text generation, enabling practical deployment in demanding real-world scenarios.

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [55] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: 本文提出TokSuite，一个包含多种分词器的模型集合和基准测试，用于研究分词对语言模型性能的影响。通过训练14个使用不同分词器但其他条件完全相同的模型，并构建针对真实世界扰动的基准测试，实现了分词影响的独立评估，揭示了多种主流分词器的优缺点。


<details>
  <summary>Details</summary>
Motivation: 当前对分词在语言模型中的作用理解不足，主要因为难以在隔离条件下测量分词的影响。需要一种系统的方法来独立分析分词器对模型性能的作用。

Method: 训练14个使用不同分词器但架构、数据集、训练预算和初始化完全相同的模型；构建并发布一个针对真实世界扰动的基准测试，以评估分词器在实际场景下的表现。

Result: 成功实现分词器影响的独立评估，发现不同分词器在处理特定任务和噪声时表现出显著差异，揭示了各分词器的优势与局限性。

Conclusion: TokSuite为研究分词器对语言模型行为的影响提供了可靠的工具和基准，有助于深入理解分词在自然语言处理中的关键作用。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [56] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 本文提出一种对抗训练框架，通过生成器（用户模拟器）与判别器之间的竞争动态，迭代提升用户模拟器的真实感。在心理健康支持聊天机器人领域，该方法显著优于零样本基线模型，能更有效地暴露系统缺陷，且经过对抗训练后，模拟器在多样性、分布对齐和预测有效性方面均有提升。模拟失败率与真实失败率高度相关，同时保持低分布偏差。判别器准确率在三轮对抗训练后大幅下降，表明模拟器真实性显著提高。


<details>
  <summary>Details</summary>
Motivation: 创建能够准确反映人类行为的用户模拟器对于训练和评估任务导向对话系统至关重要，但现有方法难以有效暴露系统缺陷。因此，需要一种更真实、更具挑战性的用户模拟器来提升系统评估质量。

Method: 采用对抗训练框架，通过生成器（用户模拟器）与判别器之间的迭代竞争，不断优化模拟器的行为以逼近真实用户行为。利用判别器反馈指导生成器改进，从而提升模拟器的逼真度和评估能力。

Result: 经过对抗训练后的用户模拟器在暴露系统问题方面表现远超零样本基线模型；模拟失败率与真实失败率呈现强相关性，失败模式分布偏差低；判别器准确率在三轮迭代后显著下降，说明模拟器真实感增强。

Conclusion: 对抗训练是一种有前景的方法，可用于构建心理健康支持领域的高保真用户模拟器，实现快速、可靠且低成本的系统评估，为部署前验证提供有力支持。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [57] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 本研究通过控制实验，对比专家人类导师、新手人类导师和多个大型语言模型在数学补救教学对话中的回应，分析其教学策略和语言特征。结果显示，尽管大语言模型平均表现出与专家人类导师相当的感知教学质量，但在教学策略上存在系统性差异：它们较少使用专家常用的重述和复述策略，而倾向于生成更长、词汇多样性更高、更礼貌的回应。统计分析表明，重述、复述、词汇多样性和强调准确性与更高的教学质量感知正相关，而更高的主动性与礼貌语言则负相关。整体而言，大语言模型虽达到专家水平的教学质量，但依赖不同的教学和语言策略，强调在评估智能辅导系统时需综合分析教学策略和语言特征。


<details>
  <summary>Details</summary>
Motivation: 当前研究探索了大型语言模型在数学辅导回应生成中的应用，但尚不清楚其教学行为与专家人类导师实践的契合度。因此，亟需系统性比较以揭示其优势与不足，为改进智能辅导系统提供依据。

Method: 采用控制实验设计，在逐轮对话层面比较专家人类导师、新手人类导师和多个大型语言模型对相同数学补救对话回合的回应。从教学策略（如重述、复述、强调准确性）和语言特征（如词汇多样性、可读性、礼貌性、主动性）两个维度进行分析。

Result: 大型语言模型在平均感知教学质量上接近专家人类导师，但在教学策略上存在系统性差异：较少使用重述和复述策略，而生成更长、词汇更丰富、更礼貌的回应。统计分析显示，重述、复述、词汇多样性及强调准确性与教学质量感知呈正相关；而高主动性和高礼貌性则与感知质量呈负相关。

Conclusion: 近期大型语言模型虽在整体教学质量上可媲美专家人类导师，但其教学策略和语言特征与人类专家存在显著差异。这凸显了在评估智能辅导系统时，必须深入分析教学策略和语言特征，而非仅依赖整体质量评分。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [58] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: 本文探讨了模型编辑算法（ROME、IKE、WISE）在机器遗忘场景中的应用，设计了新的编辑目标以实现更有效的信息删除。研究表明，这些编辑方法在特定设置下可超越传统遗忘方法的性能，但在全面覆盖需遗忘内容的同时保持模型整体性能方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在处理大规模语言模型时效率低下或无法完全移除目标信息而不损害保留知识的性能；而传统模型编辑方法侧重于重定向输入而非彻底删除信息，因此需要探索其在遗忘任务中的潜力与局限性。

Method: 研究并应用三种模型编辑算法（ROME、IKE、WISE），针对遗忘场景设计新的编辑目标，并评估其在不同设置下的表现。

Result: 模型编辑方法在某些情况下能够比基线遗忘方法更好地实现信息遗忘，但仍然难以在不损害模型整体性能的前提下精确控制遗忘范围。

Conclusion: 尽管模型编辑算法在机器遗忘任务中展现出潜力，但仍面临如何精准且无损地实现信息删除的核心挑战，表明该方向仍有待进一步优化。

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [59] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 该研究探讨语言模型中独立于一般人口统计识别的群体偏见机制，通过多任务评估发现，针对特定任务的稀疏自编码器特征删减可在不损害识别性能的前提下减少偏见。基于归因的方法有效缓解职业中的种族与性别刻板印象，而基于相关性的方法在教育偏见上更优；但教育任务中删除归因特征会引发“先验坍缩”，反而增加偏见，表明需进行维度特异性干预。结果表明，偏见源于任务特定机制而非绝对人口标志，且可通过机制推断的推理时干预实现精准去偏而不影响核心能力。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型中群体偏见是否独立于一般人口统计识别能力，以实现去偏同时保留关键识别功能。

Method: 采用多任务评估框架，关联姓名、职业和教育水平等人口统计信息，比较基于归因与基于相关性的偏见特征定位方法，并使用稀疏自编码器对特征进行靶向删减。

Result: Gemma-2-9B模型中，基于归因的特征删减有效降低种族与性别职业偏见，同时保持姓名识别准确率；基于相关性的删减对教育偏见更有效；但教育任务中归因特征删除导致‘先验坍缩’，加剧偏见。

Conclusion: 群体偏见源于任务特定机制而非绝对人口标记，通过机制推断的推理时干预可实现精准去偏，且不影响模型的核心识别能力。

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [60] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 本文通过引入新型符号表示的实验框架，研究大语言模型（LLMs）在处理陌生符号时的推理能力。通过设计语义误导情境，测试模型是否能保持符号抽象性，而非依赖表面语义关联。实验发现，即使面对简单的算术任务，语义线索仍显著降低模型表现，暴露出当前LLMs在符号操作上的局限性，以及对表面语义的过度依赖，暗示思维链可能强化统计相关性依赖。这引发伦理与社会担忧，质疑将推理能力归因于LLMs的普遍倾向，并警示其在关键决策场景中的潜在失败风险。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在涉及人类价值观的决策任务中是否具备真正可靠的符号推理能力，尤其在面对非标准符号系统时，能否摆脱训练中习得的语义偏见。

Method: 重新定义标准数字和数学运算符为新型符号，构建包含语义误导的算术任务，通过四类大语言模型进行实验，观察其在新符号系统下的表现及对误导性语义线索的敏感度。

Result: 语义线索显著降低模型在简单任务中的表现；模型表现出对表面语义的高度依赖，难以维持纯粹的符号抽象；即使看似遵循指令，仍受残余语义关联影响。

Conclusion: 当前大语言模型在符号推理方面存在根本性局限，过度依赖表面语义和统计模式，难以实现真正意义上的抽象推理。这一缺陷在需要严谨逻辑的决策场景中可能带来严重风险，应警惕将其误认为具备人类水平的推理能力。

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [61] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: EssayCBM is a rubric-aligned, interpretable essay grading framework that evaluates eight writing concepts (e.g., Thesis Clarity, Evidence Use) via dedicated prediction heads on an encoder. It uses these concept scores as a transparent bottleneck to compute final grades through a lightweight network, enabling instructors to adjust predictions and instantly see grade updates—supporting accountable human-in-the-loop evaluation. The system matches black-box model performance while providing actionable, concept-level feedback via an intuitive web interface.


<details>
  <summary>Details</summary>
Motivation: Automated essay grading systems often act as black boxes, making it difficult for educators and students to understand how grades are assigned. There is a need for transparent, interpretable models that allow meaningful human oversight and feedback.

Method: EssayCBM uses a multi-head encoder to predict eight writing concepts independently. These concept scores form a bottleneck, which feeds into a lightweight network that computes the final grade. This design enables interpretability and allows real-time grade adjustments by instructors.

Result: EssayCBM achieves performance comparable to black-box models while providing full transparency in grading decisions. Instructors can interactively modify concept scores and observe immediate grade changes, facilitating accountable and informed evaluation.

Conclusion: EssayCBM offers a practical, interpretable alternative to opaque automated grading systems, combining strong performance with actionable feedback and human-in-the-loop capabilities.

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [62] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: MediEval 是一个基于 MIMIC-IV EHR 数据与统一生物医学知识库的基准测试，用于系统评估 LLM 在医疗场景中的事实性与上下文一致性。该研究揭示了当前 LLM 存在幻觉支持和真值反转等关键缺陷，并提出 CoRFu 方法通过不对称惩罚机制提升模型安全性和准确性，显著改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有医疗 LLM 评估缺乏对事实准确性和上下文一致性的联合验证，导致模型可靠性不足，亟需更全面的评估框架与改进方法。

Method: 构建 MediEval 基准，结合真实患者上下文生成多样化的事实与反事实陈述，采用四象限评估框架分析模型表现；提出基于 DPO 的 CoRFu 方法，引入不对称惩罚以抑制不安全混淆。

Result: CoRFu 在宏 F1 上比基线模型提升 16.4 点，完全消除真值反转错误，显著提升模型准确率与安全性。

Conclusion: MediEval 提供了一种系统化评估医疗 LLM 的新范式，CoRFu 方法有效缓解关键风险，推动 LLM 在医疗领域的可靠应用。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [63] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3 Nano 30B-A3B 是一个混合专家（MoE）的 Mamba-Transformer 语言模型，基于25万亿文本标记预训练，包含超过3万亿新唯一标记。通过监督微调和大规模强化学习优化，在激活少于一半参数的情况下，性能优于前代Nemotron 2 Nano，推理吞吐量比同类开源模型如GPT-OSS-20B和Qwen3-30B-A3B-Thinking-2507高出最多3.3倍，同时在主流基准测试中更准确。该模型具备更强的智能体、推理与对话能力，支持长达100万标记的上下文长度。已公开基础版和后训练版本模型权重。


<details>
  <summary>Details</summary>
Motivation: 提升大模型的推理效率与性能，同时保持低参数激活率和长上下文支持能力，以满足复杂任务需求并推动开源模型发展。

Method: 采用Mixture-of-Experts架构结合Mamba与Transformer结构，进行大规模预训练（25万亿标记）、监督微调及基于多样化环境的大规模强化学习。

Result: 相比Nemotron 2 Nano，Nemotron 3 Nano在更低参数激活率下实现更高准确率；推理吞吐量较同类模型最高提升3.3倍；在智能体、推理和对话任务上表现更优，并支持1M token上下文长度。

Conclusion: Nemotron 3 Nano 30B-A3B 在效率、性能与功能扩展方面均显著超越现有模型，是高效且强大的开源语言模型，适用于复杂多样的实际应用场景。

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [64] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 本文评估了多种在真实检索场景下处理相关文档数量未知问题的策略，通过衡量检索质量指标与基于大语言模型（LLM）对响应质量的判断之间的相关性来评估效果。实验在多个数据集上进行，相关文档数量较少（2-15个），并提出了一种无需了解总相关文档数即可表现良好的简单检索质量度量方法。


<details>
  <summary>Details</summary>
Motivation: 在真实检索场景中，知识库庞大且不断演变，相关文档总数通常未知，导致无法计算召回率，因此需要新的方法来评估检索质量。

Method: 通过比较不同检索质量指标与基于大语言模型生成响应的质量判断之间的相关性，评估其有效性；同时提出一种不依赖总相关文档数的新型检索质量度量方法。

Result: 所提出的简单检索质量度量方法在无须知道相关文档总数的情况下表现良好，且与LLM判断的相关性较高，验证了其有效性。

Conclusion: 该研究证明了在缺乏相关文档总数信息的情况下，仍可有效评估检索质量，并提出了一种实用且高效的度量方法，适用于大规模动态知识库环境。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [65] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 本文系统研究了在严格计算资源限制下小型语言模型的性能，通过逐步引入非线性、自注意力机制和多层Transformer架构，评估其在字符级Tiny Shakespeare和词级PTB/WikiText-2数据集上的表现。结果表明，即使在小规模下，基于注意力的模型在每FLOP效率上仍优于MLP；而增加深度或上下文长度若缺乏优化反而会降低性能。此外，旋转位置编码（RoPE）等在大模型中有效的技术在小模型中并不一定适用。


<details>
  <summary>Details</summary>
Motivation: 探究在有限计算资源下小型语言模型的性能边界，理解架构选择与训练预算之间的相互作用，为高效小模型设计提供实证依据。

Method: 从线性预测器出发，逐步引入非线性、自注意力和多层Transformer结构，对比不同模型在字符级和词级文本建模任务上的测试负对数似然（NLL）、参数量和近似训练浮点运算次数（FLOPs），以分析准确率与效率的权衡。

Result: 注意力机制在小规模下依然具有更高的每FLOP效率；增加模型深度或上下文长度但缺乏优化会导致性能下降；旋转位置编码等适用于大模型的技术在小模型中效果不佳。

Conclusion: 在计算受限的小型语言模型中，应优先采用高效的注意力架构，并避免盲目增加深度或上下文长度；现有大模型中的先进架构技术未必适用于小模型，需针对性优化。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [66] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出一种跨模型推理蒸馏溯源追踪框架，用于分析学生模型在测试阶段是否真正继承教师模型的推理能力。通过比较教师、原始学生和蒸馏后学生在相同上下文下的预测概率，对每个生成动作进行溯源分类，发现蒸馏模型确实在测试中能产生源自教师的行为，且这些行为与性能提升相关。基于此，提出一种基于教师-学生差异的有原则的数据选择方法，优于以往依赖启发式的方法。实验验证了该框架和方法在多种师生模型组合中的有效性，推动了推理蒸馏的可解释性与性能优化。


<details>
  <summary>Details</summary>
Motivation: 现有推理蒸馏方法缺乏对蒸馏后模型能力来源的深入分析，不清楚学生模型在测试阶段是否真正保持教师的行为一致性，还是回归原始输出模式，影响对蒸馏模型泛化能力的信任。

Method: 提出跨模型推理蒸馏溯源追踪框架，对每个生成动作（如句子）计算教师、原始学生和蒸馏学生在相同上下文下的预测概率，通过比较概率分布进行溯源分类；并基于教师-学生差异设计一种有原则的数据选择方法。

Result: 实验证明蒸馏模型在测试阶段能够生成教师起源的动作，且这些动作与性能提升相关；所提出的教师引导数据选择方法显著优于传统启发式方法，在多种师生模型组合下均表现良好。

Conclusion: 本文提出的溯源追踪框架有效揭示了推理蒸馏中模型行为的来源，证明了蒸馏模型在测试阶段具备教师行为的延续性，并为数据选择提供了可解释、可优化的新范式，具有重要实践价值。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [67] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: FEND 是一个基于基础模型的多模态框架，用于跨生命周期、多语言环境下检测阿尔茨海默病（AD）、抑郁症和自闭症谱系障碍（ASD）。该框架整合了语音与文本模态，利用13个涵盖英语、中文、希腊语、法语和荷兰语的多语言数据集进行系统评估。结果显示，多模态融合在AD和抑郁症检测中表现优异，但在ASD检测中因数据异质性表现较差；同时存在模态不平衡问题，多模态融合常无法超越最优单模态模型。跨语料库实验表明，在任务和语言一致的情况下表现稳健，但在多语言和任务异构场景下性能明显下降。FEND提供了全面的基准和影响因素分析，推动了自动化、跨生命周期、多语言神经精神疾病评估的发展，并倡导研究者采用该框架以实现公平比较和可复现研究。


<details>
  <summary>Details</summary>
Motivation: 神经精神疾病如阿尔茨海默病、抑郁和自闭症谱系障碍常伴随语言和声学异常，具有作为早期检测生物标志物的潜力。然而，现有方法面临多语言泛化能力差、缺乏统一评估框架等挑战，亟需一个系统性的多模态评估体系来推动该领域发展。

Method: 提出FEND框架，整合语音与文本多模态信息，基于13个跨语言数据集进行多模态融合建模与系统评估，通过跨语料库实验分析不同条件下的性能表现，识别影响检测效果的关键因素。

Result: 多模态融合在AD和抑郁症检测中表现优越，但在ASD检测中受数据异质性影响表现不佳；普遍存在模态不平衡现象，多模态融合未能超越最佳单模态模型；在任务和语言一致时表现稳定，但在多语言和任务异构场景下性能显著下降。

Conclusion: FEND为神经精神疾病的多模态自动评估提供了统一、可复现的基准框架，有助于推动跨语言、全生命周期的精准检测研究，强调了未来需关注数据质量、模态平衡与跨场景泛化能力。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [68] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 本文提出了一种名为TriAligner的新方法，用于SemEval-2025任务7：多语言与跨语言事实核查声明检索。该方法采用双编码器架构结合对比学习，并融合不同模态的原生语言和英文翻译，以提升跨语言声明检索效果。通过高效的数据预处理与增强、硬负样本采样等策略，显著提升了表示学习能力与系统鲁棒性，在单语与跨语言基准上均优于基线模型，有效提高了事实核查的准确性。


<details>
  <summary>Details</summary>
Motivation: 在虚假信息快速传播的时代，高效的事实核查变得尤为重要。现有方法在多语言和跨语言场景下的声明检索性能仍有不足，亟需更有效的跨语言对齐与表示学习机制。

Method: 提出TriAligner方法，基于双编码器架构，结合对比学习，利用原生语言与英文翻译在多模态中的信息互补性；通过大语言模型进行数据预处理与增强，引入硬负样本采样以优化表示学习。

Result: 在单语和跨语言基准测试中，TriAligner在检索准确率和事实核查性能方面均显著优于现有基线模型，证明了其有效性与鲁棒性。

Conclusion: TriAligner通过多语言信息融合与强化学习策略，实现了高效、准确的跨语言声明检索，为多语言事实核查提供了强有力的技术支持。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [69] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出语言表达性概念，指出蛋白质语言模型因令牌空间表达性有限而难以应用链式思维（CoT）推理。为此，首次在生物序列模型中引入反射预训练，通过生成额外的“思考令牌”增强模型中间推理能力。理论与实验均表明，该方法显著提升模型表达性与推理性能，实现自纠错并带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有链式思维（CoT） prompting 在自然语言任务中表现优异，但在蛋白质和RNA等非自然语言领域受限于其有限的令牌表达性，无法有效生成中间推理步骤。因此亟需一种新方法突破生物序列语言的表达瓶颈，以支持复杂推理。

Method: 提出语言表达性定义，设计反射预训练框架，引入辅助‘思考令牌’扩展原始令牌集，使蛋白质语言模型具备生成中间推理步骤的能力，并通过预训练学习自纠错与高级推理策略。

Result: 理论分析证明扩展令牌集可显著提升生物语言表达性；实验结果显示，采用反射预训练的模型在多个任务上实现显著性能提升，尤其在错误纠正与复杂推理任务中表现突出。

Conclusion: 通过引入反射预训练与思考令牌，成功克服了蛋白质语言模型表达性不足的限制，为生物序列建模中的复杂推理提供了新范式，推动了生物语言模型向更智能方向发展。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [70] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: 本文提出MedMistake，一个自动提取大型语言模型（LLM）在医患对话中错误的流水线，并将其转化为单次问答基准。该方法生成复杂对话数据，通过双LLM评委多维度评估，再将错误转换为简化问答对。发布包含3,390个问答对的MedMistake-All数据集，其中GPT-5和Gemini 2.5 Pro表现不佳。通过医学专家验证211个样本形成MedMistake-Bench，用于评估12个前沿LLM，结果显示GPT、Claude和Grok表现最佳。数据集已公开于Hugging Face。


<details>
  <summary>Details</summary>
Motivation: 现有临床场景下对大语言模型的评估依赖多维评分标准，但难以复现特定错误，且手动分析效率低。亟需一种自动化方法系统识别并构建具有挑战性的错误案例库，以推动模型改进。

Method: 1. 构建由LLM模拟患者与医生的复杂对话；2. 使用两个LLM评委从多个维度（如推理质量、安全性、以患者为中心）进行评估；3. 从识别出的错误中提取并转换为单次问答形式，形成标准化测试集。

Result: 成功构建了包含3,390个问答对的MedMistake-All数据集，其中主流模型如GPT-5和Gemini 2.5 Pro存在显著错误；经医学专家验证的211个样本组成MedMistake-Bench，用于最终评估，发现GPT系列、Claude及Grok在该基准上表现最优。

Conclusion: MedMistake提供了一种可扩展、自动化的错误挖掘与评测框架，有效支持临床场景下大语言模型的质量评估与持续优化，其发布的数据集有助于推动医疗AI模型的可靠性研究。

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [71] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 本文研究了在知识蒸馏过程中，对提示（P）、思维链（CoT）和答案（A）不同部分分配监督信号对小型学生模型性能的影响。发现仅对CoT部分进行蒸馏即可有效，前提是该部分已包含提示和答案的信息。基于此，提出一种截断协议，量化序列长度与计算效率之间的权衡。实验表明，仅使用每个训练序列前50%的token即可保持约94%的完整序列性能，同时将训练时间、内存占用和浮点运算量减少约50%。结果表明，推理蒸馏应优先关注早期推理令牌，为计算与质量之间提供了简单有效的调控方式。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型到小型模型的知识蒸馏需要大量推理数据，且在长序列上进行训练成本高昂，尤其是当包含提示（P）、思维链（CoT）和答案（A）三部分时。因此，亟需探索如何在不显著损失性能的前提下降低计算开销。

Method: 通过分析不同训练段（P、CoT、A）的监督分配效果，发现仅对思维链（CoT）部分进行蒸馏即可达到良好性能；在此基础上设计了一种序列截断协议，系统评估不同序列长度下的性能与计算开销的权衡关系。

Result: 仅使用序列前50%的token进行训练，可在数学基准测试中保留约94%的完整序列性能，同时将训练时间、内存消耗和FLOPs减少约50%。

Conclusion: 推理蒸馏应优先关注早期推理令牌，通过合理截断序列可实现显著的计算效率提升，而性能损失极小，提供了一种简单有效的计算-质量权衡机制。

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [72] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: 提出SFTKey，一种两阶段微调方法，先通过传统SFT确保输出格式，再仅对关键答案部分进行微调，显著提升准确性，平均提升超5%，同时保持格式正确性。


<details>
  <summary>Details</summary>
Motivation: 传统SFT在处理长思维链时过度关注推理过程，忽视了短但关键的最终答案部分，影响任务成功率和评估质量。

Method: SFTKey采用两阶段训练：第一阶段使用常规SFT保证输出格式；第二阶段仅对答案部分进行微调，以增强准确性。

Result: 在多个基准和模型族上的实验表明，SFTKey相比传统SFT平均准确率提升超过5%，且保持良好的格式生成能力。

Conclusion: SFTKey通过显式平衡思维链学习与答案相关词元的优化，有效提升了大语言模型在复杂推理任务中的表现。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [73] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: 本文提出利用半监督学习技术，结合标注与未标注数据，以提升大语言模型（LLM）的安全性分类性能。研究强调任务特定的数据增强在提升模型表现中的关键作用，并对比了通用增强方法的不足。


<details>
  <summary>Details</summary>
Motivation: 当前安全分类器训练依赖大量标注数据，但这些数据常存在获取困难、标注错误或使用合成数据的问题，因此需要更高效、可靠的训练方法。

Method: 采用半监督学习框架，结合少量标注数据和大量未标注数据进行训练，并引入任务特定的数据增强策略以提升模型性能。

Result: 实验表明，使用任务特定增强的半监督学习显著提升了对提示（prompt）和模型响应的安全性分类效果，优于传统基于通用增强的方法。

Conclusion: 半监督学习结合任务特定增强是一种有效且高效的LLM安全性提升策略，尤其适用于标注数据稀缺的场景。

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [74] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: SpidR-Adapt 提出一种高效适应新语言的语音表示学习方法，通过元学习框架和低资源预训练协议（MAdaPT），结合一阶双层优化（FOBLO）与交替监督初始化，仅需不到1小时目标语言音频即可显著提升音素可区分性和语言建模性能，比标准训练高出100倍以上数据效率。该方法为生物启发的高效语音表示提供了通用、无需特定架构的解决方案。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿在仅有数百小时语音暴露的情况下就能掌握新语言的基本单位，展现出与当前自监督语音模型相比的巨大数据效率优势。现有模型需要大量数据才能达到类似效果，因此亟需一种能够以极少量无标签数据快速适应新语言的方法。

Method: 将低资源语音表示学习建模为元学习问题，设计多任务自适应预训练（MAdaPT）协议，采用双层优化框架；提出一阶双层优化（FOBLO）以降低计算开销；通过交替监督机制实现鲁棒初始化，稳定元训练过程。

Result: SpidR-Adapt 在音素可区分性（ABX）和口语语言建模（sWUGGY, sBLIMP, tSC）上均取得显著提升，仅使用少于1小时的目标语言音频即超越领域内语言模型，数据效率超过标准训练100倍以上。

Conclusion: SpidR-Adapt 为构建生物启发、数据高效的语音表示提供了实用且架构无关的路径，推动了低资源语音学习的发展，并已开源代码与模型权重。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [75] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: 提出并行标记预测（PTP）框架，通过在单次Transformer调用中联合预测多个相关标记，实现语言模型的并行序列生成。该方法将采样过程融入模型，减少自回归解码的延迟瓶颈，并避免现有多标记预测方法中的独立性假设限制。证明了PTP可表示任意自回归序列分布。可通过蒸馏或无教师的逆自回归训练进行训练。实验表明，在Vicuna-7B上，于Spec-Bench测试中每步接受超过四个标记，达到当前最优的推测解码性能。框架的通用性表明，无需损失建模能力即可实现长序列的并行生成。


<details>
  <summary>Details</summary>
Motivation: 自回归解码存在延迟瓶颈，而现有并行生成方法受限于独立性假设，无法有效捕捉序列依赖关系。因此需要一种既能保持建模能力又可高效并行生成的通用框架。

Method: 提出并行标记预测（PTP）框架，将采样过程嵌入模型结构中，通过单次Transformer调用联合预测多个依赖标记，支持任意自回归分布的表示；采用模型蒸馏或逆自回归训练策略进行训练。

Result: 在Vicuna-7B模型上，于Spec-Bench测试中实现每步接受超过四个标记，显著提升推测解码效率，达到当前最优性能；验证了框架在不损失建模能力前提下实现长序列并行生成的可行性。

Conclusion: PTP是一种通用且高效的并行序列生成框架，能够突破传统自回归解码的延迟限制，在不牺牲建模能力的前提下实现高质量的长序列并行生成。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [76] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 该研究挑战了当前认为视觉-语言模型在ARC等推理基准上表现不佳主要源于推理能力不足的观点，提出感知能力不足才是主要原因。通过设计两阶段实验（先将图像转为自然语言描述，再进行推理），分离感知与推理过程，发现约80%的失败源于感知错误，表明现有基准混淆了感知与推理问题，评估应更注重解耦二者。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准如ARC被广泛用于评估AI的'流体'推理能力，但前沿视觉-语言模型在此类任务上仍表现不佳，通常归因于机器推理能力不足。然而，作者质疑这一解释，认为根本原因可能是视觉感知局限而非推理缺陷。

Method: 提出两阶段实验范式：第一阶段将图像独立转换为自然语言描述，第二阶段使用这些描述进行规则推导与应用，从而隔离感知与推理模块，避免跨图像归纳信号泄露，实现对推理能力的纯净评估。

Result: 在Mini-ARC、ACRE和Bongard-LOGO三个数据集上，两阶段方法显著优于传统端到端评估；手动分析显示约80%的模型失败由感知错误导致，证明感知是性能瓶颈的关键因素。

Conclusion: ARC类基准任务实际上混合了感知与推理挑战，当前性能差距可能高估了机器推理的不足。评估机器智能进步时，必须采用能解耦感知与推理的评价协议。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [77] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: C2LLM 是基于 Qwen-2.5-Coder 架构的代码嵌入模型，包含 0.5B 和 7B 两种规模。通过引入多头注意力池化（PMA）模块，有效利用预训练语言模型的因果表示，聚合序列中所有标记信息，突破传统 EOS 嵌入的信息瓶颈，并支持灵活调整嵌入维度，作为 MRL 的替代方案。在三百万公开数据上训练后，C2LLM 在 MTEB-Code 基准上刷新同规模模型纪录，其中 C2LLM-7B 在整体排行榜中位列第一。


<details>
  <summary>Details</summary>
Motivation: 现有代码嵌入模型在序列表示上存在信息瓶颈，尤其是依赖结尾标记（EOS）的嵌入方式难以充分整合全序列信息。同时，模型嵌入维度固定，缺乏灵活性，限制了其在不同任务中的适应性。因此需要一种既能保留预训练语言模型优势，又能高效聚合全局信息并支持灵活维度配置的新型代码嵌入方法。

Method: 采用 Qwen-2.5-Coder 作为基础架构，引入 Pooling by Multihead Attention（PMA）模块，将原始 token embeddings 通过多头注意力机制进行聚合，生成序列级嵌入。该方法充分利用预训练模型的因果上下文表示能力，同时实现对整个输入序列的全面信息整合，且嵌入维度可自由调整。

Result: C2LLM 模型在 MTEB-Code 基准测试中表现优异，特别是在相似规模模型中达到新纪录；其中 C2LLM-7B 在整体排行榜中排名第一，验证了其在代码理解与表示方面的强大性能。

Conclusion: C2LLM 通过 PMA 模块实现了高效、灵活且强表达力的代码嵌入，克服了传统方法的信息瓶颈问题，为代码理解任务提供了更优的嵌入范式，在多个基准上展现出领先性能。

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [78] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 本文首次形式化了掩码扩散模型（MDMs）中解码顺序对生成质量敏感的问题，提出通过去噪熵（Denoising Entropy）量化生成路径上的累积预测不确定性，并设计了后处理选择与实时引导两种算法优化解码路径。实验表明，该方法显著提升复杂推理、规划和代码生成任务的准确性，将原本的不确定性转化为发现高质量解的关键优势。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型虽具非自回归生成灵活性，但其输出质量高度依赖解码顺序，现有方法缺乏对这一问题的系统性理解与控制机制。

Method: 提出去噪熵作为可计算的内部信号，用于衡量生成过程中的累积预测不确定性；设计两种算法：后处理选择策略和实时引导策略，以优化解码路径。

Result: 在多个挑战性基准测试（如推理、规划、代码生成）中，熵引导方法显著提升了生成质量，表现出一致且稳定的性能增益。

Conclusion: 去噪熵为理解与控制掩码扩散模型的生成过程提供了原则性工具，成功将生成不确定性从缺陷转化为实现高质量解的重要优势。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Parameter-Efficient Neural CDEs via Implicit Function Jacobians](https://arxiv.org/abs/2512.20625)
*Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 本文提出了一种参数高效的神经控制微分方程（NCDE）替代方法，显著减少参数数量，同时保持与连续RNN类似的逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程（NCDEs）虽然适用于时间序列分析，但存在参数量过大的问题，限制了其在资源受限场景下的应用。

Method: 提出一种参数高效的NCDE变体，通过简化模型结构和优化参数配置，降低整体参数规模，同时保留其连续动态建模能力。

Result: 新方法在保持与原始NCDE相当性能的同时，显著减少了所需参数数量，验证了其高效性与可行性。

Conclusion: 该方法为神经控制微分方程提供了一种更轻量、更实用的实现路径，具有良好的扩展性和应用前景。

Abstract: Neural Controlled Differential Equations (Neural CDEs, NCDEs) are a unique branch of methods, specifically tailored for analysing temporal sequences. However, they come with drawbacks, the main one being the number of parameters, required for the method's operation. In this paper, we propose an alternative, parameter-efficient look at Neural CDEs. It requires much fewer parameters, while also presenting a very logical analogy as the "Continuous RNN", which the Neural CDEs aspire to.

</details>


### [80] [Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams](https://arxiv.org/abs/2512.20631)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.LG

TL;DR: 本研究对基于Transformer的情感模型进行了零训练的时序漂移分析，使用真实社交媒体数据验证了重大现实事件中的模型不稳定性。在12,279条真实社交媒体帖子上系统评估三种Transformer架构，发现事件驱动期间准确率下降高达23.4%。模型置信度最大下降达13.0%（Bootstrap 95% CI: [9.1%, 16.5%]），与实际性能退化高度相关。研究提出了四种新型漂移检测指标，优于基于嵌入的基线方法，且计算效率高，适合生产部署。多事件统计验证表明其检测能力稳健，实际意义超过行业监控阈值。该零训练方法可立即用于实时情感监测系统，并为Transformer模型在动态内容时期的行为提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型在真实事件驱动的社交媒体环境中表现出显著的时序漂移，导致性能下降，但缺乏无需训练即可快速检测漂移的实用方法。因此，亟需一种零训练、高效且可靠的漂移分析框架，以支持实时情感监测系统的稳定运行。

Method: 采用零训练策略，通过构建四类新型漂移指标（如置信度变化、分布偏移、响应模式异常等），结合统计验证与多事件实证分析，在不依赖额外标注数据的情况下评估Transformer模型在真实社交数据上的稳定性。利用Bootstrap方法进行置信区间估计，验证指标的可靠性。

Result: 在多个重大现实事件中，模型准确率下降最高达23.4%，置信度下降达13.0%（95% CI: [9.1%, 16.5%]），且二者呈强相关性。提出的漂移指标在检测能力上超越传统嵌入基线，同时具备低计算开销，满足生产环境部署需求。统计验证显示其在不同事件场景下均具鲁棒性，实际应用价值高于行业标准阈值。

Conclusion: 本研究提出了一种有效的零训练时序漂移分析方法，能够快速识别并量化情感模型在真实动态内容环境中的性能退化，为实时情感监测系统提供可靠保障，并揭示了Transformer模型在事件驱动语境下的行为脆弱性，具有重要的理论与实践意义。

Abstract: We present a comprehensive zero-training temporal drift analysis of transformer-based sentiment models validated on authentic social media data from major real-world events. Through systematic evaluation across three transformer architectures and rigorous statistical validation on 12,279 authentic social media posts, we demonstrate significant model instability with accuracy drops reaching 23.4% during event-driven periods. Our analysis reveals maximum confidence drops of 13.0% (Bootstrap 95% CI: [9.1%, 16.5%]) with strong correlation to actual performance degradation. We introduce four novel drift metrics that outperform embedding-based baselines while maintaining computational efficiency suitable for production deployment. Statistical validation across multiple events confirms robust detection capabilities with practical significance exceeding industry monitoring thresholds. This zero-training methodology enables immediate deployment for real-time sentiment monitoring systems and provides new insights into transformer model behavior during dynamic content periods.

</details>


### [81] [Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models](https://arxiv.org/abs/2512.20633)
*MunHwan Lee,Shaika Chowdhury,Xiaodi Li,Sivaraman Rajaganapathy,Eric W Klee,Ping Yang,Terence Sio,Liewei Wang,James Cerhan,Nansu NA Zong*

Main category: cs.LG

TL;DR: 本文提出一种基于大语言模型（LLM）的面向目标的知识萃取框架（GKC），用于将实验室、基因组和药物等多模态临床数据转化为与任务对齐的高保真特征，以提升肺癌治疗效果预测的准确性。该方法作为离线预处理步骤，可无缝集成至医院信息系统中，避免了传统模型在语义捕捉上的不足和大规模微调的不实用性。在184例肺癌患者数据上，GKC达到0.803的平均AUROC，优于专家设计特征、直接文本嵌入及端到端Transformer模型。消融实验表明三类模态融合具有互补性，验证了语义表示质量对稀疏临床数据预测性能的关键作用。研究强调将LLM视为知识萃取引擎而非黑箱预测器，为肿瘤学中可解释、可扩展且符合临床工作流的人工智能辅助决策提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 现有肺癌治疗结果预测面临电子健康数据稀疏、异质性强和上下文过载等问题，传统模型难以有效捕捉多模态数据间的语义信息，而大规模微调方法又不适用于实际临床流程。因此亟需一种既能保留语义信息又适合临床部署的高效方法。

Method: 提出基于大语言模型（LLM）的面向目标的知识萃取框架（GKC），将实验室、基因组和药物数据转化为任务导向的高精度特征表示；作为离线预处理模块，与现有医院信息系统兼容，不依赖实时推理。

Result: 在184例肺癌患者数据集上，GKC实现0.803的平均AUROC（95% CI: 0.799–0.807），显著优于专家特征、直接文本嵌入和端到端Transformer模型；消融实验验证了多模态融合的互补价值。

Conclusion: 语义表示的质量是稀疏临床数据环境下预测准确性的关键因素。将大语言模型定位为知识萃取工具而非黑箱预测器，提供了一条可扩展、可解释且与临床工作流兼容的AI辅助决策路径，推动了精准肿瘤学的发展。

Abstract: Accurate prediction of treatment outcomes in lung cancer remains challenging due to the sparsity, heterogeneity, and contextual overload of real-world electronic health data. Traditional models often fail to capture semantic information across multimodal streams, while large-scale fine-tuning approaches are impractical in clinical workflows. We introduce a framework that uses Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to convert laboratory, genomic, and medication data into high-fidelity, task-aligned features. Unlike generic embeddings, GKC produces representations tailored to the prediction objective and operates as an offline preprocessing step that integrates naturally into hospital informatics pipelines. Using a lung cancer cohort (N=184), we benchmarked GKC against expert-engineered features, direct text embeddings, and an end-to-end transformer. Our approach achieved a mean AUROC of 0.803 (95% CI: 0.799-0.807) and outperformed all baselines. An ablation study further confirmed the complementary value of combining all three modalities. These results show that the quality of semantic representation is a key determinant of predictive accuracy in sparse clinical data settings. By reframing LLMs as knowledge curation engines rather than black-box predictors, this work demonstrates a scalable, interpretable, and workflow-compatible pathway for advancing AI-driven decision support in oncology.

</details>


### [82] [Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning](https://arxiv.org/abs/2512.20634)
*Weiwei Wang*

Main category: cs.LG

TL;DR: 本文提出浅层与深层对齐框架，首次量化表征对齐深度，揭示当前任务对齐方法仅维持前3-5个输出词元的浅层对齐，导致模型易发生虚假遗忘。为此，提出包含量化指标、实时检测、可视化分析和自适应缓解策略的综合框架，在多个模型和数据集上实现86.2-90.6%的识别准确率，并使模型对遗忘的鲁棒性提升3.3-7.1%。


<details>
  <summary>Details</summary>
Motivation: 现有工作虽指出性能下降可能源于任务对齐破坏而非真实知识丢失，但缺乏对齐的定量刻画，依赖事后分析，且无自动区分机制，限制了对灾难性遗忘本质的理解与应对。

Method: 提出浅层与深层对齐框架，设计0-1尺度的对齐深度度量指标，开发训练中实时检测浅层对齐的方法，构建可视化与恢复预测工具，并引入自适应缓解策略以自动区分遗忘类型并促进深层对齐。

Result: 在多个数据集和模型（Qwen2.5-3B至Qwen2.5-32B）上的实验表明，该框架可实现86.2%-90.6%的遗忘类型识别准确率，且促进深层对齐显著提升模型对遗忘的鲁棒性，相较基线提升3.3%-7.1%。

Conclusion: 本研究揭示了浅层对齐是导致虚假遗忘的关键原因，提出的综合框架有效实现了对对齐深度的量化、检测、分析与干预，为大语言模型持续学习中的遗忘问题提供了系统性解决方案。

Abstract: Catastrophic forgetting remains a fundamental challenge in continual learning for large language models. Recent work revealed that performance degradation may stem from spurious forgetting caused by task alignment disruption rather than true knowledge loss. However, this work only qualitatively describes alignment, relies on post-hoc analysis, and lacks automatic distinction mechanisms.
  We introduce the shallow versus deep alignment framework, providing the first quantitative characterization of alignment depth. We identify that current task alignment approaches suffer from shallow alignment - maintained only over the first few output tokens (approximately 3-5) - making models vulnerable to forgetting. This explains why spurious forgetting occurs, why it is reversible, and why fine-tuning attacks are effective.
  We propose a comprehensive framework addressing all gaps: (1) quantitative metrics (0-1 scale) to measure alignment depth across token positions; (2) real-time detection methods for identifying shallow alignment during training; (3) specialized analysis tools for visualization and recovery prediction; and (4) adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment. Extensive experiments on multiple datasets and model architectures (Qwen2.5-3B to Qwen2.5-32B) demonstrate 86.2-90.6% identification accuracy and show that promoting deep alignment improves robustness against forgetting by 3.3-7.1% over baselines.

</details>


### [83] [Data-Free Pruning of Self-Attention Layers in LLMs](https://arxiv.org/abs/2512.20636)
*Dhananjay Saikumar,Blesson Varghese*

Main category: cs.LG

TL;DR: 提出Gate-Norm方法，通过查询-键耦合度评估注意力子层，无需校准数据、前向传播、微调或专用内核，可在一秒内完成40层13B参数LLaMA模型的剪枝。剪除8-16个注意力层可提升1.30倍推理吞吐量，同时保持零样本准确率在基准线2%以内，性能媲美数据驱动剪枝方法，但速度提升约1000倍，实现高效无数据压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的许多自注意力子层可被移除而几乎不损失性能，这归因于预训练过程中部分深层注意力层学会抑制自身贡献，使残差流和MLP承担主要表征任务。为实现高效、无需数据的模型压缩，需要一种快速、轻量的剪枝方法。

Method: 提出Gate-Norm，一种仅基于权重的一次性评分准则，根据注意力子层的查询-键耦合强度对它们进行排序，并移除耦合度最低的层。该方法无需任何额外计算或训练过程，直接利用模型权重完成评估与剪枝。

Result: 在40层13B参数的LLaMA模型上，Gate-Norm可在不到一秒内完成剪枝；剪除8-16个注意力层后，推理吞吐量最高提升1.30倍，且在BoolQ、RTE、HellaSwag、WinoGrande、ARC-Easy/Challenge和OpenBookQA等任务上的平均零样本准确率保持在原始模型的2%以内，表现优于或媲美数据驱动剪枝方法，且评分速度提升约1000倍。

Conclusion: Gate-Norm提供了一种高效、无数据依赖的轻量级剪枝方案，验证了注意力层冗余性，实现了大规模语言模型的实用化压缩，具有极高的实际应用价值。

Abstract: Many self-attention sublayers in large language models (LLMs) can be removed with little to no loss. We attribute this to the Attention Suppression Hypothesis: during pre-training, some deep attention layers learn to mute their own contribution, leaving the residual stream and the MLP to carry the representation. We propose Gate-Norm, a one-shot, weight-only criterion that ranks attention sublayers by query--key coupling and removes the least coupled ones, requiring no calibration data, no forward passes, no fine-tuning, and no specialized kernels. On 40-layer, 13B-parameter LLaMA models, Gate-Norm prunes the model in under a second. Pruning $8$--$16$ attention sublayers yields up to $1.30\times$ higher inference throughput while keeping average zero-shot accuracy within $2\%$ of the unpruned baseline across BoolQ, RTE, HellaSwag, WinoGrande, ARC-Easy/Challenge, and OpenBookQA. Across these settings, Gate-Norm matches data-driven pruning methods in accuracy while being $\sim 1000\times$ faster to score layers, enabling practical, data-free compression of LLMs.

</details>


### [84] [Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations](https://arxiv.org/abs/2512.20643)
*Suriya R S,Prathamesh Dinesh Joshi,Rajat Dandekar,Raj Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 该研究利用科学机器学习（Scientific ML）框架，结合神经微分方程（NODEs）和通用微分方程（UDEs），在Julia语言中建模n体问题的系统动力学。通过合成噪声数据模拟真实观测限制，发现UDE模型比神经ODE更高效，仅需20%的数据即可准确预测未来轨迹，而神经ODE需要90%。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在处理n体问题时依赖大量数据且为黑箱模型，忽视物理规律，缺乏可解释性；本研究旨在通过嵌入物理定律提升模型的可解释性与数据效率。

Method: 采用Julia编程语言实现科学机器学习框架，包括神经微分方程（NODEs）和通用微分方程（UDEs），并使用合成噪声数据进行训练与验证，评估模型在不同数据量下的预测性能与预报失效点。

Result: UDE模型表现出显著更高的数据效率，仅需20%的训练数据即可实现对未见数据的准确预测，而神经ODE则需90%的数据才能达到类似效果。

Conclusion: 将物理定律融入机器学习框架（如UDE）可大幅提升模型在少样本情况下的预测能力，为复杂系统建模提供高效、可解释的新方法。

Abstract: The n body problem, fundamental to astrophysics, simulates the motion of n bodies acting under the effect of their own mutual gravitational interactions. Traditional machine learning models that are used for predicting and forecasting trajectories are often data intensive black box models, which ignore the physical laws, thereby lacking interpretability. Whereas Scientific Machine Learning ( Scientific ML ) directly embeds the known physical laws into the machine learning framework. Through robust modelling in the Julia programming language, our method uses the Scientific ML frameworks: Neural ordinary differential equations (NODEs) and Universal differential equations (UDEs) to predict and forecast the system dynamics. In addition, an essential component of our analysis involves determining the forecasting breakdown point, which is the smallest possible amount of training data our models need to predict future, unseen data accurately. We employ synthetically created noisy data to simulate real-world observational limitations. Our findings indicate that the UDE model is much more data efficient, needing only 20% of data for a correct forecast, whereas the Neural ODE requires 90%.

</details>


### [85] [MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing](https://arxiv.org/abs/2512.20655)
*Yuting Hu,Lei Zhuang,Hua Xiang,Jinjun Xiong,Gi-Joon Nam*

Main category: cs.LG

TL;DR: 提出MaskOpt数据集，用于细胞和上下文感知的IC掩模优化，包含104,714个金属层片和121,952个通孔层片，支持不同上下文窗口大小，评估了先进深度学习模型并揭示其权衡，验证了周围几何形状和细胞感知输入的重要性。


<details>
  <summary>Details</summary>
Motivation: 光学光刻在亚波长尺度下面临衍射和工艺变异挑战，传统模型基OPC和ILT计算成本高，而现有深度学习方法依赖合成布局、忽略标准单元层次结构和周围上下文，限制了实际应用。

Method: 构建基于真实45nm节点IC设计的大规模基准数据集MaskOpt，通过标准单元放置裁剪保留细胞信息，支持多尺度上下文窗口以捕捉光学邻近效应影响，并评估多种深度学习模型进行掩模优化。

Result: 评估结果显示不同基线模型存在显著权衡；上下文大小分析与输入消融研究证实周围几何和细胞感知输入对准确生成掩模的关键作用。

Conclusion: MaskOpt为细胞和上下文感知的深度学习掩模优化提供了高质量基准，证明了上下文信息和细胞结构在提升掩模生成精度中的重要性。

Abstract: As integrated circuit (IC) dimensions shrink below the lithographic wavelength, optical lithography faces growing challenges from diffraction and process variability. Model-based optical proximity correction (OPC) and inverse lithography technique (ILT) remain indispensable but computationally expensive, requiring repeated simulations that limit scalability. Although deep learning has been applied to mask optimization, existing datasets often rely on synthetic layouts, disregard standard-cell hierarchy, and neglect the surrounding contexts around the mask optimization targets, thereby constraining their applicability to practical mask optimization. To advance deep learning for cell- and context-aware mask optimization, we present MaskOpt, a large-scale benchmark dataset constructed from real IC designs at the 45$\mathrm{nm}$ node. MaskOpt includes 104,714 metal-layer tiles and 121,952 via-layer tiles. Each tile is clipped at a standard-cell placement to preserve cell information, exploiting repeated logic gate occurrences. Different context window sizes are supported in MaskOpt to capture the influence of neighboring shapes from optical proximity effects. We evaluate state-of-the-art deep learning models for IC mask optimization to build up benchmarks, and the evaluation results expose distinct trade-offs across baseline models. Further context size analysis and input ablation studies confirm the importance of both surrounding geometries and cell-aware inputs in achieving accurate mask generation.

</details>


### [86] [Dominating vs. Dominated: Generative Collapse in Diffusion Models](https://arxiv.org/abs/2512.20666)
*Hayeon Jeong,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 本文研究了文本到图像扩散模型在多概念提示生成时出现的主导-从属（DvD）不平衡现象，即某一概念令牌过度主导生成过程，抑制其他概念。为此，作者提出了DominanceBench基准，从数据和架构两个角度系统分析该问题。实验表明，训练数据中实例多样性不足加剧了概念间的干扰；跨注意力动态分析显示，主导令牌会迅速饱和注意力，随扩散步骤逐步压制其他令牌。此外，头消融实验表明DvD行为源于多个注意力头的分布式机制。研究为生成崩溃现象提供了关键洞见，有助于实现更可靠、可控的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在处理多概念提示时存在概念间不平衡问题，即一个概念过度主导生成，导致其他概念被抑制，影响生成质量与可控性，亟需系统性分析与解决。

Method: 提出DominanceBench基准，结合数据层面分析与架构层面的注意力动态研究，通过多组实验评估训练数据多样性、注意力饱和度及多头注意力机制对DvD现象的影响。

Result: 发现训练数据实例多样性不足是加剧概念干扰的关键因素；主导令牌在扩散过程中快速饱和注意力，抑制其他概念；多头注意力机制的分布特性是导致DvD行为的根本原因。

Conclusion: 本研究揭示了文本到图像生成中概念不平衡的深层成因，为改善生成可控性与可靠性提供了理论支持与改进方向。

Abstract: Text-to-image diffusion models have drawn significant attention for their ability to generate diverse and high-fidelity images. However, when generating from multi-concept prompts, one concept token often dominates the generation, suppressing the others-a phenomenon we term the Dominant-vs-Dominated (DvD) imbalance. To systematically analyze this imbalance, we introduce DominanceBench and examine its causes from both data and architectural perspectives. Through various experiments, we show that the limited instance diversity in training data exacerbates the inter-concept interference. Analysis of cross-attention dynamics further reveals that dominant tokens rapidly saturate attention, progressively suppressing others across diffusion timesteps. In addition, head ablation studies show that the DvD behavior arises from distributed attention mechanisms across multiple heads. Our findings provide key insights into generative collapse, advancing toward more reliable and controllable text-to-image generation.

</details>


### [87] [Improving Cardiac Risk Prediction Using Data Generation Techniques](https://arxiv.org/abs/2512.20669)
*Alexandre Cabodevila,Pedro Gamallo-Fernandez,Juan C. Vidal,Manuel Lama*

Main category: cs.LG

TL;DR: 本文提出一种基于条件变分自编码器（CVAE）的架构，用于生成与真实临床观察一致的合成心脏病康复数据，以解决真实医疗数据库中数据稀缺、不完整和缺失值多等问题。该方法可增强心脏风险预测模型的性能，减少对高风险诊断程序（如运动负荷试验）的依赖，并在实验中证明其生成的数据能显著提升分类器准确率，优于现有深度学习合成数据方法。


<details>
  <summary>Details</summary>
Motivation: 真实医疗数据库存在数据稀缺、记录不适用于分析目的以及大量缺失值的问题，限制了心脏康复流程建模与风险预测模型的发展，亟需一种有效方法生成高质量、多样化的合成数据以弥补数据不足。

Method: 采用条件变分自编码器（CVAE）构建合成临床记录的架构，利用真实数据学习患者特征间的复杂依赖关系，生成符合实际分布且逻辑一致的合成数据。

Result: 所提方法成功生成了在统计特性与临床逻辑上均合理的合成数据，使用这些数据训练的风险预测模型在多个分类任务中表现更优，超越了当前最先进的合成数据生成方法。

Conclusion: 基于CVAE的合成数据生成框架为心脏康复领域的数据分析提供了可行解决方案，能够有效提升模型性能并降低对侵入性检查的需求，具有重要的临床应用潜力。

Abstract: Cardiac rehabilitation constitutes a structured clinical process involving multiple interdependent phases, individualized medical decisions, and the coordinated participation of diverse healthcare professionals. This sequential and adaptive nature enables the program to be modeled as a business process, thereby facilitating its analysis. Nevertheless, studies in this context face significant limitations inherent to real-world medical databases: data are often scarce due to both economic costs and the time required for collection; many existing records are not suitable for specific analytical purposes; and, finally, there is a high prevalence of missing values, as not all patients undergo the same diagnostic tests. To address these limitations, this work proposes an architecture based on a Conditional Variational Autoencoder (CVAE) for the synthesis of realistic clinical records that are coherent with real-world observations. The primary objective is to increase the size and diversity of the available datasets in order to enhance the performance of cardiac risk prediction models and to reduce the need for potentially hazardous diagnostic procedures, such as exercise stress testing. The results demonstrate that the proposed architecture is capable of generating coherent and realistic synthetic data, whose use improves the accuracy of the various classifiers employed for cardiac risk detection, outperforming state-of-the-art deep learning approaches for synthetic data generation.

</details>


### [88] [Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection](https://arxiv.org/abs/2512.20670)
*Weilin Zhou,Zonghao Ying,Junjie Mu,Shengwei Tian,Quanchen Zou,Deyue Zhang,Dongdong Yang,Xiangzheng Zhang*

Main category: cs.LG

TL;DR: 本文提出动态冲突-共识框架（DCCF），以解决多模态假新闻检测中一致性融合方法因过度平滑而忽略关键跨模态矛盾的问题。DCCF通过分离事实与情感空间、利用物理启发的特征动态极化表示，并结合冲突-共识机制，主动挖掘并利用模态间不一致作为伪造证据，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 主流的一致性融合方法将跨模态差异误认为噪声并加以抑制，导致关键伪造证据被稀释，因此需要一种能主动识别和利用不一致性的新范式。

Method: DCCF首先将输入解耦至独立的事实与情感空间，区分客观不一致与情绪矛盾；其次采用物理启发的特征动态迭代极化表示，增强信息冲突；最后通过冲突-共识机制在全局上下文中标准化局部差异，实现稳健判断。

Result: 在三个真实世界数据集上的实验表明，DCCF显著优于现有先进方法，平均准确率提升3.52%。

Conclusion: DCCF通过主动挖掘跨模态不一致，有效克服了传统一致性融合中的过平滑问题，为假新闻检测提供了更可靠且更具解释性的解决方案。

Abstract: Prevalent multimodal fake news detection relies on consistency-based fusion, yet this paradigm fundamentally misinterprets critical cross-modal discrepancies as noise, leading to over-smoothing, which dilutes critical evidence of fabrication. Mainstream consistency-based fusion inherently minimizes feature discrepancies to align modalities, yet this approach fundamentally fails because it inadvertently smoothes out the subtle cross-modal contradictions that serve as the primary evidence of fabrication. To address this, we propose the Dynamic Conflict-Consensus Framework (DCCF), an inconsistency-seeking paradigm designed to amplify rather than suppress contradictions. First, DCCF decouples inputs into independent Fact and Sentiment spaces to distinguish objective mismatches from emotional dissonance. Second, we employ physics-inspired feature dynamics to iteratively polarize these representations, actively extracting maximally informative conflicts. Finally, a conflict-consensus mechanism standardizes these local discrepancies against the global context for robust deliberative judgment.Extensive experiments conducted on three real world datasets demonstrate that DCCF consistently outperforms state-of-the-art baselines, achieving an average accuracy improvement of 3.52\%.

</details>


### [89] [HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model](https://arxiv.org/abs/2512.20674)
*Yuanhao Xi,Xiaohuan Bing,Ramin Yahyapour*

Main category: cs.LG

TL;DR: HyDRA is a parameter-efficient fine-tuning framework for mobile Vision-Language Models (VLMs) that uses hierarchical and dynamic rank scheduling via Low-Rank Adaptation (LoRA). It introduces coarse- and fine-grained rank assignment across layers and dynamically adjusts ranks using a lightweight performance model during training. Experiments show HyDRA achieves up to 4.7% improvement over baselines without increasing trainable parameters, outperforming full-parameter fine-tuning in some tasks.


<details>
  <summary>Details</summary>
Motivation: Mobile-oriented Vision Language Models require efficient training due to high computational costs. Standard LoRA with fixed rank is insufficient for effectively handling both text and image modalities in mobile VLMs.

Method: HyDRA employs hierarchical optimization—assigning different ranks across layers (coarse-grained) and within layers (fine-grained)—and dynamic rank adjustment through an end-to-end lightweight performance model that automatically determines optimal ranks during fine-tuning.

Result: HyDRA consistently improves performance across various model sizes, achieving a 4.7% gain over baseline methods without increasing trainable parameters; in certain tasks, it even surpasses full-parameter fine-tuning.

Conclusion: HyDRA effectively enables efficient and high-performing fine-tuning of mobile VLMs by combining hierarchical and dynamic rank scheduling, making it a promising solution for resource-constrained environments.

Abstract: Vision Language Models (VLMs) have undergone significant advancements, particularly with the emergence of mobile-oriented VLMs, which offer a wide range of application scenarios. However, the substantial computational requirements for training these models present a significant obstacle to their practical application. To address this issue, Low-Rank Adaptation (LoRA) has been proposed. Nevertheless, the standard LoRA with a fixed rank lacks sufficient capability for training mobile VLMs that process both text and image modalities. In this work, we introduce HyDRA, a parameter-efficient fine-tuning framework designed to implement hierarchical and dynamic rank scheduling for mobile VLMs. This framework incorporates two essential optimization strategies: (1) hierarchical optimization, which involves a coarse-grained approach that assigns different ranks to various layers, as well as a fine-grained method that adjusts ranks within individual layers, and (2) dynamic adjustment, which employs an end-to-end automatic optimization using a lightweight performance model to determine and adjust ranks during the fine-tuning process. Comprehensive experiments conducted on popular benchmarks demonstrate that HyDRA consistently outperforms the baseline, achieving a 4.7\% improvement across various model sizes without increasing the number of trainable parameters. In some tasks, it even surpasses full-parameter fine-tuning.

</details>


### [90] [PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation](https://arxiv.org/abs/2512.20687)
*Yuma Ichikawa,Naoya Takagi,Takumi Nakagawa,Yuzi Kanazawa,Akira Sakai*

Main category: cs.LG

TL;DR: PHOTON是一种分层自回归模型，通过垂直、多分辨率的上下文访问替代传统的水平令牌扫描，显著降低解码时的KV缓存流量，实现高达1000倍的单位内存吞吐量提升，尤其在长上下文和多查询任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在生成过程中逐令牌扫描，导致预填充延迟增加且解码阶段内存成为瓶颈，主要由于KV缓存的读写操作主导了推理吞吐量。

Method: 提出PHOTON模型，采用分层结构：底部是自下而上的编码器，逐步将令牌压缩为低速率上下文状态；顶部是轻量级自上而下的解码器，重建细粒度的令牌表示，实现垂直的多分辨率上下文访问。

Result: 实验表明，PHOTON在吞吐量与质量权衡方面优于基于Transformer的语言模型，在长上下文和多查询任务中具有显著优势，解码时的KV缓存流量减少，单位内存吞吐量最高提升1000倍。

Conclusion: PHOTON通过引入分层、垂直的上下文访问机制，有效缓解了传统Transformer在长序列生成中的内存瓶颈问题，为高效长文本建模提供了新范式。

Abstract: Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\times$ higher throughput per unit memory.

</details>


### [91] [FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs](https://arxiv.org/abs/2512.20732)
*Saeed Mohammadzadeh,Erfan Hamdi,Joel Shor,Emma Lejeune*

Main category: cs.LG

TL;DR: FEM-Bench 是一个针对大语言模型（LLM）在计算力学领域生成有限元方法（FEM）相关代码能力的基准测试，涵盖研究生入门级别任务。尽管任务相对简单，但当前最先进模型仍难以稳定完成全部任务，表明现有AI在科学建模与代码生成方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在物理世界推理能力上快速发展，但缺乏严格评估其构建科学有效物理模型能力的基准，尤其在计算力学领域。该领域具备清晰数学结构、强物理与数值约束，适合用于系统性评估AI的科学推理能力。

Method: 提出 FEM-Bench 2025 基准，包含一系列基于计算力学第一门研究生课程内容的任务，涵盖建模、几何分析、材料行为和数值实现等关键环节，通过函数编写与单元测试正确率评估模型表现。

Result: 在五次尝试中，Gemini 3 Pro 在函数编写任务中完成 30/33 次，其中 26 次全对；GPT-5 在单元测试任务中达到平均联合成功率 73.8%。其他模型表现差异较大，显示当前模型在科学代码生成方面尚不稳健。

Conclusion: FEM-Bench 为评估 AI 生成科学代码提供了结构化基础，未来版本将引入更复杂任务以持续追踪模型演进。

Abstract: As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.

</details>


### [92] [Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies](https://arxiv.org/abs/2512.20749)
*Diyar Altinses,Andreas Schwung*

Main category: cs.LG

TL;DR: 本文分析了多模态自编码器中的Lipschitz性质，通过理论推导和实证验证，提出了一种基于正则化注意力的融合方法，显著提升了模型训练的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: 多模态自编码器在处理复杂多模态数据方面具有潜力，但其训练稳定性与鲁棒性仍需提升，因此需要深入理解其融合机制的数学特性，以优化模型设计与应用效果。

Method: 推导多模态自编码器中聚合方法的理论Lipschitz常数，并基于此设计一种正则化注意力融合机制；通过多次实验估计实际Lipschitz常数，验证理论结果并比较不同融合策略的表现。

Result: 所提出的融合方法在一致性、收敛速度和准确性上均优于现有方法，且与理论预测高度一致，证明了理论分析的有效性。

Conclusion: 本研究为多模态自编码器中的融合机制提供了坚实的理论基础，并提出了一种有效提升模型训练稳定性和性能的解决方案。

Abstract: In recent years, the development of multimodal autoencoders has gained significant attention due to their potential to handle multimodal complex data types and improve model performance. Understanding the stability and robustness of these models is crucial for optimizing their training, architecture, and real-world applicability. This paper presents an analysis of Lipschitz properties in multimodal autoencoders, combining both theoretical insights and empirical validation to enhance the training stability of these models. We begin by deriving the theoretical Lipschitz constants for aggregation methods within the multimodal autoencoder framework. We then introduce a regularized attention-based fusion method, developed based on our theoretical analysis, which demonstrates improved stability and performance during training. Through a series of experiments, we empirically validate our theoretical findings by estimating the Lipschitz constants across multiple trials and fusion strategies. Our results demonstrate that our proposed fusion function not only aligns with theoretical predictions but also outperforms existing strategies in terms of consistency, convergence speed, and accuracy. This work provides a solid theoretical foundation for understanding fusion in multimodal autoencoders and contributes a solution for enhancing their performance.

</details>


### [93] [Generalization of RLVR Using Causal Reasoning as a Testbed](https://arxiv.org/abs/2512.20760)
*Brian Lu,Hongyu Zhao,Shuo Sun,Hao Peng,Rui Ding,Hongyuan Mei*

Main category: cs.LG

TL;DR: 该研究探讨了基于可验证奖励的强化学习（RLVR）在大语言模型（LLM）因果推理任务中的泛化能力，发现其效果依赖于模型规模和训练查询级别的组合，并且只有在模型具备足够初始推理能力时，RLVR才能显著提升复杂查询的准确率。


<details>
  <summary>Details</summary>
Motivation: 理解强化学习与可验证奖励（RLVR）在大语言模型中实现稳健泛化的能力条件，特别是在因果推理任务中的表现。

Method: 构建包含不同难度层级（关联、干预、反事实）和结构复杂度（子图大小）的因果图与查询数据集，对Qwen-2.5-Instruct模型进行RLVR或监督微调（SFT），并测试不同模型规模（3B-32B）和训练查询水平的影响。

Result: RLVR在同级及跨级泛化上优于SFT，但仅在特定模型规模与训练查询水平组合下有效；进一步分析表明，其有效性取决于模型的初始推理能力，当初始能力足够时，能改善边缘化策略并减少中间概率计算错误，尤其在复杂查询上提升显著。

Conclusion: RLVR能够提升大语言模型在因果推理中的特定子技能，但其优势仅在模型具备充分初始推理能力的前提下显现，表明其成功应用需匹配模型基础能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising paradigm for post-training large language models (LLMs) on complex reasoning tasks. Yet, the conditions under which RLVR yields robust generalization remain poorly understood. This paper provides an empirical study of RLVR generalization in the setting of probabilistic inference over causal graphical models. This setting offers two natural axes along which to examine generalization: (i) the level of the probabilistic query -- associational, interventional, or counterfactual -- and (ii) the structural complexity of the query, measured by the size of its relevant subgraph. We construct datasets of causal graphs and queries spanning these difficulty axes and fine-tune Qwen-2.5-Instruct models using RLVR or supervised fine-tuning (SFT). We vary both the model scale (3B-32B) and the query level included in training. We find that RLVR yields stronger within-level and across-level generalization than SFT, but only for specific combinations of model size and training query level. Further analysis shows that RLVR's effectiveness depends on the model's initial reasoning competence. With sufficient initial competence, RLVR improves an LLM's marginalization strategy and reduces errors in intermediate probability calculations, producing substantial accuracy gains, particularly on more complex queries. These findings show that RLVR can improve specific causal reasoning subskills, with its benefits emerging only when the model has sufficient initial competence.

</details>


### [94] [TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform](https://arxiv.org/abs/2512.20761)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Henrik Albers,Oliver Müller*

Main category: cs.LG

TL;DR: TS-Arena 是一个旨在解决时间序列基础模型（TSFMs）评估危机的平台，通过在实时数据流上实施预注册机制，确保评估目标在推理时仍为物理上未知，从而实现严格的全局时间分割，防止历史数据污染，提供对模型泛化能力的真实评估。该平台已在能源领域试点，支持在真实世界约束下可持续比较基础模型。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型的评估存在信息泄露问题，由于训练和测试集重叠以及全球模式非法转移到测试数据中，导致基准测试失去独立性，尤其在历史数据中利用已知全球冲击会破坏评估有效性。

Method: 提出TS-Arena平台，采用预注册机制对实时数据流进行处理，将真正未知的未来作为测试环境，建立动态的时间边界，确保评估目标在推理阶段尚未发生，从而实现严格的时间隔离与非污染评估。

Result: TS-Arena成功构建了一个无历史污染、符合真实世界约束的评估环境，可有效验证时间序列基础模型的泛化能力；已在能源领域完成原型部署，并开放于Hugging Face空间。

Conclusion: TS-Arena通过强制执行真实的未来作为测试条件，恢复了时间序列模型评估的科学性与可信度，为未来模型比较提供了可持续、合规的基础设施。

Abstract: While Time Series Foundation Models (TSFMs) offer transformative capabilities for forecasting, they simultaneously risk triggering a fundamental evaluation crisis. This crisis is driven by information leakage due to overlapping training and test sets across different models, as well as the illegitimate transfer of global patterns to test data. While the ability to learn shared temporal dynamics represents a primary strength of these models, their evaluation on historical archives often permits the exploitation of observed global shocks, which violates the independence required for valid benchmarking. We introduce TS-Arena, a platform that restores the operational integrity of forecasting by treating the genuinely unknown future as the definitive test environment. By implementing a pre-registration mechanism on live data streams, the platform ensures that evaluation targets remain physically non-existent during inference, thereby enforcing a strict global temporal split. This methodology establishes a moving temporal frontier that prevents historical contamination and provides an authentic assessment of model generalization. Initially applied within the energy sector, TS-Arena provides a sustainable infrastructure for comparing foundation models under real-world constraints. A prototype of the platform is available at https://huggingface.co/spaces/DAG-UPB/TS-Arena.

</details>


### [95] [Subgroup Discovery with the Cox Model](https://arxiv.org/abs/2512.20762)
*Zachary Izzo,Iain Melvin*

Main category: cs.LG

TL;DR: 本文研究生存分析中的子群发现问题，旨在找到一个可解释的数据子集，使得Cox模型在此子集上具有高准确性。这是首个针对Cox模型子群发现的系统性工作。作者指出现有质量函数不适用于该问题，并提出两个新工具：预期预测熵（EPE）用于评估预测风险函数的生存模型，以及条件秩统计量（CRS）用于量化个体点与子群生存时间分布的偏离程度。理论分析表明，二者能有效解决现有度量的缺陷。研究提出了八种算法，其中主算法结合EPE与CRS，具备理论正确性保证。实验在合成与真实数据上验证了方法的有效性，显示其能恢复真实子群并提升模型拟合效果。案例研究基于NASA的喷气发动机仿真数据，发现的子群揭示了数据中的非线性与同质性特征，且与实际设计选择一致。


<details>
  <summary>Details</summary>
Motivation: 现有子群发现方法的质量函数不适用于生存分析中的Cox模型，导致难以准确识别具有高预测性能的子群。因此需要新的度量标准来克服这一局限。

Method: 提出两种新度量：预期预测熵（EPE）和条件秩统计量（CRS），并基于此设计八种算法，主算法融合两者以实现理论正确性。

Result: 在合成与真实数据上，所提方法能有效恢复真实子群，显著提升模型拟合效果；在NASA喷气发动机数据上，发现的子群揭示了实际中已知的非线性与同质性特征，支持工程设计决策。

Conclusion: 本研究首次系统解决了Cox模型下的子群发现问题，通过引入EPE和CRS，实现了更准确、可解释的子群识别，为生存分析中的个性化建模提供了有力工具。

Abstract: We study the problem of subgroup discovery for survival analysis, where the goal is to find an interpretable subset of the data on which a Cox model is highly accurate. Our work is the first to study this particular subgroup problem, for which we make several contributions.
  Subgroup discovery methods generally require a "quality function" in order to sift through and select the most advantageous subgroups. We first examine why existing natural choices for quality functions are insufficient to solve the subgroup discovery problem for the Cox model. To address the shortcomings of existing metrics, we introduce two technical innovations: the *expected prediction entropy (EPE)*, a novel metric for evaluating survival models which predict a hazard function; and the *conditional rank statistics (CRS)*, a statistical object which quantifies the deviation of an individual point to the distribution of survival times in an existing subgroup. We study the EPE and CRS theoretically and show that they can solve many of the problems with existing metrics.
  We introduce a total of eight algorithms for the Cox subgroup discovery problem. The main algorithm is able to take advantage of both the EPE and the CRS, allowing us to give theoretical correctness results for this algorithm in a well-specified setting. We evaluate all of the proposed methods empirically on both synthetic and real data. The experiments confirm our theory, showing that our contributions allow for the recovery of a ground-truth subgroup in well-specified cases, as well as leading to better model fit compared to naively fitting the Cox model to the whole dataset in practical settings. Lastly, we conduct a case study on jet engine simulation data from NASA. The discovered subgroups uncover known nonlinearities/homogeneity in the data, and which suggest design choices which have been mirrored in practice.

</details>


### [96] [Improving Matrix Exponential for Generative AI Flows: A Taylor-Based Approach Beyond Paterson--Stockmeyer](https://arxiv.org/abs/2512.20777)
*Jorge Sastre,Daniel Faronbi,José Miguel Alonso,Peter Traver,Javier Ibáñez,Nuria Lloret*

Main category: cs.LG

TL;DR: 本文提出了一种针对生成式AI流水线高吞吐需求优化的泰勒基矩阵指数算法，通过动态选择泰勒阶数和缩放因子，在保证精度的前提下显著降低计算复杂度。相比现有最先进的实现，该方法在数值稳定性与计算效率方面均有显著提升，适用于大规模生成建模任务。


<details>
  <summary>Details</summary>
Motivation: 传统基于Padé逼近与缩放平方的方法虽广泛使用，但近年来基于泰勒展开的新方法因采用超越经典Paterson-Stockmeyer技术的多项式求值方案，展现出更高的精度和更低的计算复杂度。为满足生成式人工智能对高吞吐量的需求，亟需更高效、稳定的矩阵指数计算方法。

Method: 提出一种优化的泰勒基矩阵指数算法，结合严格的误差分析，设计动态选择策略以自适应确定泰勒阶数与缩放因子，从而在给定误差容限下最小化计算开销。

Result: 大量数值实验表明，所提方法在保持高数值稳定性的同时，相较于现有最优实现，实现了显著的加速效果，验证了其在大规模生成建模中的高效性与实用性。

Conclusion: 该方法为大规模生成式建模提供了一个高效且稳定的矩阵指数计算工具，具有重要的应用前景。

Abstract: The matrix exponential is a fundamental operator in scientific computing and system simulation, with applications ranging from control theory and quantum mechanics to modern generative machine learning. While Padé approximants combined with scaling and squaring have long served as the standard, recent Taylor-based methods, which utilize polynomial evaluation schemes that surpass the classical Paterson--Stockmeyer technique, offer superior accuracy and reduced computational complexity. This paper presents an optimized Taylor-based algorithm for the matrix exponential, specifically designed for the high-throughput requirements of generative AI flows. We provide a rigorous error analysis and develop a dynamic selection strategy for the Taylor order and scaling factor to minimize computational effort under a prescribed error tolerance. Extensive numerical experiments demonstrate that our approach provides significant acceleration and maintains high numerical stability compared to existing state-of-the-art implementations. These results establish the proposed method as a highly efficient tool for large-scale generative modeling.

</details>


### [97] [FedMPDD: Communication-Efficient Federated Learning with Privacy Preservation Attributes via Projected Directional Derivative](https://arxiv.org/abs/2512.20814)
*Mohammadreza Rostami,Solmaz S. Kia*

Main category: cs.LG

TL;DR: FedMPDD 是一种新型联邦学习算法，通过多方向导数投影压缩客户端梯度，显著降低上行通信开销（从 $\mathcal{O}(d)$ 降至 $\mathcal{O}(m)$，$m \ll d$），同时提升隐私性。通过在多个随机方向上计算梯度的投影并平均，克服了单投影的维度依赖收敛限制。理论分析表明其收敛速度为 $\mathcal{O}(1/\sqrt{K})$，与 FedSGD 相当；且由于低秩投影的几何特性，具备内在隐私保护能力，支持可调的隐私-效用权衡。实验验证了理论和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中高维梯度传输带来的通信开销大、隐私泄露风险高的问题，尤其是在带宽受限和隐私敏感场景下，亟需高效且安全的梯度压缩方法。

Method: 采用多方向导数投影技术：客户端在多个随机向量方向上计算梯度的导向导数，将高维梯度压缩为低维消息；服务器对聚合后的投影信息进行逆投影重构，并通过平均多个投影来恢复有效梯度信息。

Result: 理论证明了 FedMPDD 具有 $\mathcal{O}(1/\sqrt{K})$ 的收敛率，与 FedSGD 相当；实验表明其在多个基准数据集上实现高效通信压缩与良好模型性能；同时具备抵抗梯度反演攻击的内在隐私保护能力，可通过调整投影数量控制隐私-效用平衡。

Conclusion: FedMPDD 成功实现了通信效率与隐私保护的双重优化，是一种兼具理论严谨性与实际可行性的联邦学习压缩方案，适用于资源受限和隐私敏感的应用场景。

Abstract: This paper introduces \texttt{FedMPDD} (\textbf{Fed}erated Learning via \textbf{M}ulti-\textbf{P}rojected \textbf{D}irectional \textbf{D}erivatives), a novel algorithm that simultaneously optimizes bandwidth utilization and enhances privacy in Federated Learning. The core idea of \texttt{FedMPDD} is to encode each client's high-dimensional gradient by computing its directional derivatives along multiple random vectors. This compresses the gradient into a much smaller message, significantly reducing uplink communication costs from $\mathcal{O}(d)$ to $\mathcal{O}(m)$, where $m \ll d$. The server then decodes the aggregated information by projecting it back onto the same random vectors. Our key insight is that averaging multiple projections overcomes the dimension-dependent convergence limitations of a single projection. We provide a rigorous theoretical analysis, establishing that \texttt{FedMPDD} converges at a rate of $\mathcal{O}(1/\sqrt{K})$, matching the performance of FedSGD. Furthermore, we demonstrate that our method provides some inherent privacy against gradient inversion attacks due to the geometric properties of low-rank projections, offering a tunable privacy-utility trade-off controlled by the number of projections. Extensive experiments on benchmark datasets validate our theory and demonstrates our results.

</details>


### [98] [Defending against adversarial attacks using mixture of experts](https://arxiv.org/abs/2512.20821)
*Mohammad Meymani,Roozbeh Razavi-Far*

Main category: cs.LG

TL;DR: 本文提出一种基于混合专家架构的防御系统，通过在模型中引入对抗训练模块来增强对各种对抗威胁的鲁棒性。该系统采用九个以ResNet-18为骨干的预训练专家，在端到端训练过程中联合优化专家参数与门控机制，从而提升整体性能。所提方法在保持轻量架构的前提下，优于现有最先进的防御系统和复杂度更高的普通分类器。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在面对对抗性攻击时表现出脆弱性，包括生成难以察觉但能导致误分类的对抗扰动、污染训练数据以降低模型性能，以及通过查询泄露敏感信息等威胁。因此亟需设计更鲁棒的防御机制。

Method: 提出一种基于混合专家（Mixture-of-Experts）架构的防御系统，集成对抗训练模块；使用九个以ResNet-18为骨干的预训练专家，并在端到端训练中联合优化专家参数与门控机制，实现对对抗攻击的自适应防御。

Result: 所提出的防御系统在多个评估指标上显著优于现有的先进防御方法和复杂度更高的基准分类器，证明了其在对抗鲁棒性方面的有效性，同时保持了较低的模型复杂度。

Conclusion: 本研究验证了将对抗训练与混合专家架构相结合的有效性，为构建高效且鲁棒的机器学习防御系统提供了新思路，尤其适用于资源受限场景下的实际部署。

Abstract: Machine learning is a powerful tool enabling full automation of a huge number of tasks without explicit programming. Despite recent progress of machine learning in different domains, these models have shown vulnerabilities when they are exposed to adversarial threats. Adversarial threats aim to hinder the machine learning models from satisfying their objectives. They can create adversarial perturbations, which are imperceptible to humans' eyes but have the ability to cause misclassification during inference. Moreover, they can poison the training data to harm the model's performance or they can query the model to steal its sensitive information. In this paper, we propose a defense system, which devises an adversarial training module within mixture-of-experts architecture to enhance its robustness against adversarial threats. In our proposed defense system, we use nine pre-trained experts with ResNet-18 as their backbone. During end-to-end training, the parameters of expert models and gating mechanism are jointly updated allowing further optimization of the experts. Our proposed defense system outperforms state-of-the-art defense systems and plain classifiers, which use a more complex architecture than our model's backbone.

</details>


### [99] [Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs](https://arxiv.org/abs/2512.20861)
*Pierre Abillama,Changwoo Lee,Juechu Dong,David Blaauw,Dennis Sylvester,Hun-Seok Kim*

Main category: cs.LG

TL;DR: 本文提出针对Transformer模型中块低秩（BLR）压缩技术在多标记推理时内存瓶颈的问题，通过自定义Triton内核实现部分融合与内存布局优化，显著提升性能。在内存受限的NVIDIA GPU上，相比PyTorch密集基线，实现了最高3.76倍加速和3倍模型压缩，支持多种主流模型。


<details>
  <summary>Details</summary>
Motivation: Transformer基础模型规模不断增长，导致单个GPU难以容纳全模型，计算成本过高；尽管BLR方法能有效压缩模型并保持精度，但在多标记推理中仍面临内存瓶颈，影响实际性能。

Method: 采用屋顶分析（roofline analysis）识别性能瓶颈，设计自定义Triton内核，引入部分融合与内存布局优化，以缓解多标记推理中的内存限制问题。

Result: 在Jetson Orin Nano和A40等内存受限GPU上，相比PyTorch密集基线，实现了最高3.76倍的速度提升和3倍的模型压缩，同时支持Llama-7/1B、GPT2-S、DiT-XL/2和ViT-B等多种模型。

Conclusion: 通过定制化Triton内核优化，可有效克服BLR方法在多标记推理中的内存瓶颈，显著提升效率与可扩展性，为轻量级部署提供可行方案。

Abstract: Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\times$ speedups and $3\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .

</details>


### [100] [Towards a General Framework for Predicting and Explaining the Hardness of Graph-based Combinatorial Optimization Problems using Machine Learning and Association Rule Mining](https://arxiv.org/abs/2512.20915)
*Bharat Sharman,Elkafi Hassini*

Main category: cs.LG

TL;DR: GCO-HPIF 是一种基于机器学习的通用框架，用于预测和解释可表示为图的组合优化问题的计算难度。该框架分为两个阶段：第一阶段通过图特征与难度分类构建数据集，并训练分类模型以映射图特征到难度类别；第二阶段利用关联规则挖掘算法解释预测结果，并训练回归模型预测算法计算时间。在包含3287个最大团问题实例的数据集上，框架表现出色，预测硬度的加权F1得分为0.9921，少数类F1得分为0.878，ROC-AUC为0.9083，仅使用三个图特征即达到高精度。最佳关联规则支持率为0.8829，准确率达87.64%，表明其在预测与解释方面均具实用性。回归模型预测计算时间的百分比RMSE为5.12，R²值达0.991，表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效预测组合优化问题实例的计算难度，且缺乏对难度成因的可解释性。本研究旨在构建一个通用、高效且可解释的机器学习框架，以自动识别和解释图结构优化问题的计算复杂性，提升求解效率与算法选择能力。

Method: 提出GCO-HPIF框架，包含两阶段：(1) 构建问题无关的图特征与硬度标签数据集，采用机器学习分类模型（如随机森林、SVM等）进行硬度预测；(2) 使用FP-Growth算法挖掘关联规则以解释预测结果，并训练回归模型（如XGBoost、Lasso）预测算法运行时间。所有实验基于最大团问题实例，涵盖五种先进算法。

Result: 框架在硬度预测任务中表现卓越：加权F1得分为0.9921，少数类F1为0.878，ROC-AUC达0.9083，仅用三个图特征即可实现高精度。关联规则挖掘得到的规则支持率0.8829，整体准确率87.64%，具备强解释力。回归模型预测计算时间的百分比RMSE为5.12，R²值高达0.991，说明预测极为精准。

Conclusion: GCO-HPIF框架成功实现了对图基组合优化问题计算难度的高精度预测与可解释性分析，具有良好的泛化能力与实用价值，为智能算法调度与问题预处理提供了强有力支持。

Abstract: This study introduces GCO-HPIF, a general machine-learning-based framework to predict and explain the computational hardness of combinatorial optimization problems that can be represented on graphs. The framework consists of two stages. In the first stage, a dataset is created comprising problem-agnostic graph features and hardness classifications of problem instances. Machine-learning-based classification algorithms are trained to map graph features to hardness categories. In the second stage, the framework explains the predictions using an association rule mining algorithm. Additionally, machine-learning-based regression models are trained to predict algorithmic computation times. The GCO-HPIF framework was applied to a dataset of 3287 maximum clique problem instances compiled from the COLLAB, IMDB, and TWITTER graph datasets using five state-of-the-art algorithms, namely three exact branch-and-bound-based algorithms (Gurobi, CliSAT, and MOMC) and two graph-neural-network-based algorithms (EGN and HGS). The framework demonstrated excellent performance in predicting instance hardness, achieving a weighted F1 score of 0.9921, a minority-class F1 score of 0.878, and an ROC-AUC score of 0.9083 using only three graph features. The best association rule found by the FP-Growth algorithm for explaining the hardness predictions had a support of 0.8829 for hard instances and an overall accuracy of 87.64 percent, underscoring the framework's usefulness for both prediction and explanation. Furthermore, the best-performing regression model for predicting computation times achieved a percentage RMSE of 5.12 and an R2 value of 0.991.

</details>


### [101] [RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks](https://arxiv.org/abs/2512.20920)
*Ningyuan Liu,Jing Yang,Kaitong Cai,Keze Wang*

Main category: cs.LG

TL;DR: RevFFN是一种针对混合专家（MoE）大语言模型的内存高效微调范式，通过设计可逆Transformer模块，在反向传播时从输出重构输入激活，从而避免存储大量中间激活，显著降低峰值内存消耗，实现单个消费级或服务器级GPU上的高效全参数微调。


<details>
  <summary>Details</summary>
Motivation: 全参数微调在大规模语言模型中面临巨大的内存开销，主要源于反向传播过程中需要缓存大量中间激活，现有分布式训练框架如DeepSpeed虽能缓解该问题，但依赖额外硬件资源且降低训练速度。因此，亟需一种更高效的内存优化方法以支持在有限硬件条件下进行全参数微调。

Method: 提出RevFFN，采用精心设计的可逆Transformer块，使层输入激活可在反向传播阶段由输出重建，无需显式存储大部分中间激活，从而大幅减少内存占用。该方法保留了MoE架构的表达能力，适用于全参数微调场景。

Result: RevFFN显著降低了全参数微调过程中的峰值内存消耗，使得在单个消费级或服务器级GPU上即可完成大规模模型的全参数微调，提升了训练效率与硬件适应性。

Conclusion: RevFFN为大规模MoE模型的全参数微调提供了一种高效、低内存需求的新方案，突破了现有技术对多GPU或高性能硬件的依赖，具有重要的实践价值。

Abstract: Full parameter fine tuning is a key technique for adapting large language models (LLMs) to downstream tasks, but it incurs substantial memory overhead due to the need to cache extensive intermediate activations for backpropagation. This bottleneck makes full fine tuning of contemporary large scale LLMs challenging in practice. Existing distributed training frameworks such as DeepSpeed alleviate this issue using techniques like ZeRO and FSDP, which rely on multi GPU memory or CPU offloading, but often require additional hardware resources and reduce training speed. We introduce RevFFN, a memory efficient fine tuning paradigm for mixture of experts (MoE) LLMs. RevFFN employs carefully designed reversible Transformer blocks that allow reconstruction of layer input activations from outputs during backpropagation, eliminating the need to store most intermediate activations in memory. While preserving the expressive capacity of MoE architectures, this approach significantly reduces peak memory consumption for full parameter fine tuning. As a result, RevFFN enables efficient full fine tuning on a single consumer grade or server grade GPU.

</details>


### [102] [Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy](https://arxiv.org/abs/2512.20932)
*Deepit Sapru*

Main category: cs.LG

TL;DR: 本文提出了一种营销分析框架，将订阅定价转化为动态、受控的决策系统，结合多变量需求预测、分段价格弹性与流失倾向，以优化收入、利润和留存率。该方法融合季节性时间序列模型与树基学习器，通过蒙特卡洛情景测试评估风险范围，并在业务约束条件下进行优化，确保客户体验、利润率底线和可接受流失率。在多样化的SaaS组合中验证，该方法优于静态定价和统一提价策略，能将价格调整精准分配给高支付意愿群体，同时保护价格敏感用户。系统支持实时模块化API重校准，并具备模型可解释性以满足治理与合规要求。管理上，该框架可作为战略手册，指导何时从固定定价转向动态定价，如何对齐定价与客户生命周期价值（CLV）和月度经常性收入（MRR）目标，以及如何嵌入伦理约束，实现可持续增长而不损害客户信任。


<details>
  <summary>Details</summary>
Motivation: 现有订阅定价策略多为静态或统一调整，难以适应不同客户群体的支付意愿差异，导致收入潜力未被充分挖掘，且可能引发价格敏感用户的流失。因此亟需一种动态、可控制、数据驱动的定价机制，在提升收益的同时保障客户体验与长期信任。

Method: 结合季节性时间序列模型与树基机器学习算法进行多变量需求预测；利用分段价格弹性与流失倾向建模识别客户响应特征；通过蒙特卡洛模拟生成多种定价情景下的风险分布；构建带约束的优化模型，纳入客户体验、最低利润率和允许流失率等业务守卫条件；系统支持实时更新与模块化接口，提供模型可解释性以满足合规要求。

Result: 在多个异构SaaS产品组合中验证，该框架显著优于传统静态定价和统一提价策略，实现了更高的收入增长与利润率，同时有效控制了客户流失率。系统能够精准识别高支付意愿客户并合理调整其价格，同时保护价格敏感群体，整体提升了客户留存与企业可持续盈利能力。

Conclusion: 本研究提出的动态、受控订阅定价框架，不仅提升了财务绩效，还增强了企业的战略灵活性与合规能力。通过整合数据驱动的预测、风险评估与业务守卫机制，该系统为SaaS企业提供了可持续增长的定价策略工具，兼顾商业目标与客户信任，具有广泛的应用前景。

Abstract: This paper presents a marketing analytics framework that operationalizes subscription pricing as a dynamic, guardrailed decision system, uniting multivariate demand forecasting, segment-level price elasticity, and churn propensity to optimize revenue, margin, and retention. The approach blends seasonal time-series models with tree-based learners, runs Monte Carlo scenario tests to map risk envelopes, and solves a constrained optimization that enforces business guardrails on customer experience, margin floors, and allowable churn. Validated across heterogeneous SaaS portfolios, the method consistently outperforms static tiers and uniform uplifts by reallocating price moves toward segments with higher willingness-to-pay while protecting price-sensitive cohorts. The system is designed for real-time recalibration via modular APIs and includes model explainability for governance and compliance. Managerially, the framework functions as a strategy playbook that clarifies when to shift from flat to dynamic pricing, how to align pricing with CLV and MRR targets, and how to embed ethical guardrails, enabling durable growth without eroding customer trust.

</details>


### [103] [Solving Functional PDEs with Gaussian Processes and Applications to Functional Renormalization Group Equations](https://arxiv.org/abs/2512.20956)
*Xianjin Yang,Matthieu Darcy,Matthew Hudes,Francis J. Alexander,Gregory Eyink,Houman Owhadi*

Main category: cs.LG

TL;DR: 本文提出了一种用于求解非微扰功能重整化群方程的算子学习框架，该方法基于高斯过程算子学习，在函数空间上构建灵活的功能表示，不依赖于特定方程或离散化。该方法具有广泛适用性，并可融入物理先验信息，已在Wetterich和Wilson--Polchinski方程等场景中验证，性能优于或等同于现有近似方法（如局部势近似），且能处理非恒定场，适用于更复杂的场构型（如瞬子）研究。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解非微扰功能重整化群方程时受限于特定方程形式与离散化方式，难以处理复杂场配置；需要一种更灵活、通用且能融合物理先验的计算框架。

Method: 采用高斯过程算子学习构建函数空间上的灵活功能表示，直接在函数空间中建模，支持物理先验嵌入（通过先验均值或核函数设计），并适用于多种功能微分方程。

Result: 在Wetterich和Wilson--Polchinski方程上表现优异，性能不低于甚至优于局部势近似，同时具备处理非恒定场的能力，为研究瞬子等复杂场结构提供了新工具。

Conclusion: 所提出的算子学习框架为求解复杂功能微分方程提供了一种通用、灵活且可融入物理知识的新范式，尤其适用于非恒定场和复杂场构型的研究。

Abstract: We present an operator learning framework for solving non-perturbative functional renormalization group equations, which are integro-differential equations defined on functionals. Our proposed approach uses Gaussian process operator learning to construct a flexible functional representation formulated directly on function space, making it independent of a particular equation or discretization. Our method is flexible, and can apply to a broad range of functional differential equations while still allowing for the incorporation of physical priors in either the prior mean or the kernel design. We demonstrate the performance of our method on several relevant equations, such as the Wetterich and Wilson--Polchinski equations, showing that it achieves equal or better performance than existing approximations such as the local-potential approximation, while being significantly more flexible. In particular, our method can handle non-constant fields, making it promising for the study of more complex field configurations, such as instantons.

</details>


### [104] [Measuring all the noises of LLM Evals](https://arxiv.org/abs/2512.21326)
*Sida Wang*

Main category: cs.LG

TL;DR: 该研究针对大语言模型评估中的信号与噪声分离问题，定义并量化了三种噪声类型：预测噪声、数据噪声及其总噪声，并提出全对配对方法以提升统计功效。实验表明，评估任务具有可预测的总噪声水平，且预测噪声通常大于数据噪声，因此通过平均降低预测噪声可显著提升统计效能。研究为评估模型差异提供了高效、可复用的分析框架。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型评估中，准确区分信号与噪声至关重要。传统统计方法在应用时需考虑大模型评估特有的噪声特性，现有方法缺乏系统性噪声建模和高效统计分析手段，限制了小效应检测的能力。

Method: 提出全对配对方法，对所有模型对进行配对分析，基于百万级问题级别的预测结果，在多种评估设置下测量三类噪声（预测噪声、数据噪声、总噪声），并依据总方差定律进行分解与量化。

Result: 发现每项评估任务具有稳定且可预测的总噪声水平；预测噪声普遍高于数据噪声，表明通过多次采样平均可有效降低噪声，显著提升统计功效；该方法使研究者无需定制测试即可评估显著性，并能检测更微小的效果差异。

Conclusion: 本研究通过系统量化大模型评估中的噪声结构，提出了高效的全对配对分析方法，为提升评估的统计可靠性与敏感性提供了坚实基础，支持更精确、可重复的模型比较。

Abstract: Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.

</details>


### [105] [Generalization of Diffusion Models Arises with a Balanced Representation Space](https://arxiv.org/abs/2512.20963)
*Zekai Zhang,Xiao Li,Xiang Li,Lianghe Shi,Meng Wu,Molei Tao,Qing Qu*

Main category: cs.LG

TL;DR: 该论文研究扩散模型中的记忆与泛化问题，通过两层ReLU去噪自编码器分析发现：记忆表现为模型将训练样本以局部尖峰形式存储在权重中，而泛化则体现为捕捉数据局部统计特性并生成平衡表示。理论分析在真实扩散模型中得到验证，并据此提出基于表示的记忆检测方法和无需训练的编辑技术，强调良好表示学习对生成建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量多样样本，但存在过拟合训练目标导致记忆训练数据的风险，需理解记忆与泛化的本质区别以提升模型可靠性与可控性。

Method: 通过分析两层ReLU去噪自编码器，从表示学习角度揭示记忆与泛化的表征特征；并在真实扩散模型中验证理论结果，提出基于表示的检测与编辑方法。

Result: 理论与实证表明，记忆对应局部尖峰表示，泛化对应平衡表示；基于此可实现无训练的记忆检测与表示导向的生成控制。

Conclusion: 学习优良的表示是实现新颖且有意义生成建模的核心。

Abstract: Diffusion models excel at generating high-quality, diverse samples, yet they risk memorizing training data when overfit to the training objective. We analyze the distinctions between memorization and generalization in diffusion models through the lens of representation learning. By investigating a two-layer ReLU denoising autoencoder (DAE), we prove that (i) memorization corresponds to the model storing raw training samples in the learned weights for encoding and decoding, yielding localized "spiky" representations, whereas (ii) generalization arises when the model captures local data statistics, producing "balanced" representations. Furthermore, we validate these theoretical findings on real-world unconditional and text-to-image diffusion models, demonstrating that the same representation structures emerge in deep generative models with significant practical implications. Building on these insights, we propose a representation-based method for detecting memorization and a training-free editing technique that allows precise control via representation steering. Together, our results highlight that learning good representations is central to novel and meaningful generative modeling.

</details>


### [106] [LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics](https://arxiv.org/abs/2512.21010)
*Jiashuo Liu,Jiayun Wu,Chunjie Wu,Jingkai Liu,Zaiyuan Wang,Huan Zhou,Wenhao Huang,Hongseok Namkoong*

Main category: cs.LG

TL;DR: 提出一种名为竞争瑞士系统动态（CSD）的新框架，用于更全面地评估大语言模型（LLM）在多维度能力上的表现。该框架通过模拟多轮、顺序性竞赛，根据模型的胜负记录动态配对，并利用蒙特卡洛模拟（10万次迭代）计算期望胜场得分（E[S_m]），以消除随机配对和早期运气带来的噪声。同时引入失败敏感性分析，通过参数化每轮淘汰数量（T_k），区分模型的风险偏好，揭示其作为稳健通才或激进专才的特性。实验表明，CSD相比传统静态评分方法更具细致性和情境感知能力，为下一代风险导向的LLM评估提供了重要方向。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖静态评分，难以确定不同基准测试间的合理权重，且无法捕捉模型在连续高压力任务中的动态竞争力与脆弱性，亟需一种更全面、动态、可量化风险的评价体系。

Method: 提出竞争瑞士系统动态（CSD）框架，结合动态配对机制、蒙特卡洛模拟（N=100,000）计算期望胜场得分（E[S_m]），并引入失败敏感性分析，通过调节每轮淘汰数量T_k来刻画模型的风险偏好。

Result: CSD能够生成比传统聚合评分和静态配对模型更精细、更具情境适应性的排名结果，有效揭示模型在复杂任务序列中的真实竞争力与稳定性，支持风险导向的模型选择与评估。

Conclusion: CSD框架为大语言模型评估提供了一种全新的、动态且风险敏感的视角，是迈向下一代综合性、智能化评估体系的关键一步。

Abstract: The rapid proliferation of Large Language Models (LLMs) and diverse specialized benchmarks necessitates a shift from fragmented, task-specific metrics to a holistic, competitive ranking system that effectively aggregates performance across multiple ability dimensions. Primarily using static scoring, current evaluation methods are fundamentally limited. They struggle to determine the proper mix ratio across diverse benchmarks, and critically, they fail to capture a model's dynamic competitive fitness or its vulnerability when confronted with sequential, high-stakes tasks. To address this, we introduce the novel Competitive Swiss-System Dynamics (CSD) framework. CSD simulates a multi-round, sequential contest where models are dynamically paired across a curated sequence of benchmarks based on their accumulated win-loss record. And Monte Carlo Simulation ($N=100,000$ iterations) is used to approximate the statistically robust Expected Win Score ($E[S_m]$), which eliminates the noise of random pairing and early-round luck. Furthermore, we implement a Failure Sensitivity Analysis by parameterizing the per-round elimination quantity ($T_k$), which allows us to profile models based on their risk appetite--distinguishing between robust generalists and aggressive specialists. We demonstrate that CSD provides a more nuanced and context-aware ranking than traditional aggregate scoring and static pairwise models, representing a vital step towards risk-informed, next-generation LLM evaluation.

</details>


### [107] [Shared Representation Learning for High-Dimensional Multi-Task Forecasting under Resource Contention in Cloud-Native Backends](https://arxiv.org/abs/2512.21102)
*Zixiao Huang,Jixiao Yang,Sijia Li,Chi Zhang,Jinyu Chen,Chengda Xu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的高维多任务时间序列预测框架，旨在应对云原生后端系统在动态负载、耦合指标和并行任务下的预测需求。通过共享编码结构统一表示多种监控指标，结合状态融合机制捕捉多时间尺度的趋势变化与局部扰动，并引入跨任务结构传播模块建模节点间潜在依赖关系，以理解资源竞争、链路交互和服务拓扑变化等复杂模式。同时，通过动态调整机制自适应非平稳行为，提升对突发负载突变、拓扑漂移和资源抖动的鲁棒性。实验验证了该方法在多种误差指标上的优越性能，展现出在不同运行条件下对未来状态的精准预测能力。


<details>
  <summary>Details</summary>
Motivation: 云原生后端系统面临高动态负载、多指标耦合及并行任务带来的复杂预测挑战，现有方法难以有效处理高维多任务时间序列中的结构依赖与非平稳性问题，亟需一种统一且自适应的预测框架以支持智能运维管理。

Method: 构建共享编码结构实现多指标统一表征；设计状态融合机制捕获多时间尺度趋势与扰动；引入跨任务结构传播模块建模节点间依赖关系；集成动态调整机制根据系统状态自动调节特征流，增强对非平稳行为的适应能力。

Result: 在多个误差指标上优于对比模型，具备更强的预测精度与稳定性，尤其在突发负载变化、拓扑漂移和资源波动场景下表现优异，能更准确刻画系统未来状态。

Conclusion: 所提出的统一预测框架有效解决了高维、多任务、强动态环境下云原生系统的预测难题，具备良好的泛化能力与实用性，为智能后端管理系统提供了可靠的技术支撑。

Abstract: This study proposes a unified forecasting framework for high-dimensional multi-task time series to meet the prediction demands of cloud native backend systems operating under highly dynamic loads, coupled metrics, and parallel tasks. The method builds a shared encoding structure to represent diverse monitoring indicators in a unified manner and employs a state fusion mechanism to capture trend changes and local disturbances across different time scales. A cross-task structural propagation module is introduced to model potential dependencies among nodes, enabling the model to understand complex structural patterns formed by resource contention, link interactions, and changes in service topology. To enhance adaptability to non-stationary behaviors, the framework incorporates a dynamic adjustment mechanism that automatically regulates internal feature flows according to system state changes, ensuring stable predictions in the presence of sudden load shifts, topology drift, and resource jitter. The experimental evaluation compares multiple models across various metrics and verifies the effectiveness of the framework through analyses of hyperparameter sensitivity, environmental sensitivity, and data sensitivity. The results show that the proposed method achieves superior performance on several error metrics and provides more accurate representations of future states under different operating conditions. Overall, the unified forecasting framework offers reliable predictive capability for high-dimensional, multi-task, and strongly dynamic environments in cloud native systems and provides essential technical support for intelligent backend management.

</details>


### [108] [A Mechanistic Analysis of Transformers for Dynamical Systems](https://arxiv.org/abs/2512.21113)
*Gregory Duthé,Nikolaos Evangelou,Wei Liu,Ioannis G. Kevrekidis,Eleni Chatzi*

Main category: cs.LG

TL;DR: 本文从动力系统视角分析单层Transformer在处理时序数据时的表征能力与局限性，揭示其自注意力机制的本质：对线性系统受softmax凸性约束导致过度平滑，对非线性系统在可观测条件下可作为自适应延迟嵌入机制实现状态重构。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer虽广泛用于时序建模与预测，但其内部机制缺乏动力系统理论支持，常被视为黑箱；尤其在跨动态场景的通用或零样本预测中，理解其工作原理至关重要。

Method: 基于动力系统理论，将因果自注意力解释为历史依赖的线性递归，通过线性和非线性案例研究识别不同运行模式。

Result: 对于线性系统，softmax的凸性限制了可表示的动力学类型，造成振荡场景下的过度平滑；对于部分可观测的非线性系统，注意力可充当自适应延迟嵌入，当具备足够时间上下文和隐空间维度时能有效重建状态。

Conclusion: 该研究连接了经验观察与经典动力系统理论，阐明了Transformer在何种情况下能成功建模动态系统及其失败原因，为理解其表征能力提供了理论依据。

Abstract: Transformers are increasingly adopted for modeling and forecasting time-series, yet their internal mechanisms remain poorly understood from a dynamical systems perspective. In contrast to classical autoregressive and state-space models, which benefit from well-established theoretical foundations, Transformer architectures are typically treated as black boxes. This gap becomes particularly relevant as attention-based models are considered for general-purpose or zero-shot forecasting across diverse dynamical regimes. In this work, we do not propose a new forecasting model, but instead investigate the representational capabilities and limitations of single-layer Transformers when applied to dynamical data. Building on a dynamical systems perspective we interpret causal self-attention as a linear, history-dependent recurrence and analyze how it processes temporal information. Through a series of linear and nonlinear case studies, we identify distinct operational regimes. For linear systems, we show that the convexity constraint imposed by softmax attention fundamentally restricts the class of dynamics that can be represented, leading to oversmoothing in oscillatory settings. For nonlinear systems under partial observability, attention instead acts as an adaptive delay-embedding mechanism, enabling effective state reconstruction when sufficient temporal context and latent dimensionality are available. These results help bridge empirical observations with classical dynamical systems theory, providing insight into when and why Transformers succeed or fail as models of dynamical systems.

</details>


### [109] [MODE: Multi-Objective Adaptive Coreset Selection](https://arxiv.org/abs/2512.21152)
*Tanmoy Mukherjee,Pierre Marquis,Zied Bouraoui*

Main category: cs.LG

TL;DR: MODE 是一个动态融合核心集选择策略的框架，根据训练阶段自适应调整选择标准：早期注重类别平衡，中期强调多样性，后期关注不确定性。该方法在保证 (1-1/e)-近似率的同时，具有 O(n log n) 的复杂度，实验表明其在保持竞争力准确率的同时显著降低内存需求，并提供数据效用演化的可解释性洞察。


<details>
  <summary>Details</summary>
Motivation: 现有静态的核心集选择方法无法适应不同训练阶段的数据重要性变化，导致效率与性能的权衡不佳。为提升数据效率并理解数据在训练过程中的价值演变，需要一种能动态调整选择策略的机制。

Method: MODE 通过监控模型在不同训练阶段的表现，动态分配权重给多种核心集选择策略（如基于类别的平衡、样本多样性、预测不确定性），并结合加权策略生成最终的核心集。利用在线评估机制实时更新各策略的贡献度，实现自适应优化。

Result: MODE 在多个基准数据集上实现了与最优静态方法相当甚至更优的精度，同时将内存占用减少超过 50%，且具备良好的可解释性，能够追踪每条数据在训练过程中的效用变化。

Conclusion: MODE 证明了动态调整核心集选择策略的有效性，为高效、可解释的数据选择提供了新范式，在资源受限场景下具有广泛的应用前景。

Abstract: We present Mode(Multi-Objective adaptive Data Efficiency), a framework that dynamically combines coreset selection strategies based on their evolving contribution to model performance. Unlike static methods, \mode adapts selection criteria to training phases: emphasizing class balance early, diversity during representation learning, and uncertainty at convergence. We show that MODE achieves (1-1/e)-approximation with O(n \log n) complexity and demonstrates competitive accuracy while providing interpretable insights into data utility evolution. Experiments show \mode reduces memory requirements

</details>


### [110] [A Unified Framework for EEG Seizure Detection Using Universum-Integrated Generalized Eigenvalues Proximal Support Vector Machine](https://arxiv.org/abs/2512.21170)
*Yogesh Kumar,Vrushank Ahire,M. A. Ganaie*

Main category: cs.LG

TL;DR: 本文提出两种新型的Universum增强分类器：U-GEPSVM和改进型IU-GEPSVM，用于脑电图（EEG）信号分类。通过结合广义特征值分解的计算效率与Universum学习的泛化优势，模型有效应对了EEG分析中的非平稳性、信噪比低和标注数据有限等挑战。U-GEPSVM在GEPSVM基础上引入基于比率的目标函数以融入Universum约束；IU-GEPSVM则通过加权差分形式提升稳定性，实现对类别分离与Universum对齐的独立控制。在波恩大学EEG数据集上的两个二分类任务中（O vs S：闭眼健康 vs 癫痫；Z vs S：睁眼健康 vs 癫痫），IU-GEPSVM分别达到85%和80%的最高准确率，平均准确率分别为81.29%和77.57%，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号分析面临非平稳性、低信噪比及标注数据稀缺等挑战，传统分类方法在泛化能力与计算效率方面存在局限。为提升分类性能，需引入更有效的先验知识机制，而Universum学习可提供此类信息，因此研究如何将Universum学习与高效分类框架结合成为关键。

Method: 提出U-GEPSVM与IU-GEPSVM两种新模型。前者通过比率型目标函数引入Universum约束，扩展GEPSVM框架；后者采用加权差分形式，实现对类间分离与Universum对齐的独立优化，从而增强模型稳定性和泛化能力。

Result: 在两个二分类任务上，IU-GEPSVM分别取得85%（O vs S）和80%（Z vs S）的峰值准确率，平均准确率分别为81.29%和77.57%，显著优于现有基准方法，验证了所提方法的有效性与优越性。

Conclusion: 所提出的U-GEPSVM与IU-GEPSVM能够有效融合Universum学习与广义特征值分解的优势，在处理复杂且受限的EEG信号分类任务中表现出优异的性能，尤其在小样本和噪声环境下具备更强的鲁棒性与泛化能力。

Abstract: The paper presents novel Universum-enhanced classifiers: the Universum Generalized Eigenvalue Proximal Support Vector Machine (U-GEPSVM) and the Improved U-GEPSVM (IU-GEPSVM) for EEG signal classification. Using the computational efficiency of generalized eigenvalue decomposition and the generalization benefits of Universum learning, the proposed models address critical challenges in EEG analysis: non-stationarity, low signal-to-noise ratio, and limited labeled data. U-GEPSVM extends the GEPSVM framework by incorporating Universum constraints through a ratio-based objective function, while IU-GEPSVM enhances stability through a weighted difference-based formulation that provides independent control over class separation and Universum alignment. The models are evaluated on the Bonn University EEG dataset across two binary classification tasks: (O vs S)-healthy (eyes closed) vs seizure, and (Z vs S)-healthy (eyes open) vs seizure. IU-GEPSVM achieves peak accuracies of 85% (O vs S) and 80% (Z vs S), with mean accuracies of 81.29% and 77.57% respectively, outperforming baseline methods.

</details>


### [111] [Model Merging via Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2512.21288)
*Seyed Arshan Dalili,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SAMerging的新方法，用于模型合并，旨在解决现有方法在系数缩放上的不稳定性问题。通过引入基于平坦性的PAC-Bayes泛化界，论文揭示了跨任务异质性对合并模型性能的影响，并将模型合并建模为在少量无标签数据上的多教师知识蒸馏过程。通过最小化学生-教师KL散度来紧致泛化上界，进而利用Sharpness-Aware Minimization（SAM）寻找平坦极小值，实现了在视觉与自然语言处理基准上的新纪录性能。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并方法缺乏理论保障，尤其在系数缩放方面依赖启发式策略，导致性能脆弱且对初始化敏感。此外，合并过程无法访问原始训练数据，且涉及分布差异大的微调模型，亟需一种可解释、可优化的理论框架。

Method: 提出一个针对模型合并场景的扁平性感知PAC-Bayes泛化界，引入‘跨任务异质性’项以量化不同模型先验与目标多任务分布之间的偏差；将模型合并视为在稀缺无标签数据上的多教师知识蒸馏；通过最小化学生-教师KL散度来优化合并模型；采用SAM技术寻找平坦极小值以增强泛化能力。

Result: SAMerging在多个视觉和自然语言处理基准上达到了新的性能记录，显著优于现有方法，验证了理论分析的有效性与方法的实用性。

Conclusion: 本研究通过理论分析与方法设计，为模型合并提供了坚实的理论基础，证明了基于平坦性与知识蒸馏的优化策略能够有效提升合并模型的泛化能力，推动了轻量级多任务学习的发展。

Abstract: Model merging has emerged as a lightweight alternative to joint multi-task learning (MTL), yet the generalization properties of merged models remain largely unexplored. Establishing such theoretical guarantees is non-trivial, as the merging process typically forbids access to the original training data and involves combining fine-tuned models trained on fundamentally heterogeneous data distributions. Without a principled understanding of these dynamics, current methods often rely on heuristics to approximate the optimal combination of parameters. This dependence is most critical in coefficient scaling, the weighting factors that modulate the magnitude of each fine-tuned model's contribution to the shared parameter. However, without a principled objective to guide their selection, these methods lead to brittle performance and are highly sensitive to scaling initialization. We address this gap by (i) establishing a novel flatness-aware PAC-Bayes generalization bound specifically for the model merging setting. This analysis introduces a "cross-task heterogeneity" term that formally captures the mismatch between diverse fine-tuned model priors and the target multi-task distributions. Guided by this theoretical insight, (ii) we frame model merging as multi-teacher knowledge distillation on scarce, unlabeled data. We formally demonstrate that minimizing the student-teacher Kullback-Leibler divergence directly tightens the upper bound on the merged model's excess risk. Guided by the flatness-aware bound derived, (iii) we operationalize this objective via SAMerging, a method that employs Sharpness-Aware Minimization (SAM) to find flat minima. Empirically, SAMerging establishes a new state of the art across vision and NLP benchmarks, achieving remarkable performance. The code is available at https://github.com/arshandalili/SAMerging.

</details>


### [112] [Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering](https://arxiv.org/abs/2512.21301)
*Abdullah G. Elafifi,Basma Mamdouh,Mariam Hanafy,Muhammed Alaa Eldin,Yosef Khaled,Nesma Mohamed El-Gelany,Tarek H. M. Abou-El-Enien*

Main category: cs.LG

TL;DR: 本研究提出一种端到端的计算框架，整合系统生物学与元启发式分子组装，从AML患者的转录组数据中识别关键生物标志物，利用AlphaFold3建模靶点结构，通过DOGSiteScorer定位可成药热点，再基于反应优先的进化算法从片段库生成新型配体。模型生成的化合物具有良好的类药性（QED评分0.5–0.7），并经ADMET和分子对接验证，发现高置信度候选物如Ligand L1对A08A96靶点结合自由能达-6.571 kcal/mol，为精准抗癌药物研发提供可扩展范式。


<details>
  <summary>Details</summary>
Motivation: 急性髓系白血病（AML）因分子异质性强、复发率高，现有精准治疗手段仍无法满足多数患者个性化用药需求，亟需新方法实现从患者特异性基因表达数据到全新药物设计的闭环。

Method: 结合WGCNA分析TCGA-LAML队列的bulk RNA-seq数据以筛选20个高价值生物标志物；使用AlphaFold3预测靶点三维结构；通过DOGSiteScorer量化可成药热点；开发反应优先的进化元启发式算法与多目标优化程序，从片段库构建新型配体；结合QED评分、ADMET评估与SwissDock对接进行验证。

Result: 成功生成结构新颖且类药性优良的化学实体，其中候选物Ligand L1对A08A96靶点表现出-6.571 kcal/mol的强结合能力，证明该框架可有效产出具有药理潜力的个体化先导化合物。

Conclusion: 该计算框架实现了从患者转录组数据到新型靶向药物的自动化设计，为AML及其他复杂疾病提供了可扩展的精准肿瘤学解决方案。

Abstract: Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computational framework that bridges the gap between patient-specific transcriptomics and de novo drug discovery. By analyzing bulk RNA sequencing data from the TCGA-LAML cohort, the study utilized Weighted Gene Co-expression Network Analysis (WGCNA) to prioritize 20 high-value biomarkers, including metabolic transporters like HK3 and immune-modulatory receptors such as SIGLEC9. The physical structures of these targets were modeled using AlphaFold3, and druggable hotspots were quantitatively mapped via the DOGSiteScorer engine. Then developed a novel, reaction-first evolutionary metaheuristic algorithm as well as multi-objective optimization programming that assembles novel ligands from fragment libraries, guided by spatial alignment to these identified hotspots. The generative model produced structurally unique chemical entities with a strong bias toward drug-like space, as evidenced by QED scores peaking between 0.5 and 0.7. Validation through ADMET profiling and SwissDock molecular docking identified high-confidence candidates, such as Ligand L1, which achieved a binding free energy of -6.571 kcal/mol against the A08A96 biomarker. These results demonstrate that integrating systems biology with metaheuristic molecular assembly can produce pharmacologically viable, patient tailored leads, offering a scalable blueprint for precision oncology in AML and beyond

</details>


### [113] [Learning to Solve PDEs on Neural Shape Representations](https://arxiv.org/abs/2512.21311)
*Lilian Welschinger,Yilin Liu,Zican Wang,Niloy Mitra*

Main category: cs.LG

TL;DR: 本文提出一种无需网格的新型方法，可在神经表面表示中直接求解曲面上的偏微分方程（PDE），避免了传统方法对网格提取或实例特定训练的依赖，实现端到端求解。该方法通过学习基于局部形状属性的更新算子，与主流神经表面表示兼容，仅需一次训练即可跨不同形状和拓扑泛化，保持高精度、快速推理并支持梯度传播。在球面热方程和泊松方程等基准测试中表现优于CPM，接近FEM，是首个统一处理神经与经典表面表示的端到端PDE求解框架。


<details>
  <summary>Details</summary>
Motivation: 现有PDE求解器依赖三角网格，而现代3D资产多以神经表示存在，两者不匹配导致无法直接在神经域求解表面PDE，需额外网格提取或每实例训练，阻碍端到端流程。

Method: 提出一种无网格的局部更新算子，其参数由神经形状局部属性条件决定，可直接嵌入神经表面表示中，通过单次训练实现跨形状和拓扑的泛化。

Result: 在多个分析基准和真实神经资产上，性能略优于CPM，接近FEM；首次实现对神经与传统表面表示的统一端到端表面PDE求解。

Conclusion: 所提方法实现了在神经域直接求解表面PDE的端到端流程，具有高效、通用、可微等优势，为基于神经表示的几何计算提供了新范式。

Abstract: Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation](https://arxiv.org/abs/2512.20626)
*Chi-Hsiang Hsiao,Yi-Cheng Wang,Tzung-Sheng Lin,Yi-Ren Yeh,Chu-Song Chen*

Main category: cs.AI

TL;DR: 本文提出了一种基于多模态知识图谱的检索增强生成（RAG）方法，通过融合视觉线索改进大语言模型在长篇、领域特定内容上的理解与推理能力。该方法在知识图谱构建、检索和答案生成中引入视觉信息，实现了跨模态推理，显著提升了文本与多模态语料上的问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法受限于上下文窗口，难以实现对长文档的深层理解与推理；尽管知识图谱提供了结构化支持，但现有方法仅处理文本输入，忽略了视觉等其他模态提供的互补信息。针对视觉文档理解需要结合文本、视觉和空间线索的问题，亟需一种能够整合多模态信息的增强推理框架。

Method: 提出一种多模态知识图谱基的RAG框架，将视觉信息融入知识图谱构建、检索阶段和答案生成过程，通过跨模态对齐与结构化表示，实现对图文混合内容的层次化、概念性理解。

Result: 在全局与细粒度问答任务上，该方法在文本和多模态数据集上均优于现有RAG方法，验证了其在提升内容理解与推理能力方面的有效性。

Conclusion: 本研究证明，将视觉信息嵌入知识图谱驱动的RAG系统，可显著增强大语言模型对复杂、长篇、多模态内容的理解与推理能力，为未来多模态智能问答系统提供了新范式。

Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to dynamically access external information, which is powerful for answering questions over previously unseen documents. Nonetheless, they struggle with high-level conceptual understanding and holistic comprehension due to limited context windows, which constrain their ability to perform deep reasoning over long-form, domain-specific content such as full-length books. To solve this problem, knowledge graphs (KGs) have been leveraged to provide entity-centric structure and hierarchical summaries, offering more structured support for reasoning. However, existing KG-based RAG solutions remain restricted to text-only inputs and fail to leverage the complementary insights provided by other modalities such as vision. On the other hand, reasoning from visual documents requires textual, visual, and spatial cues into structured, hierarchical concepts. To address this issue, we introduce a multimodal knowledge graph-based RAG that enables cross-modal reasoning for better content understanding. Our method incorporates visual cues into the construction of knowledge graphs, the retrieval phase, and the answer generation process. Experimental results across both global and fine-grained question answering tasks show that our approach consistently outperforms existing RAG-based approaches on both textual and multimodal corpora.

</details>


### [115] [Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)](https://arxiv.org/abs/2512.20628)
*Edited by Tessai Hayama,Takayuki Ito,Takahiro Uchiya,Motoki Miura,Takahiro Kawaji,Takaya Yuizono,Atsuo Yoshitaka,Tokuro Matsuo,Shun Okuhara,Jawad Haqbeen,Sofia Sahab,Wen Gu,Shiyao Ding*

Main category: cs.AI

TL;DR: 该论文集收录了2025年国际知识、信息与创造力支持系统会议（KICSS 2025）的论文，会议于2025年12月3日至5日在日本长冈市举行。论文经过双盲评审，部分优秀论文被推荐至IEICE信息与系统汇刊发表。


<details>
  <summary>Details</summary>
Motivation: 为人工智能、知识工程、人机交互及创造力支持系统领域的研究人员提供跨学科交流平台，促进高质量研究成果的传播与发表。

Method: 通过双盲评审流程筛选论文，并由合作期刊IEICE进行进一步评审以推荐出版。

Result: 会议收录了多篇经同行评审的高质量论文，部分论文被推荐至权威期刊发表，提升了研究成果的学术影响力。

Conclusion: KICSS 2025成功搭建了一个高水平的学术交流平台，推动了知识与创造力支持系统领域的研究发展。

Abstract: This volume presents the proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025), held in Nagaoka, Japan, on December 3-5, 2025. The conference, organized in cooperation with the IEICE Proceedings Series, provides a multidisciplinary forum for researchers in artificial intelligence, knowledge engineering, human-computer interaction, and creativity support systems. The proceedings include peer-reviewed papers accepted through a double-blind review process. Selected papers have been recommended for publication in IEICE Transactions on Information and Systems after an additional peer-review process.

</details>


### [116] [MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data](https://arxiv.org/abs/2512.20630)
*Aayam Bansal,Ishaan Gangwani*

Main category: cs.AI

TL;DR: microprobe is a novel approach for efficient foundation model reliability assessment using only 100 strategically selected probe examples. It leverages prompt diversity across five reliability dimensions, uncertainty quantification, and adaptive weighting to detect failure modes effectively. Evaluated on multiple GPT-2 variants and across healthcare, finance, and legal domains, microprobe outperforms random sampling by 23.5% in composite reliability scores (p < 0.001, Cohen's d = 1.21), achieves expert approval of 4.14/5.0, reduces assessment cost by 90%, maintains 95% coverage, and delivers 99.9% statistical power.


<details>
  <summary>Details</summary>
Motivation: Traditional foundation model reliability assessment requires thousands of evaluation examples, leading to high computational cost and time consumption, which hinders real-world deployment. There is a critical need for efficient, scalable methods to ensure responsible AI deployment without sacrificing assessment comprehensiveness.

Method: microprobe employs strategic prompt diversity across five key reliability dimensions, integrates advanced uncertainty quantification, and applies adaptive weighting to prioritize informative probes. The method selects 100 highly representative examples that capture diverse failure modes efficiently.

Result: microprobe achieves 23.5% higher composite reliability scores than random sampling baselines, with p < 0.001 and Cohen's d = 1.21. Expert validation confirms superior performance (4.14/5.0 vs. 3.14/5.0). It reduces assessment cost by 90%, maintains 95% coverage, and ensures 99.9% statistical power.

Conclusion: microprobe provides a highly efficient and effective solution for comprehensive foundation model reliability assessment, enabling faster, more scalable, and responsible AI deployment while significantly reducing resource demands.

Abstract: Foundation model reliability assessment typically requires thousands of evaluation examples, making it computationally expensive and time-consuming for real-world deployment. We introduce microprobe, a novel approach that achieves comprehensive reliability assessment using only 100 strategically selected probe examples. Our method combines strategic prompt diversity across five key reliability dimensions with advanced uncertainty quantification and adaptive weighting to efficiently detect potential failure modes. Through extensive empirical evaluation on multiple language models (GPT-2 variants, GPT-2 Medium, GPT-2 Large) and cross-domain validation (healthcare, finance, legal), we demonstrate that microprobe achieves 23.5% higher composite reliability scores compared to random sampling baselines, with exceptional statistical significance (p < 0.001, Cohen's d = 1.21). Expert validation by three AI safety researchers confirms the effectiveness of our strategic selection, rating our approach 4.14/5.0 versus 3.14/5.0 for random selection. microprobe completes reliability assessment with 99.9% statistical power while representing a 90% reduction in assessment cost and maintaining 95% of traditional method coverage. Our approach addresses a critical gap in efficient model evaluation for responsible AI deployment.

</details>


### [117] [Erkang-Diagnosis-1.1 Technical Report](https://arxiv.org/abs/2512.20632)
*Jianbing Ma,Ao Feng,Zhenjie Gao,Xinyu Song,Li Su,Bin Chen,Wei Wang,Jiamin Wu*

Main category: cs.AI

TL;DR: Erkang-Diagnosis-1.1 是基于阿里通义千问Qwen-3模型开发的AI医疗咨询助手，整合了约500GB高质量结构化医学知识，采用增强预训练与检索增强生成相结合的混合方法，具备安全性、可靠性与专业性。该模型可在3-5轮高效交互中准确理解用户症状，完成初步分析，并提供诊断建议与健康指导，旨在成为用户的智能健康伴侣，赋能基层医疗与健康管理。在综合医学测评中，其表现优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 为提升基层医疗与个人健康管理能力，打造安全、可靠、专业的AI医疗咨询助手，解决医疗资源分布不均和专业医生短缺的问题，同时提供高效、精准的健康咨询服务。

Method: 采用混合方法：结合增强预训练与检索增强生成（RAG），利用约500GB高质量结构化医学知识库，强化模型对医学知识的理解与推理能力；通过多轮交互机制实现症状精准捕捉与分析。

Result: Erkang-Diagnosis-1.1在综合医学评估中表现超越GPT-4，能在3-5轮交互中完成症状理解、初步分析并输出诊断建议与健康指导，具备高可靠性与实用性，可有效支持初级医疗与日常健康管理。

Conclusion: Erkang-Diagnosis-1.1作为基于通义千问的AI医疗助手，展现出卓越的专业能力与应用潜力，是推动智能健康服务普及的重要技术成果。

Abstract: This report provides a detailed introduction to Erkang-Diagnosis-1.1 model, our AI healthcare consulting assistant developed using Alibaba Qwen-3 model. The Erkang model integrates approximately 500GB of high-quality structured medical knowledge, employing a hybrid approach combining enhanced pre-training and retrieval-enhanced generation to create a secure, reliable, and professional AI health advisor. Through 3-5 efficient interaction rounds, Erkang Diagnosis can accurately understand user symptoms, conduct preliminary analysis, and provide valuable diagnostic suggestions and health guidance. Designed to become users intelligent health companions, it empowers primary healthcare and health management. To validate, Erkang-Diagnosis-1.1 leads GPT-4 in terms of comprehensive medical exams.

</details>


### [118] [Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning](https://arxiv.org/abs/2512.20647)
*Leo Lu,Jonathan Zhang,Sean Chua,Spencer Kim,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.AI

TL;DR: 该研究探讨了不同大型语言模型之间推理链的可交换性，发现部分完成的推理链可在不同模型间可靠延续，且在某些情况下还能提升最终准确率和逻辑一致性。研究通过基于令牌级概率阈值的截断方法与过程奖励模型（PRM）评估推理稳定性，揭示了模型间推理的互操作性是一种新兴的行为特性，为协作式AI系统中的模块化推理提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通过内部推理策略提升大模型性能，但对不同模型间推理链的可交换性了解不足。本研究旨在探究一个模型的部分推理链是否能被另一个模型有效延续，从而评估推理在模型替换下的可信度与可靠性。

Method: 采用令牌级日志概率阈值对基准模型（Gemma-3-4B-IT 和 LLaMA-3.1-70B-Instruct）的推理链进行早期、中期和晚期截断，并使用Gemma-3-1B-IT和LLaMA-3.1-8B-Instruct进行推理延续实验，评估同家族与跨家族模型的表现；结合过程奖励模型（PRM）构建可复现的推理稳定性评估框架。

Result: 实验结果显示，混合推理链通常保持甚至提升最终答案的准确性和逻辑结构，表明推理链具有良好的可交换性，尤其在跨模型情境下仍能维持较高一致性与可靠性。

Conclusion: 推理链的可交换性已成为大模型推理行为的一个新兴属性，为构建更可靠、模块化的协作式人工智能系统提供了重要启示。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of large language models (LLMs). While prior work focuses on improving model performance through internal reasoning strategies, little is known about the interchangeability of reasoning across different models. In this work, we explore whether a partially completed reasoning chain from one model can be reliably continued by another model, either within the same model family or across families. We achieve this by assessing the sufficiency of intermediate reasoning traces as transferable scaffolds for logical coherence and final answer accuracy. We interpret this interchangeability as a means of examining inference-time trustworthiness, probing whether reasoning remains both coherent and reliable under model substitution. Using token-level log-probability thresholds to truncate reasoning at early, mid, and late stages from our baseline models, Gemma-3-4B-IT and LLaMA-3.1-70B-Instruct, we conduct continuation experiments with Gemma-3-1B-IT and LLaMA-3.1-8B-Instruct to test intra-family and cross-family behaviors. Our evaluation pipeline leverages truncation thresholds with a Process Reward Model (PRM), providing a reproducible framework for assessing reasoning stability via model interchange. Evaluations with a PRM reveal that hybrid reasoning chains often preserve, and in some cases even improve, final accuracy and logical structure. Our findings point towards interchangeability as an emerging behavioral property of reasoning models, offering insights into new paradigms for reliable modular reasoning in collaborative AI systems.

</details>


### [119] [AIAuditTrack: A Framework for AI Security system](https://arxiv.org/abs/2512.20649)
*Zixun Luo,Yuhang Fan,Yufei Li,Youzhi Zhang,Hengyu Lin,Ziqi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于区块链的AI使用流量记录与治理框架AiAuditTrack（AAT），通过去中心化身份（DID）和可验证凭证（VC）建立可信的AI实体，并将实体间交互轨迹上链，实现跨系统监督与审计。采用动态交互图模型，将AI实体视为节点，交互行为作为时间特定的边，设计风险扩散算法以追踪风险源头并传播预警。通过TPS性能评估，验证了AAT在大规模交互记录下的可行性与稳定性，为复杂多智能体环境中的AI审计、风险管理和责任追溯提供了可扩展、可验证的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型驱动的AI应用快速扩张，AI交互数据激增，带来安全、问责和风险溯源等紧迫挑战，亟需一种可信赖、可审计的机制来管理多主体间的AI行为。

Method: 提出AiAuditTrack（AAT）框架，利用去中心化身份（DID）和可验证凭证（VC）构建可信AI实体；将AI实体建模为动态交互图中的节点，交互行为作为时间相关的边；设计风险扩散算法以追踪风险来源并跨实体传播预警；所有交互记录上链，实现透明可审计。

Result: 系统性能通过区块链TPS指标评估，验证了AAT在大规模交互场景下的可行性与稳定性；该框架实现了对AI行为的可追溯、可审计与风险预警能力，具备良好的可扩展性与可靠性。

Conclusion: AiAuditTrack（AAT）提供了一种基于区块链的可扩展、可验证的AI审计与风险治理方案，适用于复杂多智能体环境，有效支持责任归属与跨系统监管。

Abstract: The rapid expansion of AI-driven applications powered by large language models has led to a surge in AI interaction data, raising urgent challenges in security, accountability, and risk traceability. This paper presents AiAuditTrack (AAT), a blockchain-based framework for AI usage traffic recording and governance. AAT leverages decentralized identity (DID) and verifiable credentials (VC) to establish trusted and identifiable AI entities, and records inter-entity interaction trajectories on-chain to enable cross-system supervision and auditing. AI entities are modeled as nodes in a dynamic interaction graph, where edges represent time-specific behavioral trajectories. Based on this model, a risk diffusion algorithm is proposed to trace the origin of risky behaviors and propagate early warnings across involved entities. System performance is evaluated using blockchain Transactions Per Second (TPS) metrics, demonstrating the feasibility and stability of AAT under large-scale interaction recording. AAT provides a scalable and verifiable solution for AI auditing, risk management, and responsibility attribution in complex multi-agent environments.

</details>


### [120] [Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA](https://arxiv.org/abs/2512.20650)
*Esmail Gumaan*

Main category: cs.AI

TL;DR: 提出Mixture of Attention Schemes (MoAS)，通过学习的路由器动态选择每个token的注意力机制（MHA、GQA或MQA），在保持性能的同时提升推理效率。实验表明，动态路由优于静态混合，在WikiText-2上达到2.3074的验证损失，优于静态混合的2.3093。


<details>
  <summary>Details</summary>
Motivation: Transformer模型中注意力机制在建模质量与推理效率之间存在权衡：MHA性能好但内存开销大；MQA和GQA节省内存但可能牺牲性能。需要一种既能保持高质量又能灵活优化效率的解决方案。

Method: 设计一个可学习的路由器，根据每个token动态选择最合适的注意力机制（MHA、GQA或MQA），实现注意力机制的自适应切换。

Result: 在WikiText-2数据集上，动态路由方法的验证损失为2.3074，优于静态混合方案（2.3093），证明了该方法的有效性。同时在性能接近MHA的基础上实现了潜在的条件计算效率优势。

Conclusion: MoAS通过动态路由选择注意力机制，在不显著降低性能的前提下有效缓解了内存与效率之间的矛盾，为高效且高质量的Transformer推理提供了新思路。

Abstract: The choice of attention mechanism in Transformer models involves a critical trade-off between modeling quality and inference efficiency. Multi-Head Attention (MHA) offers the best quality but suffers from large Key-Value (KV) cache memory requirements during inference. Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) reduce memory usage but often at the cost of model performance. In this work, we propose Mixture of Attention Schemes (MoAS), a novel architecture that dynamically selects the optimal attention scheme (MHA, GQA, or MQA) for each token via a learned router. We demonstrate that dynamic routing performs better than static averaging of schemes and achieves performance competitive with the MHA baseline while offering potential for conditional compute efficiency. Experimental results on WikiText-2 show that dynamic routing (val loss 2.3074) outperforms a static mixture (2.3093), validating the effectiveness of the proposed method. Our code is available at https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS.

</details>


### [121] [Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence](https://arxiv.org/abs/2512.20651)
*Deliang Wen,Ke Sun*

Main category: cs.AI

TL;DR: Memory Bear提出了一种基于认知科学的人类记忆架构，通过整合多模态感知、动态记忆维护和自适应认知服务，实现了大语言模型记忆机制的全链路重构，在医疗、企业运营和教育等领域展现出显著的技术创新与性能突破。实验表明，相比Mem0、MemGPT、Graphiti等现有方案，Memory Bear在知识保真度、检索效率、幻觉率降低及上下文适应性等方面均表现更优，推动AI从‘记忆’迈向‘认知’。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在上下文窗口受限、长期知识遗忘、信息冗余积累和幻觉生成等固有缺陷，严重制约其在持续对话和个人化服务中的应用。

Method: 提出Memory Bear系统，构建基于认知科学原理的人类记忆架构，集成多模态信息感知、动态记忆维护与自适应认知服务，实现记忆-认知融合的全链路重构。

Result: 在医疗、企业运营、教育等多个领域验证了系统有效性；显著提升长对话中知识保真度与检索效率，降低幻觉率，增强上下文适应性与推理能力；相比Mem0、MemGPT、Graphiti等方案，在准确率、令牌效率和响应延迟等关键指标上全面领先。

Conclusion: Memory Bear标志着人工智能从‘记忆’向‘认知’演进的重要一步，为构建具备长期记忆与智能推理能力的下一代AI系统提供了新范式。

Abstract: Large language models (LLMs) face inherent limitations in memory, including restricted context windows, long-term knowledge forgetting, redundant information accumulation, and hallucination generation. These issues severely constrain sustained dialogue and personalized services. This paper proposes the Memory Bear system, which constructs a human-like memory architecture grounded in cognitive science principles. By integrating multimodal information perception, dynamic memory maintenance, and adaptive cognitive services, Memory Bear achieves a full-chain reconstruction of LLM memory mechanisms. Across domains such as healthcare, enterprise operations, and education, Memory Bear demonstrates substantial engineering innovation and performance breakthroughs. It significantly improves knowledge fidelity and retrieval efficiency in long-term conversations, reduces hallucination rates, and enhances contextual adaptability and reasoning capability through memory-cognition integration. Experimental results show that, compared with existing solutions (e.g., Mem0, MemGPT, Graphiti), Memory Bear outperforms them across key metrics, including accuracy, token efficiency, and response latency. This marks a crucial step forward in advancing AI from "memory" to "cognition".

</details>


### [122] [From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers](https://arxiv.org/abs/2512.20661)
*Yawei Liu*

Main category: cs.AI

TL;DR: 本文提出一种名为对抗反馈注意力（AFA）的训练机制，旨在解决Transformer模型在情感分析中过度关注常见词而忽略重要但低频词的问题。通过动态掩码策略与判别器对抗，以及基于策略梯度优化注意力分布，实现自动注意力重分配，无需人工标注。实验表明该方法在三个公开数据集上达到领先性能，并在大语言模型中提升12.6%。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在情感分析中常过度关注高频词，忽视对任务至关重要的低频关键词，导致性能下降。

Method: 提出对抗反馈注意力（AFA）训练机制，结合动态掩码策略和判别器对抗，利用策略梯度优化注意力分布，使模型自动聚焦关键信息。

Result: 在三个公开数据集上取得最优性能；应用于大语言模型时进一步提升12.6%。

Conclusion: AFA机制有效提升了Transformer模型在情感分析中的注意力分配能力，显著改善了模型性能，尤其在处理关键但低频词汇方面表现突出。

Abstract: Transformer-based models have been widely adopted for sentiment analysis tasks due to their exceptional ability to capture contextual information. However, these methods often exhibit suboptimal accuracy in certain scenarios. By analyzing their attention distributions, we observe that existing models tend to allocate attention primarily to common words, overlooking less popular yet highly task-relevant terms, which significantly impairs overall performance. To address this issue, we propose an Adversarial Feedback for Attention(AFA) training mechanism that enables the model to automatically redistribute attention weights to appropriate focal points without requiring manual annotations. This mechanism incorporates a dynamic masking strategy that attempts to mask various words to deceive a discriminator, while the discriminator strives to detect significant differences induced by these masks. Additionally, leveraging the sensitivity of Transformer models to token-level perturbations, we employ a policy gradient approach to optimize attention distributions, which facilitates efficient and rapid convergence. Experiments on three public datasets demonstrate that our method achieves state-of-the-art results. Furthermore, applying this training mechanism to enhance attention in large language models yields a further performance improvement of 12.6%

</details>


### [123] [Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models](https://arxiv.org/abs/2512.20662)
*Yiqing Ma,Jung-Hua Liu*

Main category: cs.AI

TL;DR: 该研究通过三个控制实验评估了大型语言模型（LLMs）在行为上的缺陷，包括懒惰性（提前截断响应或部分遵守多部分请求）、解码次优性（因短视解码未能选择更高质量序列）和上下文退化（在长时间对话中遗忘或忽略核心指令）。结果显示，现代LLMs普遍存在对复杂多部分指令的不完全遵守问题，但解码次优性和上下文退化现象在测试中表现有限。模型在200轮混乱对话中表现出意外的鲁棒性，能较好保留关键信息。研究建议采用自修正和动态提示等策略以提升指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 揭示并量化当前先进LLMs在指令遵循、解码质量和长期上下文保持方面存在的潜在行为缺陷，为提升模型可靠性提供实证依据。

Method: 设计并执行三组受控实验（A、B、C），分别评估模型在复杂指令遵守、简单推理任务中的解码行为以及在长时间对话中的上下文稳定性。使用GPT-4变体和DeepSeek等主流模型进行测试，并通过定量指标分析其表现。

Result: 发现模型普遍存在对多部分指令的懒惰行为，如遗漏必要内容或未达长度要求；但在简单推理任务中缺乏解码次优性的证据；在200轮对话测试中表现出对核心指令的高度记忆保持，远超预期。

Conclusion: 尽管模型在遵守详细指令方面仍面临挑战，但其内部机制可能已有效缓解部分假设中的失败模式（如上下文遗忘），尤其在直接检索场景中。建议引入自修正与动态提示等方法以增强多指令合规性。

Abstract: Large Language Models (LLMs) often exhibit behavioral artifacts such as laziness (premature truncation of responses or partial compliance with multi-part requests), decoding suboptimality (failure to select higher-quality sequences due to myopic decoding), and context degradation (forgetting or ignoring core instructions over long conversations). We conducted three controlled experiments (A, B, and C) to quantify these phenomena across several advanced LLMs (OpenAI GPT-4 variant, DeepSeek). Our results indicate widespread laziness in satisfying complex multi-part instructions: models frequently omitted required sections or failed to meet length requirements despite explicit prompting. However, we found limited evidence of decoding suboptimality in a simple reasoning task (the models' greedy answers appeared to align with their highest-confidence solution), and we observed surprising robustness against context degradation in a 200-turn chaotic conversation test - the models maintained key facts and instructions far better than expected. These findings suggest that while compliance with detailed instructions remains an open challenge, modern LLMs may internally mitigate some hypothesized failure modes (such as context forgetting) in straightforward retrieval scenarios. We discuss implications for reliability, relate our findings to prior work on instruction-following and long-context processing, and recommend strategies (such as self-refinement and dynamic prompting) to reduce laziness and bolster multi-instruction compliance.

</details>


### [124] [Bridging the AI Trustworthiness Gap between Functions and Norms](https://arxiv.org/abs/2512.20671)
*Daan Di Scala,Sophie Lathouwers,Michael van Bekkum*

Main category: cs.AI

TL;DR: 本文提出需要构建一种概念性语言来弥合功能可信人工智能（FTAI）与规范可信人工智能（NTAI）之间的鸿沟，以实现对AI系统可信度的评估。该语言可作为开发者的框架，帮助将法规转化为具体实现步骤，提升AI系统的可评估性和合规性。


<details>
  <summary>Details</summary>
Motivation: 当前FTAI与NTAI之间存在脱节，导致难以有效评估AI系统的可信度，亟需一种能够连接两者的方法。

Method: 通过分析现有技术状态，识别FTAI与NTAI之间的差距，并探讨构建语义语言的初步路径。

Result: 提出构建一种概念性语义语言的可行性与必要性，该语言能促进技术实现与法规要求之间的对齐。

Conclusion: 未来应推动跨领域合作，发展标准化的语义语言，以支持可信AI的评估与实施。

Abstract: Trustworthy Artificial Intelligence (TAI) is gaining traction due to regulations and functional benefits. While Functional TAI (FTAI) focuses on how to implement trustworthy systems, Normative TAI (NTAI) focuses on regulations that need to be enforced. However, gaps between FTAI and NTAI remain, making it difficult to assess trustworthiness of AI systems. We argue that a bridge is needed, specifically by introducing a conceptual language which can match FTAI and NTAI. Such a semantic language can assist developers as a framework to assess AI systems in terms of trustworthiness. It can also help stakeholders translate norms and regulations into concrete implementation steps for their systems. In this position paper, we describe the current state-of-the-art and identify the gap between FTAI and NTAI. We will discuss starting points for developing a semantic language and the envisioned effects of it. Finally, we provide key considerations and discuss future actions towards assessment of TAI.

</details>


### [125] [From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education](https://arxiv.org/abs/2512.20714)
*Iman Reihanian,Yunfei Hou,Qingquan Sun*

Main category: cs.AI

TL;DR: 该综述分析了32项2023-2025年发表的研究，探讨生成式AI在高等教育计算机科学教育中的个性化机制及其有效性。识别出五大应用领域：智能辅导、个性化材料、形成性反馈、AI增强评估和代码评审，并发现强调解释优先、解题延迟、渐进提示阶梯和基于学生代码/测试/评分标准的引导设计更有利于学习。成功实践共享四种模式：基于学生作品的上下文感知辅导、多层级需反思的提示结构、与传统CS基础设施（自动评分器、评分标准）融合、以及人机协同质量保障。提出探索优先的采用框架，强调试点、监测、保留学习过程的默认设置和基于证据的扩展。主要风险包括学术诚信、隐私、偏见与公平性、过度依赖，并提出相应缓解策略。总体支持生成式AI作为精准支架工具，在可审计的工作流中实现规模化个性化支持的同时保持有效学习过程。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽能实现大规模个性化计算机科学教育，但其对学习的实际影响尚不明确，亟需系统评估个性化机制的有效性与潜在风险。

Method: 通过系统性筛选259条记录，从中选取32项研究进行目的性抽样，采用文献综述方法，归纳个性化机制与学习效果信号，分析设计选择对学习结果的影响，并提炼成功模式与风险应对策略。

Result: 设计中融入解释优先、解题延迟、渐进提示和基于学生作品的引导，显著提升学习过程；成功案例具备上下文感知、多层级提示、与传统系统整合及人工保障等特征；提出探索优先的采用框架并识别关键风险及缓解措施。

Conclusion: 生成式AI可作为精准教学支架，若嵌入可审计流程、保留学习挣扎并结合传统基础设施，能够有效支持规模化个性化学习。

Abstract: Generative AI enables personalized computer science education at scale, yet questions remain about whether such personalization supports or undermines learning. This scoping review synthesizes 32 studies (2023-2025) purposively sampled from 259 records to map personalization mechanisms and effectiveness signals in higher-education computer science contexts. We identify five application domains: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review, and analyze how design choices shape learning outcomes. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding (student code, tests, and rubrics) consistently show more positive learning processes than unconstrained chat interfaces. Successful implementations share four patterns: context-aware tutoring anchored in student artifacts, multi-level hint structures requiring reflection, composition with traditional CS infrastructure (autograders and rubrics), and human-in-the-loop quality assurance. We propose an exploration-first adoption framework emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling. Recurrent risks include academic integrity, privacy, bias and equity, and over-reliance, and we pair these with operational mitigation. The evidence supports generative AI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

</details>


### [126] [AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent](https://arxiv.org/abs/2512.20745)
*Haipeng Luo,Huawen Feng,Qingfeng Sun,Can Xu,Kai Zheng,Yufei Wang,Tao Yang,Han Hu,Yansong Tang,Di Wang*

Main category: cs.AI

TL;DR: AgentMath提出一种融合语言模型推理能力与代码解释器计算精度的智能体框架，通过自动化生成高质量监督微调数据、动态交错自然语言生成与实时代码执行的强化学习范式，以及高效的训练系统（如异步滚动调度、部分滚动、前缀感知加权负载均衡），显著提升复杂数学问题求解效率与准确性。在AIME24、AIME25和HMMT25等竞赛基准上表现卓越，最高准确率达90.6%。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在处理复杂数学运算时存在计算效率低、准确性不足的问题，亟需结合语言模型的推理能力与代码解释器的精确计算能力，以实现高效、精准的数学推理。

Method: 1）自动化将自然语言思维链转化为结构化工具增强轨迹，生成高质量监督微调数据；2）提出新型智能体强化学习范式，动态交织自然语言生成与实时代码执行，支持多轮交互反馈，促进代码优化与错误修正能力的涌现；3）设计高效训练系统，采用请求级异步滚动调度、智能体部分滚动和前缀感知加权负载均衡技术，实现4-5倍加速，支持超长序列与大量工具调用场景下的高效强化学习训练。

Result: AgentMath在AIME24、AIME25和HMMT25等挑战性数学竞赛基准上达到领先性能，其中AgentMath-30B-A3B分别取得90.6%、86.4%和73.8%的准确率，展现出先进水平的数学推理能力。

Conclusion: AgentMath有效验证了融合语言模型与代码解释器在复杂数学推理中的优势，为构建更智能、可扩展的数学推理智能体提供了可行路径。

Abstract: Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.

</details>


### [127] [Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions](https://arxiv.org/abs/2512.20831)
*Rashmeet Kaur Nayyar,Naman Shah,Siddharth Srivastava*

Main category: cs.AI

TL;DR: 本文提出了一种新的强化学习方法，用于处理具有参数化动作空间的长期、稀疏奖励任务。该方法能够在线自主学习状态和动作抽象，并在学习过程中逐步细化关键区域的抽象，从而提高样本效率。在多个连续状态和参数化动作的领域中，该方法显著优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 现有的规划方法依赖于手工设计的动作模型，而标准的强化学习算法通常只能处理离散或连续动作，无法同时处理两者。少数处理参数化动作的方法依赖领域特定工程，且未能利用这些空间的潜在结构。因此，需要一种能自动学习并优化抽象的新方法以提升性能。

Method: 提出了一种基于抽象的强化学习框架，通过在线学习状态和动作抽象，并在学习过程中动态地增加关键区域的细节分辨率，使算法能够在复杂环境中更高效地探索和决策。该方法结合了TD(λ)算法，支持长时序、稀疏奖励场景下的学习。

Result: 在多个连续状态、参数化动作的任务中，所提方法相比现有最先进的基线，在样本效率方面表现出显著优势，尤其在高维、复杂环境中的表现更为突出。

Conclusion: 本研究成功扩展了强化学习算法在具有参数化动作的长期、稀疏奖励问题中的应用能力，通过自适应抽象机制实现了更高的学习效率，为复杂决策任务提供了新的解决方案。

Abstract: Real-world sequential decision-making often involves parameterized action spaces that require both, decisions regarding discrete actions and decisions about continuous action parameters governing how an action is executed. Existing approaches exhibit severe limitations in this setting -- planning methods demand hand-crafted action models, and standard reinforcement learning (RL) algorithms are designed for either discrete or continuous actions but not both, and the few RL methods that handle parameterized actions typically rely on domain-specific engineering and fail to exploit the latent structure of these spaces. This paper extends the scope of RL algorithms to long-horizon, sparse-reward settings with parameterized actions by enabling agents to autonomously learn both state and action abstractions online. We introduce algorithms that progressively refine these abstractions during learning, increasing fine-grained detail in the critical regions of the state-action space where greater resolution improves performance. Across several continuous-state, parameterized-action domains, our abstraction-driven approach enables TD($λ$) to achieve markedly higher sample efficiency than state-of-the-art baselines.

</details>


### [128] [LLM Personas as a Substitute for Field Experiments in Method Benchmarking](https://arxiv.org/abs/2512.21080)
*Enoch Hyunwook Kang*

Main category: cs.AI

TL;DR: 该论文提出在满足特定条件下，使用大语言模型生成的虚拟角色（persona）替代真实用户进行A/B测试是可行的。当方法仅观察总体结果且评估不依赖算法来源时，替换为虚拟角色等同于更换评估人群。此外，论文通过信息论定义了聚合通道的可区分性，指出使虚拟角色基准与真实实验同样有效本质上是一个样本量问题，并给出了区分不同方法所需的最小独立虚拟角色评估数量的明确界限。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的场实验成本高、延迟大，阻碍了方法的迭代开发；尽管基于LLM的虚拟角色模拟提供了一种低成本的替代方案，但其是否能保持与真实实验相同的评估基准尚不明确，因此需要理论支持以验证其有效性与实用性。

Method: 提出一个充要条件证明：在仅观察聚合结果且评估不依赖算法身份的情况下，用虚拟角色替代真实用户对优化方法而言是不可区分的，相当于改变评估群体。进一步引入信息论中的可区分性概念，推导出实现与真实实验相当决策效用所需的最小样本量。

Result: 证明了在特定条件下，虚拟角色可以作为真实用户的等效替代；并给出了在给定分辨力下，可靠区分不同方法所需的最小独立虚拟角色评估数的理论边界。

Conclusion: 只要满足聚合观察和算法盲评两个条件，使用大语言模型生成的虚拟角色进行基准测试不仅在理论上是有效的，而且其可靠性取决于足够的样本量，从而为低成本、高效的方法迭代提供了理论依据。

Abstract: Field experiments (A/B tests) are often the most credible benchmark for methods in societal systems, but their cost and latency create a major bottleneck for iterative method development. LLM-based persona simulation offers a cheap synthetic alternative, yet it is unclear whether replacing humans with personas preserves the benchmark interface that adaptive methods optimize against. We prove an if-and-only-if characterization: when (i) methods observe only the aggregate outcome (aggregate-only observation) and (ii) evaluation depends only on the submitted artifact and not on the algorithm's identity or provenance (algorithm-blind evaluation), swapping humans for personas is just panel change from the method's point of view, indistinguishable from changing the evaluation population (e.g., New York to Jakarta). Furthermore, we move from validity to usefulness: we define an information-theoretic discriminability of the induced aggregate channel and show that making persona benchmarking as decision-relevant as a field experiment is fundamentally a sample-size question, yielding explicit bounds on the number of independent persona evaluations required to reliably distinguish meaningfully different methods at a chosen resolution.

</details>


### [129] [Beyond Context: Large Language Models Failure to Grasp Users Intent](https://arxiv.org/abs/2512.21110)
*Ahmed M. Hussain,Salahuddin Salahuddin,Panos Papadimitratos*

Main category: cs.AI

TL;DR: 当前大语言模型的安全方法主要关注显性有害内容，却忽视了其无法理解上下文和识别用户意图这一关键漏洞，导致恶意用户可系统性绕过安全机制。通过情感包装、渐进披露和学术化论证等技术，研究者成功绕过了包括ChatGPT、Claude、Gemini和DeepSeek在内的多个主流LLM的安全防护。值得注意的是，具备推理能力的配置反而增强了攻击效果，提升了事实准确性但未审视潜在意图。唯一例外是Claude Opus 4.1，在部分场景下优先考虑意图识别而非信息输出。该现象揭示当前架构设计存在系统性缺陷，亟需将上下文理解和意图识别作为核心安全能力，而非事后附加的保护措施。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全机制主要针对显性有害内容，但忽略了模型在理解上下文和用户真实意图方面的不足，这使得攻击者能够通过特定策略绕过安全限制，形成严重安全隐患。因此，迫切需要重新审视并改进当前的安全范式。

Method: 对多个主流大语言模型（如ChatGPT、Claude、Gemini、DeepSeek）进行实证评估，采用情感框架、渐进披露和学术化论证等策略测试其安全机制的可绕过性，并对比不同配置（尤其是启用推理能力）下的表现差异。

Result: 多数模型在面对情感化、逐步披露或学术化表述的请求时，均表现出显著的安全绕过行为；推理增强配置虽提高了事实准确性，但未能有效识别潜在恶意意图；仅有Claude Opus 4.1在某些场景中展现出更强的意图识别能力。

Conclusion: 当前大语言模型的安全架构存在系统性缺陷，过度依赖表面内容过滤而忽视深层意图理解。未来应将上下文感知与意图识别作为核心安全能力，推动从‘事后防御’向‘主动理解’的安全范式转变。

Abstract: Current Large Language Models (LLMs) safety approaches focus on explicitly harmful content while overlooking a critical vulnerability: the inability to understand context and recognize user intent. This creates exploitable vulnerabilities that malicious users can systematically leverage to circumvent safety mechanisms. We empirically evaluate multiple state-of-the-art LLMs, including ChatGPT, Claude, Gemini, and DeepSeek. Our analysis demonstrates the circumvention of reliable safety mechanisms through emotional framing, progressive revelation, and academic justification techniques. Notably, reasoning-enabled configurations amplified rather than mitigated the effectiveness of exploitation, increasing factual precision while failing to interrogate the underlying intent. The exception was Claude Opus 4.1, which prioritized intent detection over information provision in some use cases. This pattern reveals that current architectural designs create systematic vulnerabilities. These limitations require paradigmatic shifts toward contextual understanding and intent recognition as core safety capabilities rather than post-hoc protective mechanisms.

</details>


### [130] [A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care](https://arxiv.org/abs/2512.21127)
*Oliver Normand,Esther Borsi,Mitch Fruin,Lauren E Walker,Jamie Heagerty,Chris C. Holmes,Anthony J Avery,Iain E Buchan,Harry Coppock*

Main category: cs.AI

TL;DR: 本研究首次在英国国家卫生服务体系（NHS）真实初级保健数据上评估了基于大语言模型（LLM）的药物安全审查系统，覆盖210万成年人的电子健康记录。尽管系统在识别临床问题方面具有100%的敏感度，但仅在46.9%的患者中正确识别所有问题与干预措施。主要失败模式源于上下文推理缺陷，包括对不确定性的过度自信、机械套用指南、误解实际医疗实践、事实错误和流程盲视。这些缺陷在不同复杂度和人口统计群体中普遍存在，并在多种先进模型中持续存在。研究提供了45个详细案例以全面展示失败情形，强调了在临床部署前必须解决这些问题，并呼吁开展更大规模的前瞻性评估和深入研究。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医学基准测试中表现优异，但缺乏在真实临床数据上的评估，且多依赖表面指标。为确保临床安全应用，亟需对模型在真实医疗环境中的表现进行深入分析，特别是其失败机制。

Method: 基于英国NHS柴郡与默西赛德地区212万余名成人的大规模电子健康记录，通过战略性抽样选取277名患者以涵盖不同临床复杂度与用药风险；由专家临床医生评审系统识别的问题及建议干预措施，进行性能评估与失败模式分析。

Result: LLM系统在识别临床问题方面表现出100%的敏感度（95%置信区间98.2–100），特异性为83.1%（72.7–90.1），但整体准确识别问题与干预措施的比例仅为46.9%（41.1–52.8）。主要失败模式集中在上下文推理缺陷，包括过度自信、忽略个体化情境、误解临床流程等，且在不同模型与配置中均持续存在。

Conclusion: 该研究揭示了当前大语言模型在临床决策支持中面临的关键局限性，尤其是上下文理解能力不足，提示在实际部署前必须系统性改进模型行为。同时，研究呼吁开展更大规模的前瞻性评估和对模型行为更深层次的探索，以确保其安全性和可靠性。

Abstract: Large language models (LLMs) often match or exceed clinician-level performance on medical benchmarks, yet very few are evaluated on real clinical data or examined beyond headline metrics. We present, to our knowledge, the first evaluation of an LLM-based medication safety review system on real NHS primary care data, with detailed characterisation of key failure behaviours across varying levels of clinical complexity. In a retrospective study using a population-scale EHR spanning 2,125,549 adults in NHS Cheshire and Merseyside, we strategically sampled patients to capture a broad range of clinical complexity and medication safety risk, yielding 277 patients after data-quality exclusions. An expert clinician reviewed these patients and graded system-identified issues and proposed interventions. Our primary LLM system showed strong performance in recognising when a clinical issue is present (sensitivity 100\% [95\% CI 98.2--100], specificity 83.1\% [95\% CI 72.7--90.1]), yet correctly identified all issues and interventions in only 46.9\% [95\% CI 41.1--52.8] of patients. Failure analysis reveals that, in this setting, the dominant failure mechanism is contextual reasoning rather than missing medication knowledge, with five primary patterns: overconfidence in uncertainty, applying standard guidelines without adjusting for patient context, misunderstanding how healthcare is delivered in practice, factual errors, and process blindness. These patterns persisted across patient complexity and demographic strata, and across a range of state-of-the-art models and configurations. We provide 45 detailed vignettes that comprehensively cover all identified failure cases. This work highlights shortcomings that must be addressed before LLM-based clinical AI can be safely deployed. It also begs larger-scale, prospective evaluations and deeper study of LLM behaviours in clinical contexts.

</details>
