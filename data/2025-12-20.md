<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 24]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict 是一种几何感知的视觉-语言-动作（VLA）框架，通过引入预测性的运动学和几何先验来增强连续动作策略，解决现有VLA模型在3D推理中的不足。其核心包括轨迹级模块预测机器人手臂的多步3D关键点轨迹，以及基于追踪引导优化的3D高斯几何预测模块。这些模块仅在训练阶段提供监督信号，推理时仅需轻量级查询标记，无需复杂3D解码。在RoboCasa Human-50、LIBERO及真实世界操作任务中，GeoPredict显著优于多个强基线模型，尤其在几何密集型和空间要求高的场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在机器人操作中虽具强大泛化能力，但主要为反应式且以2D为中心，难以可靠处理需要精确3D推理的任务。因此亟需提升模型对三维空间结构与动态轨迹的理解能力。

Method: GeoPredict引入两个关键模块：1）轨迹级模块，编码运动历史并预测多步3D关键点轨迹；2）预测性3D高斯几何模块，基于未来关键点轨迹进行追踪引导的几何重构。所有预测模块仅用于训练阶段的深度渲染监督，推理阶段仅增加轻量查询令牌，不涉及3D解码。

Result: 在RoboCasa Human-50、LIBERO以及真实世界操作任务上，GeoPredict均显著超越现有强基线模型，尤其在几何复杂度高、空间约束严苛的任务中表现出更强鲁棒性和性能优势。

Conclusion: GeoPredict成功实现了对3D几何与运动轨迹的高效预测，通过轻量级设计实现高性能推理，在复杂空间任务中展现出卓越的泛化能力，为下一代视觉-语言-动作模型提供了有效的3D感知范式。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究评估了YOLOv8系列模型在车牌识别（LPR）和字符识别任务中的表现，提出了一种基于x轴位置的字符排序方法，并构建了由YOLOv8 Nano（LPR）与YOLOv8 Small（字符识别）组成的优化流水线，在保持计算效率的同时实现了高精度，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 提升交通管理与车辆监控中车牌检测与识别的实时准确率，尤其在复杂环境下的表现，以支持智能交通系统的发展。

Method: 采用两个不同数据集进行训练与评估；使用YOLOv8 Nano进行车牌定位，YOLOv8 Small进行字符识别；引入基于x轴位置的字符排序方法优化识别结果。

Result: YOLOv8 Nano在LPR任务中达到0.964精度和0.918 mAP50；YOLOv8 Small在字符识别任务中实现0.92精度和0.91 mAP50；优化后的流水线兼具高效性与高准确性。

Conclusion: 所提出的基于YOLOv8 Nano和YOLOv8 Small的优化方案为智能交通系统在边缘设备上的实际应用提供了可靠的技术基础，推动了智慧城市建设。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [3] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 本文提出一种轻量级图像到文本架构，仅需单张正位胸片即可生成胸部X光报告的发现部分。模型结合冻结的DINOv3 ViT编码器与增强型GPT-2解码器，通过分层高斯平滑融合肺部和心脏分割掩码，以无训练参数方式引导注意力聚焦于临床相关区域。在MIMIC-CXR数据集上，该方法在CheXpert和RadGraph指标上均取得显著提升：五种关键病理的宏平均F1提升168%，微平均F1提升146%，14个观察项总体性能提升86%，结构连贯性（RadGraph F1）提高9.7%。结果表明，解码器层面的解剖学引导可有效提升空间定位能力与临床相关区域的生成一致性。代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的放射学报告生成系统依赖大规模多模态训练、临床元数据和多视角影像，资源消耗大，难以在多数临床环境中部署。因此亟需一种轻量、高效且无需复杂输入的替代方案，以实现普适性应用。

Method: 采用冻结的DINOv3 Vision Transformer作为编码器，搭配经过层间解剖注意力增强的GPT-2解码器；通过分层高斯平滑融合肺部与心脏分割掩码，以非训练方式引导解码器关注临床相关区域，实现无需额外参数的解剖学先验注入。

Result: 在MIMIC-CXR数据集上，CheXpert Macro-F1从0.083提升至0.238（+168%），Micro-F1从0.137提升至0.337（+146%）；14项观察总体性能从0.170升至0.316（+86%）；RadGraph F1提升9.7%，表明模型在病理检测准确性和报告结构连贯性方面均有显著改进。

Conclusion: 尽管模型规模小且仅基于单张图像，但通过解码器级别的解剖学注意力机制，能有效提升空间感知能力和临床相关区域的生成质量，证明了轻量化设计在自动放射学报告生成中的可行性与有效性。代码已开源，支持广泛部署。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [4] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch是首个真实场景下基于第一人称视角的全手触觉数据集，包含5.1小时同步视频-触觉-姿态数据及2,900个带详细文本注释的片段。该研究提出触觉检索与分类基准，验证触觉信号在提升抓握理解、跨模态对齐和从自然视频中可靠检索触觉信息方面的有效性，并推动多模态第一人称感知、具身学习与接触密集型机器人操作的发展。


<details>
  <summary>Details</summary>
Motivation: 当前第一人称视觉感知缺乏对手部接触时间、位置和力度的准确识别，且缺少真实环境中同步视频与全手触觉数据的公开数据集，限制了多模态感知与具身智能的发展。

Method: 构建OpenTouch数据集，采集真实场景下的第一人称视频、全手触觉信号与姿态数据，进行标注并设计触觉检索与分类任务作为基准测试。

Result: 触觉信号被证明是理解抓握行为的紧凑而有力的线索，能增强跨模态对齐，并可从真实世界视频中可靠检索；数据集与基准已发布，促进后续研究。

Conclusion: OpenTouch填补了真实场景下第一人称全手触觉数据的空白，为多模态感知、具身学习与机器人交互提供了重要基础资源。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [5] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 该论文指出文本到图像（T2I）模型评估面临挑战，现有基准如GenEval因无法跟上模型进步而出现基准漂移问题，导致其与人类判断的偏差高达17.7%。为此，作者提出新基准GenEval 2，提升对基础视觉概念和组合性的覆盖，并引入Soft-TIFA评估方法，通过分解视觉原语进行评分，以增强与人类判断的一致性并降低未来漂移风险。研究强调持续评估与改进的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型评估依赖静态基准和判别模型，但随着模型能力提升，这些基准逐渐失效，导致评估结果与人类判断脱节，即出现基准漂移现象。尤其在GenEval基准上，这种漂移已显著影响评估有效性。

Method: 提出GenEval 2新基准，强化基础视觉概念与组合性设计；引入Soft-TIFA评估方法，基于视觉原语的联合判断提升评估一致性，并通过大规模人类实验验证其有效性。

Result: GenEval 2比原有基准更具挑战性，且软性分解式评估方法（Soft-TIFA）更贴近人类判断，显著降低未来基准漂移风险。同时，实证显示GenEval已严重饱和，需更新替代。

Conclusion: 尽管GenEval 2有望提供长期有效的评估标准，但避免基准漂移仍具挑战；研究强调必须持续审计与迭代自动化评估体系，以确保其与人类认知保持同步。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [6] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan提出了一种基于区域对齐规划的图像编辑框架，通过视觉-语言规划器将复杂指令分解并精准定位到目标区域，结合无需训练的注意力区域注入机制实现高效、并行的多区域编辑。利用GRPO强化学习优化规划能力，并构建IV-Edit基准测试复杂场景下的细粒度编辑任务。在高复杂度场景下，RePlan显著优于大量数据训练的基线模型，提升区域精度与整体保真度。


<details>
  <summary>Details</summary>
Motivation: 现有指令式图像编辑模型在处理复杂指令与混乱或模糊场景时表现不佳，缺乏对指令与视觉内容的精确对齐和可靠推理能力。

Method: 提出RePlan框架，包含一个基于GRPO强化学习的视觉-语言规划器，用于分解指令并显式定位目标区域；配合训练-free的注意力区域注入机制，实现无需迭代修复的并行多区域编辑。

Result: 在IV-Complex设置下，RePlan显著超越多个强基线模型，提升区域精确性和整体编辑保真度，且仅使用1K指令样本即可取得优异效果。

Conclusion: RePlan通过计划-执行范式与强化学习优化，有效应对指令-视觉复杂性挑战，在精细控制与跨区域编辑方面展现出卓越性能，为复杂自然语言指令下的图像编辑提供了新思路。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [7] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal 提出了一种新的图像和视频隐写技术，解决了现有方法在鲁棒性与不可察觉性之间的平衡难题。通过采用仅对抗训练范式、三阶段训练调度以及高分辨率适应策略，显著提升了水印的性能。


<details>
  <summary>Details</summary>
Motivation: 现有隐写方法依赖于不准确的感知损失（如MSE、LPIPS），导致可见的水印伪影；同时存在优化不稳定性和高分辨率下鲁棒性下降的问题，难以在实际应用中实现理想效果。

Method: 提出仅对抗训练，避免使用不可靠的像素级不可察觉损失；设计三阶段训练流程以解耦鲁棒性与不可察觉性，稳定收敛；引入基于JND的衰减和训练时推理模拟，解决高分辨率下的上采样伪影问题。

Result: 在多种图像类型和变换下，Pixel Seal 在鲁棒性和不可察觉性方面均优于当前最先进方法，且能高效适配视频场景，具备良好的实用性和可扩展性。

Conclusion: Pixel Seal 代表了图像和视频隐写领域的最新进展，为真实世界中的内容溯源提供了可靠、高效且可扩展的解决方案。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [8] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3 是一种无需训练的记忆增强型扩展，用于改进 SAM3 在内窥镜视频中的手术器械分割性能。通过引入相关性感知记忆过滤、分段插值和基于特征的重新识别模块，有效解决了频繁遮挡、快速运动和长期重入带来的挑战。在 EndoVis17 和 EndoVis18 数据集上，零样本设置下分别实现了约 7% 和 16% 的 mcIoU 提升，超越了以往基于训练的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如 SAM3 在内窥镜视频中因无差别记忆更新、固定记忆容量和遮挡后身份恢复能力弱，导致分割性能受限，尤其在复杂手术场景中表现不佳。因此需要一种无需训练但能提升记忆管理与身份恢复能力的解决方案。

Method: 提出 ReMeDI-SAM3，包含三个核心组件：(i) 相关性感知记忆过滤与专用遮挡感知记忆，用于存储遮挡前帧；(ii) 分段插值方案以扩大有效记忆容量；(iii) 基于特征的重新识别模块结合时间投票机制，实现遮挡后的可靠身份判别。

Result: 在 EndoVis17 和 EndoVis18 数据集的零样本测试中，mcIoU 分别提升约 7% 和 16%，显著优于原始 SAM3，并超过多个训练依赖型方法。

Conclusion: ReMeDI-SAM3 通过无需训练的方式有效提升了内窥镜视频中手术器械分割的鲁棒性，特别是在处理遮挡和长时重入方面表现出色，为计算机辅助手术提供了更可靠的视觉支持。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [9] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 本文提出Multi-material Physical Gaussians (M-PhyGs)，用于从自然场景视频中估计复杂多材料物体（如花朵）的材料组成和连续介质力学参数。该方法通过级联3D与2D损失、时间小批量处理，实现高效联合分割与参数恢复，并在新构建的Phlowers数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 真实世界物体常具有复杂的多材料组成和几何结构，而现有方法受限于均质单材料、预训练动力学或简单拓扑假设，难以应对此类挑战。以花朵为例，亟需一种能从视频中同时估计材料分布与物理参数的新方法。

Method: M-PhyGs结合3D与2D级联损失，利用时间小批量策略，在短自然视频基础上联合进行材料分割与连续介质力学参数估计，同时考虑重力影响。

Result: 在Phlowers数据集上的实验表明，M-PhyGs能够准确估计多材料花的物理参数，各组件协同工作显著提升性能，验证了方法的有效性与鲁棒性。

Conclusion: M-PhyGs为复杂多材料自然物体的物理属性估计提供了一种高效且准确的新范式，尤其适用于现实场景下的动态物体建模与预测。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [10] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut 是首个基于视频大语言模型（VLLM）的视频推荐方法，直接从原始视频帧中提取语义化、知识感知的标记，支持多视频历史输入和低延迟推理，克服了传统方法在生成延迟、多视频处理和视觉细节丢失方面的局限。通过跨层知识融合的MoE机制，实现个性化、可解释的推荐，无需手工标签，在标准基准上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在视频推荐任务中面临高延迟、不支持多视频输入、输出受限于语言而丢失细粒度视觉信息等问题，根本原因在于缺乏既能保留像素级细节又能利用世界知识的统一表示。

Method: LinkedOut 利用VLLM从原始视频帧中提取语义化、知识感知的标记，通过可提示查询和可选辅助模态引导；引入跨层知识融合的MoE模块，动态选择合适抽象层次的特征，实现高效、可解释的推荐。

Result: LinkedOut 在标准视频推荐基准上取得最先进性能，支持多视频输入与低延迟推理，且无需人工标注；消融实验和可解释性分析验证了层级多样性与分层融合的有效性。

Conclusion: LinkedOut 为视频推荐等下游视觉任务提供了一种高效、可解释、基于世界知识的新范式，充分释放了VLLM在视觉理解中的潜力。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [11] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait提出一种端到端的视频扩散变换器，可生成保持身份一致性的无限长度视频，并实现高达6倍的推理加速。通过身份无关的表情特征提取与归一化面部表情模块，提升面部建模的身份稳定性；在推理阶段采用动态滑动窗口与加权融合策略，确保长视频中过渡平滑和身份一致；结合高阶潜在变量导数预测未来潜在状态，跳过多个去噪步骤，显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的长肖像动画加速方法难以保证身份一致性，亟需一种既能保持身份稳定又能高效生成长视频的方法。

Method: 使用预训练提取器获取身份无关的表情特征；引入归一化面部表情块对特征进行归一化以匹配扩散潜在空间；设计动态滑动窗口机制并结合重叠区域加权融合，确保长序列生成的一致性；利用高阶潜在变量导数根据当前时刻信息直接预测未来潜在状态，跳过多步去噪过程实现加速。

Result: 在多个基准测试上，FlashPortrait在视觉质量和定量指标上均表现出色，实现了高达6倍的推理速度提升，同时保持了高度的身份一致性。

Conclusion: FlashPortrait成功解决了长视频生成中身份不一致与推理效率低的问题，为高质量、高效率的肖像动画生成提供了有效方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [12] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Alchemist is a meta-gradient-based framework for automatic, scalable data selection in Text-to-Image model training. It improves data efficiency by learning sample influence through gradient information and uses a two-stage process: data rating with multi-granularity perception and Shift-Gsampling for pruning. Experiments show it outperforms full-data training on both synthetic and web-crawled datasets.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models are limited by low-quality or redundant training data from web-crawled and synthetic sources. Manual curation is costly, and heuristic filtering methods are insufficient. There is a need for an automated, scalable approach to select high-quality data subsets.

Method: Alchemist employs a two-stage framework: (1) a lightweight rater trained via meta-gradients to assess each sample's influence using gradient information and multi-granularity perception; (2) Shift-Gsampling strategy to select informative subsets for efficient training.

Result: Alchemist achieves superior visual quality and downstream performance. Training on only 50% of the data selected by Alchemist outperforms training on the full dataset across multiple benchmarks.

Conclusion: Alchemist is the first automatic, scalable, meta-gradient-based data selection method for text-to-image models, significantly improving data efficiency and model performance without requiring manual labeling.

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [13] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA 是一种用于基于指令的视频编辑的可扩展框架，通过视觉语言模型（VLM）引导编码和奖励优化来提升对复杂、多样化自然语言指令的泛化能力。该方法利用 VLM 将文本指令、源视频首帧和可选参考图像编码为具有细粒度空间与语义上下文的表示，并引入 Edit-GRPO 后训练策略，基于相对奖励直接优化模型在遵循指令、保持内容真实性和美学质量方面的表现。同时，提出数据构建流水线以合成高质量的配对视频-指令数据。实验表明，VIVA 在指令遵循、泛化能力和编辑质量上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的方法主要依赖于简单编辑操作的成对数据进行训练，难以泛化到多样且复杂的现实世界指令，因此需要一种能更好处理复杂指令并保持内容一致性的新方法。

Method: 提出 VIVA 框架，包含：1）基于 VLM 的指导器，将文本指令、源视频第一帧及可选参考图像融合为视觉-语言联合表示；2）后训练阶段 Edit-GRPO，采用组相对策略优化技术，利用相对奖励信号优化模型以实现更忠实于指令、内容保真且美观的编辑结果；3）设计数据构造流程，自动生成多样化的高保真配对视频-指令数据集以支持训练。

Result: 在多个基准测试中，VIVA 显著优于当前最先进的方法，在指令遵循准确性、跨指令泛化能力以及生成视频的质量（如内容一致性、视觉美感）方面均表现出色。

Conclusion: VIVA 通过结合 VLM 引导的表示学习与基于相对奖励的策略优化，有效解决了现有方法在复杂指令泛化上的瓶颈，为高质量、可控的视频编辑提供了新的解决方案。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [14] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集和EgoMAN模型，通过视觉-语言推理与运动生成的轨迹-标记接口，实现阶段感知的3D手部轨迹预测，提升真实场景下的泛化能力与精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究受限于数据集将运动与语义监督分离，以及模型对推理与动作的弱关联，导致推理与运动生成缺乏一致性。

Method: 构建EgoMAN数据集，包含219K 6DoF轨迹和300万结构化QA对，用于语义、空间与运动推理；设计EgoMAN模型，通过轨迹-标记接口连接视觉-语言推理与运动生成，并采用渐进式训练使推理与运动动态对齐。

Result: 所提方法在真实世界场景中实现了高精度、阶段感知的3D手部轨迹预测，具备良好的泛化能力。

Conclusion: EgoMAN数据集与模型为交互阶段感知的3D手部轨迹预测提供了强有力支持，推动了推理与运动生成的深度融合。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [15] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 本文提出SceneDiff基准和方法，用于多视角下物体的添加、删除或移动检测。该基准包含350个多样化的视频对，带有实例标注，是首个此类多视角变化检测基准。提出的SceneDiff方法无需训练，利用预训练的3D、分割和图像编码模型，在不同视角下实现鲁棒的改变检测。通过三维对齐、提取物体区域并比较空间与语义特征来识别变化。在多视角和双视角基准上均显著优于现有方法（相对AP提升94%和37.4%）。代码和数据集将公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角变化检测中易受视角变化影响，导致误判。需要一个具有实例标注的多视角基准和鲁棒的检测方法以提升准确性。

Method: SceneDiff方法通过预训练的3D、分割和图像编码模型，将图像对齐至三维空间，提取物体区域，并比较其空间与语义特征以检测变化，整个过程无需训练。

Result: 在多视角和双视角基准上，相比现有方法，分别实现了94%和37.4%的相对平均精度（AP）提升，表明该方法在跨视角变化检测中具有显著优势。

Conclusion: 本文提出的SceneDiff基准和方法为多视角物体变化检测提供了新的标准和高效解决方案，推动了机器人清理、施工监控等应用的发展。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [16] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 本文提出MomaGraph，一种统一的场景表示方法，整合空间-功能关系与部件级交互元素，以支持家庭环境中移动操作机器人的导航与操作。为此，作者构建了MomaGraph-Scenes数据集和MomaGraph-Bench评估基准，并开发了基于强化学习训练的7B视觉语言模型MomaGraph-R1，该模型在零样本任务规划中表现优异，在基准测试中达到71.6%准确率（比最佳基线提升11.4%），并可在真实机器人上有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有场景图方法通常将空间与功能关系分离，忽略物体状态和时间动态，且缺乏对当前任务相关性的关注，难以满足家庭环境中移动操作机器人对语义丰富、紧凑场景表示的需求。

Method: 提出MomaGraph统一框架，融合空间-功能关系与部件级交互；构建大规模标注数据集MomaGraph-Scenes与系统化评估基准MomaGraph-Bench；训练7B视觉语言模型MomaGraph-R1，采用强化学习进行任务导向场景图预测，并在Graph-then-Plan框架下实现零样本任务规划。

Result: MomaGraph-R1在MomaGraph-Bench上达到71.6%准确率，较最佳基线提升11.4%，在多个公开基准上表现出良好泛化能力，并成功应用于真实机器人实验，验证了其有效性与实用性。

Conclusion: MomaGraph及其配套数据与模型为家庭场景中的移动操作提供了高效、可扩展的统一场景表示方案，显著提升了任务规划与理解能力，推动了具身智能系统的发展。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [17] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok 是一种用于高分辨率图像生成的离散图像分词器，通过多步迭代机制实现高精度重建。它结合自强化引导视觉重建和去偏-拟合训练策略，解决了多步过程中的训练-推理不一致问题，在仅使用64个令牌每图像的高压缩率下，实现了ImageNet上的先进重建质量（rFID = 1.21）和出色的图文生成性能（gFID = 2.29）。


<details>
  <summary>Details</summary>
Motivation: 当前离散分词器在多步重建过程中存在训练与推理不一致的问题，限制了其在多模态系统中的应用。尽管其天然适配自回归范式，但重建质量仍落后于连续分词器。

Method: 提出SFTok，采用多步迭代机制，结合自强化引导视觉重建和去偏-拟合训练策略，以提升重建精度并解决训练-推理差异。

Result: 在64个令牌/图像的高压缩率下，SFTok在ImageNet上达到rFID = 1.21，在类到图像生成任务中取得gFID = 2.29，表现优于现有方法。

Conclusion: SFTok通过创新的训练机制显著提升了离散分词器的重建质量，使其在高分辨率图像生成中具备竞争力，为多模态系统提供了高效且高质量的离散表示方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [18] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniStereo数据集和StereoPilot模型，解决单目转立体视频中的误差传播、深度模糊和格式不一致问题，实现高效、高质量的立体视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有单目转立体视频方法受限于多阶段深度-扭曲-修补（DWI）流程，存在误差传播、深度模糊及平行与汇聚立体格式不一致等问题，亟需更高效、统一的解决方案。

Method: 构建UniStereo大规模统一数据集，设计StereoPilot端到端前馈模型，通过可学习域切换器和循环一致性损失，直接合成目标视图，无需显式深度图或迭代采样。

Result: 实验表明，StereoPilot在视觉保真度和计算效率上均显著优于现有方法，能有效适应不同立体格式并保持高一致性。

Conclusion: UniStereo与StereoPilot共同推动了立体视频转换技术的发展，为未来高质量3D内容生成提供了可靠基准与高效工具。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [19] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V 是一种自适应工具使用的多模态大语言模型，通过引入 AT-GRPO 强化学习算法，根据每个样本的工具收益得分动态调整奖励尺度，仅在必要时调用视觉工具，从而减少推理开销并提升性能。研究构建了两个数据集（AdaTooler-V-CoT-100k 和 AdaTooler-V-300k）支持监督微调和强化学习训练。在十二个基准测试中表现优异，7B 版本在高分辨率基准 V* 上达到 89.8% 准确率，优于 GPT-4o 和 Gemini 1.5 Pro。所有代码、模型和数据均已开源。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在推理时存在盲目使用视觉工具的问题，即使不需要也频繁调用，导致推理延迟增加且性能下降。因此需要一种机制来判断是否真正需要工具调用，以提升效率与准确性。

Method: 提出 AdaTooler-V 模型，采用 AT-GRPO 强化学习算法，基于工具收益得分动态调节奖励，实现自适应工具使用；构建 AdaTooler-V-CoT-100k 和 AdaTooler-V-300k 两个数据集，分别用于 SFT 冷启动和 RL 训练，覆盖单图、多图和视频场景。

Result: 在十二个视觉推理任务基准上表现卓越，尤其是 AdaTooler-V-7B 在 V* 高分辨率基准上达到 89.8% 的准确率，超越 GPT-4o 与 Gemini 1.5 Pro；显著降低不必要的工具调用，提升推理效率。

Conclusion: AdaTooler-V 通过自适应工具使用机制有效解决了盲目调用工具带来的性能损耗问题，实现了高效且精准的多模态推理，在多个关键任务中达到领先水平，具备广泛的应用潜力。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [20] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V 是一个简单有效的基于指令的视频编辑框架，通过构建多样化的视频对数据、简化模型设计（利用预训练文本到视频模型并结合轻量级LoRA微调）以及统一时空控制机制，实现了灵活输入下的先进视频编辑性能。


<details>
  <summary>Details</summary>
Motivation: 视频编辑相比图像编辑仍面临一致性、控制性和泛化性挑战，亟需更高效且通用的解决方案。

Method: 在数据层面，通过组合具有快速逆变换的专家生成多样化视频对，利用单帧监督和伪对引入共享仿射运动，挖掘密集标注片段以构建视频对，并添加过渡监督以学习编辑过程；在模型层面，基于预训练文本到视频模型具备编辑能力的观察，采用简单的序列拼接作为条件输入，配合轻量级LoRA微调；在控制层面，统一使用单一掩码机制实现时空控制，并支持可选参考图像。

Result: EasyV2V 能够处理多种灵活输入形式（如视频+文本、视频+掩码+文本等），在多项指标上超越同期及商业系统，达到当前最佳视频编辑效果。

Conclusion: EasyV2V 证明了通过合理设计数据、简化模型架构与统一控制机制，可在保持灵活性的同时实现高性能的视频编辑，为未来研究提供了有效范式。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [21] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM 是一个通过强化学习训练的自动化框架，旨在发现并纠正多模态大模型（MLLM）的失败模式。它通过生成具有挑战性的提问和反事实图像来最大化不同模型间的分歧，从而揭示模型弱点，并提供无需标注的数据用于改进。在Gemmas-3和PaliGemma-2等先进模型上，AuditDM发现了超过20种不同的失败类型，基于这些发现进行微调后，所有模型在16个基准测试中均表现提升，甚至使3B模型超越28B模型。研究表明，当数据规模扩展进入收益递减阶段时，针对性的模型审计是诊断与优化的有效路径。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法对多模态大模型缺乏可解释性，难以全面揭示模型间的关键能力差距。需要一种更有效、可解释的方式识别模型缺陷并推动改进。

Method: 采用强化学习训练一个审计员（auditor），使其能够生成能引发目标模型间最大分歧的挑战性问题和反事实图像；利用这些高差异样本作为无标注数据，用于发现模型缺陷并指导模型修正。

Result: 在多个主流模型上发现超过20种新型失败模式；基于审计结果微调后，所有模型在16个基准上性能提升，3B模型甚至超越28B模型。

Conclusion: 随着数据规模扩展带来的收益递减，通过主动审计发现模型缺陷并针对性改进，是一种高效且可解释的模型优化路径。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [22] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出一种基于生成预训练的自监督视觉学习方法——Next-Embedding Predictive Autoregression (NEPA)，通过预测未来图像块嵌入来训练Transformer模型，无需像素重建、离散标记或对比损失。该方法在ImageNet-1k上取得83.8%（ViT-B）和85.3%（ViT-L）的准确率，且在ADE20K语义分割任务上表现良好，展现出架构简洁、可扩展性强的特点。


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成预训练成功的启发，探索是否可将类似原则应用于视觉领域，从学习表示转向学习模型，以实现更简单、可扩展的自监督视觉学习。

Method: 采用因果掩码与停止梯度技术，训练Transformer模型根据历史图像块嵌入预测未来嵌入，即下一代嵌入预测自回归（NEPA），仅以预测任务作为唯一学习目标。

Result: NEPA在ImageNet-1k上达到83.8%（ViT-B）和85.3%（ViT-L）的top-1准确率，且在ADE20K语义分割任务中表现出良好的迁移能力，验证了其有效性与泛化性。

Conclusion: 生成式嵌入预训练提供了一种简单、可扩展且潜在跨模态通用的自监督视觉学习替代方案，为视觉学习范式提供了新思路。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [23] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Generative Refocusing的两步法，用于解决单图像重聚焦难题。该方法结合DeblurNet恢复全清晰图像和BokehNet生成可调控虚化效果，创新性地采用半监督训练策略，融合合成配对数据与真实无配对虚化图像，并利用EXIF元数据捕捉真实光学特性，从而实现更真实的虚化效果与更高的控制自由度。实验表明，该方法在去模糊、虚化合成及重聚焦任务中均达到顶尖性能，并支持文本引导调整与自定义光圈形状。


<details>
  <summary>Details</summary>
Motivation: 当前单图像重聚焦方法存在依赖全清晰输入、依赖模拟数据、控制能力有限等缺陷，难以实现真实自然的虚化效果与灵活的焦点控制。因此需要一种无需全清晰输入、能利用真实光学特征且具备高度可控性的新方法。

Method: 提出Generative Refocusing框架，包含两个阶段：1）使用DeblurNet从各种输入图像中恢复全清晰图像；2）通过BokehNet生成具有可控参数的逼真虚化背景。核心是半监督训练机制，将合成配对数据与真实未配对的虚化图像结合，并利用EXIF元数据建模真实相机光学特性。

Result: 在多个去模糊、虚化合成与重聚焦基准测试中表现最优。能够生成高质量、自然的bokeh效果，支持文本引导的焦点调节和自定义光圈形状，显著提升用户对成像效果的控制能力。

Conclusion: Generative Refocusing通过半监督学习有效融合合成与真实数据，突破了传统方法对模拟数据和固定光圈的依赖，实现了高保真、可定制的单图像重聚焦，为摄影创作提供了强大工具。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [24] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas 是一个结合文本、轨迹和参考图像的可提示世界事件框架，支持用户主导的丰富模拟，通过多模态输入实现对多智能体交互、物体进出、外观变化等复杂场景的可控生成，并保持时间连贯性和对象一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成复杂动态场景时受限于单一模态输入（如仅文本或轨迹），难以同时满足语义理解、视觉一致性和用户控制力；因此需要一种融合多模态信息的系统来实现更真实、可控且具交互性的世界模拟。

Method: 采用多模态融合策略：利用自然语言表达语义意图，轨迹编码运动、时间和可见性信息，参考图像提供对象身份的视觉锚定，三者协同驱动视频生成，确保事件的连贯性与对象一致性。

Result: 生成的视频展现出高水平的时间连贯性与对象一致性，即使物体暂时消失也能保持身份识别，支持复杂的多智能体交互和反直觉事件，实现了从被动预测到主动交互式模拟的跃迁。

Conclusion: WorldCanvas 通过多模态融合显著提升了世界模型的可控性与表现力，推动其从静态预测工具演变为可由用户塑造的交互式仿真平台。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: 本文提出了一种名为Grammar Forced Translation (GraFT) 的新框架，用于将自然语言（NL）翻译为时序逻辑（TL）。该框架通过限制每一步的输出词集，减少解空间复杂度，从而提升原子命题提取和翻译的准确性。相比现有方法，GraFT在多个基准测试中显著提升了端到端翻译准确率（平均提升5.49%）和跨域翻译准确率（平均提升14.06%）。


<details>
  <summary>Details</summary>
Motivation: 现有NL到TL翻译方法在原子命题提取、共指消解和小样本学习方面存在挑战，尤其受限于语言模型在全词汇表上逐词生成带来的高复杂性与不准确性。

Method: GraFT通过基于问题特性的约束机制，将语言模型的输出候选从全词汇表缩减为少量有效词，实现解空间压缩。该方法结合语法引导策略，分步控制生成过程，降低错误传播风险，并提供理论支持解释其高效学习能力。

Result: 在CW、GLTL和Navi等基准上，GraFT相较于现有最先进方法，平均提升了5.49%的端到端翻译准确率和14.06%的跨域翻译准确率，验证了其优越性。

Conclusion: GraFT通过解空间压缩与语法引导机制，有效解决了自然语言到时序逻辑翻译中的关键挑战，在准确性和泛化能力上均表现优异，具有良好的应用前景。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [26] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出一种信息论方法，量化语音韵律（非文本）所传递的信息量及其内容。通过大型语音与语言模型估计语义维度（如情感、讽刺）与通信通道（如音频、文本）之间的互信息，发现对于讽刺和情感，音频通道（即韵律）传递的信息远超文本，尤其在缺乏长时上下文时；而对于疑问语气，韵律提供的额外信息较少。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析难以捕捉语音中的韵律信息，而韵律在传达情感、讽刺等语义特征中起关键作用。现有研究缺乏对韵律独立贡献的量化评估，因此需要一种系统方法来衡量韵律单独传递了多少信息及其具体内容。

Method: 利用大模型估计不同语义维度（如情绪、讽刺、疑问）与通信通道（音频或文本）之间的互信息，基于电视和播客语音数据进行实证分析。

Result: 对于讽刺和情感，音频通道传递的信息量比文本通道高出一个数量级以上；对于疑问语气，韵律提供的额外信息有限。

Conclusion: 该方法可推广至更多语义维度、通信渠道及语言，为理解语音韵律在交流中的作用提供新范式。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [27] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache 是一种基于语义相似性的分层缓存框架，通过重用中间激活来加速 Transformer 模型的推理。它不依赖特定模型，适用于编码器和解码器架构，并可在任意层进行缓存。采用轻量级指纹匹配机制识别相似输入，并结合自适应淘汰策略管理缓存过期问题。在 BERT 和 GPT-2 上的实验表明，推理速度最高提升 3.1 倍，准确率下降小于 0.5%。


<details>
  <summary>Details</summary>
Motivation: 现有缓存机制（如逐标记的键值缓存）仅限于自回归解码场景，适用范围有限。为解决 Transformer 模型高推理延迟的问题，需要一种更通用、高效的缓存方法以支持实时与大规模部署。

Method: 提出 LLMCache 框架，利用语义相似性匹配输入序列，通过轻量级指纹生成实现快速匹配；设计分层缓存机制，在任意 Transformer 层上重用中间激活；引入自适应淘汰策略应对缓存老化问题。

Result: 在 SQuAD、WikiText-103 和 OpenBookQA 数据集上，对 BERT 与 GPT-2 的测试显示，推理速度最高提升 3.1 倍，同时准确率损失低于 0.5%，验证了该方法的有效性与实用性。

Conclusion: LLMCache 是一种模型无关、跨架构、可扩展的通用推理优化方案，能够显著降低 Transformer 模型的推理延迟，适用于真实世界中的大规模部署场景。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [28] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: 提出首个针对多模态理解与交错生成的奖励模型综合基准MMRB2，涵盖四大任务：图文生成、图像编辑、交错生成和多模态推理，每项任务包含1000个专家标注的偏好对，覆盖23个模型与智能体。通过集成过滤策略确保人类专家共识，评估现有判别器表现，发现最新Gemini 3 Pro达75-80%准确率，优于GPT-4o（59%），但仍低于人类（>90%）。开源模型Qwen3-VL-32B表现接近Gemini 2.5 Flash（64%）。MMRB2性能与下游任务成功率高度相关，揭示未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型研究主要集中在纯文本场景，缺乏对处理交错图像与文本序列的多模态模型的系统性评估工具。为填补这一空白，亟需一个全面、具有挑战性且基于高一致性人类判断的基准来推动多模态奖励模型的发展。

Method: 构建MMRB2基准，包括设计实际且具挑战性的提示，收集来自前沿模型与智能体的响应，并通过集成过滤策略筛选出具有强人类专家共识的偏好对。利用该基准评估多种判别器在不同子任务上的表现，结合Best-of-N采样分析其与下游任务成功的关系。

Result: Gemini 3 Pro在多数任务中达到75-80%准确率，领先于GPT-5与Gemini 2.5 Pro（66-75%），但显著低于人类水平（>90%）。开源模型Qwen3-VL-32B表现接近Gemini 2.5 Flash（64%）。MMRB2性能与下游任务成功率高度相关，验证其有效性。

Conclusion: MMRB2是首个系统评估多模态奖励模型的综合性基准，揭示了当前模型与人类之间的差距，明确了提升方向。其性能指标可有效预测下游任务表现，为未来多模态奖励模型的设计与优化提供了重要参考。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [29] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 本文研究了当Transformer模型在符号意义仅通过相互作用决定的序列上进行算术训练时所形成的机制。与以往固定符号值的研究不同，本文设计了一种新任务，其中符号对代数群元素的赋值在不同序列间变化。尽管挑战性大，模型仍达到接近完美的准确率，并能泛化到未见过的代数群。通过设计针对性的数据分布，作者验证并识别出三种模型一致学习到的机制：交换复制（专用注意力头复制答案）、单位元识别（区分包含单位元的事实）和基于封闭性的消去（跟踪群成员身份以限制有效答案）。这些发现表明，在变量意义不固定的上下文中，模型会发展出符号推理机制，而非仅依赖几何表示。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在变量符号意义不固定的情况下如何进行符号推理，突破以往固定符号值设置下的几何嵌入范式，揭示其在动态语境中生成符号逻辑能力的可能性。

Method: 设计新的算术任务，其中符号对代数群元素的映射随序列变化；构建特定数据分布以进行因果测试；通过分析注意力头行为、激活模式和推理路径，识别模型内部机制。

Result: 模型在复杂动态符号任务中达到近完美准确率，具备跨代数群泛化能力；识别出三种稳定存在的机制：交换复制、单位元识别、基于封闭性的消去。

Conclusion: 当面对非固定符号意义的算术任务时，Transformer不仅能够学习几何结构，还会发展出符号推理机制，表明其具备在上下文推理中处理抽象变量的能力，拓展了对模型内在认知机制的理解。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本文提出了一种基于联邦学习（FL）的全球协作框架，用于罕见病胶原蛋白VI相关肌营养不良症（COL6-RD）的诊断。通过在两个国际组织间使用分布式数据集进行模型训练，保持患者数据本地化以保障隐私，成功构建了一个能准确分类三种主要致病机制（外显子跳跃、甘氨酸替代、伪外显子插入）的机器学习模型，F1分数达0.82，显著优于单一机构模型（0.57–0.75）。该方法提升了诊断准确性与泛化能力，有望辅助解释意义未明的变异并指导新致病变异的发现。


<details>
  <summary>Details</summary>
Motivation: 罕见病如COL6-RD的诊断受限于数据稀缺与分散，跨机构合作面临隐私、监管和物流障碍。传统集中式机器学习难以应用，亟需一种既能保护数据隐私又能实现协同建模的方法。

Method: 采用Sherpa.ai FL平台，实施联邦学习，在两个国际组织的分布式数据集上训练模型，利用患者来源成纤维细胞的胶原蛋白VI免疫荧光显微图像进行分类任务，实现跨机构协作而不共享原始数据。

Result: 所构建的联邦学习模型在三类致病机制分类中达到F1-score 0.82，显著优于单机构模型（0.57–0.75），证明其在诊断准确性与泛化能力上的优势。

Conclusion: 联邦学习为罕见病的精准诊断提供可行路径，不仅提升模型性能，还支持对未知变异的解读与新致病变异的发现，具有重要临床与科研价值。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [31] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 本文提出Generative Adversarial Reasoner，一种基于对抗强化学习的联合训练框架，通过协同优化语言模型推理器与判别器，提升数学推理中的逻辑严谨性与准确性。采用高效的分片审查机制，判别器对每个推理片段进行结构化评估，提供密集且校准良好的步骤级奖励，增强信用分配与样本效率。在多个数学基准上显著优于现有方法，如AIME24上将DeepSeek-R1-Distill-Qwen-7B性能从54.0提升至61.3，将DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7。该框架还支持灵活的奖励塑造，适用于教师蒸馏、偏好对齐及证明型推理等任务。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备显式推理能力，但在数学推理中仍存在计算错误、逻辑脆弱和表面合理但无效的步骤等问题，亟需更精细的监督信号以提升推理质量。

Method: 提出一种基于对抗强化学习的在线联合训练框架，将语言模型推理器与判别器共同优化；采用计算高效的推理链分片策略，判别器对每个逻辑完整片段进行简洁结构化评估，并生成步骤级奖励信号；推理器根据逻辑一致性与答案正确性获得奖励，判别器则根据误检率与区分能力获得反馈，形成互补信号闭环。

Result: 在多个数学推理基准测试中表现优异，尤其在AIME24上实现显著提升：DeepSeek-R1-Distill-Qwen-7B从54.0升至61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7升至53.7（+10.0）；同时具备良好可扩展性，支持多种下游任务的奖励设计。

Conclusion: 该框架通过生成密集、校准的步骤级奖励，有效缓解了传统稀疏匹配奖励带来的信用分配难题，显著提升了大语言模型的数学推理能力与鲁棒性，为构建更可靠、可解释的推理系统提供了新范式。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [32] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE提出一种基于信号时序逻辑（STL）的大型语言模型压缩框架，通过STL鲁棒性引导的贝叶斯优化，在无需微调的情况下系统搜索层间量化与剪枝配置，实现对语言属性的形式化保障。在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B上评估，最多降低3.3倍计算成本（FLOPs）和68.8%模型大小，同时满足所有语言特性要求。这是首个将形式化方法融入LLM压缩的工作，推动了边缘设备上高效可验证的LLM部署。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术如量化和剪枝常导致语言属性退化，且缺乏对模型行为的正式保证，限制了在资源受限边缘设备上的可靠部署。

Method: 采用信号时序逻辑（STL）形式化定义语言属性，利用STL鲁棒性引导的贝叶斯优化，自动搜索最优的层级量化与剪枝策略，生成满足约束的压缩模型，无需重新训练或微调。

Result: 在四个主流LLM架构上实现最高3.3倍的计算成本降低和68.8%的模型尺寸压缩，同时严格满足预设的语言学性质。

Conclusion: TOGGLE是首个将形式化方法引入大语言模型压缩的研究，实现了高效、可验证的边缘部署，为可信轻量化LLM提供了新范式。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [33] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出社会责任栈（SRS）——一个六层架构框架，将社会价值作为显式约束、防护机制、行为接口、审计手段和治理流程嵌入AI系统全生命周期，通过闭环监督控制模型实现对公平性、自主性、认知负担和解释质量的持续监控与强制执行，并在临床决策支持、自动驾驶车辆和公共部门系统中验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有负责任AI和治理努力虽提供重要规范原则，但缺乏可执行的工程机制来贯穿系统全生命周期，亟需一种能将社会价值观转化为可操作技术控制的系统性方法。

Method: 提出统一的基于约束的建模方法，引入安全边界与反馈解释，构建六层架构以实现设计期防护与运行期监控的融合，结合控制理论与治理机制，形成闭环监督控制框架。

Result: 案例研究表明，SRS可有效将规范目标转化为可实施的工程与运营控制，在临床决策、自动驾驶及公共部门系统中实现可问责、可适应、可审计的智能系统。

Conclusion: SRS整合伦理、控制理论与AI治理，为构建负责任、透明且动态适应的智能系统提供了可落地的技术基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>
