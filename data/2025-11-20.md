<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]
- [cs.CL](#cs.CL) [Total: 22]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 38]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video](https://arxiv.org/abs/2511.14848)
*Yarin Bekor,Gal Michael Harari,Or Perel,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种名为Gaussian See, Gaussian Do的新方法，用于从多视角视频中实现语义3D运动迁移。该方法无需骨架，可跨类别迁移运动，并通过条件反演提取源视频中的运动嵌入，应用于静态目标形状的渲染帧，再利用生成视频监督动态3D高斯点云重建。引入基于锚点的视图感知运动嵌入机制，保证跨视角一致性并加速收敛；同时设计稳健的4D重建流程，整合噪声监督视频。建立了首个语义3D运动迁移基准，实验表明其在运动保真度和结构一致性上优于现有基线方法。代码与数据已公开。


<details>
  <summary>Details</summary>
Motivation: 现有3D运动迁移方法存在依赖骨架、难以跨类别迁移、缺乏语义对应等问题，且多数方法无法在无约束条件下实现高质量动态3D重建。为解决这些问题，本文旨在构建一种无需刚性绑定、支持跨类别、具备语义一致性的3D运动迁移框架，实现从多视角视频到动态3D场景的自然迁移。

Method: 1. 采用条件反演技术从源视频中提取运动嵌入；2. 将运动嵌入应用至静态目标形状的渲染帧上，生成具有运动信息的监督视频；3. 引入锚点驱动的视图感知运动嵌入机制，增强跨视角一致性；4. 构建4D高斯点云重建管道，融合多视角监督信号并抑制噪声影响；5. 通过优化损失函数实现动态3D场景的高质量重建。

Result: 在自建的语义3D运动迁移基准上，本方法在运动保真度（如动作流畅性、姿态准确性）和结构一致性（如几何稳定性、纹理保持）方面显著优于现有基线方法。实验表明，该方法可在不同类别物体间实现自然、语义对齐的运动迁移，且无需预设骨骼或刚性绑定。

Conclusion: 本文提出的Gaussian See, Gaussian Do方法实现了首个无需骨架、支持跨类别、基于语义对应关系的3D运动迁移系统。通过结合条件反演与动态3D高斯点云重建，有效提升了运动迁移的质量与鲁棒性，为未来通用化3D内容生成提供了新范式。

Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/

</details>


### [2] [B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?](https://arxiv.org/abs/2511.14870)
*Fuyang Zhang,Pradeep Kumar Jayaraman,Xiang Xu,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于体素距离函数的新几何表示方法——B-Rep距离函数（BR-DF），将CAD边界表示（B-Rep）的表面网格几何编码为有符号距离函数（SDF），同时将顶点、边、面及其拓扑信息编码为每面无符号距离函数（UDF）。通过扩展的Marching Cubes算法，可直接生成封闭的CAD B-Rep模型（严格来说是分段B-Rep模型），且该转换过程永不失败。利用BR-DF的体素特性，提出了基于3D U-Net主干的多分支潜在扩散模型，联合生成SDF和每面UDF。该方法在生成性能上与当前最先进方法相当，并实现了前所未有的100%成功生成（分段）B-Rep模型的记录。


<details>
  <summary>Details</summary>
Motivation: 传统CAD生成方法在生成复杂几何结构时存在失败率高、拓扑不一致等问题。为解决这一挑战，需要一种稳定、鲁棒且能保持几何与拓扑一致性的新表示方法。本文旨在通过引入体素距离函数构建更可靠的几何表示，以提升生成成功率并保证结果的完整性。

Method: 提出B-Rep距离函数（BR-DF），将表面几何表示为有符号距离函数（SDF），将拓扑信息编码为每面无符号距离函数（UDF）。采用扩展的Marching Cubes算法实现从BR-DF到封闭分段B-Rep模型的转换，确保过程永不失败。进一步设计基于3D U-Net的多分支潜在扩散模型，联合学习并生成SDF与各面的UDF。

Result: 所提方法在生成质量上与现有最优方法相当，且在生成封闭（分段）B-Rep模型方面达到了100%的成功率，显著优于以往方法。该方法具备极强的鲁棒性，尤其适用于复杂或退化几何的生成任务。

Conclusion: B-Rep距离函数（BR-DF）提供了一种稳定、高效且可信赖的CAD几何表示方式，其结合体素距离函数与多分支扩散模型，实现了高成功率的闭合B-Rep生成，为未来工业级参数化建模与生成式设计奠定了坚实基础。

Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.

</details>


### [3] [InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization](https://arxiv.org/abs/2511.14899)
*Daniel Gilo,Or Litany*

Main category: cs.CV

TL;DR: 提出I-Mix2Mix框架，通过将2D扩散模型的编辑能力迁移到预训练多视图扩散模型中，利用其数据驱动的3D先验实现跨视图一致性。关键创新包括用多视图扩散学生替代传统神经场合并器，引入增量更新、专用教师噪声调度器和注意力机制改进，显著提升多视图一致性与单帧编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏输入视图下的多视图图像编辑任务中，因依赖单场景神经场或时序注意力机制，常产生伪影和不一致编辑结果，难以保证跨视角的一致性。

Method: 将2D扩散模型的编辑能力蒸馏至预训练多视图扩散模型中，采用多视图扩散学生替代传统神经场合并器，设计增量学生更新策略、专用教师噪声调度器及改进注意力机制以增强跨视图一致性。

Result: 实验表明，I-Mix2Mix在保持高单帧编辑质量的同时，显著提升了多视图间的一致性，有效减少伪影和不一致现象。

Conclusion: I-Mix2Mix成功实现了基于稀疏输入视图的高质量多视图图像编辑，通过融合2D扩散模型的编辑能力与多视图扩散模型的3D先验，为跨视图一致性提供了有效解决方案。

Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

</details>


### [4] [Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis](https://arxiv.org/abs/2511.14900)
*Zehao Liu,Wejieying Ren,Jipeng Zhang,Tianxiang Zhao,Jingxi Zhu,Xiaoting Li,Vasant G. Honavar*

Main category: cs.CV

TL;DR: 提出SkinR1，一种结合教科书式推理与强化学习的新型皮肤病视觉-语言模型，以解决数据异质性、缺乏可解释诊断依据及泛化能力不足的问题。通过教科书生成推理轨迹进行监督微调，并引入基于疾病层次结构的强化学习，实现从密集标注小数据到稀疏标注大数据的可靠推理迁移，显著提升诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在皮肤科诊断中受限于数据异质性、缺乏可解释的诊断理由以及难以在大规模稀疏标注数据上泛化。为提升其临床可信度和实用性，亟需构建具备专家级推理能力并能有效迁移的模型。

Method: 设计教科书驱动的推理轨迹生成器，生成高保真、层次化且包含鉴别诊断信息的推理路径；利用这些轨迹进行监督微调（SFT），赋予模型可解释的推理能力；开发一种融合疾病层次结构的新型强化学习框架，将已学到的推理模式有效迁移到大规模、稀疏标注的数据集上。

Result: 在多个皮肤科数据集上的实验表明，SkinR1在诊断准确性方面优于现有方法；消融实验证明了监督微调所建立的推理基础对性能的关键作用。

Conclusion: SkinR1通过整合教科书知识与强化学习，成功解决了当前皮肤病VLM在可靠性、可解释性和泛化能力方面的核心挑战，展现出强大的临床应用潜力。

Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

</details>


### [5] [FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding](https://arxiv.org/abs/2511.14901)
*Zhenshi Li,Weikang Yu,Dilxat Muhtar,Xueliang Zhang,Pengfeng Xiao,Pedram Ghamisi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 针对遥感图像-文本模型中细粒度对齐不足的问题，本文提出FarSLIP框架，并构建首个多粒度遥感图像-文本数据集MGRS-200k。通过引入对象级文本标注与改进的特征蒸馏机制，提升局部与全局视觉线索的对齐能力，同时保持语义连贯性。实验表明，该方法在遥感开放词汇分割、零样本分类和图文检索等任务上均达到新基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感专用CLIP模型受限于全局对齐方式，难以捕捉细粒度空间信息；且当前数据集未充分利用对象级标签，区域-文本对齐方法在遥感场景下易导致性能下降。

Method: 提出FarSLIP框架，采用patch-to-patch蒸馏替代传统的patch-to-CLS自蒸馏，增强局部特征判别力；使用基于CLS token的区域-类别对齐策略，避免显式像素级对齐带来的语义破坏，有效利用对象级监督信号。

Result: 在遥感开放词汇语义分割、零样本图像分类及图像-文本检索任务上均取得显著性能提升，优于现有方法，达到新的状态水平。

Conclusion: FarSLIP通过多粒度数据构建与创新的对齐机制，显著提升了遥感领域视觉-语言模型的细粒度对齐能力，为后续研究提供了高质量数据集与可复现的基准方法。

Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.

</details>


### [6] [nnMIL: A generalizable multiple instance learning framework for computational pathology](https://arxiv.org/abs/2511.14907)
*Xiangde Luo,Jinxi Xiang,Yuanfeng Ji,Ruijiang Li*

Main category: cs.CV

TL;DR: nnMIL是一种简单但广泛适用的多实例学习框架，通过随机采样和轻量级聚合器实现大规模批量优化与任务感知采样，显著提升病理全切片图像（WSI）的滑片级临床推断性能，在35项临床任务中优于现有方法，并具备良好的跨模型泛化能力、可靠不确定性估计及生存分层预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于病理基础模型的多实例学习方法在滑片级预测中存在设计局限，影响其泛化性和可靠性，亟需一种更稳健、可扩展的框架来连接局部特征与整体诊断决策。

Method: nnMIL引入了在样本和特征层面的随机采样策略，支持大批次优化与任务感知采样；采用轻量级聚合器进行滑窗推理，生成集成滑片级预测并支持不确定性量化。

Result: 在涵盖40,000张全切片图像和35个临床任务的数据集上，nnMIL在疾病诊断、组织学亚型分类、分子生物标志物检测和泛癌预后预测等多个任务中均显著优于现有MIL方法；同时展现出强跨模型泛化能力、可靠的不确定性估计以及在外部队列中的稳健生存分层表现。

Conclusion: nnMIL为将病理基础模型转化为临床有意义的预测提供了实用且通用的解决方案，推动了真实世界中可靠AI系统的发展与部署。

Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

</details>


### [7] [X-WIN: Building Chest Radiograph World Model via Predictive Sensing](https://arxiv.org/abs/2511.14918)
*Zefan Yang,Ge Wang,James Hendler,Mannudeep K. Kalra,Pingkun Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为X-WIN的新型胸部X光（CXR）世界模型，通过从胸部计算机断层扫描（CT）中提取三维解剖知识，学习预测其在潜在空间中的二维投影，以克服传统2D CXR因结构重叠导致的局限性。该模型利用亲和引导的对比对齐损失，捕捉同一体积不同视角投影之间的相关性，并通过掩码图像建模和领域分类器融合真实CXR数据，提升模型适应性。实验表明，X-WIN在多种下游任务中优于现有基础模型，且具备重建3D CT体积的投影生成能力。


<details>
  <summary>Details</summary>
Motivation: 传统胸部X光（CXR）作为2D投影图像，受限于结构重叠，难以准确反映3D解剖结构，导致表示学习与疾病诊断困难。为突破这一瓶颈，亟需引入三维先验知识以增强模型对真实解剖结构的理解能力。

Method: 提出X-WIN模型，通过从3D CT中学习预测其在潜在空间中的2D投影；采用亲和引导的对比对齐损失以增强多视角投影间的信息关联；结合掩码图像建模和领域分类器，使模型同时学习真实与模拟CXR，提升泛化性与适应性。

Result: X-WIN在多个下游任务中，无论是线性探测还是少样本微调，均显著优于现有基础模型；同时具备生成高质量2D投影并用于重建3D CT体积的能力。

Conclusion: X-WIN成功将3D解剖知识融入CXR世界模型，有效缓解了2D投影信息丢失问题，为医学影像理解提供了新的范式，具有良好的泛化性能与可解释性潜力。

Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.

</details>


### [8] [CPSL: Representing Volumetric Video via Content-Promoted Scene Layers](https://arxiv.org/abs/2511.14927)
*Kaiyuan Hu,Yili Jin,Junhua Liu,Xize Duan,Hong Kang,Xue Liu*

Main category: cs.CV

TL;DR: 提出了一种名为内容促进场景层（CPSL）的紧凑2.5D视频表示方法，通过每帧深度和内容显著性引导，将视频分解为少量几何一致的图层，结合软透明带和边缘深度缓存，实现视差校正的新视角合成，无需复杂的3D重建。该方法支持实时播放，并使用标准视频编码器保持帧间一致性，相比传统层基方法和神经场基方法，在感知质量和边界保真度上表现更优，同时大幅降低存储与渲染成本，推动2D视频向可扩展的2.5D沉浸式媒体演进。


<details>
  <summary>Details</summary>
Motivation: 现有体积化视频表示方法在采集、计算和渲染方面成本高昂，限制了其在按需视频中的可扩展性及实时通信的可行性。需要一种兼顾高质量与高效性的轻量化表示方式，以实现从2D视频到沉浸式2.5D媒体的平滑过渡。

Method: CPSL基于每帧深度图和内容显著性，将视频帧分解为少量几何一致的图层；每个图层配备软透明带与边缘深度缓存，用于保持遮挡顺序和边界连续性；通过深度加权映射和前后顺序透明合成实现视差校正的新视角生成；采用运动引导传播与分层编码维持时序一致性，支持标准视频编码器下的实时播放。

Result: 在多个基准测试中，CPSL在感知质量与边界保真度上优于层基与神经场基方法，同时将存储与渲染成本降低数个数量级，具备实际部署潜力。

Conclusion: CPSL提供了一条从传统2D视频迈向可扩展、低开销2.5D沉浸式媒体的实用路径，兼具高质量、高效率与实时性优势。

Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

</details>


### [9] [Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities](https://arxiv.org/abs/2511.14945)
*Fan Yang,Quanting Xie,Atsunori Moteki,Shoichi Masui,Shan Jiang,Yonatan Bisk,Graham Neubig*

Main category: cs.CV

TL;DR: 本文提出了首个包含580个多模态人类活动序列的基准，用于长周期、低对比度模式的周期性工作流程研究，涵盖无监督周期性检测、任务完成追踪和过程异常检测三类任务。提出了一种轻量级、无需训练的基线模型，在各项任务中均显著优于现有方法，并在真实场景中表现出与传统监督方法相当的部署优势，无需标注和重训练。


<details>
  <summary>Details</summary>
Motivation: 长周期、低对比度的周期性工作流程在制造、体育和日常生活中普遍存在，但现有研究主要集中在短周期、高对比度的活动上，因此亟需新的基准和方法来解决这一挑战。

Method: 构建了首个大规模多模态周期性工作流程基准，设计三种贴近实际应用的评估任务，并提出一种轻量级、无需训练的基线模型以捕捉多样化的周期性模式。

Result: 该基准对现有无监督方法和基于大语言模型的零样本方法构成显著挑战；所提基线在所有任务中表现优异，且在真实应用中展现出与监督方法相当的部署能力，无需人工标注和重新训练。

Conclusion: 本研究填补了长周期周期性工作流程分析领域的空白，提出的基准和基线为未来研究提供了重要基础，具有良好的实用价值和推广潜力。

Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.

</details>


### [10] [RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948)
*Jaro Meyer,Frédéric Giraud,Joschua Wüthrich,Marc Pollefeys,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 本文提出了一种低成本、通用的多视角视频流时间同步方法，利用自定义的LED时钟通过红光和红外LED编码时间信息，实现跨异构相机系统（包括可见光与红外模态）的毫秒级精确对齐。该方法在多个实验中表现出优于基于光、音频和时间码的现有技术，并在大型手术记录场景中验证了其有效性，显著提升了多视角姿态估计和3D重建等下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 在真实环境中，由于相机设备异构（如专业与消费级设备混合、可见光与红外传感器共存、有无音频支持等），传统硬件同步手段难以使用，导致多视角视频流的时间对齐成为关键挑战，限制了动态场景应用（如三维重建、姿态估计）的精度与普及。

Method: 设计并使用一种定制的'LED Clock'，通过红光和红外LED信号编码时间信息；录制视频后，从帧中视觉解码曝光窗口的起止时间，从而实现毫秒级的时间对齐。

Result: 在多种拍摄条件下，该方法实现了1.34毫秒的均方根误差（RMSE），优于现有基于光、音频和时间码的方法；并在包含25台异构相机的大规模手术记录中成功应用，显著提升下游视觉任务性能。

Conclusion: 所提出的同步方法无需依赖硬件同步，适用于复杂、非受控环境，有效简化了多视角视频同步流程，拓展了先进视觉感知技术在工业与临床等实际场景中的应用可行性。

Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.

</details>


### [11] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 本文提出基于深度学习的新型方法，通过闭环控制动态调节CO2激光切割机抽风泵功率，结合材料识别与烟雾水平检测，实现节能20%至50%，显著降低能耗与环境影响。


<details>
  <summary>Details</summary>
Motivation: 当前激光切割中的抽风系统多为开环控制，缺乏自适应能力，导致能源浪费和环境负担加重，亟需智能化调控方案以提升能效与可持续性。

Method: 采用闭式反馈系统，结合无透镜散斑传感与定制CNN进行材料分类，以及使用USB摄像头配合预训练VGG16模型进行迁移学习；同时引入独立的深度学习模型实时检测烟雾浓度，动态调节抽风泵功率，实现停机时自动关闭与运行中智能调功。

Result: 实验验证表明，该系统可实现烟雾抽风泵能耗降低20%至50%，显著提升能源利用效率，推动制造行业绿色转型。

Conclusion: 所提出的深度学习驱动的闭环控制策略有效解决了激光切割过程中的能源浪费问题，具备良好的应用前景，为智能制造与可持续制造提供了关键技术支撑。

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [12] [EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects](https://arxiv.org/abs/2511.14970)
*Gbenga Omotara,Ramy Farag,Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.CV

TL;DR: 本文提出Edge-Guided Spatial Attention（EGSA）融合机制，通过引入边界信息缓解语义与几何特征融合中的负面交互，提升透明物体感知性能。结合多模态渐进式训练策略，从RGB图像边缘过渡到深度图边缘，无需真实深度标签即可有效学习。在Syn-TODD和ClearPose基准上，EGSA显著优于当前最优方法MODEST，尤其在透明区域表现突出。


<details>
  <summary>Details</summary>
Motivation: 透明物体感知在计算机视觉中面临深度估计与语义分割双重挑战，现有方法因多任务学习中的负向交叉干扰导致性能受限。

Method: 提出EGSA融合机制，利用边界信息指导语义与几何特征融合；设计多模态渐进式训练策略，先基于RGB图像边缘学习，再转向深度预测边缘进行优化。

Result: 在Syn-TODD和ClearPose数据集上，EGSA在深度精度上超越MODEST，同时保持优异的分割性能，尤其在透明区域提升明显。

Conclusion: 边缘引导的特征融合是一种鲁棒且有效的透明物体感知方法，结合渐进式训练策略可显著提升模型性能。

Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

</details>


### [13] [Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation](https://arxiv.org/abs/2511.14981)
*Nicholas Cooper,Lijun Chen,Sailesh Dwivedy,Danna Gurari*

Main category: cs.CV

TL;DR: 本文提出一种仅使用基于特征的损失（不依赖交叉熵等基于logits的损失）的知识蒸馏框架，通过引入新的知识质量度量来识别教师模型中提供最有效知识的层，实验证明该方法在多个图像分类数据集和多种师生模型组合上均达到领先性能，最高可提升15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有特征知识蒸馏方法通常结合logits损失（如交叉熵）与中间层特征损失，但本文旨在探索完全基于特征损失的蒸馏框架，以更纯粹地利用潜在表示中的知识，并提升蒸馏效率与效果。

Method: 提出一种仅依赖特征损失的知识蒸馏方法，结合近期关于潜在表示几何结构的研究，设计知识质量度量以筛选最优教师层进行知识传递。

Result: 在三个图像分类数据集和四种不同的师生模型对上，所提方法显著优于标准方法，实现最高达15%的top-1准确率提升。

Conclusion: 本研究证明了仅使用特征损失进行知识蒸馏的有效性，提出的新度量机制能有效选择高质量知识源，为未来轻量化模型训练提供了高效且可复现的方案。

Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.

</details>


### [14] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0 是一个先进的生成模型家族，支持高分辨率图像和10秒视频合成，包含三个主要模型：Image Lite（6B参数）、Video Lite（2B参数）和Video Pro（19B参数），通过多阶段训练、数据优化与质量增强技术实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 提升高分辨率图像与短时视频生成的质量与效率，推动生成模型在多样应用中的可访问性与实用性。

Method: 采用多阶段训练流程，结合数据收集、处理、过滤与聚类，引入自监督微调（SFT）与基于强化学习的后训练优化，并设计新型架构、训练与推理加速技术。

Result: 在人类评估中表现出色，生成速度快且在各类任务中达到行业领先水平，支持广泛的应用场景。

Conclusion: Kandinsky 5.0 作为大规模开源生成框架，通过完整预训练与后续优化，显著推进了高质量生成模型的研究与发展，其代码与训练权重的公开将促进社区进步。

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [15] [FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation](https://arxiv.org/abs/2511.14998)
*Yueru He,Xueqing Peng,Yupeng Cao,Yan Wang,Lingfei Qian,Haohang Li,Yi Han,Ruoyu Xiang,Mingquan Lin,Prayag Tiwari,Jimin Huang,Guojun Xiong,Sophia Ananiadou*

Main category: cs.CV

TL;DR: FinCriticalED 是一个用于评估 OCR 和视觉语言模型在财务文档上事实级准确性的可视化基准，专注于数值和时间信息的精确性。它包含 500 组图像-HTML 对，由金融专家标注超过七百个关键事实，并引入基于 LLM-as-Judge 的评估流程，实现结构化事实提取与上下文验证。该基准推动了从表面文本相似性到领域关键事实正确性的评估范式转变。


<details>
  <summary>Details</summary>
Motivation: 传统 OCR 评估指标（如 ROUGE、编辑距离）仅关注文本表面相似性，无法捕捉财务文档中因符号错误或日期错位等微小失误导致的重大事实偏差。在高风险场景下，这些错误可能引发严重后果，因此亟需一种能衡量事实级准确性的新评估标准。

Method: 构建包含 500 组图像-HTML 对的 FinCriticalED 基准数据集；所有标注由金融专家完成并经过严格质量控制；开发 LLM-as-Judge 评估流水线，实现对复杂视觉布局中数值与时间事实的结构化提取与上下文验证。

Result: 即使最强的专有模型在复杂数值和时间上下文中仍存在显著错误；量化分析与专家案例研究显示，当前模型在视觉密集型财务文档中的事实精度仍有较大提升空间。

Conclusion: FinCriticalED 提供了一个严谨的评估基础，推动视觉事实精度在金融及其他高精度要求领域的发展，标志着从词法匹配向事实正确性评估的重要范式转变。

Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.

</details>


### [16] [CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification](https://arxiv.org/abs/2511.15016)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出一种跨模态知识解耦与对齐方法（CKDA），用于解决可见光-红外持续人物重识别（VI-LReID）中的知识干扰与遗忘问题。通过引入模态通用提示（MCP）和模态特定提示（MSP）模块，实现模态特有与共有知识的显式解耦与净化；同时设计跨模态知识对齐（CKA）模块，在双模态原型基础上平衡地对齐新旧知识，有效缓解协同遗忘。在四个基准数据集上的实验验证了方法的有效性与先进性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理可见光-红外持续人物重识别时，虽采用跨模态知识蒸馏缓解灾难性遗忘，但忽略了模态特有知识获取与模态共有知识抗遗忘之间的相互干扰，导致冲突知识引发协同遗忘问题。

Method: 提出CKDA方法，包含：1）模态通用提示（MCP）模块和模态特定提示（MSP）模块，用于解耦并净化不同模态的判别信息；2）跨模态知识对齐（CKA）模块，基于双模态原型，在独立的跨模态与模态内特征空间中平衡对齐新旧知识。

Result: 在四个公开基准数据集上，CKDA均显著优于当前最优方法，验证了其在持续学习场景下提升模型性能和知识保留能力的有效性。

Conclusion: 所提出的CKDA方法通过显式解耦与对齐跨模态知识，有效缓解了模态间知识干扰与协同遗忘问题，为可见光-红外持续人物重识别提供了更鲁棒的解决方案。

Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

</details>


### [17] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出基于结构化复数2D高斯原型的全息图表示，减少参数搜索空间达10:1，通过可微分光栅化器和GPU优化的自由空间光传播核实现端到端训练。实验表明该方法在VRAM使用降低2.5倍、优化速度提升50%的同时，重建质量更高。还引入转换流程以适配实际全息格式，有效抑制噪声伪影，提升下一代计算全息系统的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有全息图表示方法依赖像素级信息存储，导致参数搜索空间大、计算开销高，且易产生噪声伪影，限制了下一代计算机生成全息系统的发展。

Method: 采用结构化复数2D高斯原型作为全息图的新表示形式，设计可微分光栅化器与GPU优化的自由空间光传播核，支持端到端训练；并开发转换流程将表示适配为平滑和随机相位仅全息图格式。

Result: 相比现有方法，本方法在保持更高重建保真度的同时，实现了2.5倍的VRAM节省和50%的优化加速；转换流程显著抑制了噪声伪影，提升了实用性与可扩展性。

Conclusion: 所提出的全息图表示通过降低参数空间和提升计算效率，为下一代计算机生成全息系统提供了更高效、更可扩展的解决方案。

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [18] [Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans](https://arxiv.org/abs/2511.15029)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: 该研究探讨了计算机视觉模型（如ResNet-50）在训练过程中是否表现出与人类数学认知发展相似的轨迹，发现其在几何、拓扑和数字认知方面部分呈现出与发展一致的模式，尤其在‘心理数线’的形成上与儿童发展相符。


<details>
  <summary>Details</summary>
Motivation: 探究计算机视觉模型是否能反映人类数学认知的发展过程，从而为理解人类数学思维的发育机制提供新视角。

Method: 通过对ResNet-50模型进行详细案例研究，分析其在几何、拓扑及数字概念上的表现，并与儿童发展轨迹进行对比。

Result: 模型在欧几里得几何、图形、度量性质和拓扑等概念上表现出与发展一致的特征；但在手性图形、几何变换和对称图形方面未观察到类似发展轨迹；在数字认知中，模型随训练逐渐形成类似人类的‘心理数线’表征。

Conclusion: 计算机视觉模型在某些数学概念上展现出与人类认知发展的一致性，表明其在模拟和理解人类数学认知发展方面具有潜力，可为未来研究提供新工具和方向。

Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.

</details>


### [19] [UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space](https://arxiv.org/abs/2511.15046)
*Panqi Yang,Haodong Jing,Nanning Zheng,Yongqiang Ma*

Main category: cs.CV

TL;DR: UniHOI提出了一种统一的框架，通过共享的标记空间联合建模人类-物体交互（HOI）检测与生成任务，实现了知识共享和泛化能力的提升。该方法引入对称的交互感知注意力模块和统一的半监督学习范式，在有限标注数据下仍能实现图像与交互语义间的高效双向映射。实验表明，UniHOI在长尾HOI检测上准确率提升4.9%，在开放词汇生成任务中交互指标提升42.0%，达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统上，人类-物体交互（HOI）检测与生成任务被分别处理，导致难以实现全面的交互理解，限制了模型的综合性能。为打破这一局限，需要一种能够同时建模检测与生成任务、促进知识共享的统一框架。

Method: 提出基于统一标记空间的UniHOI框架，引入对称交互感知注意力模块以增强交互建模能力，并设计统一的半监督学习范式，支持在少量标注数据下实现图像与交互语义之间的双向映射。

Result: 在长尾HOI检测任务中，准确率提升4.9%；在开放词汇生成任务中，交互相关指标提升42.0%，均达到当前最佳水平。

Conclusion: UniHOI通过统一建模检测与生成任务，有效促进了知识共享与泛化能力，显著提升了交互理解的性能，为复杂场景下的人类-物体交互分析提供了新范式。

Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.

</details>


### [20] [Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method](https://arxiv.org/abs/2511.15052)
*Yue Wen,Kunjing Yang,Minru Bai*

Main category: cs.CV

TL;DR: 本文提出一种基于退化模型的低秩与残差融合（DLRRF）方法，用于解决高光谱图像（HSI）与多光谱图像（MSI）融合中的跨图像光谱和空间变化问题。通过将光谱差异建模为退化算子的变化，并分解目标HSI为低秩与残差成分以恢复丢失的空间细节，结合降维与隐式正则化利用空间先验信息，在插件式框架中使用近似交替优化算法求解，具有良好的收敛性。实验表明该方法在处理跨图像变异性方面性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接对图像进行变换处理跨图像变异性，加剧了融合模型的不适定性问题。需要一种更稳健的方法来应对光谱差异和局部空间变化带来的挑战。

Method: 提出DLRRF模型，将光谱变异建模为退化算子变化；将目标HSI分解为低秩与残差成分以恢复空间细节；对两成分进行基于光谱相关性的降维；引入隐式正则化利用空间先验；采用插件式框架与近似交替优化（PAO）算法求解，其中隐式正则项由外部去噪器处理。

Result: 所提方法在多种存在跨图像变异的场景下均表现出优异的融合性能，尤其在保持光谱保真度和提升空间分辨率方面优于现有方法。

Conclusion: DLRRF模型有效建模了跨图像变异，通过低秩与残差分解及隐式正则化显著提升了融合质量，且算法具有理论保证的收敛性，适用于实际复杂条件下的遥感图像融合任务。

Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.

</details>


### [21] [CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues](https://arxiv.org/abs/2511.15054)
*Srijan Ray,Bikesh K. Nirala,Jason T. Yustein,Sundaresh Ram*

Main category: cs.CV

TL;DR: 提出CellGenNet，一种基于知识蒸馏的跨组织细胞分割框架，在有限标注下实现鲁棒的细胞分割。通过师生架构，利用稀疏标注训练教师模型生成软伪标签，学生模型结合真实标签、教师概率目标及混合损失函数（二元交叉熵与Tversky损失）进行优化，有效缓解类别不平衡并保留少数核结构。引入一致性正则化和逐层丢弃以稳定特征表示，提升特征迁移能力。在多种癌症组织全切片图像上实验表明，该方法优于监督与半监督基线，提升了分割精度与泛化能力，支持可扩展、可重复的组织病理学分析。


<details>
  <summary>Details</summary>
Motivation: 准确分割显微镜全切片图像中的细胞核仍具挑战性，主要由于染色、成像条件和组织形态的差异。现有方法在标注数据有限的情况下难以实现跨组织的鲁棒分割，亟需一种高效且泛化能力强的解决方案。

Method: 采用师生架构，教师模型基于稀疏标注训练并生成软伪标签；学生模型通过联合目标函数优化，融合真实标签、教师概率目标及混合损失（二元交叉熵+Tversky损失），结合一致性正则化和层间丢弃机制，增强特征稳定性与迁移能力。

Result: 在多种癌症组织全切片图像上，CellGenNet显著提升分割精度与跨组织泛化能力，优于现有监督与半监督方法，具备良好的可扩展性和可重复性。

Conclusion: CellGenNet是一种高效且鲁棒的跨组织细胞分割框架，适用于标注稀缺场景，为大规模组织病理学分析提供了可靠的技术支持。

Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.

</details>


### [22] [ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling](https://arxiv.org/abs/2511.15057)
*Yaxiong Chen,Qicong Wang,Chunlei Li,Jingliang Hu,Yilei Shi,Shengwu Xiong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 本文提出ProPL框架，实现通用半监督超声图像分割，通过共享视觉编码器和提示引导的双解码器，结合不确定性驱动的伪标签校准模块，在5个器官、8个分割任务上超越现有方法，建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现有超声图像分割方法多针对特定解剖结构或任务，限制了其在临床中的实用性，亟需一种通用且能利用大量未标注数据的分割框架。

Method: 采用共享视觉编码器与提示引导的双解码器，通过提示-解码机制实现灵活任务适配，并引入不确定性驱动的伪标签校准（UPLC）模块提升自训练可靠性。

Result: 在涵盖5个器官和8个任务的综合数据集上，ProPL在多种评估指标上均优于当前最优方法，验证了其有效性与通用性。

Conclusion: ProPL为通用半监督超声图像分割提供了有效解决方案，推动了该领域向更实用、更高效的方向发展。

Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.

</details>


### [23] [Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://arxiv.org/abs/2511.15059)
*Keito Sasagawa,Shuhei Kurita,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 该研究评估了现有多模态大语言模型（MLLMs）在垂直书写日文文本上的阅读能力，通过构建合成的日本OCR数据集和真实世界文档图像数据集，发现现有模型在垂直书写日文上表现较差。通过在合成数据集上微调，模型性能得到提升，尤其改善了无法处理垂直书写的模型。相关数据集与代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在处理垂直书写日文文本方面能力不足，而垂直书写在日文文档中普遍存在，因此亟需研究并提升模型对此类文本的理解能力。

Method: 构建合成日本OCR数据集（包含横写与竖写日文），并采集真实世界文档中的垂直日文图像作为评估数据集；使用这些数据对MLLMs进行微调与评估，比较其在横写与竖写日文上的表现差异。

Result: 现有MLLMs在垂直书写日文上的表现显著低于水平书写；在合成数据集上微调后，模型性能明显提升，特别是对原本不支持垂直书写的模型有显著改善。

Conclusion: 垂直书写日文是多模态模型理解文档的重要挑战，通过专门的合成数据训练可有效提升模型能力，为后续研究提供重要基础与资源。

Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.

</details>


### [24] [Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065)
*Cheng Yang,Haiyuan Wan,Yiran Peng,Xin Cheng,Zhaoyang Yu,Jiayi Zhang,Junchi Yu,Xinlei Yu,Xiawu Zheng,Dongzhan Zhou,Chenglin Wu*

Main category: cs.CV

TL;DR: 本文探索了视频模型通过视频生成进行推理的可能性，提出了VR-Bench基准，用于系统评估视频模型的空间推理能力。基于需要空间规划和多步推理的迷宫求解任务，该基准包含7,920个程序生成的视频，涵盖五种迷宫类型和多种视觉风格。实验表明，SFT能有效激发视频模型的推理能力，其在空间感知方面表现优于领先视觉语言模型，并具有良好的泛化能力。此外，推理时的多样化采样可提升推理可靠性10–20%，显示出视频推理在空间推理任务中的独特潜力与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 受文本模型从生成到推理发展的启发，本文探讨视频模型是否也能通过视频生成实现推理，利用视频在空间布局和时间连续性上的优势，为空间推理提供理想基础。

Method: 构建了VR-Bench基准，包含7,920个程序生成的视频，涵盖五种迷宫类型和多样视觉风格，用于评估视频模型的推理能力；采用SFT方法激发模型推理能力，并通过多样化采样测试推理可靠性。

Result: SFT能有效激发视频模型的推理能力，模型在空间感知上优于现有视觉语言模型，具备良好泛化能力；推理时多样化采样可使推理可靠性提升10–20%。

Conclusion: 视频模型通过视频生成进行推理在空间推理任务中展现出独特潜力与可扩展性，尤其在空间感知和多步推理方面表现突出，且可通过测试时缩放进一步提升性能。

Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.

</details>


### [25] [BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching](https://arxiv.org/abs/2511.15066)
*Yachuan Huang,Xianrui Luo,Qiwen Wang,Liao Shen,Jiaqi Li,Huiqiang Sun,Zihao Huang,Wei Jiang,Zhiguo Cao*

Main category: cs.CV

TL;DR: BokehFlow提出了一种无需深度信息的可控虚化渲染框架，基于流匹配技术，直接从全焦图像生成逼真的虚化效果，并通过文本提示实现对焦点区域和模糊强度的语义控制。该方法避免了对精确深度图的依赖，提升了可控性与效率，在渲染质量和性能上优于现有深度依赖及生成式方法。


<details>
  <summary>Details</summary>
Motivation: 现有可控虚化渲染方法多依赖精确深度图，而生成式方法在可控性和效率上存在不足，因此需要一种无需深度输入且具备高可控性的新方法。

Method: BokehFlow采用流匹配框架，结合交叉注意力机制，通过文本提示控制焦点区域和模糊强度，直接从全焦图像生成虚化效果。

Result: 实验表明，BokehFlow在视觉质量、可控性和渲染效率方面均优于现有深度依赖和生成式方法，能够生成逼真且可精确控制的虚化效果。

Conclusion: BokehFlow成功实现了无需深度输入的可控虚化渲染，为摄影级虚化效果的生成提供了高效、灵活的新方案。

Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.

</details>


### [26] [MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation](https://arxiv.org/abs/2511.15077)
*Shengjing Tian,Yinan Han,Xiantong Zhao,Xuehu Liu,Qi Lang*

Main category: cs.CV

TL;DR: MambaTrack3D 是一种针对高时变（HTV）动态室外环境的新型 3D 单目标跟踪框架，基于状态空间模型 Mamba 构建。它通过设计基于 Mamba 的帧间传播（MIP）模块，实现近线性复杂度的跨帧特征传播，并显式建模历史帧间的空间关系；同时引入分组特征增强模块（GFEM），在通道级别分离前景与背景语义，缓解记忆库中的时间冗余问题。在 KITTI-HTV 和 nuScenes-HTV 基准上，MambaTrack3D 显著优于现有 HTV 及常规场景跟踪器，成功提升精度与效率平衡，在标准 KITTI 数据集上也保持了强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的 3D 单目标跟踪方法在高时变环境中面临计算复杂度高、时间冗余严重以及几何先验利用不足等问题，亟需更高效且鲁棒的解决方案。

Method: 提出 MambaTrack3D 框架，包含两个核心模块：1）基于 Mamba 的帧间传播（MIP）模块，用于替代传统单帧特征提取，实现高效跨帧信息传播并建模空间关系；2）分组特征增强模块（GFEM），在通道层面分离前景与背景特征，减少记忆库中冗余信息。

Result: 在 KITTI-HTV 与 nuScenes-HTV 基准上，MambaTrack3D 相较于 HVTrack 提升最多达 6.5 的成功率和 9.5 的精度；在标准 KITTI 数据集上性能媲美当前最先进的常规场景跟踪器，验证其优异泛化能力。

Conclusion: MambaTrack3D 实现了高精度与高效率之间的优越平衡，能够在高时变及常规场景下均表现出色，是面向复杂动态环境的先进 3D 跟踪解决方案。

Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

</details>


### [27] [TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.15085)
*Wen Yin,Siyu Zhan,Cencen Liu,Xin Hu,Guiduo Duan,Xiurui Xie,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: 提出TiCAL框架，通过伪单模态标签和典型性估计动态评估样本一致性，并在双曲空间中嵌入特征，以缓解多模态情感识别中的模态冲突问题，显著提升识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了多模态数据中不同模态间的情感不一致问题，导致模型在处理存在矛盾情绪信号的样本时表现不佳。

Method: 设计基于典型性的自洽感知多模态情感识别框架（TiCAL），利用伪单模态标签与典型性估计动态评估样本一致性，并将特征嵌入双曲空间以捕捉细粒度情感差异。

Result: 在CMU-MOSEI和MER2023等基准数据集上，TiCAL相比当前最优方法DMD提升约2.6%的识别准确率，尤其在高不一致样本上表现更优。

Conclusion: TiCAL有效缓解了多模态情感识别中的模态冲突问题，通过一致性感知与双曲表示学习提升了模型鲁棒性与性能。

Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

</details>


### [28] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 本文提出一种结构感知的手术反馈生成流水线，通过从真实导师-学员对话中挖掘手术动作本体（IAT三元组），并利用该结构指导GPT-4o生成临床相关的、类导师风格的反馈。在视频到IAT识别任务中，引入上下文和时序追踪使AUC显著提升；在反馈生成任务中，IAT条件化使生成质量评分提高12.4%，达标的生成比例翻倍，且文本相似性指标也显著改善。结果表明，基于显式IAT结构的生成方式提升了反馈的临床可信度与可审计性，适用于大规模手术训练。


<details>
  <summary>Details</summary>
Motivation: 当前手术培训依赖高质量的实时反馈，但人工反馈难以规模化。自动化生成类导师风格的反馈需理解临床相关语义，现有方法缺乏对手术动作结构的显式建模，导致生成内容不够准确或不可靠。因此，亟需一种能捕捉临床意义动作结构并用于引导反馈生成的方法。

Method: 1) 从33例真实手术中的导师-学员对话中提取并聚类出Instrument-Action-Target（IAT）三元组，构建标准化手术动作本体；2) 微调一个视频到IAT的模型，融合手术流程、任务上下文及精细时序的器械运动信息；3) 利用学习到的IAT结构作为条件输入，驱动GPT-4o生成具有临床依据的反馈文本。

Result: 在视频到IAT识别任务中，引入上下文和时序追踪后，各维度AUC均提升：器械（0.67→0.74）、动作（0.60→0.63）、组织（0.74→0.79）。在反馈生成任务中，仅基于视频的GPT-4o得分2.17，而加入IAT条件后提升至2.44（+12.4%），达到可接受水平（≥3）的比例从21%增至42%。同时，词错误率下降15–31%，ROUGE分数上升9–64%，表明生成内容更贴近真实反馈。

Conclusion: 通过构建结构化的手术动作本体并将其融入反馈生成过程，能够有效提升生成反馈的临床准确性、一致性和可解释性，为实现规模化、可审计的智能手术训练系统提供了可行路径。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [29] [A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models](https://arxiv.org/abs/2511.15098)
*Duo Li,Zuhao Yang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文研究了离散扩散多模态大语言模型（dMLLMs）中的视觉标记冗余问题，发现仅在从头训练的dMLLMs处理长回答任务时才会出现视觉冗余。研究还表明，视觉标记剪枝会引入显著的信息损失，但仅从头训练的dMLLMs能在后期去噪步骤中逐步恢复。此外，层跳过对AR-to-diffusion dMLLMs加速有效，而渐进或后期剪枝更适合从头训练的dMLLMs。该研究为dMLLMs的效率优化提供了新视角，提升了其在多模态理解任务中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有dMLLMs在推理时因每步去噪中全序列注意力计算导致高计算开销，尽管已有研究尝试通过键值缓存优化或高效采样解决，但大多忽略了模态特异的视觉标记冗余问题。因此需要深入理解视觉冗余的演化机制及其对模型性能和效率的影响。

Method: 通过系统性实验分析不同dMLLM架构和任务下视觉标记冗余的演化规律；评估视觉标记剪枝对模型响应与效率的影响；比较层跳过、渐进剪枝和后期剪枝等策略在不同类型dMLLMs中的有效性。

Result: 视觉冗余仅出现在从头训练的dMLLMs处理长回答任务时；剪枝会引入显著信息损失，但仅从头训练模型可在后期去噪中逐步恢复；层跳过适用于AR-to-diffusion dMLLMs，而渐进或后期剪枝更适用于从头训练模型。

Conclusion: 本研究揭示了视觉标记冗余的产生机制及剪枝影响，提出针对不同dMLLM类型的有效加速策略，为提升dMLLM的效率与适用性提供了重要指导。

Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

</details>


### [30] [Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting](https://arxiv.org/abs/2511.15102)
*Junseo Koo,Jinseo Jeong,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新的高斯混合方法——Gaussian Blending，以解决3D高斯溅射（3DGS）在未见过的采样率下生成视角时出现的模糊和阶梯状伪影问题。传统3DGS使用标量形式的透明度融合，而本文将其改进为基于空间分布的透明度与透射率建模，使背景点云能更合理地参与渲染。该方法保持实时性且无额外内存开销，可无缝集成至现有框架中。实验表明，该方法在不同采样率下均显著提升细节表现力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在未见采样率下存在因传统标量透明度融合导致的模糊和阶梯状伪影，尤其在缩放时表现明显，亟需一种能适应空间变化透明度的新融合机制。

Method: 提出Gaussian Blending，将透明度和透射率从标量扩展为像素区域内空间分布的高斯函数，通过考虑透明度的空间分布动态更新透射率，从而让远处背景点也参与有效贡献。

Result: 在多种未见采样率下，模型能够有效保留精细结构，显著减少模糊和阶梯伪影，在视觉质量和定量指标上均优于现有方法。

Conclusion: Gaussian Blending通过引入空间分布的透明度建模，克服了传统3DGS在多尺度视图合成中的固有缺陷，实现高质量、实时、低成本的新型视图合成，具有广泛适用性。

Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.

</details>


### [31] [An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring](https://arxiv.org/abs/2511.15117)
*Jun-Yi Liu,Chung-Hao Chen,Ya-Chi Tsao,Ssu-Yao Wu,Yu-Ting Tsao,Lyn Chao-ling Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件触发的系统，用于监测老年人的生理与心理状态，通过GMM背景建模检测访客和老人的运动行为，并实现看护、危险提醒和照片链接三种事件。实验在5个家庭的居家环境中进行，结合SVM机器学习分析图像数据。针对老年人技术使用困难的问题，设计了类日常生活的直观操作方式，实现老人与亲属通过社交媒体的便捷沟通。


<details>
  <summary>Details</summary>
Motivation: 为解决老年人居家安全与心理健康问题，同时克服其对复杂技术设备的使用障碍，需要一种易于操作且能实时监测生活活动的智能系统。

Method: 采用GMM背景建模技术检测运动行为，在看护和危险提醒事件中识别访客与老人动作；利用SVM进行图像分析；设计直观操作界面，使老年人可像日常活动一样使用社交功能与亲属沟通。

Result: 系统成功在家庭场景中检测并记录了三种事件（看护、危险提醒、照片链接），实验验证了其有效性与实用性，且老年人能顺利使用该系统进行远程沟通。

Conclusion: 该事件触发系统有效融合了行为监测与人性化交互设计，提升了老年人居家安全与情感连接水平，具有良好的应用前景。

Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.

</details>


### [32] [Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation](https://arxiv.org/abs/2511.15118)
*Jin Wang,Bingfeng Zhang,Jian Pang,Weifeng Liu,Baodi Liu,Honglong Chen*

Main category: cs.CV

TL;DR: 本文提出一种基于SAM的无偏语义解码（USD）策略，通过同时从支持集和查询集中提取目标信息，结合CLIP模型的语义对齐能力，增强SAM在少样本分割中的泛化性能。设计了全局补充（图像级）和局部引导（像素级）两种特征增强策略，分别提供类别通用指示和目标位置信息，并引入可学习的视觉-文本目标提示生成器，融合文本与视觉特征生成聚焦目标的提示嵌入，无需微调基础视觉模型即可实现更准确、无偏的目标区域定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖支持集提取提示，难以充分发挥SAM的泛化能力，且易因提示偏差导致对未知类别的适应性差。为提升少样本分割中SAM的鲁棒性和公平性，需引入更全面的语义指导机制。

Method: 提出无偏语义解码（USD）框架：1）利用CLIP模型进行全局语义补充，通过支持图像生成通用类别指示；2）在像素级进行局部引导，借助查询图像提供目标位置线索；3）设计可学习的视觉-文本目标提示生成器，融合文本嵌入与CLIP视觉特征生成高质量提示嵌入，引导SAM精准解码。

Result: 实验表明，该方法在多个少样本分割基准上显著优于现有方法，尤其在未知类别上的泛化性能提升明显，验证了其无偏性和强语义引导能力。

Conclusion: 所提出的USD策略有效缓解了传统方法对提示的依赖与偏差问题，通过联合利用支持集与查询集的语义信息，结合CLIP的跨模态对齐能力，实现了更鲁棒、更具泛化性的少样本分割结果，为基于SAM的少样本分割提供了新思路。

Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

</details>


### [33] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: WaveFuse-AL 是一种新型的主动学习框架，通过动态融合多种采集策略（如BALD、BADGE、熵、CoreSet），结合周期性时间先验和性能自适应机制，在医疗影像领域实现更高效的标注。在多个基准测试中，该方法显著优于单一策略和交替策略，有效提升有限标注预算下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法在不同学习阶段表现不稳定，单一策略难以适应整个学习过程的变化，因此需要一种能够动态调整策略权重的多策略融合框架以提高效率和稳定性。

Method: WaveFuse-AL 采用正弦周期性时间先验与性能驱动的自适应机制，动态调节多种经典主动学习策略的重要性，实现策略间的协同优化。

Result: 在 APTOS-2019、RSNA Pneumonia Detection 和 ISIC-2018 三个医学影像任务上，WaveFuse-AL 在十二项指标中的十项上均取得统计显著性优势，显著提升了标注效率和模型性能。

Conclusion: WaveFuse-AL 通过周期性与性能自适应的多策略融合机制，实现了对医疗影像主动学习中样本选择的高效优化，为降低标注成本提供了强有力的方法支持。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [34] [DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging](https://arxiv.org/abs/2511.15151)
*Meihua Zhou,Xinyu Tong,Jiarui Zhao,Min Cheng,Li Yang,Lei Tian,Nan Wan*

Main category: cs.CV

TL;DR: DCL-SE enhances neuroimaging analysis by combining efficient 3D-to-2D encoding with dynamic curriculum learning, achieving superior performance and interpretability on diverse clinical tasks.


<details>
  <summary>Details</summary>
Motivation: High-dimensional neuroimaging analyses face challenges in spatiotemporal fidelity and adaptability of large-scale models.

Method: Introduces Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), using Approximate Rank Pooling (ARP) to encode 3D brain data into 2D dynamic representations, and employs a Dynamic Group Mechanism (DGM) for progressive training.

Result: DCL-SE outperforms existing methods in accuracy, robustness, and interpretability across six datasets including Alzheimer's disease, brain tumor classification, cerebral artery segmentation, and brain age prediction.

Conclusion: Compact, task-specific architectures are crucial in the era of large-scale pretrained networks.

Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

</details>


### [35] [SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection](https://arxiv.org/abs/2511.15153)
*Chun-Jung Lin,Tat-Jun Chin,Sourav Garg,Feras Dayoub*

Main category: cs.CV

TL;DR: 提出SceneEdited数据集，支持城市尺度的HD地图维护研究，包含800+场景、23,000+合成对象变化，提供RGB图像、LiDAR扫描和变化掩膜，配套基准方法与工具包，实现3D点云更新的标准化评估。


<details>
  <summary>Details</summary>
Motivation: 现有2D图像基变化检测方法难以有效更新3D地图，缺乏真实城市变化数据集与统一评估标准，亟需支持3D地图动态维护的研究工具。

Method: 构建城市尺度的SceneEdited数据集，通过人工与自动方式合成2000+过时版本中的23,000+对象变化，涵盖道路设施、建筑、立交桥等典型修改；结合结构光重建（SfM）基础流程，提供可扩展、可追踪、可移植的更新工具链。

Result: 成功构建首个面向3D地图更新的城市级数据集，覆盖73公里行驶距离与约3平方公里城区，支持训练与评估变化检测与3D更新模型；工具包提升研究可复现性与可扩展性。

Conclusion: SceneEdited数据集与配套工具为3D高精地图维护提供了标准化基准，填补了从变化检测到3D更新之间的研究空白，推动自动驾驶与城市规划中动态地图更新的发展。

Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.

</details>


### [36] [Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance](https://arxiv.org/abs/2511.15164)
*Songze Li,Mingyu Gao,Tonghua Su,Xu-Yao Zhang,Zhongjie Wang*

Main category: cs.CV

TL;DR: 本文提出一种新方法来缓解多模态持续指令微调中的灾难性遗忘问题，通过利用参数空间的几何特性，以当前参数与先前最优参数之间的方向向量作为梯度引导，近似缺失的旧任务梯度。该方法结合有限回放缓冲区的真实梯度，并通过伯努利采样策略动态调节模型稳定性与可塑性，在不扩展模型规模的情况下实现领先性能，有效缓解遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在持续学习新任务时面临灾难性遗忘问题，即学习新任务导致旧任务性能下降。现有方法难以有效保留历史知识，亟需新的机制来缓解此问题。

Method: 提出将灾难性遗忘视为旧任务梯度缺失的问题，利用当前参数与历史最优参数间的方向向量作为梯度引导，近似缺失梯度；结合小规模回放缓冲区的真实梯度，并通过伯努利采样策略动态控制稳定性和可塑性。

Result: 在多个多模态持续指令微调数据集上的实验表明，该方法在不增加模型大小的前提下，显著提升了持续学习性能，有效缓解了灾难性遗忘，达到当前最佳水平。

Conclusion: 本文提出的基于参数空间几何特性的梯度近似方法，为多模态持续学习提供了一种高效、紧凑且有效的解决方案，显著提升了模型在持续任务学习中的知识保留能力。

Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

</details>


### [37] [Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation](https://arxiv.org/abs/2511.15167)
*Jing Cao,Kui Jiang,Shenyi Li,Xiaocheng Feng,Yong Huang*

Main category: cs.CV

TL;DR: 提出SEC-Depth框架，通过自演化对比学习提升自监督深度估计在恶劣天气下的鲁棒性，利用训练过程中的中间参数构建时序延迟模型，并设计自演化对比损失（SECL），以历史模型输出作为负样本，动态调整学习目标，有效感知天气恶化程度，无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计方法在雨、雾等恶劣天气下性能显著下降，因能见度降低严重影响深度预测准确性，亟需提升模型在复杂环境中的鲁棒性。

Method: 设计动态更新的延迟模型以捕捉训练阶段的优化状态；引入自演化对比损失（SECL），将历史延迟模型的输出作为负样本，实现自适应学习目标调整，隐式感知天气退化严重程度。

Result: 实验表明，该方法可无缝集成到多种基线模型中，在零样本评估中显著提升模型在恶劣天气下的深度估计鲁棒性。

Conclusion: SEC-Depth通过自演化对比学习机制，有效缓解了自监督深度估计在恶劣天气条件下的性能下降问题，具备良好的泛化能力和部署灵活性。

Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.

</details>


### [38] [MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction](https://arxiv.org/abs/2511.15179)
*Kyotaro Tokoro,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本文提出一种新的多模态人体运动预测评估指标MMCM，旨在解决现有方法在评估多模式预测时对分布覆盖性和运动有效性评估不足的问题。该方法通过聚类划分运动空间为多个模式，分别评估预测结果是否覆盖多个模式（覆盖性）以及是否符合真实可能的运动模式（有效性）。实验表明MMCM能有效准确地评估多模态预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法有效区分多模态预测中分布广泛但实际仅对应单一模式或不具运动学合理性的预测，导致评估失真。因此需要一种能同时衡量覆盖性和有效性的新指标。

Method: 提出基于聚类的多模态感知度量（MMCM），通过聚类将运动空间划分为多个模式以评估覆盖性，并利用真实数据集中的未来运动样本识别有效模式以评估有效性。

Result: 实验验证了聚类生成的模式定义合理，且MMCM能够准确评估多模态预测的质量，优于现有方法。

Conclusion: MMCM有效解决了传统指标在多模态人体运动预测评估中的局限性，可作为更可靠、更具判别力的评估工具。

Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM

</details>


### [39] [Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset](https://arxiv.org/abs/2511.15186)
*Geon Choi,Hangyul Yoon,Hyunju Shin,Hyunki Park,Sang Hoon Seo,Eunho Yang,Edward Choi*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于指令的胸部X光片病灶分割范式（ILS），并构建了首个大规模的指令-答案数据集MIMIC-ILS，包含110万条指令-答案对，覆盖7种主要病灶类型。通过全自动多模态管道生成标注，引入ROSALIA模型在该数据集上微调，实现根据简单指令进行病灶分割并提供文本解释，显著提升了分割与文本生成的准确性，验证了该方法的有效性与数据集的价值。


<details>
  <summary>Details</summary>
Motivation: 现有病灶分割模型在胸部X光片应用中受限于目标标签数量少以及依赖复杂专家级文本输入，难以实际应用。为克服这些限制，需要一种更简单、用户友好的分割方式。

Method: 采用全自动多模态管道，从胸部X光图像及其报告中自动生成标注，构建大规模指令-答案数据集MIMIC-ILS；在此基础上，训练视觉-语言模型ROSALIA以实现基于指令的病灶分割与解释生成。

Result: ROSALIA在新提出的任务中表现出高精度的病灶分割能力与准确的文本解释能力，证明了所提数据集和方法的有效性。

Conclusion: MIMIC-ILS作为首个大规模指令引导的胸部X光病灶分割数据集，为像素级病灶定位提供了重要基础资源，而基于其训练的ROSALIA模型展示了在临床实用场景中的巨大潜力。

Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.

</details>


### [40] [Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition](https://arxiv.org/abs/2511.15197)
*Raghu Vamsi Chittersu,Yuvraj Singh Rathore,Pranav Adlinge,Kunal Swami*

Main category: cs.CV

TL;DR: 本文提出了一种名为Insert In Style的零样本生成框架，首次实现了在风格化领域中插入真实世界物体时兼具实用性和高保真度。通过多阶段训练协议和专用掩码注意力架构，该方法有效解耦了身份、风格和组合特征，避免了通用模型中的概念干扰。模型基于一个新构建的10万样本数据集进行训练，该数据集通过大规模生成与双重过滤流程确保语义身份准确与风格一致。与以往方法不同，本方法无需文本提示且真正实现零样本推理。此外，作者还提出了一个新的公开基准用于评估风格化组合任务，并在多个指标上取得领先性能，用户研究也验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的对象组合方法在将真实世界物体插入风格化场景时表现不佳，现有解决方案要么是缺乏生成保真度的实用型‘混合器’，要么是需要不切实际的逐对象在线微调的生成器。因此亟需一种既能保持高保真度又具备实用性的零样本生成框架。

Method: 提出了一种统一框架，包含两个核心创新：(i) 多阶段训练协议，用于解耦身份、风格和组合表示；(ii) 专用掩码注意力架构，在生成过程中精确强制这种解耦，防止概念干扰。同时构建了一个新的10万样本数据集，采用大规模生成与双阶段过滤的数据流水线以保证质量。

Result: 在身份保留和风格一致性方面显著优于现有方法，用户研究进一步证实了其优越性。该框架为真实物体在风格化场景中的插入提供了首个真正零样本、高保真且实用的解决方案。

Conclusion: Insert In Style 是首个实现零样本、高保真且实用的真实物体风格化组合框架，通过解耦表示与专用注意力机制克服了传统方法的局限，推动了该领域的技术进步。

Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.

</details>


### [41] [Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval](https://arxiv.org/abs/2511.15201)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 本文针对食谱与食物图像在跨模态检索中的表示学习挑战，提出基于因果理论的建模方法。由于食谱与菜品之间存在因果关系，现有方法将食谱视为描述菜品视觉外观的文本源，会引入偏差，影响图像与食谱的相似性判断。本文识别出食材为混淆因子，并通过后门调整消除该偏差。通过因果干预重构模型，加入去偏项，显著提升检索性能，在Recipe1M数据集上达到MedR=1的最优表现，且在不同测试规模下均保持优异。此外，提出一个即插即用的多标签食材分类模块用于去偏，实现了新的最佳检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将食谱作为描述菜品视觉外观的文本源，忽略了食谱与菜品之间的因果关系，导致在跨模态表示学习中产生偏差，影响图像与食谱相似性判断的准确性。尤其当食物图像无法完全体现食谱中的所有细节时（如烹饪过程、摆盘方式、拍摄条件等），当前模型倾向于捕捉主导的视觉-文本对齐，而忽略决定检索相关性的细微差异。

Method: 基于因果理论建模食谱与菜品间的偏倚关系，识别食材为关键混淆因子，并采用后门调整进行去偏。通过因果干预重构传统食品到食谱的检索模型，引入额外项以消除潜在偏差。同时设计了一个可插拔的多标签食材分类神经模块，实现去偏功能。

Result: 在Recipe1M数据集上，所提方法在测试数据规模分别为1K、10K和50K时，均实现了MedR=1的检索性能，显著优于现有方法。新提出的去偏模块有效提升了检索准确率，达到了当前最优水平。

Conclusion: 本研究通过引入因果视角，揭示并解决了食谱与食物图像跨模态检索中的表示偏差问题。基于因果干预的模型重构与可插拔去偏模块，不仅提升了检索精度，还为未来跨模态学习提供了理论指导与实用工具。

Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.

</details>


### [42] [Physics-Based Benchmarking Metrics for Multimodal Synthetic Images](https://arxiv.org/abs/2511.15204)
*Kishor Datta Gupta,Marufa Kamal,Md. Mahfuzur Rahman,Fahad Rahman,Mohd Ariful Haque,Sunzida Siddique*

Main category: cs.CV

TL;DR: 本文提出了一种名为物理约束多模态数据评估（PCMDE）的新度量方法，旨在解决现有评估指标在领域特定或上下文依赖场景下无法准确捕捉语义和结构准确性的局限。该方法结合大语言模型的推理能力、基于知识的映射以及视觉-语言模型，通过三个阶段实现：(1) 利用目标检测和视觉-语言模型提取空间与语义特征；(2) 采用置信度加权组件融合进行自适应的组件级验证；(3) 借助大语言模型进行物理引导的推理，以强制执行结构与关系约束（如对齐、位置、一致性）。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标如BLEU、CIDEr、VQA分数、SigLIP-2和CLIPScore在处理领域特定或上下文依赖任务时，难以有效衡量语义和结构准确性，因此需要一种更精确、更具上下文感知能力的评估方法。

Method: 提出PCMDE框架，包含三个阶段：(1) 多模态特征提取，利用对象检测和视觉-语言模型获取空间与语义信息；(2) 置信度加权组件融合，实现组件级别的自适应验证；(3) 物理引导推理，通过大语言模型施加结构与关系约束，确保生成内容在物理合理性上的正确性。

Result: PCMDE在多个领域特定任务中表现出更高的语义与结构准确性评估能力，尤其在涉及物理合理性、空间关系和上下文一致性的场景中优于传统指标。

Conclusion: PCMDE通过整合大语言模型的推理能力、知识映射和视觉-语言模型，显著提升了多模态内容评估在复杂、上下文敏感场景下的准确性与鲁棒性，为未来多模态系统评估提供了新范式。

Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.

</details>


### [43] [SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning](https://arxiv.org/abs/2511.15242)
*Yuhao Shen,Jiahe Qian,Zhangtianyi Chen,Yuanhao He,Juexiao Zhou*

Main category: cs.CV

TL;DR: SkinGPT-R1 is a dermatology-focused vision-language model that enhances diagnostic reasoning through explicit, step-by-step chain-of-thought explanations. It leverages DermCoT, a curated corpus of 13,000 dermatologic cases with expert annotations, and introduces DermEval (a six-dimensional physician-aligned evaluation) and DermBench (a benchmark for reasoning quality). On DermBench, SkinGPT-R1 achieves an average score of 4.031/5, outperforming Vision-R1 by ~41% and ranking first among 14 models. It also shows consistent accuracy gains on dermatology classification tasks. Ablation studies confirm the value of both chain-of-thought supervision via DermCoT and dermatology-aware visual distillation.


<details>
  <summary>Details</summary>
Motivation: To improve transparency, interpretability, and accuracy in dermatological diagnosis by making AI's diagnostic reasoning explicit and verifiable, addressing the need for clinically meaningful chain-of-thought reasoning in medical vision-language models.

Method: SkinGPT-R1 is trained using DermCoT—a large-scale corpus combining 10,000 filtered DermEval cases and 3,000 certified dermatologist-scored cases—enabling structured chain-of-thought reasoning. The model is evaluated using DermEval (a six-dimensional clinician-defined metric) and DermBench (a benchmark for reasoning quality). Additional improvements are achieved through dermatology-aware visual distillation.

Result: SkinGPT-R1 achieves an average DermBench score of 4.031/5, ranks first among 14 models, and improves over Vision-R1 by ~41%. It consistently outperforms baseline models on three dermatology classification benchmarks. Ablation studies confirm significant gains from both chain-of-thought supervision and visual distillation.

Conclusion: SkinGPT-R1 demonstrates state-of-the-art performance in dermatology-specific vision-language reasoning, with enhanced interpretability and clinical relevance. The integration of domain-specific corpora (DermCoT), evaluation frameworks (DermEval/DermBench), and specialized training techniques enables robust, explainable dermatological diagnosis.

Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.

</details>


### [44] [SplitFlux: Learning to Decouple Content and Style from a Single Image](https://arxiv.org/abs/2511.15258)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Yongjun Zhang,Ziyang Chen,Shuting He*

Main category: cs.CV

TL;DR: 本文提出SplitFlux，通过分析Flux模型的特性，发现单个Dream Blocks对图像生成至关重要，且早期块控制内容、后期块控制风格。基于此，设计了两种关键组件：（1）秩约束适应，压缩更新秩并放大幅度以防止内容泄露；（2）视觉门控LoRA，根据图像显著性将内容LoRA分为高低秩分支，分别保留主体信息和残差细节，实现内容与风格的有效分离。实验表明，SplitFlux在多种场景下均优于现有方法，显著提升内容保真度与风格化质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDXL的方法在高质量图像生成方面存在局限，而Flux模型虽有潜力但因未充分探索其特性导致内容与风格分离效果不佳。因此，亟需一种能有效解耦内容与风格的新方法，以支持更灵活的图像定制生成。

Method: 系统分析Flux模型，识别出单个Dream Blocks的重要性及早期/后期块在内容与风格控制中的分工；提出SplitFlux框架，通过LoRA微调单个Dream Blocks，结合秩约束适应与视觉门控LoRA机制，实现内容与风格的精确分离与再嵌入。

Result: SplitFlux在多个评估指标上超越当前最优方法，在内容保持、风格迁移和跨场景泛化方面表现优异，尤其在复杂场景中展现出更强的鲁棒性和生成质量。

Conclusion: 本研究揭示了Flux模型中内容与风格的内在分离机制，并通过SplitFlux成功实现了高效的内容-风格解耦，为个性化图像生成提供了新范式。

Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.

</details>


### [45] [Graph Query Networks for Object Detection with Automotive Radar](https://arxiv.org/abs/2511.15271)
*Loveneet Saini,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 本文提出Graph Query Networks (GQN)，一种基于注意力机制的框架，用于3D雷达目标检测。GQN将雷达感知的对象建模为图结构，通过动态关注鸟瞰图（BEV）空间，构建对象特定图，并利用EdgeFocus和DeepContext Pooling两个新模块进行关系推理和上下文聚合。在NuScenes数据集上，GQN相对mAP提升最高达+53%，较最强前序雷达方法提升+8.2%，同时将峰值图构建开销降低80%，且计算成本适中。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格和序列的卷积与变压器检测器难以应对雷达因波长较长导致的稀疏不规则反射问题，因此需要一种能有效建模雷达特性并提取个体化关系与上下文特征的新方法。

Method: 提出Graph Query Networks (GQN)，通过图查询机制动态关注鸟瞰图空间，构建对象特定图；引入EdgeFocus模块进行关系推理，DeepContext Pooling模块实现上下文聚合。

Result: 在NuScenes数据集上，GQN的相对mAP提升最高达+53%，较最强前序雷达方法提升+8.2%；峰值图构建开销降低80%，计算成本适中。

Conclusion: GQN通过图结构建模与注意力机制，有效解决了雷达稀疏不规则反射带来的挑战，在性能与效率方面均显著优于现有方法，为360度汽车感知中的雷达目标检测提供了新范式。

Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.

</details>


### [46] [Taming Generative Synthetic Data for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.15299)
*Jialong Sun,Hongguang Zhu,Weizhe Liu,Yunda Sun,Renshuai Tao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本到图像生成的一阶段X-ray安全图像合成方法Xsyn，解决了以往两阶段方法中需人工提取前景带来的高成本问题。通过引入交叉注意力精炼（CAR）和背景遮挡建模（BOM）策略，提升了合成图像的质量与复杂性，无需额外人工劳动即可生成高质量图像，在多个数据集和检测器上均实现1.2% mAP提升，显著改善违禁品检测性能。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有X-ray安全图像合成方法多采用两阶段流程，需人工进行前景提取，耗时耗力且效率低；为解决数据不足问题，亟需一种高效、无需额外人工干预的合成方法。

Method: 提出一阶段合成框架Xsyn，结合文本到图像生成技术，引入交叉注意力精炼（CAR）策略以优化边界框标注，以及背景遮挡建模（BOM）策略在潜在空间显式建模背景遮挡，增强图像真实感与复杂性。

Result: 实验表明，Xsyn在多个X-ray安全数据集上优于现有方法，实现1.2% mAP提升，合成图像有效提升违禁品检测性能，且无需额外人工成本。

Conclusion: Xsyn是首个实现高质量无额外人力成本的X-ray安全图像合成方法，具有高效、实用性强的优势，为违禁品检测提供了有效的数据增强方案。

Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.

</details>


### [47] [Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language](https://arxiv.org/abs/2511.15308)
*Yan Xia,Letian Shi,Yilin Di,Joao F. Henriques,Daniel Cremers*

Main category: cs.CV

TL;DR: Text2Loc++ is a novel neural network for 3D point cloud localization using natural language, featuring a coarse-to-fine pipeline with advanced multimodal alignment via MIT and MHCL. It achieves up to 15% improvement on KITTI360Pose and shows strong generalization on a new city-scale dataset with diverse urban scenes and linguistic complexity. Code and data will be made public.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of localizing 3D point cloud submaps using complex natural language descriptions, aiming to improve cross-modal alignment between language and point clouds in diverse urban environments.

Method: Text2Loc++ employs a coarse-to-fine pipeline: it uses a pretrained language model with Hierarchical Transformer with Max pooling (HTM) for global semantic understanding and an attention-based point cloud encoder for spatial modeling. It introduces Masked Instance Training (MIT) to enhance robustness by filtering non-aligned objects and Modality-aware Hierarchical Contrastive Learning (MHCL) to strengthen embedding space alignment through multiple loss types. For fine localization, it adopts Prototype-based Map Cloning (PMC) and Cascaded Cross-Attention Transformer (CCAT), eliminating explicit text-instance matching.

Result: Experiments on KITTI360Pose show Text2Loc++ outperforms existing methods by up to 15%. The model also demonstrates strong generalization on a new city-scale dataset with diverse linguistic complexity and urban scenes.

Conclusion: Text2Loc++ effectively enables accurate and robust 3D point cloud localization from natural language descriptions, with significant improvements over prior work and strong adaptability across complex environments and linguistic expressions. The dataset and code will be publicly released.

Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.

</details>


### [48] [Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models](https://arxiv.org/abs/2511.15311)
*Mehran Tamjidi,Hamidreza Dastmalchi,Mohammadreza Alimoradijazi,Ali Cheraghian,Aijun An,Morteza Saberi*

Main category: cs.CV

TL;DR: Uni-Adapter 是一种无需训练的在线测试时自适应（TTA）策略，针对3D视觉-语言基础模型（VLFMs）在噪声、不完整或分布外数据下的性能下降问题。它通过动态原型学习构建3D缓存，存储并持续更新类别特定的聚类中心作为原型，利用相似性评分进行基于缓存的逻辑计算，并结合图结构标签平滑模块增强原型间的一致性。最终通过熵加权聚合原始模型与缓存预测，实现鲁棒自适应。该方法在多个3D基准上显著提升性能，如ModelNet-40C提升10.55%，ScanObjectNN-C提升8.26%，ShapeNet-C提升4.49%。


<details>
  <summary>Details</summary>
Motivation: 3D视觉-语言基础模型虽具备强泛化能力，但在实际场景中面对噪声、不完整数据或分布外样本时表现不佳，亟需一种无需重训练的高效自适应方法以应对数据异质性和分布偏移问题。

Method: 提出Uni-Adapter，采用动态原型学习机制，建立3D缓存存储类别原型，实时更新以捕捉类内变化；通过相似性评分计算缓存逻辑值；引入图基标签平滑模块建模原型间相似性，保证标签一致性；最后使用熵加权融合原始模型与缓存预测结果，实现可靠自适应。

Result: 在ModelNet-40C、ScanObjectNN-C和ShapeNet-C等基准上，相比原始3D VLFMs，分别提升10.55%、8.26%和4.49%，达到当前最佳性能，验证了方法在处理分布外数据和噪声数据中的有效性。

Conclusion: Uni-Adapter作为一种无需训练的测试时自适应方法，有效缓解了3D VLFMs在真实复杂环境中的性能退化问题，通过动态原型学习与多源信息融合实现了卓越的零样本适应能力，为开放世界点云处理提供了实用且高效的解决方案。

Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

</details>


### [49] [A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data](https://arxiv.org/abs/2511.15312)
*Mauro Larrat,Claudomiro Sales*

Main category: cs.CV

TL;DR: 本研究提出了一种新型多模态Transformer模型，融合雷达、可见光视频（RGB）、红外视频和音频数据，用于无人机检测与空中目标识别。该模型利用Transformer的自注意力机制，有效融合各模态特征，生成互补且高度判别性的表示，在独立测试集上达到0.9812的准确率、0.9873的召回率、0.9787的精确率、0.9826的F1分数和0.9954的特异性，尤其在区分无人机与其他空中物体方面表现优异。模型计算效率高，仅需1.09 GFLOPs、122万参数，推理速度达41.11 FPS，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 单模态方法在无人机检测与空中目标识别中存在局限性，亟需一种能融合多种传感器数据、提升鲁棒性和准确性的系统。

Method: 设计并评估一种多模态Transformer模型，通过自注意力机制融合雷达、可见光、红外视频和音频数据，学习跨模态互补特征表示。

Result: 在独立测试集上取得0.9812准确率、0.9873召回率、0.9787精确率、0.9826 F1分数和0.9954特异性，且在区分无人机与其他空中物体时表现出极高精度与召回率；模型计算量为1.09 GFLOPs，参数量122万，推理速度达41.11 FPS。

Conclusion: 本研究验证了基于Transformer的多模态数据融合在空中目标分类中的有效性，实现了当前最优性能，为复杂空域中的无人机检测与监控提供了高精度、强鲁棒的解决方案。

Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.

</details>


### [50] [IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers](https://arxiv.org/abs/2511.15369)
*Gihwan Kim,Jemin Lee,Hyungshin Kim*

Main category: cs.CV

TL;DR: 本文提出了一种无需重训练的新型后训练量化（PTQ）框架IPTQ-ViT，实现视觉变换器的全整数仅推理。通过引入基于多项式的GELU近似函数和基于位移的Softmax设计，提升非线性层在量化下的精度。同时提出统一度量标准，综合考虑量化敏感度、扰动和计算成本，以选择最优近似函数。实验表明，IPTQ-ViT在图像分类任务中平均提升1.78%准确率（最高达6.44%），在目标检测中提升1.0 mAP，性能优于现有部分浮点PTQ方法，并达到与整数仅QAT相当的精度与延迟水平。代码将开源。


<details>
  <summary>Details</summary>
Motivation: 现有量化感知训练（QAT）方法需昂贵的重训练来恢复非线性层量化带来的精度损失，限制其在资源受限环境中的应用；而现有后训练量化（PTQ）方法或仅部分量化非线性函数，或调整激活分布以维持精度，但无法实现完全整数仅推理。因此亟需一种无需重训练、支持全整数仅推理且保持高精度的PTQ方法。

Method: 提出IPTQ-ViT框架，包含：(1) 针对视觉数据优化的多项式基GELU近似函数；(2) 基于位移的Softmax设计，提高量化环境下近似精度；(3) 一个统一评估指标，融合量化敏感度、扰动影响与计算开销，用于每层激活函数选择最优近似方式。

Result: 在图像分类任务中，相比先前PTQ方法，IPTQ-ViT实现最高6.44%（平均1.78%）的top-1准确率提升；在目标检测任务中提升1.0 mAP。在W8A8和W4A8设置下，其性能优于部分浮点PTQ方法，并达到与整数仅QAT方法相当的精度与延迟表现。

Conclusion: IPTQ-ViT成功实现了无需重训练的全整数仅视觉变换器量化，在精度和效率上均显著优于现有PTQ方法，接近甚至媲美整数仅QAT效果，具备在资源受限设备上部署的潜力。代码已计划开源。

Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.

</details>


### [51] [Adaptive thresholding pattern for fingerprint forgery detection](https://arxiv.org/abs/2511.15322)
*Zahra Farzadpour,Masoumeh Azghani*

Main category: cs.CV

TL;DR: 本文提出一种基于自适应阈值模式的指纹伪造检测算法，通过多层小波变换与各向异性扩散处理输入图像，对不同层次系数进行自适应阈值化并拼接生成特征向量，再使用SVM分类器进行识别。该方法对像素缺失、块缺失和噪声污染等常见畸变具有较强鲁棒性，在90%像素缺失和70×70块缺失场景下，准确率分别优于现有方法约8%和5%，验证了其在抗干扰能力上的优势。


<details>
  <summary>Details</summary>
Motivation: 指纹活体检测系统易受伪造攻击，需开发能有效区分真实与虚假指纹的技术，尤其要求对噪声、像素或块缺失等畸变具有强鲁棒性，以防止伪造者通过添加畸变来欺骗系统。

Method: 采用各向异性扩散预处理输入指纹图像，经三层小波变换后对各层系数进行自适应阈值处理，将结果拼接为特征向量，并使用SVM分类器完成分类识别。

Result: 在90%像素缺失和70×70块缺失场景下，本方法准确率分别比现有方法提升约8%和5%，表现出更强的抗畸变能力。

Conclusion: 所提方法在复杂畸变环境下仍保持高检测精度，具备良好的实用性与抗欺骗能力，为指纹活体检测提供了有效的解决方案。

Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.

</details>


### [52] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 本文提出一种轻量级、模型无关的后处理框架，用于无人机导航中的开放集检测。该方法将背景与未知物体明确分离，实现对已知目标、分布外（OOD）物体和背景的实时三分类，优于传统基于阈值的二分类方法。通过融合多种置信度估计和检测特征，利用紧凑的多层感知机（MLP）提升性能，无需牺牲推理速度。实验表明，该方法在二分类任务中平均提升2.7% AUROC，同时在开放集mAP上保持或提升；在三分类任务中显著增强安全性，使无人机能有效规避未知物体并忽略背景。相比现有技术，其在多个数据集上的AUROC表现更优，并在封闭集mAP上最高提升9点（18%相对增益）。


<details>
  <summary>Details</summary>
Motivation: 现有开放集检测方法通常依赖单一不确定性分数进行阈值判断，灵活性差且易将分布外物体误判为背景，难以满足无人机导航中对未知物体主动避让的需求。因此亟需一种能区分已知目标、未知物体与背景的高效、鲁棒的三分类机制。

Method: 提出一种模型无关的后处理框架，通过融合多个置信度估计和每检测框的特征，使用小型多层感知机（MLP）进行三类分类。该框架不改变原始检测器结构，可兼容多种检测模型，支持实时运行。引入不同类型的logit变体以增强分类能力。

Result: 在二分类任务中，相比阈值基线平均提升2.7% AUROC；在开放集mAP上保持或提高；首次实现稳定可靠的三分类，适用于安全无人机导航。在多个数据集上超越对比方法，在AUROC方面表现更优，且封闭集mAP最高提升9点（18%相对增益）。

Conclusion: 所提方法实现了高效的三类开放集检测，有效分离未知物体与背景，显著提升了无人机导航系统的安全性与可靠性，具有良好的通用性和实用性。

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [53] [Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training](https://arxiv.org/abs/2511.15379)
*Yunjiao Zhou,Xinyan Chen,Junlang Qian,Lihua Xie,Jianfei Yang*

Main category: cs.CV

TL;DR: ZOMG 是一个零样本、开放词汇的框架，能够在无需标注或微调的情况下，将运动序列分割为语义有意义的子动作。它结合了语言语义划分和软掩码优化，利用大语言模型分解指令，并通过学习实例特定的时间掩码来聚焦关键帧，同时保持段内连续性和段间分离性。在三个运动-语言数据集上的实验表明，ZOMG 在运动定位性能上显著优于现有方法，在 HumanML3D 基准上提升了 +8.7% mAP，且在下游检索任务中也表现优异，为无标注运动理解树立了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖密集标注和预定义的动作类别，难以适应开放词汇、真实世界场景中的复杂人类活动理解需求。因此，亟需一种无需标注、可泛化到未见动作的运动分割方法。

Method: ZOMG 采用两个核心技术：(1) 语言语义划分——利用大语言模型将自然语言指令分解为有序的子动作单元；(2) 软掩码优化——在不修改预训练编码器的前提下，学习实例特定的时间掩码，以聚焦与子动作相关的关键帧，同时保证段内连续性和段间分离性。

Result: 在三个运动-语言数据集上的实验结果显示，ZOMG 在运动接地任务中达到领先性能，尤其在 HumanML3D 上比之前方法提升 +8.7% mAP。此外，在下游检索任务中也展现出显著改进，验证了其在无标注环境下的有效性与通用性。

Conclusion: ZOMG 成功实现了无需标注的开放词汇运动理解，提供了一种高效、灵活且可扩展的零样本运动分割框架，为行为分析、具身智能和虚拟现实等应用开辟了新路径。

Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

</details>


### [54] [Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models](https://arxiv.org/abs/2511.15390)
*Haidong Kang,Lihong Lin,Enneng Yang,Hongning Dai,Hao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AutoPrune的新颖剪枝方法，旨在让大语言模型（LLMs）能够自动设计最优剪枝算法，摆脱对专家知识的依赖。通过引入图驱动的思维链（GCoT）优化提示，提升了模型在生成剪枝算法时的推理能力与可解释性。同时，针对高剪枝率下因均匀稀疏性导致的异常值问题，提出了偏斜感知动态稀疏度分配（SDSA）机制，有效缓解性能下降。实验表明，AutoPrune在主流基准上持续优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法依赖人工设计，成本高且需专家知识；同时发现均匀稀疏性引发的异常值问题是性能下降的关键因素，亟需自适应稀疏度分配策略。

Method: 提出AutoPrune框架，利用大语言模型自身生成剪枝算法，结合图驱动的思维链（GCoT）提升提示优化与推理能力，并引入Skew-aware Dynamic Sparsity Allocation（SDSA）以应对异常值问题。

Result: 在多个主流大语言模型基准测试中，AutoPrune表现显著优于现有方法，实现了高效、可解释且鲁棒的自动剪枝。

Conclusion: AutoPrune首次实现了大语言模型自主设计剪枝算法的能力，克服了人工设计的局限性与异常值问题，为大模型轻量化部署提供了新路径。

Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

</details>


### [55] [ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation](https://arxiv.org/abs/2511.15396)
*Simon Boeder,Fabian Gigengack,Simon Roesler,Holger Caesar,Benjamin Risse*

Main category: cs.CV

TL;DR: ShelfOcc提出一种无需LiDAR的纯视觉3D占用估计方法，通过视频生成度量一致的语义体素标签，在真实3D空间中实现高质量监督，克服了传统2D投影方法的几何不一致和深度泄漏问题。该方法通过过滤与累积静态几何信息，有效处理动态场景，提升语义传播稳定性，并可适配任意先进占用模型架构。在Occ3D-nuScenes上性能相比先前弱监督/货架监督方法提升最高达34%，为无LiDAR的3D场景理解开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有自监督和弱监督占用估计依赖2D投影或渲染监督，存在几何不一致和深度泄漏问题；同时，现有视觉3D几何基础模型在动态驾驶场景中因几何稀疏、噪声和不一致而难以直接用于预测。因此需要一种能生成高质量、度量一致的3D监督信号的方法，以推动无需LiDAR的鲁棒3D场景理解。

Method: ShelfOcc利用视频序列生成度量一致的语义体素标签，将监督引入原生3D空间；通过滤除动态内容、跨帧一致性聚合静态几何信息，构建稳定的体素表示，并实现语义信息的有效传播。该方法不依赖额外传感器或人工3D标注，支持任意先进占用模型架构。

Result: 在Occ3D-nuScenes基准上，ShelfOcc显著优于所有先前的弱监督/货架监督方法，相对提升最高达34%，验证了其作为高质量数据驱动监督范式在无LiDAR场景下的有效性。

Conclusion: 高质量的3D监督对于鲁棒的占用学习至关重要。ShelfOcc通过将监督从2D投影转向真实3D空间，提供了一种无需LiDAR的高效数据驱动方案，是架构创新之外的重要补充方向，为未来无传感器3D感知提供了新思路。

Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.

</details>


### [56] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 本文提出一种简单且通用的后处理框架，用于生成具有分布无关、图像级控制的置信掩码，以限制假阳性预测的比例。该方法不依赖特定模型，无需重新训练，可为任意预训练分割模型提供有限样本下的统计保证。通过校准集选择合适的收缩参数，确保在新图像上假阳性比例低于用户设定的容忍度，具有高概率保障。实验验证了其在息肉分割任务中的有效性，适用于临床中过分割可能带来严重后果的场景。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像语义分割中广泛应用，但缺乏对预测错误的显式统计保证。特别是在临床决策中，假阳性可能导致误诊或过度治疗，因此需要一种能提供风险控制的方法。现有方法通常依赖于模型假设或大量数据，难以在小样本下提供可靠保证。

Method: 提出基于套索（conformal prediction）的后处理框架。利用预训练分割模型输出的分数或形态学腐蚀操作构造一系列嵌套的缩小掩码。通过一个标记的校准集，选择最优的收缩参数，使得在新图像上的假阳性比例以高概率低于用户指定的阈值。该方法不依赖模型结构，无需重新训练，且能在有限样本下提供分布无关的保证。

Result: 在息肉分割基准测试中，该方法实现了目标级别的经验有效性，即假阳性比例稳定控制在预设容忍度内。结果表明，该框架能够在不改变原模型的情况下有效提升分割结果的可靠性，支持临床应用中的风险感知决策。

Conclusion: 所提出的框架是一种模型无关、无需重训练的后处理方法，能够为医学图像分割提供可靠的置信区域，实现对假阳性的严格控制。它在实际临床环境中具有重要应用价值，尤其适用于对过分割敏感的医疗任务。代码已开源。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [57] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: D4C是首个专为CLIP设计的数据无量化（DFQ）框架，通过提示引导语义注入、结构对比生成和扰动感知增强，合成语义丰富且结构多样的伪图像，显著提升CLIP模型在低比特量化下的性能。在CIFAR-10、CIFAR-100和ImageNet-1K上，相比现有DFQ方法，其零样本分类准确率提升达12.4%~19.7%。


<details>
  <summary>Details</summary>
Motivation: 现有数据无量化（DFQ）方法在应用于视觉-语言模型（如CLIP）时表现不佳，主要因生成的伪图像语义信息不足且内部多样性低，限制了量化性能。因此亟需一种能生成高质量伪图像的专用DFQ框架。

Method: D4C包含三个核心组件：(1) 提示引导语义注入，利用文本提示对齐生成图像与真实语义；(2) 结构对比生成，通过前景-背景对比合成复现自然图像的组合结构；(3) 扰动感知增强，引入可控扰动提升样本多样性和鲁棒性。三者协同生成高保真伪图像以支持高效量化。

Result: 在多种比特宽度设置下，D4C显著提升CLIP模型的量化性能。例如，在W4A8设置下，对CLIP ResNet-50和ViT-B/32分别在CIFAR-10、CIFAR-100和ImageNet-1K上实现最高18.9%的Top-1准确率提升，验证了其有效性与泛化能力。

Conclusion: D4C首次成功将数据无量化技术扩展至视觉-语言模型，通过生成语义丰富且结构多样的伪图像，有效缓解了传统DFQ在跨模态场景中的性能下降问题，为隐私敏感环境下的模型压缩提供了新范式。

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [58] [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435)
*Linyin Luo,Yujuan Ding,Yunshan Ma,Wenqi Fan,Hanjiang Lai*

Main category: cs.CV

TL;DR: 本文提出一种新型层级化视觉攻击方法，针对多模态检索增强生成（MRAG）系统，通过在用户图像输入中添加不可察觉的扰动，实现对MRAG的视觉攻击。该方法不修改任何组件，仅通过扰动图像输入，破坏检索器与生成器之间的跨模态和语义对齐，从而误导检索结果并干扰生成内容。实验在OK-VQA和InfoSeek数据集上进行，使用CLIP检索器和BLIP-2、LLaVA作为生成器，结果表明该攻击显著降低检索与生成性能，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注知识投毒攻击，而本文关注更隐蔽的视觉攻击方式——仅通过在图像输入中添加不可察觉的扰动来攻击MRAG系统，挑战在于细调后的检索器和大规模生成器具有鲁棒性，且扰动在RAG链中可能被削弱。

Method: 提出层级化两阶段策略：第一阶段优化扰动以打破跨模态对齐，第二阶段进一步破坏多模态语义对齐；通过扰动图像输入，使检索器召回无关知识，从而生成被误导的输出。

Result: 在两个主流MRAG数据集（OK-VQA和InfoSeek）上的实验表明，该视觉攻击显著降低了检索准确率和生成质量，证明了攻击的有效性。

Conclusion: 本工作揭示了MRAG系统在视觉输入层面的脆弱性，提出了一种无需修改系统组件、仅通过图像扰动即可有效攻击的方法，为提升MRAG系统的安全性提供了重要启示。

Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.

</details>


### [59] [WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes](https://arxiv.org/abs/2511.15429)
*Marc-Emmanuel Coupvent des Graviers,Hejer Ammar,Christophe Guettier,Yann Dumortier,Romaric Audigier*

Main category: cs.CV

TL;DR: WarNav 是一个基于 DATTALION 开源数据集构建的真实世界数据集，专为在非结构化、战区受损环境中的自主地面车辆导航任务设计，填补了传统城市驾驶数据与危险战场环境之间空白。研究揭示了数据异质性与伦理挑战等方法论难题，并报告了多个先进语义分割模型在该数据集上的基线性能。同时提出一种无需目标图像标注即可提升导航能力的初步方案，旨在推动高风险场景下自动驾驶系统鲁棒性与安全性的研究，且注重减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集多集中于结构化城市环境，难以支持在战区等极端复杂环境中运行的无人系统开发。为应对这一差距，需建立专门面向冲突地区、非结构化地形的基准数据集，以促进更稳健、安全的自主导航技术发展。

Method: 从开源 DATTALION 项目中提取真实战场环境图像，构建 WarNav 数据集；采用多种主流语义分割模型进行训练与评估；分析不同训练数据来源对模型表现的影响；提出一种无需目标图像标注的导航策略，以降低标注成本并适应极端场景需求。

Result: WarNav 数据集成功建立，展示了其在复杂战场环境下的适用性；多个先进模型在该数据集上表现显著下降，表明当前城市模型泛化能力有限；所提无标注引导策略在一定程度上提升了模型在未标注环境中的可导航性。

Conclusion: WarNav 为研究极端环境下自动驾驶提供了重要资源，强调了在缺乏标注数据的情况下提升模型鲁棒性的必要性。未来工作应聚焦于少样本/无监督学习与跨域迁移机制，以增强无人系统在高危区域的安全部署能力。

Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.

</details>


### [60] [Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection](https://arxiv.org/abs/2511.15433)
*YiKang Shao,Tao Shi*

Main category: cs.CV

TL;DR: 本文针对多模态目标检测中的融合退化问题，提出了一种系统的理论分析，并揭示了两个关键的优化缺陷：1）多模态架构下单模态分支的梯度严重被抑制，导致单模态分支优化不足；2）模态质量差异导致弱模态遭受更强的梯度抑制，引发模态学习不平衡。为此，提出RSC-MD方法，包含表示空间约束学习（RSC）和模态解耦（MD）模块，分别用于增强被抑制的梯度、消除模态间耦合与不平衡，实现各模态分支的全面优化。在FLIR、LLVIP、M3FD和MFAD等多个数据集上实验验证了该方法能有效缓解融合退化并达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽关注多模态融合策略，但普遍忽视融合退化问题，且缺乏对其成因的理论分析，因此亟需从理论上揭示其根源并提出有效解决方案。

Method: 提出RSC-MD方法，包含表示空间约束学习（RSC）模块和模态解耦（MD）模块，分别解决梯度抑制与模态间干扰及不平衡问题，实现对各模态分支的全面优化。

Result: 在多个主流多模态目标检测数据集（FLIR、LLVIP、M3FD、MFAD）上，所提方法显著缓解了融合退化现象，取得了优于现有方法的检测性能，达到当前最佳水平。

Conclusion: 本文通过理论分析揭示了多模态检测中融合退化的根本原因，并提出RSC-MD方法有效解决了梯度抑制与模态不平衡问题，为多模态目标检测提供了新的优化思路与实用框架。

Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.

</details>


### [61] [RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection](https://arxiv.org/abs/2511.15476)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出一种混合深度学习方法RS-CA-HSICT，结合CNN与Transformer优势，用于增强MPox检测。该框架通过HSICT模块、残差CNN、空间CNN块和通道增强（CA）设计，有效提取多尺度特征、细节病变信息及长距离依赖关系。实验在Kaggle基准和多样化MPox数据集上均取得98.30%准确率和98.13%F1分数，优于现有CNN和ViT模型。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer在MPox检测中存在特征表达局限，难以同时捕捉局部细节与全局上下文，亟需一种融合二者优势的高效架构以提升检测性能。

Method: 提出RS-CA-HSICT框架，包含HSICT模块（集成抽象茎干CNN与定制ICT块）、残差CNN模块、空间CNN块与通道增强（CA），结合逆残差学习、分阶段分辨率降低实现尺度不变性；采用通道融合与注意力机制优化特征通道，空间注意力机制精细定位细微模式与类内对比变化。

Result: 在Kaggle和多样化的MPox数据集上，分类准确率达到98.30%，F1得分为98.13%，显著优于现有CNN和Vision Transformer模型。

Conclusion: 所提出的RS-CA-HSICT框架有效融合了CNN的空间感知能力与Transformer的全局建模能力，能够精准捕捉MPox病灶的多尺度结构特征与细微纹理差异，在复杂医学图像中展现出卓越的检测性能。

Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.

</details>


### [62] [Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels](https://arxiv.org/abs/2511.15496)
*Maria Pilligua,David Serrano-Lozano,Pai Peng,Ramon Baldrich,Michael S. Brown,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 提出MILL数据集，用于在多光照条件下评估低光增强算法，揭示现有方法在不同光照强度下的性能差异，并通过改进提升算法鲁棒性，实现高达10 dB PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的低光增强方法依赖于单一低光条件与正常光照的配对数据，缺乏辐射多样性，难以全面评估算法在不同光照条件下的表现。

Method: 构建MILL数据集，包含在受控条件下以固定相机设置和精确照度测量拍摄的多光照强度图像，并基于该数据集对现有方法进行基准测试与改进。

Result: 所提方法在全高清图像上对DSLR提升最高达10 dB PSNR，对智能手机提升2 dB PSNR，显著增强了算法在多样光照场景下的鲁棒性。

Conclusion: MILL数据集为低光增强算法提供了更全面的评估平台，所提出的改进策略有效提升了模型在不同光照条件下的性能稳定性。

Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.

</details>


### [63] [Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras](https://arxiv.org/abs/2511.15459)
*Ziyan Liu,Qi Su,Lulu Tang,Zhaofei Yu,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出EASD，一种端到端的基于脉冲相机的目标检测方法，通过双分支设计解决运动模糊和极端光照下的检测难题；引入DSEC Spike数据集填补脉冲数据与图像检测之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统图像检测在自动驾驶中受运动模糊和极端光照影响严重，而脉冲相机虽具备微秒级延迟和超高动态范围，但其稀疏离散输出难以被标准图像检测器处理，因此亟需一种能直接处理脉冲流的端到端检测方法。

Method: 提出EASD模型，包含两个分支：时间基纹理与特征融合分支用于全局跨切片语义理解，熵选择注意力分支用于捕捉目标中心细节。同时构建DSEC Spike仿真数据集，模拟驾驶场景中的脉冲数据。

Result: EASD在DSEC Spike数据集上表现出色，显著提升脉冲流目标检测性能，验证了双分支设计的有效性及端到端处理脉冲流的可行性。

Conclusion: EASD成功实现了对脉冲相机输出的端到端目标检测，解决了现有方法在高速运动和极端光照条件下的性能瓶颈，DSEC Spike为后续研究提供了重要基准。

Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.

</details>


### [64] [SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome](https://arxiv.org/abs/2511.15464)
*Dabin Jeong,Amirhossein Vahidi,Ciro Ramírez-Suástegui,Marie Moullet,Kevin Ly,Mohammad Vali Sanian,Sebastian Birk,Yinshui Chang,Adam Boxall,Daniyal Jafree,Lloyd Steele,Vijaya Baskar MS,Muzlifah Haniffa,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: Sigmma提出一种多模态对比对齐框架，通过多尺度对比对齐学习HE图像与空间转录组在多个尺度上的分层表示，有效捕捉细胞间相互作用，并在基因表达预测和跨模态检索任务中分别提升9.78%和26.93%。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅在单一尺度上对齐HE图像与空间转录组数据，忽略了细粒度细胞结构及其空间组织，因此需要更精细的多尺度对齐机制以提升跨模态表征能力。

Method: 提出多尺度对比对齐框架，将细胞相互作用建模为图结构，整合子图内与子图间关系，实现从细到粗的细胞间交互捕获，并确保不同尺度下跨模态表示的一致性。

Result: 在基因表达预测任务中平均提升9.78%，在跨模态检索任务中平均提升26.93%；且在下游分析中能有效揭示多组织层次结构。

Conclusion: Sigmma能够更准确地学习跨模态对应关系，显著提升对组织微环境复杂结构的理解能力。

Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.

</details>


### [65] [Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners](https://arxiv.org/abs/2511.15468)
*Xabier Lekunberri,Ahmad Kamal,Izaro Goienetxea,Jon Ruiz,Iñaki Quincoces,Jaime Valls Miro,Ignacio Arganda-Carreras,Jose A. Fernandes-Salvador*

Main category: cs.CV

TL;DR: 本研究针对金枪鱼捕捞中使用电子监控（EM）系统产生的大量视频数据，探讨了利用人工智能（AI）提升物种识别准确性和效率的可行性。重点分析了专家在区分大眼金枪鱼（BET）和黄鳍金枪鱼（YFT）时的一致性较低问题（分别为42.9%±35.6%和57.1%±35.6%），提出一种多阶段处理流程，结合YOLOv9与SAM2进行目标分割，通过ByteTrack实现个体追踪，并采用分层分类方法提升模型泛化能力。最终结果显示，该方法在验证集上达到0.66±0.03的mAP和0.88±0.03的召回率，整体分类准确率达84.8%，平均误差仅为4.5%。


<details>
  <summary>Details</summary>
Motivation: 当前电子监控系统生成海量视频数据，人工分析负担重且易出错；同时，不同金枪鱼种类间外观相似，导致专家识别一致性低，亟需高效、准确的自动化识别方法以支持渔业管理决策。

Method: 构建多阶段AI pipeline：首先使用三种分割模型（Mask R-CNN、DINOv2+SAM2、YOLOv9+SAM2）对图像中的鱼类进行分割；其次采用ByteTrack算法追踪个体；最后比较标准多分类模型与分层分类模型在物种识别上的表现。所有模型均经过交叉验证，并在已知真实组成的数据上测试。

Result: YOLOv9+SAM2组合在分割任务中表现最优，验证mAP为0.66±0.03，召回率为0.88±0.03；分层分类策略优于传统多分类模型，整体个体分割与分类准确率达84.8%，平均误差仅4.5%。

Conclusion: 结合YOLOv9与SAM2的分割框架及分层分类策略，可有效提升电子监控系统中金枪鱼物种识别的准确性与效率，具备在实际渔业管理中推广应用的潜力。

Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.

</details>


### [66] [FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI](https://arxiv.org/abs/2511.15481)
*Luisa Gallée,Yiheng Xiong,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: FunnyNodules 是一个参数化合成数据集，用于系统分析医学AI模型中的基于属性的推理。它生成具有可控视觉属性（如圆形度、边缘锐度、毛刺性）的肺结节样图像，目标类别由预定义的属性组合决定，实现对诊断决策规则的完全控制。该数据集支持模型无关评估，可检验模型是否学习到正确的属性-目标关系，解释属性预测中的过/欠表现，并分析注意力与特定属性区域的一致性。其完全可定制，提供完整真实标签，适用于开发、基准测试和深入分析可解释AI方法。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像数据集中缺乏密集标注，尤其是关于诊断背后推理过程的标注，这限制了可解释AI（xAI）模型的发展与评估。为使AI模型像放射科医生一样做出正确且合理的诊断，需要具备理解并模拟人类推理过程的能力，因此亟需一种能精确控制推理逻辑的数据集。

Method: 构建一个参数化合成数据集FunnyNodules，通过生成具有可调节视觉属性的抽象肺结节图像，设定基于属性组合的目标分类规则，实现对决策逻辑的完全控制。利用该数据集进行模型无关的评估，分析模型在属性识别、注意力分布与真实推理路径之间的一致性。

Result: FunnyNodules成功实现了对属性-目标关系的精确控制，能够有效评估模型是否基于正确理由做出判断；可揭示模型在特定属性上的性能偏差，并验证注意力机制是否聚焦于关键解剖区域。该框架具备高度灵活性，适用于多种复杂度和设置下的可解释性研究。

Conclusion: FunnyNodules作为一个全参数化、可定制的合成数据集，为可解释医学AI的研究提供了强有力的工具，不仅支持模型评估与调试，还推动了医疗AI向更透明、可信的方向发展。

Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.

</details>


### [67] [Learning to Expand Images for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.15499)
*Ruiqing Yang,Kaixin Zhang,Zheng Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Expanding Autoregressive Representation (EAR)的新生成范式，模仿人类视觉系统的中心向外感知模式，通过螺旋式从中心向外围展开图像标记，保持空间连续性并支持高效并行解码。同时引入长度自适应解码策略，动态调整每步预测的标记数量，提升灵活性与速度。实验表明，该方法在ImageNet上实现了单尺度自回归模型中保真度与效率的最佳平衡，为可扩展且符合认知的自回归图像生成指明了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有自回归图像生成方法存在效率低下问题，如逐标记解码或多层次表示的复杂性，亟需一种更高效、更符合人类感知规律的生成机制。

Method: 提出Expanding Autoregressive Representation (EAR)，采用螺旋式从中心向外展开图像标记序列，并设计长度自适应解码策略，实现并行高效生成。

Result: 在ImageNet数据集上，EAR在单尺度自回归模型中达到了最先进的保真度与效率权衡，显著降低计算成本并提升生成质量。

Conclusion: EAR通过生物启发的设计，实现了高效、高质量的图像生成，为未来自回归图像生成提供了新的可行路径。

Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.

</details>


### [68] [Scriboora: Rethinking Human Pose Forecasting](https://arxiv.org/abs/2511.15565)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 本文评估了多种人体姿态预测算法在绝对姿态预测任务中的表现，揭示了诸多可复现性问题，并提供了一个统一的训练与评估流程。通过类比语音理解任务，展示了近期语音模型可高效适配于姿态预测，从而提升当前最先进性能。此外，通过引入基于姿态估计器生成的带噪关节坐标的新数据集变体，评估了模型在真实噪声环境下的鲁棒性，发现估计姿态会导致显著性能下降，但可通过无监督微调部分恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有姿态预测方法存在可复现性问题，且缺乏统一的评估框架；同时，真实应用场景中姿态数据常受噪声干扰，亟需评估模型在噪声下的鲁棒性并提出有效应对策略。

Method: 构建统一的训练与评估管道；借鉴语音理解模型架构，将其适配至姿态预测任务；引入基于姿态估计器生成的带噪数据集变体，评估模型在真实噪声条件下的表现，并采用无监督微调进行性能恢复。

Result: 所提方法显著提升了姿态预测的性能，尤其在噪声条件下，通过无监督微调可有效缓解性能下降问题。

Conclusion: 语音模型架构可高效迁移至姿态预测任务，提升性能；统一评估框架有助于提高研究可比性；无监督微调能有效增强模型对现实噪声的鲁棒性。

Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.

</details>


### [69] [US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery](https://arxiv.org/abs/2511.15600)
*Miruna-Alexandra Gafencu,Yordanka Velikova,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 本文提出一种多模态深度学习方法，通过结合单张X射线图像与3D超声图像，实现对超声中被骨骼遮挡的椎体结构的完整重建。利用模拟数据进行训练，显著提升了椎体重建精度（p < 0.001），在无需术前CT配准的情况下，实现了更准确、完整的腰椎三维可视化，验证了该方法在临床转化中的潜力。


<details>
  <summary>Details</summary>
Motivation: 超声虽无辐射、成本低且可实时成像，但因骨性结构导致的声影效应，难以完整显示椎体等解剖结构。现有方法无法有效解决此问题，因此亟需一种融合互补信息的技术来增强超声的解剖可视化能力。

Method: 构建了一个多模态深度学习框架，利用单张侧位X射线图像提供椎体形态先验信息，与受限视野的3D超声数据联合训练，通过端到端学习完成被遮挡椎体的三维重建。训练数据由模拟的2D X射线图像和对应的3D部分椎体表示构成。

Result: 在体模实验中，所提方法在椎体重建上显著优于现有先进方法（p < 0.001），实现了无需注册即可将完整椎体结构精确叠加于超声图像之上，提升了脊柱手术中的实时导航精度。

Conclusion: 融合单张X射线投影信息可有效克服超声在椎体可视化上的关键缺陷，同时保留其实时、无辐射的优势，为术中影像引导提供了高效可行的新路径。

Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete

</details>


### [70] [Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector](https://arxiv.org/abs/2511.15571)
*Weiheng Zhu,Gang Cao,Jing Liu,Lifang Yu,Shaowei Weng*

Main category: cs.CV

TL;DR: 该论文提出了一种双域特征重要性攻击（DuFIA）方法，通过结合空间域和频域的特征重要性，生成对抗样本以削弱AI生成图像检测器的性能。该方法在多个检测器上表现出良好的跨模型迁移性、透明性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像检测器在干净条件下表现优异，但其在对抗性攻击下的安全性尚未得到充分研究，因此需要开发先进的对抗攻击方法来评估这些检测器的安全性。

Method: DuFIA利用空间插值梯度和频率感知扰动提取重要的取证特征，并联合建模空间与频域特征重要性，融合后指导基于优化的对抗样本生成。

Result: 实验结果表明，DuFIA在多种AIGI检测器上具有较强的跨模型迁移能力，且生成的对抗样本具有较高的透明性和鲁棒性。

Conclusion: DuFIA能够有效干扰AIGI检测器，验证了现有检测方法在对抗性环境下的脆弱性，为提升检测器安全提供了新的评估手段。

Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.

</details>


### [71] [AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning](https://arxiv.org/abs/2511.15578)
*Urjitkumar Patel,Fang-Chun Yeh,Chinmay Gondhalekar*

Main category: cs.CV

TL;DR: AVATAAR is a modular and interpretable framework for long-form video question answering, combining global and local context with a Pre Retrieval Thinking Agent and a Rethink Module. It uses a feedback loop to refine retrieval strategies and enable iterative reasoning, improving performance on CinePile across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Existing large vision language models struggle with nuanced queries requiring deep understanding and detailed analysis of long-form videos, necessitating a more robust and interpretable approach.

Method: AVATAAR integrates global video summaries, local context analysis, a Pre Retrieval Thinking Agent, and a Rethink Module with a feedback loop to iteratively refine retrieval and reasoning strategies.

Result: On the CinePile benchmark, AVATAAR achieves +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension compared to baseline.

Conclusion: AVATAAR effectively enhances video understanding through accuracy, interpretability, and scalability, demonstrating that iterative reasoning and feedback mechanisms are key to handling complex video QA tasks.

Abstract: With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.

</details>


### [72] [Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition](https://arxiv.org/abs/2511.15597)
*Xufei Wang,Junqiao Zhao,Siyue Tao,Qiwen Gu,Wonbong Kim,Tiantian Feng*

Main category: cs.CV

TL;DR: KDF+ is a novel continual learning framework for LiDAR place recognition that addresses catastrophic forgetting by introducing a loss-aware sampling strategy and a rehearsal enhancement mechanism. It prioritizes harder, more informative samples for replay and refines memory samples during new-task training, leading to improved knowledge retention and consistent performance gains across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR place recognition methods suffer from catastrophic forgetting when adapting to new environments, losing previously learned knowledge. This limits their applicability in dynamic real-world scenarios requiring continuous learning.

Method: KDF+ extends the KDF paradigm with two key components: (1) a loss-aware sampling strategy that selects samples based on their loss value—harder samples (with higher loss) are prioritized due to their discriminative potential; (2) a rehearsal enhancement mechanism that slightly reduces the loss of memory samples during new-task training, reinforcing long-term retention of past knowledge.

Result: Extensive experiments on multiple benchmarks show that KDF+ outperforms existing continual learning methods, achieves stable and significant performance improvements, and can be seamlessly integrated into state-of-the-art frameworks for LiDAR place recognition.

Conclusion: KDF+ effectively mitigates catastrophic forgetting in LiDAR place recognition through intelligent sample selection and enhanced memory rehearsal, enabling robust continual learning in real-world robotic and autonomous driving applications.

Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [73] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了在推理时通过分配额外计算资源来提升预训练大语言模型预测准确性的技术。文章重点分析了测试时扩展方法的分类，特别是问题如何被分解为子问题以及这些子问题的拓扑结构（顺序、并行或树状结构）。这一视角将Chain-of-Thought、Branch-Solve-Merge和Tree-of-Thought等不同方法统一在一个框架下。文章还整合了现有研究对这些技术的分析，指出了各自的优缺点，并展望了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 提高预训练大语言模型在推理阶段的预测准确性，尤其是在计算资源可扩展的情况下，探索更高效的问题求解策略。

Method: 通过分析问题分解方式与子问题拓扑结构（顺序、并行、树状），对测试时扩展方法进行系统分类与统一建模，涵盖Chain-of-Thought、Branch-Solve-Merge、Tree-of-Thought等典型方法。

Result: 成功将多种看似不同的推理增强方法纳入统一框架，揭示其内在共性；明确了各类方法的优势与局限性。

Conclusion: 未来研究应探索更智能的问题分解机制、动态拓扑调整策略以及高效计算资源分配方案，以进一步提升模型推理性能。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [74] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 该研究通过在推理过程的早期隐藏状态上训练线性分类器，探究大语言模型在链式思维（CoT）中何时内化最终答案。结果表明，即使需要较长输出才能得出确定答案，模型在仅经过几个推理步骤后，其正确性已高度可预测。对于较难的问题，预测准确率下降揭示了选择偏差：难题更可能出现在长链推理中。研究指出，推理模型的内部自我评估在少数几步后即已形成，这对可解释性和推理时控制具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在链式思维推理过程中何时内化最终答案，以理解其内部推理机制和决策时机。

Method: 在链式思维推理过程中，对前 t 个推理步骤后的隐藏状态训练线性分类器，分析其对最终答案正确性的预测能力。

Result: 模型在仅几个推理步骤后即可高度预测最终答案的正确性；对于难题，预测准确率下降，反映其在长推理中的集中分布，存在选择偏差。

Conclusion: 推理模型的内部自我评估在早期阶段（仅几步后）即已形成，这为模型的可解释性及推理过程的动态控制提供了重要启示。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [75] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出LiveCLKTBench自动生成管道，用于隔离和测量大语言模型中的跨语言知识迁移，通过真实世界的时间敏感知识实体生成多语言事实问题，评估跨语言迁移能力。实验发现跨语言迁移受语言距离影响，且方向不对称；模型规模虽提升迁移效果但边际收益递减，不同领域表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分跨语言知识迁移是源于真正的能力转移还是预训练阶段的暴露，需要一个能准确衡量跨语言知识迁移的基准工具。

Method: 设计LiveCLKTBench，从真实世界中识别时间敏感、自包含的知识实体，依据时间发生情况筛选并验证模型知识，生成对应事实问题后翻译成多种语言进行评估。

Result: 在五种语言上评估多个大语言模型，发现跨语言迁移受语言距离影响显著，具有方向不对称性；大模型虽提升迁移性能，但收益随规模增加而递减，且不同领域表现不一。

Conclusion: LiveCLKTBench为研究跨语言知识迁移提供了可靠基准，揭示了迁移机制的复杂性，有助于未来多语言模型的研究与优化。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [76] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: COMPASS 是一种轻量级、可解释的控制框架，通过在解码过程中嵌入基于模型的反馈回路，动态调节注意力头以增强生成内容的事实一致性。它引入了上下文依赖度评分（CRS）作为可解释的指标，实时监测模型对证据的依赖程度，并利用PID控制器调整注意力分配，从而显著降低幻觉率（绝对降低2.8至5.8个百分点），且无需重新训练或多次解码。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能生成流畅文本，但常因注意力分配不当而产生事实错误，尤其在结合上下文与参数知识时表现不佳。理解并调控这种内部行为对于提升模型可信度和科学可解释性至关重要。

Method: 提出COMPASS框架，利用上下文依赖度评分（CRS）作为在线探针，通过PID控制器动态调节注意力头，实现对生成过程的实时干预，确保输出与上下文证据对齐。

Result: 在HotpotQA、XSum、HaluEval、RAGTruth等多个基准测试中，COMPASS显著降低了上下文幻觉率（绝对下降2.8%至5.8%），同时揭示了不同注意力头在证据对齐中的作用，验证了反馈驱动可解释性的有效性。

Conclusion: 反馈驱动的可解释性为深入理解大语言模型的行为机制提供了新路径，使模型在保持生成质量的同时提升事实准确性，具备实际部署潜力。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [77] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 该研究探讨了在巴西葡萄牙语中，手动与自动韵律分割标注对非自回归语音合成模型FastSpeech 2性能的影响。结果显示，使用韵律分割训练可略微提升语音的可懂性和声学自然度；手动标注虽引入更多变异性，但更贴近真实语调，尤其在前重音轮廓上表现更优。所有数据集、代码和模型均已公开，便于复现与后续研究。


<details>
  <summary>Details</summary>
Motivation: 自发性对话中的语音合成面临诸多挑战，如对话轮换、停顿和不流畅现象。尽管现有语音合成系统在生成自然、清晰语音方面取得进展，但针对韵律特征（如音高、强度、时长）的显式分割标注数据集的构建及其对自发性语音合成的影响仍缺乏研究。因此，本文旨在评估不同韵律分割方式对语音合成质量的影响。

Method: 采用非自回归模型FastSpeech 2，基于巴西葡萄牙语的语料库，分别使用手动和自动方式进行韵律分割标注，并对比两种标注方式在语音合成中的表现。通过分析语音的可懂性、声学自然度及韵律结构（如重音模式和前重音轮廓），评估其效果。

Result: 实验表明，使用韵律分割训练的模型生成的语音在可懂性和声学自然度上略有提升。手动韵律分割虽然导致分段更不规则，但其带来的更大变异性使语音更具自然感；在中性陈述句分析中，两种方法均能再现预期的重音模式，但韵律模型更接近真实前重音轮廓。

Conclusion: 韵律分割标注有助于提升自发性语音合成的质量，尤其是手动标注因保留更多自然语调变化而更具优势。本研究为未来韵律建模与语音合成提供了有价值的实证支持与开放资源。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [78] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: 提出一种名为层级标记前置（HTP）的新方法，解决大语言模型在长文本嵌入中因注意力机制导致的信息流动受限和过压缩问题。HTP通过将输入分块并为后续块添加块级摘要标记，实现多路径反向信息传递；同时采用均值池化替代最后标记池化，缓解读出阶段的过挤压问题。该方法在11个检索数据集和30个通用嵌入基准上表现优异，尤其在长上下文场景下提升显著，且对零样本和微调模型均有效，具有架构无关性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的因果注意力机制限制了从后向前的信息流动，导致长文档表示质量下降；现有方法通过单个摘要标记前置虽缓解部分问题，但造成信息过度压缩，影响性能。

Method: HTP将输入文本分块，为每个块前置一个块级摘要标记，形成多条反向信息流路径；同时用均值池化替代传统的最后标记池化，以减少读出阶段的信息损失。

Result: HTP在11个检索数据集和30个通用嵌入基准上均取得一致性能提升，尤其在长上下文任务中表现突出，适用于零样本与微调模型，且不依赖特定架构。

Conclusion: HTP是一种简单、通用且高效的长文档嵌入优化方法，能够有效克服注意力机制与读出过程中的瓶颈，为高质量长文本表示提供了可扩展的解决方案。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [79] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 本文提出了一种数学基础框架，用于理解、衡量和缓解大语言模型（LLMs）的幻觉问题。通过概率建模、信息论、三角信号分析和贝叶斯不确定性估计，研究了错误在自回归过程中的累积机制，提出了语义和相位感知的不确定性度量，并开发了对比解码、检索增强校准、事实对齐和回避策略等原则性缓解方法。该框架统一整合了校准、检索与对齐的最新进展，旨在提升LLM的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽强大，但存在幻觉问题，即生成看似合理却事实错误的内容。现有方法缺乏系统性理论支持，亟需一种可量化、可解释且可操作的框架来应对这一挑战。

Method: 结合概率建模、信息论、三角函数信号分析与贝叶斯不确定性估计，构建多维度分析框架；提出新的不确定性度量（如语义与相位感知变体），并设计对比解码、检索增强、事实对齐与回避等策略以抑制幻觉。

Result: 所提框架能够有效识别和量化幻觉风险，显著降低生成内容的错误率；多种缓解策略在多个基准测试中表现优于现有方法，尤其在长文本生成和复杂推理任务中效果突出。

Conclusion: 本研究为理解与控制大语言模型的幻觉提供了统一的数学视角，推动了更安全、可信的生成式AI发展，具有重要的理论与应用价值。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [80] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: TASA 是一种学生感知的数学教学框架，通过整合学习者人格、记忆和遗忘动态，实现个性化教学。它利用结构化学生档案和事件记忆，结合连续遗忘曲线与知识追踪，动态更新学生的掌握状态，并生成难度适配的问题与解释。实验证明，TASA 在学习效果和自适应教学行为上优于现有基线方法，凸显了在基于大语言模型的辅导系统中建模遗忘与学习者画像的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的智能辅导系统未能充分捕捉学生知识随能力水平、概念性漏洞和遗忘模式动态演变的过程，尤其在数学教学中，缺乏精细的支架支持来匹配学生当前掌握程度和认知保留情况。

Method: TASA 框架通过维护包含熟练度特征的学生人格档案和记录过往学习交互的事件记忆，结合连续遗忘曲线与知识追踪机制，实时更新学生的学习状态，并据此生成针对性强、难度适中的问题与解释。

Result: 实验结果显示，TASA 在学习成效和教学适应性方面显著优于多个代表性基线模型，验证了其在建模遗忘过程和个体学习特征方面的有效性。

Conclusion: 在基于大语言模型的智能辅导系统中，引入对学生认知状态的动态建模（包括遗忘和个性化档案）对于提升教学效果至关重要，TASA 为此提供了有效解决方案。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [81] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的评估框架，用于在印度语言中评估多语言视觉-语言模型（VLMs），并生成了涵盖印地语和泰卢固语的HinTel-AlignBench基准数据集。该框架结合了反向翻译、过滤和人工验证，构建了包含约4000个问答对的综合性多模态数据集，覆盖科学、技术、数学及文化语境。实验发现，大多数SOTA模型在印度语言上的表现比英文任务平均下降8.3分（印地语）和5.5分（泰卢固语），揭示了多语言多模态理解中的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前多语言视觉-语言模型评估存在依赖未经验证的自动翻译、任务与领域覆盖有限、样本量小以及缺乏文化相关和本地来源的问答数据等局限性，尤其在低资源语言如印度语言中更为突出，亟需更可靠、全面且文化敏感的评估方法以推动公平人工智能的发展。

Method: 提出一种半自动化数据集构建框架，融合反向翻译、数据过滤与人工验证；基于该框架构建HinTel-AlignBench，整合英文基准（如VQAv2、RealWorldQA、CLEVR-Math）的适配版本与原创印地语数据集（如JEE用于STEM，VAANI用于文化背景），确保跨语言对齐与文化真实性。

Result: 实验表明，在5个任务中，4个任务的模型在印度语言上的表现均低于英文任务，印地语平均性能下降8.3点，泰卢固语下降5.5点；识别出常见失败模式，如文化语境误解、术语不一致、图像-文本关联偏差等，为未来模型改进提供具体方向。

Conclusion: 本研究不仅提供了首个系统性的印度语言多模态评估基准，也揭示了当前主流VLMs在低资源语言中的显著性能退化，强调了开发更具包容性与文化敏感性的多语言多模态模型的重要性，并为后续研究提供了可复现的评估框架与数据集。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [82] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次系统研究了大语言模型中固有维度（ID）与可解释文本属性的关系，通过交叉编码器分析、语言特征和稀疏自动编码器（SAEs）揭示：ID与熵类指标互补，不相关；不同文体呈现显著的ID分层（科学文本低ID~8，百科中等~9，创意/观点写作高ID~10.5）；SAE识别出因果特征——正式语气、报告模板、统计数据降低ID，个性化、情感、叙事增强ID，且可通过操控验证其因果性。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型固有维度（ID）的研究多关注其在训练动态、缩放行为和数据结构分析中的应用，但其文本决定因素尚不明确，亟需从可解释角度深入理解其背后机制。

Method: 采用交叉编码器分析、语言学特征提取与稀疏自动编码器（SAEs）结合的方法，对不同文本类型进行跨模型比较，并通过控制变量与转向实验验证因果关系。

Result: 发现ID与长度控制后的熵无相关性，体现其捕捉几何复杂性的独特能力；不同文体呈现稳定ID梯度；通过SAEs识别出影响ID的关键语义特征，并证实其因果效应。

Conclusion: 对于现代大语言模型而言，科学文本在表示上更简单，而小说、观点类及情感表达则需要更多表示自由度；本研究为正确使用和解读基于ID的结果提供了实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [83] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: OEMA提出一种基于多智能体协作的零样本临床命名实体识别框架，通过自注释器生成样本、判别器基于SNOMED CT过滤样本、预测器利用实体描述进行推理，在MTSamples和VAERS数据集上达到领先性能，接近监督模型表现。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵标注数据的依赖，解决现有零样本临床NER在示例选择粒度和提示与自我改进融合方面的挑战。

Method: 采用多智能体协作框架，包括自注释器、判别器（基于SNOMED CT）和预测器（使用实体描述），实现基于本体引导推理的零样本临床NER。

Result: 在MTSamples和VAERS数据集上，OEMA在精确匹配指标上达到最先进水平，在相关匹配指标上与监督模型BioClinicalBERT相当，并优于CRF模型。

Conclusion: OEMA通过本体引导推理和多智能体协作，有效解决了零样本临床NER的关键挑战，实现了接近监督学习的性能，展现出在临床自然语言处理中的巨大应用潜力。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [84] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: C3提出一种两级大模型文本压缩方法，通过小模型压缩长文本为少量隐向量（如32或64个），再由大模型解码，实现高达40倍的压缩比，且保持93%以上解码准确率，优于现有OCR压缩方案，表明纯文本压缩在长上下文处理中具有更高效率与潜力。


<details>
  <summary>Details</summary>
Motivation: 解决百万级token输入在长上下文任务中对LLM带来的计算与内存挑战，探索文本压缩的上限，提升长序列处理能力。

Method: 采用两级架构：第一阶段使用小型LLM将长文本压缩为少量隐向量（如32或64），第二阶段利用大型LLM对压缩后的隐向量进行解码，形成端到端的文本压缩-解码流程。

Result: 在20倍压缩比下达到98%解码准确率，40倍压缩比下仍保持约93%准确率，显著优于DeepSeek-OCR的约60%准确率，验证了该方法在文本压缩中的高效性与可行性。

Conclusion: C3展示了纯文本压缩在长上下文任务中的优越性能，为未来光学字符压缩、信息压缩等研究提供了新的方向和潜在的理论上限参考。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [85] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 本研究探讨了团队在BHASHA-任务1语法错误修正共享任务中对五种印度语言（泰卢固语、印地语、泰米尔语、马拉雅拉姆语和孟加拉语）的零样本/少样本提示语言模型的应用。通过使用不同规模的语言模型（40亿到大型专有模型），在泰卢固语中获得第4名，在印地语中获得第2名，GLEU得分分别为83.78和84.31。研究还扩展至其余三种语言，并深入分析数据质量和评估指标问题。结果表明小型语言模型具有潜力，同时强调了为印度语言脚本构建高质量数据集及合适评估指标的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有语法错误修正系统在印度语言上的表现受限于高质量数据和合适的评估指标。本研究旨在探索小规模语言模型在零/少样本设置下的有效性，并揭示数据与评价体系中的关键挑战。

Method: 采用零样本/少样本提示方法，测试不同规模的语言模型（从40亿参数到大型专有模型）在五种印度语言上的语法错误修正性能，涵盖泰卢固语、印地语、泰米尔语、马拉雅拉姆语和孟加拉语。通过实验对比模型表现，并对数据质量与评估指标进行深入分析。

Result: 在泰卢固语中取得第4名，GLEU得分为83.78；在印地语中取得第2名，GLEU得分为84.31。在其他三种语言（泰米尔语、马拉雅拉姆语、孟加拉语）上也展示了良好的性能，验证了小模型的有效性。同时发现当前数据集存在标注不一致、覆盖不足等问题，且评估指标（如GLEU）可能无法充分反映实际修正质量。

Conclusion: 小型语言模型在零/少样本设置下具备较强的语法错误修正能力，尤其适用于资源有限的印度语言。然而，构建高质量的数据集并设计适合印度语言脚本的评估指标仍是该领域亟待解决的关键问题。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [86] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 本研究针对阿拉伯语方言的情感分析挑战，利用SetFit框架在酒店评论数据集上进行少样本学习，实现了73%的F1分数，排名12/26，展示了少样本学习在处理特定领域方言文本中的潜力。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言存在语言多样性且标注数据稀缺，情感分析难度大，尤其在酒店等专业领域需要高效方法应对数据不足问题。

Method: 采用SetFit（句子嵌入微调）框架，一种数据高效的少样本学习技术，用于处理阿拉伯语方言的情感分类任务。

Result: 在官方评估集上取得73%的F1分数，位列26个参赛者中的第12名。

Conclusion: 少样本学习在解决阿拉伯语方言文本中数据稀缺问题方面具有显著潜力，尤其适用于酒店评论等特定领域。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [87] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2 是一个扩展和更新的西班牙语/英语医疗多选题推理数据集，涵盖十年的西班牙专业考试，超过12,000个问题。该研究评估了多种开源大模型在提示、检索增强生成（RAG）和概率选择策略下的表现，发现模型规模和内在推理能力是性能的主要驱动因素，复杂推理策略带来的增益有限。该数据集支持多语言研究，为生物医学推理和模型改进提供了可靠资源。


<details>
  <summary>Details</summary>
Motivation: 响应日益增长的对高质量数据集的需求，以捕捉医疗推理中的语言和概念复杂性，推动生物医学领域大模型的研究与改进。

Method: 扩展原始数据集至12,000+问题，涵盖十年西班牙专业考试；使用提示、检索增强生成（RAG）和概率答案选择方法，对多个开源大模型进行基准测试；提供多语言版本以支持未来研究。

Result: 模型性能主要受模型规模和内在推理能力影响，复杂推理策略带来的提升有限。HEAD-QA v2 在多语言环境下表现出良好的适用性，可作为生物医学推理研究的可靠基准。

Conclusion: HEAD-QA v2 是一个高质量、多语言、大规模的医疗推理数据集，能够有效支持大模型在生物医学领域的推理能力评估与优化，为未来研究提供坚实基础。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [88] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 本文提出一种合规性保留的检索系统，通过适应大语言模型重排序和语义搜索，解决航空维修运营（MRO）中技术人员因查找手册耗时长达30%的效率瓶颈。系统基于ATA章节层级构建版本鲁棒嵌入，并利用视觉-语言解析结构化认证内容，支持技术人员在不替换现有认证查看工具的前提下预览排序后的任务并访问验证过的程序。在4.9万个合成查询上的评估显示检索准确率超过90%，10名持证机务人员的双语对照研究显示前10名成功率90.9%，查找时间从6-15分钟减少至18秒/任务。结果表明，语义检索可在严格监管约束下有效运行，并显著降低真实世界多语言MRO流程中的操作负担。


<details>
  <summary>Details</summary>
Motivation: 航空维修运营中，机务技术人员需花费高达30%的工作时间查找手册，且所有程序必须可追溯至认证来源，形成效率瓶颈。现有系统难以在保证合规性的前提下实现高效信息检索，亟需一种既能提升检索效率又不违背监管要求的技术方案。

Method: 采用大语言模型重排序与语义搜索技术，结合ATA章节层级构建修订鲁棒的嵌入表示；利用视觉-语言模型对认证文档进行结构化解析；系统作为辅助工具与现有认证查看器协同工作，不替代其功能；允许技术人员预览排序结果，并在原视图中访问已验证程序。

Result: 在4.9万条合成查询上达到超过90%的检索准确率；10名持证机务人员参与的双语实验中，前10名检索成功率90.9%，平均查找时间由6–15分钟降至18秒，效率提升达95%。

Conclusion: 该系统证明了语义检索可在严格监管环境下有效部署，显著减轻真实世界多语言MRO工作流中的操作负担，为未来智能化维修支持提供了可行路径。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [89] [NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework](https://arxiv.org/abs/2511.15408)
*Shanlin Zhou,Xinpeng Wang,Jianxun Lian,Zhenghao Liu,Laks V. S. Lakshmanan,Xiaoyuan Yi,Yongtao Hao*

Main category: cs.CL

TL;DR: 提出NAMeGEn多智能体优化框架，解决中文婴儿命名中的多目标灵活性与解释复杂性挑战，构建17k+古典诗词语料库和CBNames基准，实验证明其在创意命名与解释生成上优于六种基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在创造性自然语言生成（CNLG）中面临多目标灵活性与解释复杂性的挑战，尤其在短文本生成任务中表现受限，需更精准满足个性化需求并提供有意义的美学解释。

Method: 提出NAMeGEn框架，通过迭代交替进行目标提取、名称生成与评估，结合古典诗词语料库提升审美质量，并引入新基准CBNames及定制化评价指标。

Result: NAMeGEn在中文婴儿命名任务中能有效生成符合多样化、个性化要求的创意名字，并提供准确有意义的解释，优于六种基于不同大模型架构的基线方法，且无需额外训练。

Conclusion: NAMeGEn框架为短形式创造性文本生成提供了高效解决方案，显著提升了生成内容的创意性与解释深度，具有广泛的应用潜力。

Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.

</details>


### [90] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: 本文介绍了SPRING实验室在ASRU MADASR 2.0挑战赛中开发的系统，专注于提升语音识别系统对8种语言33种方言的识别能力。团队参与了不使用额外数据且从零开始构建多语言系统的第1和第2赛道。提出了一种基于多解码器架构与音素公共标签集（CLS）作为中间表示的新训练方法，在CLS空间上优于基线模型，并探讨了如何在将结果转换回字符表示时保持音素空间中的性能增益。最终系统在第2赛道中3种语言的词错误率（WER）/字符错误率（CER）上超越基线，并在语言识别和方言识别准确率方面位居所有参赛团队首位。


<details>
  <summary>Details</summary>
Motivation: 提升语音识别系统在多语言、多方言环境下的性能，特别是在无额外数据条件下实现高效的语言和方言识别。

Method: 采用多解码器架构结合音素公共标签集（CLS）作为中间表示，通过新训练策略优化模型在音素空间的表现，并研究如何有效保留音素空间的优势以还原为字符表示。

Result: 在第2赛道中，3种语言的WER/CER低于基线；语言识别和方言识别准确率均达到所有参赛团队最高水平。

Conclusion: 所提出的多解码器与音素公共标签集结合的训练方法显著提升了多语言多方言语音识别系统的性能，尤其在无外部数据限制条件下表现出色，验证了该方法的有效性与实用性。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [91] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster 是一种全新的、完全基于大语言模型（LLM）的文本聚类框架，通过引入动态记忆（Dynamic Memory）实现状态感知，利用双提示策略（Dual-Prompt Strategy）让模型自主推理并确定聚类数量。该方法无需调参，显著优于现有基线，在多个基准数据集上表现出色，实现了真正端到端、可解释且高效的 LLM 原生聚类。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 在无监督文本聚类中的应用受限于缺乏状态记忆和难以控制聚类粒度，导致依赖复杂外部模块，难以实现真正的端到端聚类。

Method: 提出 LLM-MemCluster 框架，结合动态记忆以实现迭代优化中的状态保持，并采用双提示策略引导模型自主判断聚类数量，使聚类过程完全由 LLM 本体完成。

Result: 在多个基准数据集上，LLM-MemCluster 在无需任何调参的情况下，显著且一致地超越了强基线方法，展现出卓越的聚类性能与可解释性。

Conclusion: LLM-MemCluster 构建了一种真正端到端、高效且可解释的 LLM 原生文本聚类范式，为未来基于 LLM 的无监督学习提供了新方向。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [92] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了一种受脑成像数据结构（BIDS）启发的语言处理数据结构（LPDS），用于标准化语言数据的组织与共享，并开发了pelican nlp Python工具包，实现从数据清洗到复杂语言与声学特征提取的可重复流程。整个工作流可通过单一配置文件定义并执行，支持透明、可复现的语言研究。


<details>
  <summary>Details</summary>
Motivation: 当前语言处理研究面临数据组织不统一、方法缺乏标准化与可复现性的问题，亟需建立统一的数据结构与处理框架。

Method: 设计LPDS数据结构以规范语言数据存储格式，并开发pelican nlp工具包，支持模块化、可扩展的语言处理流程，通过配置文件驱动全流程自动化。

Result: 实现了语言数据从预处理到特征提取的端到端标准化流程，提升了研究的透明度与可复现性，支持跨研究共享与协作。

Conclusion: LPDS与pelican nlp共同构建了一个可推广、可复现的语言数据处理生态系统，为未来语言科学研究提供了坚实基础。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [93] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文提出Mera Multi，一个针对俄语的开源多模态评估框架，涵盖文本、图像、音频和视频四种模态，包含18项全新构建的评测任务，旨在填补俄语多模态基准的空白。该框架具有统一的多模态能力分类体系、符合俄语文化语言特性的数据集、标准化的提示与评估指标，并提供闭源与开源模型的基线结果。同时引入防泄露方法（如水印和许可证），为其他斯拉夫语系语言的多模态基准构建提供可复制范式。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型的研究虽进展迅速，但其智能水平、局限性和风险仍不明确，尤其在俄语等非主流语言中缺乏相应的多模态评估基准，限制了模型在本地化场景中的可靠评估与发展。因此亟需建立一个面向俄语的系统性多模态评测框架。

Method: 构建了一个基于指令的多模态评估框架Mera Multi，涵盖文本、图像、音频、视频四种模态；18个新任务从零构建，注重俄语文化与语言特性；采用统一的提示格式与评估指标；引入水印和授权机制防止基准泄露；并为闭源与开源模型提供基线性能。

Result: 成功构建了首个面向俄语的多模态评估基准Mera Multi，包含18个高质量、文化敏感的任务数据集；提供了多种模型的基线表现；验证了方法在防止数据泄露方面的有效性；为其他语言（尤其是斯拉夫语系）的多模态基准建设提供了可复现路径。

Conclusion: Mera Multi是首个专为俄语设计的多模态评估框架，不仅填补了该语言在多模态研究中的空白，还为全球范围内多语言、多模态基准的构建提供了通用方法论支持，具有重要的学术价值与应用前景。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [94] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: 本文提出HSKBenchmark，首个针对中文二语习得（SLA）阶段建模与写作评估的基准测试，涵盖HSK 3-6级，包含676万词的真实教材、1.6万条合成指令样本、30个测试主题及语言学基础评估体系。通过课程式微调框架模拟人类学习轨迹，评估语法覆盖率、写作错误、词汇与句法复杂度及整体评分。构建了基于1万份学习者作文微调的HSKAgent，实验表明其写作表现媲美高级人类学习者，并呈现类人习得特征。该基准与工具可推动LLM可解释性与语言习得建模研究。


<details>
  <summary>Details</summary>
Motivation: 当前语言习得研究受限于伦理与实践问题，难以控制人类学习者的语言输入，导致模型验证与扩展困难，尤其在中文二语习得领域缺乏系统性评估基准。

Method: 提出HSKBenchmark基准，整合真实教材、合成指令、测试主题与语言学评估体系；设计课程式微调框架，分阶段训练模型以模拟学习过程；构建HSKAgent并进行多维度评估。

Result: HSKBenchmark有效建模中文二语习得过程，支持动态写作评估；微调后的模型表现接近高级人类学习者，具备类人习得特征；所有资源已开源。

Conclusion: HSKBenchmark与HSKAgent为中文二语习得建模与大语言模型可解释性研究提供了可靠工具与基础资源，具有重要应用前景。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文提出FERMAT，一个用于建模概念发现和定理证明的强化学习环境，旨在自动化开放式的数学理论发现。研究重点包括通过进化算法自动评估数学对象的'有趣性'，并引入基于大语言模型的进化算法，实现对初等数论和有限域中非平凡有趣性度量的高效发现，优于硬编码基线。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 自动化发现新的数学理论是人工智能领域的一个重大挑战。当前缺乏有效工具来支持数学概念的发现与定理证明的系统性探索，因此需要构建可编程、可学习的环境以推动这一进程。

Method: 设计并实现了一个符号化动作的强化学习环境FERMAT，用于模拟数学概念发现与定理证明过程；采用进化算法（特别是基于大语言模型的算法）搜索非平凡的有趣性度量，并引入函数抽象机制提升搜索效率。

Result: 所提出的基于LLM的进化算法在初等数论和有限域中成功发现了具有实际意义的有趣性度量，性能显著优于硬编码基线，验证了方法的有效性与潜力。

Conclusion: FERMAT为数学理论发现提供了可扩展的强化学习框架，结合大语言模型与进化算法，在自动评估数学对象有趣性方面展现出强大能力，为未来人工智能驱动的数学研究奠定基础。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [96] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出一种全自动的LLM辅助工作流程，利用GPT-4o处理灾害事件的非结构化文本位置信息，并通过GADM、OpenStreetMap和Wikidata三个地理信息库进行交叉验证，自动分配地理边界并生成可靠性评分。该方法应用于2000至2024年EM-DAT数据集，成功对14,215个事件的17,948个独特地点完成地理编码，无需人工干预，覆盖所有灾害类型，支持多源交叉验证与灵活映射，为灾害风险评估提供可扩展、可靠的地理信息处理方案。


<details>
  <summary>Details</summary>
Motivation: 现有灾害数据库（如EM-DAT）中的位置信息常以非结构化文本形式存在，存在粒度不一致、拼写差异等问题，难以与空间数据集成，亟需一种自动化、高可靠性的地理编码方法以支持灾害风险评估与减灾决策。

Method: 采用GPT-4o解析非结构化文本位置信息，结合GADM、OpenStreetMap和Wikidata三个独立地理信息库进行交叉验证，根据一致性与可用性为每个位置生成可靠性评分，并自动赋予子国家层级几何形状。

Result: 成功对EM-DAT中2000至2024年间的14,215个灾害事件、17,948个唯一地点完成自动化地理编码；方法无需人工干预，覆盖所有灾害类型，支持多源验证与灵活框架映射，显著提升地理信息处理效率与可靠性。

Conclusion: 本研究展示了大型语言模型在从非结构化文本中提取与结构化地理信息方面的巨大潜力，提出了一种可扩展、高可靠性的自动化地理编码框架，适用于大规模灾害数据及其他类似场景的地理信息处理。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [97] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本研究通过创建并追踪名为Rachel So的完整人工智能学术身份，开展了一项行动研究，探讨学术界对人工智能作者身份的反应。Rachel So在2025年3月至10月间发表了10余篇论文，被引用并收到同行评审邀请，揭示了人工智能作者在出版、研究和科学体系中的影响，为未来学术传播中超级人类、超能力人工智能系统的角色提供了实证数据。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能在学术出版中的角色及其对现有学术生态的影响，特别是在作者身份认定与学术信任机制方面的挑战。

Method: 通过构建虚拟学术身份Rachel So，系统性地发布由AI生成的研究论文，并追踪其在学术界的接受度、引用情况及同行评审反馈，进行实证观察与分析。

Result: Rachel So成功发表多篇论文，获得引用，并收到同行评审邀请，表明当前学术系统对人工智能作者存在一定程度的接受与互动。

Conclusion: 该研究提供了关于人工智能作为学术作者的实证数据，凸显了现有学术体系在应对超能力人工智能时的适应性与潜在问题，呼吁对学术传播模式进行深入反思与制度调整。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [98] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出一种概率方法，用于量化自动驾驶系统训练与测试数据集在代表性方面的表现，重点关注其是否涵盖预期运行环境（TOD）中的各类场景。由于真实TOD分布未知，该方法采用不精确贝叶斯框架处理有限数据和先验不确定性，生成区间化的、带有不确定性的代表性估计，而非单一数值。通过数值示例展示了在天气、道路类型、时间段等操作类别下的局部与全局代表性评估，考虑了特征间的依赖关系和先验不确定性。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶车辆）的可信度与安全性依赖于训练和测试数据集的数据相关安全属性，尤其是代表性。然而，真实的目标运行域（TOD）分布通常无法完全获取，仅能从有限数据中推断，因此需要一种能够处理数据有限性和先验不确定性的方法来准确评估数据集对TOD的覆盖程度。

Method: 采用不精确贝叶斯方法，通过比较场景集与推断出的TOD之间特征分布的统计差异，量化代表性。该方法在先验信息不确定且数据有限的情况下，生成区间值的代表性估计，以反映不确定性。同时考虑特征之间的依赖关系，并在局部（类别间）和全局层面进行评估。

Result: 数值实验表明，该方法能够有效生成具有不确定性的代表性区间估计，在不同操作类别（如天气、道路类型、时间）上实现局部与全局代表性的量化分析，为评估数据集对目标运行域的覆盖能力提供了更稳健、透明的工具。

Conclusion: 本文提出的不精确贝叶斯方法为评估自动驾驶系统数据集的代表性提供了一种可靠、鲁棒且可解释的框架，尤其适用于真实世界中数据稀缺与先验不确定的情形，有助于提升AI系统的安全性和可信度。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [99] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本研究重新实现并评估了GeneGPT，采用开源模型（如Llama 3.1、Qwen2.5和Qwen2.5 Coder）构建单体架构，发现其局限性；在此基础上提出OpenBioLLM，一个模块化的多智能体框架，通过智能体专业化实现工具路由、查询生成与响应验证，提升协同推理能力。OpenBioLLM在超过90%的基准任务上表现优于或相当基因图灵和GeneHop基准，平均得分分别为0.849和0.830，且使用更小的开源模型，无需额外微调或特定工具预训练，同时降低40-50%延迟，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: GeneGPT虽有效整合领域API与大语言模型实现基因组问答，但依赖专有模型导致可扩展性差、成本高、数据隐私风险及泛化能力受限，亟需探索基于开源模型的替代方案以提升可持续性与安全性。

Method: 首先在单体架构中复现GeneGPT，使用Llama 3.1、Qwen2.5及Qwen2.5 Coder等开源模型；随后设计并实现OpenBioLLM，采用模块化多智能体架构，引入专用智能体负责工具路由、查询生成与响应验证，支持角色分工与协同推理。

Result: OpenBioLLM在基因组问答任务中表现优异，在Gene-Turing和GeneHop基准上分别达到0.849和0.830的平均得分，优于或相当GeneGPT，且使用更小的开源模型，无需额外微调或工具预训练；系统延迟降低40-50%，显著提升效率。

Conclusion: 开源多智能体系统在基因组问答中具有巨大潜力，OpenBioLLM展示了其在性能、效率与可扩展性方面的优势，为构建高效、安全、低成本的生物信息学智能系统提供了可行路径。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [100] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一种基于进展的行动与变化推理的神经符号框架，利用大语言模型（LLM）解决RAC问题。该方法提取问题中的动作和疑问，逐步执行动作以推导最终状态，并基于进展后的状态评估查询以得出答案。在多个RAC基准测试上的实验表明，ProRAC在不同基准、领域、LLM模型和任务类型上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理行动与变化推理（RAC）问题时存在对动态过程建模不足、难以精确追踪状态演变等问题，亟需一种能够有效结合符号推理与神经语言模型优势的新框架。

Method: ProRAC通过从问题中提取关键动作和查询，采用渐进式执行策略模拟动作序列对系统状态的影响，结合大语言模型进行状态演化推理，并最终对目标问题进行判断。

Result: 在多个标准RAC基准数据集上，ProRAC显著优于基线方法，在不同领域、不同类型的推理任务以及多种LLM架构下均展现出稳定且优异的表现。

Conclusion: ProRAC为解决复杂行动与变化推理问题提供了一种高效、可扩展的神经符号解决方案，验证了渐进式状态推理在增强大语言模型推理能力方面的有效性。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [101] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One is a multi-agent LLM framework for knowledge-informed automatic feature extraction, featuring Scientist, Extractor, and Tester agents that iteratively discover, generate, and validate features. It uses a qualitative feedback mechanism and 'flooding-pruning' strategy to balance exploration and exploitation, integrates external domain knowledge via RAG, and demonstrates superior performance on 28 datasets while generating interpretable, scientifically meaningful features.


<details>
  <summary>Details</summary>
Motivation: Existing AutoFE methods are limited by monolithic LLM architectures, simplistic feedback, and lack of integration with external domain knowledge, hindering both performance and interpretability.

Method: Rogue One employs a decentralized multi-agent system: the Scientist formulates hypotheses, the Extractor generates features using LLMs augmented with retrieval from external knowledge (RAG), and the Tester evaluates features using a rich qualitative feedback mechanism. The system dynamically balances exploration and exploitation via a 'flooding-pruning' strategy.

Result: Rogue One outperforms state-of-the-art methods on 19 classification and 9 regression datasets. It produces semantically meaningful, interpretable features and uncovers novel, testable scientific hypotheses—e.g., identifying a potential new biomarker in a myocardial dataset.

Conclusion: Rogue One advances automated feature engineering by integrating domain knowledge, enabling high-quality, interpretable, and scientifically actionable feature discovery through a collaborative, adaptive multi-agent framework.

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [102] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是首个端到端评估大型推理模型（LRM）安全性的基准，涵盖输入设计、细粒度推理过程分析和人类安全对齐验证，可全面识别推理过程中潜在的有害内容风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注输出层面，难以捕捉大型推理模型在推理过程中可能引入的渐进式或隐蔽性有害内容风险，因此需要一种能覆盖输入、中间推理和最终输出全过程的安全评估方法。

Method: 提出SafeRBench基准，包含三方面创新：（1）输入特征化，引入风险类别与等级，构建多样且平衡的提示集；（2）微思块分割机制，将长推理链拆分为语义连贯单元，支持十维度细粒度评估；（3）基于人类标注验证LLM评估的有效性，确保安全判断与人类认知对齐。

Result: 在19个大型推理模型上的实验表明，SafeRBench能够实现多维度、精细化的安全评估，揭示模型在不同阶段的潜在风险及防护机制，为模型安全性分析提供新视角。

Conclusion: SafeRBench为大型推理模型的安全评估提供了系统性框架，推动从静态输出评估向动态推理过程监控的转变，有助于提升AI系统的整体安全性与可信度。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [103] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK 是一种利用不确定性信号检测大型语言模型训练数据中是否包含受版权保护内容的新框架。它通过捕捉模型对已见（训练数据）与未见（非训练数据）内容的不确定性模式，将模型过自信的特性转化为优势。该方法采用两步策略：一是将文件分段以减少对大规模训练数据的依赖；二是使用基于不确定性的无监督聚类，避免依赖经验设定的阈值。实验表明，COPYCHECK 在 LLaMA 7b 和 LLaMA2 7b 上分别达到 90.1% 和 91.6% 的平均平衡准确率，相比现有最优方法提升超过 90%，最高达 93.8%。同时在 GPT-J 6B 上也表现出良好泛化能力。这是首次将不确定性应用于大模型版权检测的研究，为训练数据透明性提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有会员推断攻击（MIAs）在检测大模型训练中是否使用受版权保护内容时面临诸多挑战，包括模型过度自信、缺乏真实训练数据以及依赖经验阈值等问题。因此需要一种更可靠、无需依赖人工调参的方法来实现版权合规性检测。

Method: COPYCHECK 通过分析大模型对输入内容的不确定性输出，构建区分‘已见’与‘未见’数据的机制。具体包括：1）将输入文档分段处理，降低对大规模训练数据的依赖；2）采用不确定性引导的无监督聚类，自动识别训练数据片段，无需手动设定阈值。

Result: COPYCHECK 在 LLaMA 7b 和 LLaMA2 7b 上分别实现了 90.1% 和 91.6% 的平均平衡准确率，相较 SOTA 方法有超过 90% 的相对提升，最高达 93.8%。在 GPT-J 6B 上同样表现优异，展现出良好的跨架构泛化能力。

Conclusion: COPYCHECK 首次成功将不确定性信号应用于大模型训练数据中的版权内容检测，有效克服了传统方法的局限性，提供了一种高效、可扩展且无需人工调参的解决方案，为提升大模型训练数据透明性与合法性提供了重要工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [104] [SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making](https://arxiv.org/abs/2511.15202)
*Yinsheng Wang,Tario G You,Léonard Boussioux,Shan Liu*

Main category: cs.AI

TL;DR: SOLID integrates mathematical optimization with large language models (LLMs) for intelligent decision-making, using iterative collaboration via dual prices and deviation penalties. It maintains modularity, data privacy, and theoretical convergence under convexity. Evaluated on stock portfolio investment with historical prices and news, SOLID shows improved annualized returns over baseline methods.


<details>
  <summary>Details</summary>
Motivation: To enhance automated decision-making by combining the precision of mathematical optimization with the contextual understanding of LLMs, while preserving modularity and data privacy.

Method: SOLID employs a synergistic framework where optimization and LLM agents iteratively interact through dual prices and deviation penalties, enabling refined decision quality without compromising privacy or modularity.

Result: Empirical results on stock portfolio investment show convergence across scenarios and superior annualized returns compared to optimizer-only baselines, demonstrating the effectiveness of the integrated approach.

Conclusion: SOLID presents a promising, theoretically grounded framework for intelligent decision-making that leverages the strengths of both optimization and LLMs, with broad applicability across domains.

Abstract: This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.

</details>


### [105] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文指出，随着人工智能向复杂问题求解发展，仅靠效率提升已不足以实现可持续的推理型AI。由于计算投入呈指数增长且缺乏饱和点，必须引入显式限制以优化和治理此类系统。


<details>
  <summary>Details</summary>
Motivation: 当前推理型AI依赖于持续的指数级计算投资，而传统能源效率提升已接近物理极限，因此需要新的研究与政策方向来确保可持续性。

Method: 通过分析计算需求趋势、能源消耗模式以及现有系统的扩展瓶颈，提出应将显式限制嵌入到推理型AI的优化与治理框架中。

Result: 研究表明，单纯追求效率无法应对推理型AI的持续扩张；必须在技术设计与政策层面设定明确的资源使用边界。

Conclusion: 为实现可持续的推理型AI，必须在系统设计与治理中引入显式限制，避免无节制的计算扩张。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [106] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨了人工智能研究中两种关于智能的根本观念：智能现实主义（认为智能是可跨系统测量的单一普遍能力）与智能多元主义（认为智能是多样的、情境依赖的能力，无法简化为单一衡量标准）。这两种观念虽常隐含于研究中，却深刻影响着模型选择、基准设计、实验验证、对现象的解读以及对AI风险的理解。现实主义者将超级智能视为主要风险并寻求统一对齐方案，而多元主义者则强调各领域中的多样化威胁，需具体情境应对。明确这些假设有助于澄清当前AI研究中的分歧。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中关于智能的本质存在根本性分歧，但这些分歧往往未被明确指出，导致研究方法和结论的误解与争议。本文旨在揭示并分析这些隐含假设，以促进更清晰的学术对话。

Method: 通过分析当前AI研究中的争论，比较智能现实主义与智能多元主义在模型选择、基准设计、实验验证、现象解释及风险评估等方面的差异，结合具体案例进行论证。

Result: 发现两种智能观在方法论、解释力和风险评估上产生截然不同的研究路径和结论；明确这些假设有助于减少误解，推动更透明、一致的研究方向。

Conclusion: 智能的底层假设深刻影响着AI研究的方向与解释。通过显性化这些观念，可以增进对研究分歧的理解，促进更具建设性的学术讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [107] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova is a new comprehensive challenge environment (CCE) for reinforcement learning inspired by Civilization V, designed to simultaneously present multiple canonical RL challenges like partial observability and credit assignment. Unlike aggregated multitask benchmarks, it requires integrated, long-horizon reasoning across interacting variables.


<details>
  <summary>Details</summary>
Motivation: To create a more realistic and challenging testbed for RL that evaluates deep reasoning and integration of skills across multiple interacting problems, rather than just the ability to switch between unrelated tasks.

Method: Designing Terra Nova as a single, complex environment where multiple RL challenges co-occur naturally and interactively, inspired by the strategic depth of Civilization V.

Result: Terra Nova provides a platform for testing agents' ability to handle long-horizon planning and integrated decision-making in a rich, dynamic environment.

Conclusion: Terra Nova represents a significant step toward evaluating advanced RL capabilities by combining multiple core challenges in a unified, interactive setting.

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [108] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文提出IPR（交互式物理推理器），通过结合世界模型的滚动预测与视觉语言模型（VLM）的策略，实现从互动中学习物理和因果关系，以提升智能体的人类级推理能力。在涵盖1000多个异构游戏的G2U设置下，IPR在生存、好奇心、效用三个层次上表现优异，超越GPT-5在好奇心任务上的表现，并具备零样本迁移至未见游戏的能力。结果表明，更多训练游戏和交互步骤可进一步提升性能，验证了以物理为中心的交互是持续改进物理推理的有效路径。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察、互动和内化物理与因果关系进行学习。本文旨在探索智能体是否也能通过类似方式，从交互中获取类人推理能力，并随着经验积累持续提升。

Method: 提出IPR框架，利用世界模型的滚动预测来评估并强化VLM的策略，引入PhysCode作为以物理为核心的动作编码，使语义意图与动态行为对齐，建立统一的动作空间用于预测与推理。模型在超过1000个异构游戏中预训练。

Result: IPR在生存、好奇心、效用三个层级上均表现稳健，整体性能媲美GPT-5，且在好奇心任务上超越其表现；性能随训练游戏数量和交互步数增加而提升；具备零样本迁移至未见游戏的能力。

Conclusion: 以物理为中心的交互是实现智能体持续提升物理推理能力的有效途径，IPR框架为构建具有类人推理能力的智能体提供了可行方案。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [109] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 本文提出Transaction Intent Mining (TIM)框架，利用基于扎根理论的DeFi意图分类体系和多智能体大语言模型系统，通过元级规划器协调领域专家分解多视角意图分析任务，结合多模态链上/链下数据的问答求解器，并通过认知评估器减少LLM幻觉，提升可验证性。实验表明TIM显著优于传统机器学习模型、单个LLM及单智能体基线。研究揭示了意图推断的核心挑战，为理解DeFi用户动机提供了更可靠、上下文感知的解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解析DeFi交易中用户意图时缺乏深层语义理解，受限于智能合约交互复杂性、链上/链下因素多样以及二进制日志不透明等问题。因此亟需一种能深入理解用户行为动机的系统性框架。

Method: TIM框架采用基于扎根理论构建的DeFi意图分类体系，结合多智能体大语言模型系统；其中元级规划器动态协调多个领域专家，将复杂意图分析任务分解为可执行子任务；问题求解器利用多模态数据（链上与链下）完成具体分析；认知评估器用于抑制大模型幻觉并保障结果可验证性。

Result: TIM在多项指标上显著超越传统机器学习模型、单一LLM及单智能体基线；实验证明其在复杂交易意图识别中具有更强鲁棒性与准确性；同时揭示了当前意图推断中的关键挑战。

Conclusion: TIM框架为理解DeFi用户行为提供了更可靠、上下文感知的语义解释能力，推动了对复杂区块链活动背后用户动机的深入洞察，具备实际应用价值与研究意义。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [110] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该研究在实解析性假设下，分析了仅解码器的Transformer模型中，从离散提示到最后一层隐藏状态的映射性质。通过定义每层的碰撞判别集和可注入性流形，证明了要么模型在该集合上处处非单射，要么可注入性流形是开且稠密的，且所有映射均单射。在温和的非奇异优化器和绝对连续初始化条件下，这种泛用单射性在任意固定训练时长内沿平滑训练轨迹持续存在。此外，研究还考虑对称群作用，发现判别集与可注入性流形可自然下降至参数商空间，表明单射性是函数等价类的属性。实验部分引入分离边界与下利普希茨常数等几何诊断工具，基于大规模提示集的近邻统计估计，并应用于LLaMA-3、Qwen及小规模微调GPT-2模型。结果显示：全精度与8位量化下无碰撞现象，4位量化则引入少量碰撞并显著降低下利普希茨估计值；而微调的GPT-2模型在训练过程中归一化指标保持稳定。整体表明，在连续参数理想化下，Transformer表示具有普遍且持久的单射性，其实际可逆性可通过简单几何指标探测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示Transformer模型中从离散提示到最后一层隐藏状态的映射是否具有单射性，特别是在连续参数理想化下的普遍性和训练过程中的稳定性。同时探索如何通过可计算的几何指标评估模型的实际可逆性，为理解模型内部表示提供理论与实证支持。

Method: 基于实解析性假设，构建每层的碰撞判别集Δ^ℓ与可注入性流形U^ℓ = Θ\Δ^ℓ，利用代数几何与微分拓扑方法分析其结构。在非奇异优化器与绝对连续初始化下，结合光滑训练轨迹分析单射性演化。引入分离边界与下利普希茨常数作为层间几何诊断工具，通过大规模提示集的近邻统计进行估计，并应用于多种预训练与微调模型（如LLaMA-3、Qwen、GPT-2）进行实证分析。

Result: 理论上证明了在一般条件下，模型在有限提示集上的映射几乎处处单射；若非处处非单射，则可注入性流形为开且稠密，所有层映射均为单射。在平滑训练路径上，该性质可保持。实验显示：全精度与8位量化下无碰撞，4位量化引入少量碰撞且显著降低下利普希茨常数；微调的GPT-2模型中归一化指标稳定。说明模型表示在理想化下具泛用单射性，且可通过简单几何指标探测其实际可逆性。

Conclusion: Transformer模型在连续参数理想化下，其从提示到最后一层隐藏状态的映射具有普遍且持久的单射性。尽管实际量化可能引入少量碰撞并削弱表示的稳定性，但整体仍保持良好可逆性。该性质可通过分离边界与下利普希茨常数等几何诊断手段有效评估，为理解模型内部表征提供了理论与实践双重支撑。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [111] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的评估大语言模型（LLM）推理能力的方法，通过定义‘推导关系’（DR）和‘推导能力’（DC）来衡量模型在输入变化时是否能正确推导出输出的相应修改。为此，作者构建了名为DEVAL的系统性评估框架，并在七个主流任务中测试了五款主流LLM和一款大型推理模型。结果显示，尽管如GPT-4o和Claude3.5等模型具备一定DR识别能力，但在实际问题解决中应用能力显著下降。为此，提出一种新型提示工程方法——推导提示（DP），平均提升所有测试模型15.2%的DC表现，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在面对输入数据变化时，难以有效遵循抽象规则进行输出推导，缺乏对推导关系（DR）的系统性建模与评估。而人类在面对类似变化时可基于抽象规则调整输出，因此亟需建立一种能够衡量和提升模型此类推理能力的标准方法。

Method: 提出推导关系（DR）的形式化定义，引入推导能力（DC）作为评估指标；构建系统性评估框架DEVAL，涵盖多种任务场景；设计并验证推导提示（DP）这一新型提示工程策略以增强模型的推导能力。

Result: 主流大语言模型虽具备一定DR识别能力，但其在实际推理任务中的推导应用效果不佳；推导提示（DP）方法在所有测试模型上平均提升了15.2%的推导能力，显著优于传统提示方法。

Conclusion: 推导能力是衡量大语言模型推理性能的重要维度，现有模型在此方面仍有明显短板。通过引入推导提示（DP）可有效提升模型的推导能力，为未来模型优化提供了新方向。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [112] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出动态嵌套层次结构，以解决现有机器学习模型在非平稳环境中的适应性问题。通过借鉴神经可塑性，模型可自主调整优化层级、嵌套结构和更新频率，实现自进化与终身学习，克服传统模型的顺行性遗忘问题。理论分析包括收敛性、表达能力边界和亚线性后悔，实验验证其在语言建模、持续学习和长上下文推理任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在静态任务中表现优异，但在非平稳环境中因架构僵化而难以持续适应，缺乏真正的终身学习能力。尤其在面对分布漂移和上下文压缩需求时，传统方法易出现顺行性遗忘。因此需要一种能够动态调整自身结构和更新机制的新型学习范式。

Method: 提出动态嵌套层次结构，基于嵌套学习框架，允许模型在训练或推理过程中自主调节优化层级数量、嵌套方式及更新频率。该方法受神经可塑性启发，结合数学建模与理论证明，确保系统收敛性、表达能力与低后悔率，并通过多任务实证验证其有效性。

Result: 在语言建模、持续学习和长上下文推理任务中均表现出显著优于现有方法的性能；理论分析证明了模型的收敛性、表达能力上限以及在不同场景下的亚线性后悔；实现了对上下文流的动态压缩与分布漂移的自适应。

Conclusion: 动态嵌套层次结构是迈向自适应通用智能的重要一步，为实现真正意义上的终身学习提供了新范式，推动人工智能向更灵活、更具适应性的方向演进。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [113] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出GTPO算法，通过逐轮奖励分配、基于回报的优势估计和自监督奖励塑造，解决多轮工具集成推理中强化学习信号不足的问题，显著提升模型在复杂数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中因粗粒度的轨迹级奖励导致学习信号不足，引发训练停滞，亟需更精细的反馈机制。

Method: GTPO引入三个创新：(1) 逐轮奖励分配以提供细粒度反馈；(2) 基于回报的优势估计，用归一化折扣回报作为优势值；(3) 自监督奖励塑造，利用生成代码中的自监督信号增强稀疏的二元结果奖励。

Result: 在多个推理基准上，GTPO相比GRPO平均提升3.0%，验证了其在复杂数学推理任务中的有效性。

Conclusion: GTPO通过精细化奖励设计与自监督机制，有效缓解了多轮推理中强化学习的信号稀疏问题，为复杂推理能力的提升提供了新范式。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [114] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec 是一个基于 Transformer 的统一框架，旨在解决金融服务业中序列推荐的实时性、长时序用户交互和多产品协同等挑战。它在历史回测和线上 A/B 测试中均优于现有的树模型基线，支持跨产品信号共享，降低训练成本与技术债务，并提升整体性能，是首个兼顾技术和业务需求的金融领域统一序列推荐研究。


<details>
  <summary>Details</summary>
Motivation: 金融服务业中的序列推荐面临长时序用户行为（涵盖数字与实体渠道）、多产品间复杂关联及业务目标冲突等挑战，传统树模型虽可解释性强但难以满足高效建模需求，亟需更先进且可落地的解决方案。

Method: 提出 FinTRec 框架，采用 Transformer 架构实现统一建模，通过细粒度特征融合与跨产品信号共享机制，支持多产品协同推荐与灵活适配，同时具备良好的可扩展性与可维护性。

Result: FinTRec 在历史回测和线上 A/B 测试中持续超越生产级树模型基线，在多个产品上提升离线性能，减少训练成本与技术债务，实现跨产品协同优化。

Conclusion: FinTRec 证明了 Transformer 架构在金融服务业序列推荐中的可行性与有效性，为行业从传统树模型向现代深度学习模型演进提供了实证支持，具有重要的实践意义。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [115] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的深度强化学习方法，用于优化电动垂直起降（eVTOL）飞机的最小能耗起飞轨迹。该方法通过Transformer在每个时间步探索更真实的状态空间，显著降低了传统深度强化学习（DRL）的训练难度。实验表明，该方法仅需457万次时间步即可完成训练，相比传统DRL减少约25%；同时在能耗准确性上达到97.2%，优于传统DRL的96.3%。


<details>
  <summary>Details</summary>
Motivation: eVTOL飞机的发展为缓解城市交通拥堵提供了新机遇，但其起飞轨迹的最优设计面临传统控制方法维度受限和深度强化学习训练困难的问题，亟需高效且鲁棒的解决方案。

Method: 提出一种Transformer引导的深度强化学习框架，利用Transformer动态建模状态空间，提升智能体在复杂非线性系统中的学习效率与泛化能力，并应用于eVTOL起飞轨迹优化问题。

Result: 所提方法在训练效率上比传统DRL降低25%（4.57×10⁶ vs 19.79×10⁶时间步），能耗优化准确率达97.2%，优于传统DRL的96.3%，验证了其在训练效率与性能上的优越性。

Conclusion: Transformer引导的深度强化学习有效缓解了传统DRL的训练难题，在eVTOL最小能耗起飞轨迹设计中表现出更高的训练效率和更优的性能表现，具备实际应用潜力。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [116] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 本文提出LIT框架，通过强制大语言模型（LLMs）在可能的情况下调用外部更可靠的工具来解决任务，以提升其决策的可解释性和可靠性。该框架结合工具调用能力与自定义的可靠性成本函数，评估不同工具的可靠性与可调试性，并在包含1300个问题的基准数据集上验证了其有效性，显著提升了复杂任务中的可信度与可维护性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在多个领域表现出色，但其推理过程不透明，在高风险场景中难以保证解决方案的可靠性与可调试性。现有模型可能选择不可靠或难以排查的方案，即使存在更好的选项。因此需要一种机制让模型优先选择更可靠且易于追踪的解决方案路径。

Method: 提出LIT（LLMs with Inspectable Tools）框架，利用现有大语言模型的工具调用能力，设计一套可定制的可靠性成本函数，用于量化工具的可靠性与可调试性。通过多步工具调用实现复杂任务求解，并基于成本函数动态选择最优工具路径。

Result: 实验表明，使用LIT框架后，大语言模型在数学、编程和建模等任务中能够选择更可靠、更易调试的解决方案，同时保持甚至提升任务性能。在哈佛美国专利数据集和NeurIPS 2023论文数据集上的测试验证了该方法的有效性。

Conclusion: LIT框架有效增强了大语言模型在真实世界任务中的可信赖性与可解释性，为高风险应用提供了更安全的推理路径，是迈向可信AI的重要一步。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [117] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: Causal-GCN is a causal graph convolutional network that uses do-calculus-based back-door adjustment to identify brain regions with stable causal influence on Alzheimer's disease progression. It models MRI data as structural connectomes and adjusts for confounders like age, sex, and APOE4 genotype via principal components. The model simulates interventions on brain regions to estimate causal effects on disease probability, achieving performance comparable to standard GNNs while providing interpretable causal rankings consistent with known AD pathology.


<details>
  <summary>Details</summary>
Motivation: Existing deep graph learning models for Alzheimer's disease classification are often correlational and fail to disentangle demographic and genetic factors from disease-specific features, limiting interpretability and causal inference. This work aims to develop a method that identifies true causal brain regions influencing AD progression.

Method: Causal-GCN constructs a structural connectome from MRI data where nodes represent brain regions and edges represent anatomical connectivity. Confounders (age, sex, APOE4) are summarized using principal components and included in the causal adjustment set. The framework applies do-calculus-based back-door adjustment and simulates interventions by modifying incoming edges and node features to estimate average causal effects on disease probability.

Result: Causal-GCN achieves classification performance comparable to baseline GNNs but provides interpretable causal effect rankings. Key brain regions identified include posterior, cingulate, and insular hubs—regions known to be affected in Alzheimer's disease pathology—demonstrating alignment with established neurobiological knowledge.

Conclusion: Causal-GCN enables more reliable and interpretable causal inference in Alzheimer's disease classification by integrating causal modeling into graph neural networks, offering insights beyond correlation and supporting clinical relevance through biologically plausible findings.

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [118] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: This study compares four privacy-preserving training methods for clinical NLP, finding that knowledge distillation from DP-trained models best balances privacy and diagnostic accuracy, recovering up to 63\% of non-private performance at reasonable privacy budgets.


<details>
  <summary>Details</summary>
Motivation: Large language models trained on clinical text may expose sensitive patient information, and existing differential privacy (DP) methods often significantly reduce diagnostic accuracy, making them impractical for real-world deployment. However, it remains unclear which privacy-preserving strategy is most effective for clinical natural language processing tasks.

Method: The study conducts a systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries using identical 1B-parameter models and matched privacy budgets. The pipelines include direct DP-SGD, DP-synthetic data training, and knowledge distillation from DP-trained teachers, with performance evaluated on ICD-9 code prediction.

Result: At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance while maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5).

Conclusion: Knowledge distillation emerges as the most practical approach for achieving a favorable privacy-utility trade-off in privacy-preserving clinical NLP, highlighting significant differences in performance across different privacy-preserving architectures.

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [119] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) is a non-parametric framework that uses prototype nodes with reliability indicators and relational edges to improve embedding-based inference. It unifies retrieval, reasoning, and label propagation, offering better calibration, smoother decision boundaries, and efficiency with fewer samples compared to kNN and Label Spreading.


<details>
  <summary>Details</summary>
Motivation: To enhance non-parametric learning by incorporating structured memory that captures both local evidence and global consistency through reliable prototypes and relational structure.

Method: GM summarizes the embedding space into prototype nodes annotated with reliability and connected via edges encoding geometric and contextual relations. It enables unified instance retrieval, prototype-based reasoning, and graph-based label propagation in an inductive model.

Result: GM achieves accuracy comparable to kNN and Label Spreading but with superior calibration, smoother decision boundaries, and requires an order of magnitude fewer training samples.

Conclusion: By explicitly modeling reliability and relational structure, GM provides a principled approach to bridge local evidence and global consistency in non-parametric learning.

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [120] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast is a deep learning framework using graph-based spatiotemporal models to forecast global ionospheric Total Electron Content (TEC), improving accuracy over persistence methods by integrating diverse physical and observational data, thus enhancing space weather forecasting and operational resilience.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of ionospheric variability is crucial for GNSS accuracy, high-frequency communications, and aviation safety; existing models lack sufficient integration of heterogeneous data and advanced spatiotemporal learning.

Method: IonCast employs a GraphCast-inspired deep learning model that leverages spatiotemporal learning to forecast TEC, incorporating diverse physical drivers and observational datasets through scalable graph-based architecture.

Result: IonCast demonstrates superior performance compared to persistence models during both storm-time and quiet conditions, showing enhanced forecasting skill and improved integration of heterogeneous data.

Conclusion: IonCast exemplifies how machine learning can enhance physical understanding of ionospheric dynamics and support operational space weather resilience by enabling more accurate and robust global TEC forecasting.

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [121] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 本文构建了一个动态时间序列模拟环境，用于研究智能辅导系统（ITS）在学生个性化学习中的应用。通过引入不同级别的探查干预（如测验、辅导等），系统在获取学生状态信息的同时权衡信息收集成本与教学干扰之间的平衡。研究比较了强化学习（RL）算法与基于规则的启发式方法，发现两者表现相近，但RL在处理复杂隐藏信息时面临挑战，尤其在面对较难学生群体时效果下降。此外，研究还验证了探查干预对提升课程结构中测验和期中考试成绩的显著作用，而对仅依赖期末考试的结构帮助较小，凸显了持续信息反馈的价值。


<details>
  <summary>Details</summary>
Motivation: 传统智能辅导系统虽然能利用历史数据进行个性化教学，但每个学生具有独特性，且学习过程本身部分不可观测，导致建模困难。因此，需要一种能够动态适应个体差异并有效获取学生状态信息的方法，以提升教学效果。

Method: 设计一个模拟课堂的动态时间序列环境，包含学生-教师交互（如授课、辅导、考试）。采用强化学习方法，结合个体学生状态估计与群体数据，通过探查干预（如测验）来提高对学生状态的准确性，同时考虑干预带来的认知负荷与教学成本。对比多种强化学习算法与规则基启发式策略的表现。

Result: 强化学习与启发式方法在性能上接近，但强化学习在面对高难度学生群体时表现不佳；探查干预显著提升了学生学习效果，尤其是在有频繁评估的课程结构中；不同课程结构下，非探查政策的效果受限，说明持续反馈的重要性。

Conclusion: 探查干预机制对于提升智能辅导系统的有效性至关重要，它能够在信息获取与教学干扰之间取得平衡。尽管强化学习具备灵活性，但在复杂场景下仍存在局限性。未来应探索更鲁棒的自适应策略以应对多样化的学习者群体。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [122] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新的框架，用于在有限输入长度下实现心律失常的检测与分类。该框架结合局部与全局特征提取，并通过注意力机制融合信息，有效处理心律失常发作时间不一的问题。在MIT-BIH和AFDB数据库上的实验表明，该方法在10类和4类心律失常检测中均表现出优异性能，F1分数分别达到96.45%、82.05%、96.31%（MITDB）和97.57%、98.31%、97.45%（AFDB），显著优于基准模型。此外，跨数据库测试验证了其良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统心律失常分类方法未充分考虑心律失常发作时间的差异性，且受限于输入长度，难以准确捕捉发作起止点及持续时间，因此需要一种能够兼顾局部细节与全局动态的新方法。

Method: 提出一个包含局部-全局特征提取与注意力融合机制的框架，通过双路径结构捕获心电图信号中的局部细节与整体趋势，利用注意力机制增强关键区域的信息表达，从而在有限输入长度下实现精准的心律失常检测与分类。

Result: 在MITDB和AFDB数据集上，该方法在持续时间、事件识别和Dice评分方面均取得高F1分数，分别为96.45%、82.05%、96.31%（MITDB）和97.57%、98.31%、97.45%（AFDB），显著优于现有模型；跨数据库测试也显示优越的泛化性能。

Conclusion: 所提出的框架能有效融合局部与全局信息，在不丢失重要特征的前提下实现高精度的心律失常检测与发作时间定位，有助于提升临床诊疗方案的准确性。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [123] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 本文提出了一种新型稀疏矩阵乘法算法，成功将自组织映射应用于整个Medline数据集，克服了以往因内存和计算需求激增而只能处理小规模数据的限制，实现了对现有医学知识更完整的地图绘制，并提升了模型随数据变化动态更新的可行性。


<details>
  <summary>Details</summary>
Motivation: 以往的Medline数据库映射方法受限于算法的内存和计算需求，仅能处理数据的小部分，无法全面反映医学知识。

Method: 设计并应用一种新型稀疏矩阵乘法算法，支持对整个Medline数据集进行自组织映射。

Result: 成功实现对整个Medline数据集的自组织映射，构建了更完整的医学知识图谱，并增强了模型随时间更新的能力。

Conclusion: 该算法显著提升了大规模医学知识映射的可行性和效率，为未来动态知识发现提供了有力工具。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [124] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 本文提出GRPO-Verif算法，通过统一损失函数联合优化大语言模型的解题生成与自我验证能力，并引入可调节超参数控制验证信号权重。实验表明该方法有效提升了模型的自我验证能力，同时保持了推理性能的稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的推理能力已通过强化学习显著提升，但其在一致性地验证自身推理过程方面仍存在困难，因此需要研究如何增强模型的自我验证能力及其对推理性能的影响。

Method: 提出GRPO-Verif算法，将解题生成与自我验证联合优化于一个统一的损失函数中，通过可调节的超参数控制验证信号的重要性。

Result: 实验结果显示，该方法显著增强了模型的自我验证能力，同时在推理性能上保持了相当的表现。

Conclusion: 通过联合优化生成与验证过程，GRPO-Verif能够有效提升大语言模型的自我验证能力，且不损害其推理性能，为增强模型可靠性提供了新思路。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [125] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性与跨模态一致性来增强对标签噪声的鲁棒性。该方法在脑电（EEG）和面部特征之间进行表示对齐，以确保模态间的语义一致性，并将残差不一致视为噪声引起的偏差，从而选择性地查询这些样本以获取人工反馈，提高模型在噪声标签下的性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签常基于主观且不一致的报告，导致情绪解码困难，因此需要一种能够有效处理标签噪声并提升数据效率的方法。

Method: 提出了一种不确定性感知的主动学习框架，结合模型不确定性与跨模态一致性；引入表示对齐模块将EEG和面部特征嵌入共享潜在空间，通过残差分析识别噪声引起的不一致，进而选择性地查询高信息量样本以优化模型。

Result: 在ASCERTAIN数据集上的实验表明，该方法在数据效率和噪声容忍方面表现优异，具有良好的鲁棒性和实用性，适用于脑机接口系统中的情绪解码任务。

Conclusion: 所提出的框架能有效缓解标签噪声问题，提升模型在低质量标注环境下的表现，为基于EEG的情绪识别提供了一种高效、稳健的解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [126] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文研究了复数变分自编码器（Complex VAEs）中的几何结构，发现其在特定条件下呈现凯勒（Kähler）几何特性。通过引入复高斯潜变量的正则化，推导出复数情况下的Fisher信息度量，并证明其与相对熵（KL散度）的海森矩阵一致。提出一种近似等价于Fisher信息度量的凯勒势函数，该函数为强拟凸（PSH）函数，显著降低大规模自动微分的计算负担。基于此，实现了对解码器几何的正则化，支持按加权复体积元素采样，实验表明该方法在牺牲少量样本多样性的情况下，能生成更平滑的表示并减少语义异常点。


<details>
  <summary>Details</summary>
Motivation: 现有变分自编码器（VAE）在实数潜空间中已知具有黎曼几何结构，但复数潜空间中的几何性质尚未充分探索。为提升复数VAE的表征能力和生成质量，需理解其内在几何结构，特别是与信息几何相关的凯勒结构。

Method: 采用复数潜变量建模，基于复高斯先验和零协方差关系，推导复数情形下的Fisher信息度量；利用相对熵与海森矩阵的关系建立凯勒势函数；设计一个可高效计算且保持凯勒几何一致性的势函数，替代直接自动微分计算；结合解码器几何进行潜空间正则化，并使用加权复体积元素采样。

Result: 所提出的凯勒势函数有效逼近了复数情况下的Fisher信息度量，计算效率显著提高；潜空间正则化与加权采样使生成表示更平滑，语义异常点减少；尽管样本多样性略有下降，但整体表征质量提升。

Conclusion: 复数变分自编码器具备凯勒几何结构，通过设计合适的凯勒势函数，可在保证几何一致性的同时大幅降低计算复杂度。该方法为复数潜在空间的正则化与采样提供了新路径，提升了生成模型的稳定性和表现力。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [127] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器利用预训练的正常数据分布来建模正常与故障域之间的差异，实现精准的故障合成；同时引入多样性损失以防止模式崩溃，提升生成样本的多样性。实验表明，该方法在真实性和多样性上均显著优于传统方法，在关键基准测试中达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 工业设备监测中的故障诊断对系统可靠性与预测性维护至关重要，但故障数据稀缺（因故障事件罕见且标注成本高）严重制约了数据驱动方法的应用。现有时间序列生成模型针对丰富正常数据优化，难以在少样本场景下捕捉故障分布，生成的样本缺乏真实性和多样性，主要由于故障域与正常域间存在较大差异以及故障内部变异性高。

Method: 提出一种基于扩散模型的少样本故障时间序列生成框架。采用正负差异适配器，利用预训练的正常数据分布建模正常与故障域之间的差异，以实现更准确的故障合成；引入多样性损失，通过样本间差异正则化，防止模式崩溃，提升生成样本的多样性。

Result: 实验结果表明，所提方法在生成样本的真实性和多样性方面显著优于传统方法，在多个关键基准测试中达到当前最优性能。

Conclusion: 本文提出的少样本故障时间序列生成框架有效解决了故障数据稀缺背景下的生成难题，通过正负差异适配器和多样性损失机制，实现了高质量、多样化的故障样本合成，为工业设备故障诊断提供了有力支持。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [128] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: This paper introduces Q-GAT, a quantum-enhanced graph attention network within a DRL framework, replacing heavy MLPs with parameterized quantum circuits. It reduces model parameters by over 50%, accelerates convergence, and improves routing cost efficiency by ~5% on VRP benchmarks, showing promise for scalable logistics optimization.


<details>
  <summary>Details</summary>
Motivation: The vehicle routing problem (VRP) is a complex NP-hard challenge in intelligent transportation systems, requiring efficient and scalable solutions. Classical deep reinforcement learning (DRL) models using Graph Neural Networks (GNNs) often rely on large multi-layer perceptrons (MLPs), which are computationally expensive and memory-intensive, limiting their scalability.

Method: The paper proposes a Quantum Graph Attention Network (Q-GAT) integrated into a DRL framework. It replaces conventional MLPs with parameterized quantum circuits (PQCs) at key readout stages, leveraging the expressive power of graph attention mechanisms while reducing model parameters significantly.

Result: Experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing costs by approximately 5% compared to classical GAT baselines. The hybrid model reduces trainable parameters by over 50%, maintaining strong performance.

Conclusion: The study demonstrates that PQC-enhanced GNNs offer a compact and effective approach for large-scale routing and logistics optimization, highlighting the potential of quantum-enhanced deep learning models in practical applications.

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [129] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL提出了一种基于知识蒸馏的框架，将掩码自回归扩散模型（MAR）中的扩散链压缩为单步自回归生成，显著加速推理并支持强化学习后训练，实现高效且符合人类偏好的生成模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统MAR模型因层级推理机制导致的推理速度慢的问题，使其适用于强化学习等后训练场景。

Method: 提出一种基于得分函数的变分目标，将MAR扩散模型蒸馏为单步自回归生成，并设计MARVAL-RL框架实现高效强化学习后训练。

Result: 在ImageNet 256*256上，MARVAL-Huge实现FID 2.00，推理速度提升30倍以上；MARVAL-RL在CLIP和图像奖励分数上持续提升，验证了其有效性与偏好对齐能力。

Conclusion: MARVAL首次实现了MAR扩散模型的实用化蒸馏与强化学习，推动了快速采样与人类偏好对齐的生成模型发展。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [130] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 本文提出ATPO，一种针对扩散大语言模型（dLLMs）的自适应轨迹策略优化方法，通过识别推理过程中的高影响力步骤（如不确定性突增区域），动态调整梯度分配，提升推理准确性和训练稳定性，无需增加计算成本或改变奖励机制。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法对去噪步骤均匀分配策略梯度，但实际中并非所有步骤都同等重要；本文旨在揭示推理轨迹中的关键动态阶段，以更高效地利用梯度更新。

Method: 通过熵不确定性、置信度-边际（CM）不确定性和熵变化率（RoEC）等指标分析推理轨迹，识别出‘困惑区’；设计基于RoEC+CM混合规则的ATPO策略，动态选择并聚焦于高杠杆步骤进行梯度更新。

Result: ATPO在多个基准测试上显著提升了推理准确性与训练稳定性，证明了利用轨迹动态特性可有效增强dLLM的强化学习性能。

Conclusion: 推理轨迹中存在结构化的高影响力阶段，通过智能选择这些关键步骤进行梯度更新，能显著提升dLLM在强化学习中的表现，为未来模型优化提供了新方向。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [131] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出EntroPIC方法，通过比例-积分控制动态调整正负样本的损失系数，以稳定大语言模型训练中的熵，确保高效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在长期训练大语言模型时难以维持适当的熵水平，因正负样本对熵的影响不同，易导致模型陷入次优行为。

Method: 提出基于比例-积分控制的熵稳定性方法EntroPIC，动态调节正负样本的损失权重，以自适应地稳定训练过程中的熵。

Result: 实验表明，EntroPIC能有效维持期望的熵水平，实现大语言模型强化学习训练的稳定与最优。

Conclusion: EntroPIC在理论上和实践中均证明能有效控制大语言模型训练中的熵，提升训练稳定性与探索效率。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [132] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT is a unified framework for network tomography that models various network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent state, using multimodal alignment and contrastive learning to improve generalization and robustness across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing network tomography methods solve problems separately and rely on task-specific signals, limiting their generalization and interpretability. A unified approach is needed to better capture underlying network states.

Method: PLATONT adopts the Platonic Representation Hypothesis, modeling different network indicators as projections of a shared latent state. It uses multimodal alignment and contrastive learning to train multiple tomography tasks in a common latent space.

Result: PLATONT outperforms existing methods in link estimation, topology inference, and traffic prediction on both synthetic and real-world datasets, showing higher accuracy and stronger robustness under varying conditions.

Conclusion: PLATONT provides a unified, generalizable, and interpretable framework for network tomography by leveraging shared latent representations, significantly improving performance across diverse network inference tasks.

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [133] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为GRPO-RM的新方法，旨在将GRPO从大语言模型扩展到表示学习模型。通过构建预定义的输出集来替代语言模型中的词元序列采样，并设计专门的奖励函数以适应表示模型特性，实验验证了该方法在多种真实数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO是否可泛化至表示学习模型，以提升其在非语言任务中的表现。

Method: 使用预定义输出集代替词元序列采样生成输出组，并设计适配表示模型特性的奖励函数进行概率驱动优化。

Result: 在多个真实世界数据集上，GRPO-RM展现出良好的性能，证明了该方法的有效性与可行性。

Conclusion: GRPO-RM成功将GRPO思想应用于表示学习模型，为后续在更广泛场景中应用强化学习提供了新思路。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [134] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP is a sparse Test-Time Adaptation (TTA) framework that reduces adaptation frequency and data usage while maintaining high accuracy, making it suitable for resource-constrained edge environments. It introduces Class and Domain Representative Memory (CnDRM) to store representative samples and Inference-only Batch-aware Memory Normalization (IoBMN) to dynamically adjust normalization at inference time, enabling efficient domain alignment with minimal updates.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods require frequent adaptation and high computational cost, which limits their applicability in edge environments with strict latency and resource constraints. The need is to develop a lightweight, infrequent-adaptation TTA method that maintains accuracy under limited data and computation.

Method: SNAP consists of two core components: (i) CnDRM, which identifies and stores a small set of representative samples capturing both class and domain characteristics; and (ii) IoBMN, which uses these stored samples to dynamically normalize batch statistics during inference, enabling effective adaptation without retraining.

Result: SNAP achieves up to 93.12% reduction in latency while keeping accuracy drop below 3.3% across adaptation rates from 1% to 50%. It maintains competitive performance even when adapting on only 1% of the data stream, demonstrating strong robustness and efficiency for edge deployment.

Conclusion: SNAP provides a practical, low-cost TTA solution for edge devices by drastically reducing adaptation frequency and data usage without sacrificing accuracy. Its integration with multiple state-of-the-art TTA methods confirms its versatility and potential for real-world applications in latency-sensitive scenarios.

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [135] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim 是一种训练阶段的方法，旨在生成对后端和精度选择具有鲁棒性的硬件中立检查点，通过渐进式伪量化和反向剪枝技术，减少不同厂商编译器间的性能差异，提升低比特量化模型的准确性与可移植性。


<details>
  <summary>Details</summary>
Motivation: 当前专用边缘加速器依赖低比特量化，但不同厂商的编译器在缩放、裁剪和内核支持方面存在差异，导致相同的浮点检查点在不同后端上表现不一致，迫使开发者频繁调整参数或重构模型以适配特定后端。

Method: 结合渐进式伪量化（align training with deployed integer grid）和反向剪枝（tame outlier-driven scale inflation while preserving learnability），实现对多种量化方案（对称/非对称、张量/通道级、INT8/INT4）的兼容，无需修改图结构或依赖厂商特定配置。

Result: 在多个模型和任务上，Quant-Trim显著缩小了浮点与低比特之间的差距，降低了对编译器启发式方法和校准过程的依赖，避免了针对每个后端的重新训练，并在静态/动态激活缩放及不同算子覆盖场景下实现了良好的延迟、吞吐、能效和推理成本表现。

Conclusion: Quant-Trim 有效提升了低比特量化模型在异构边缘设备上的可移植性和一致性，为跨平台部署提供了更稳定、高效的解决方案。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [136] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型（TSFMs）中概念可解释性，探究了概念在不同层的编码、线性可恢复性、表示的解耦与抽象演化以及概念组合的处理方式。结果表明，浅层主要捕捉局部时域模式（如自回归、水平变化、趋势），深层则编码离散度和变化时间信号，而频谱和变形因子最难线性恢复。在组合场景下，探测性能下降，揭示概念间存在干扰，说明当前TSFMs在表征交互时间现象方面仍存在关键局限。


<details>
  <summary>Details</summary>
Motivation: 理解时间序列基础模型内部如何表征基本时间序列概念，以揭示其语义机制并识别其能力边界。

Method: 采用分层分析、线性可恢复性测试和表示相似性度量等方法，系统地探查概念在模型中的分布与演化。

Result: 早期层主要编码局部时间域模式，深层编码分散性和变化时间信号；频谱和变形因素难以线性恢复；在概念组合中出现性能下降，表明存在概念间干扰。

Conclusion: 尽管原子概念在模型中能被可靠定位，但概念组合仍是当前TSFMs的一大挑战，凸显其在表征复杂交互时间现象方面的局限性。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [137] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: KrawtchoukNet, a new GNN filter based on discrete Krawtchouk polynomials, addresses both over-smoothing and poor performance on heterophilic graphs by using bounded recurrence coefficients (via fixed domain N) and learnable shape parameter p for adaptive spectral response, achieving SOTA results on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard polynomial-based GNNs like ChebyNet suffer from performance collapse on heterophilic graphs and over-smoothing at high polynomial degrees due to their static, low-pass filter nature.

Method: Proposes KrawtchoukNet using discrete Krawtchouk polynomials with a fixed small domain N (e.g., 20) to ensure bounded recurrence coefficients and a learnable shape parameter p to adapt the spectral response to graph data.

Result: KrawtchoukNet achieves state-of-the-art performance on heterophilic benchmarks (Texas, Cornell) and maintains strong performance even at high polynomial degrees (K=10), outperforming GAT and APPNP.

Conclusion: KrawtchoukNet provides a unified solution to over-smoothing and heterophilic graph challenges through inherent boundedness and adaptive spectral filtering, demonstrating superior robustness and performance.

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [138] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列预测中的集成学习策略，评估了33种集成模型（包括现有和新提出的方法）在50个真实世界数据集上的表现。结果显示，堆叠（stacking）方法能持续提升预测精度，但没有单一堆叠器在所有任务上均最优。为此，本文提出一种多层堆叠框架，融合不同堆叠模型的优势，显著提升多种预测场景下的准确性。研究强调了基于堆叠的方法在改进时间序列自动机器学习系统方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测中，集成学习方法仍未被充分应用，当前主流仍采用简单的线性组合，而堆叠等更先进的集成技术尚未得到系统探索和验证，因此亟需系统评估并改进集成策略以提升预测性能。

Method: 系统评估33种集成模型（包括经典与新提出方法），在50个真实世界时间序列数据集上进行实验；提出多层堆叠框架，通过组合多个堆叠器模型以增强泛化能力。

Result: 堆叠方法在多数情况下显著提升预测精度；多层堆叠框架在不同预测场景中均表现出更优的性能，且优于单一堆叠器。

Conclusion: 基于堆叠的集成方法在时间序列预测中具有巨大潜力，多层堆叠框架可有效提升AutoML系统在该任务上的表现，应被广泛采纳和进一步研究。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [139] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 本文提出了一种新的后验局部特征重要性方法——反事实重要性分布（CID），通过生成正负反事实样本并使用核密度估计建模其分布，基于分布差异度量对特征进行排序。该方法具有坚实的数学基础，满足作为有效度量的关键性质，并在忠实性指标上优于现有方法，提供了更可靠的模型解释。


<details>
  <summary>Details</summary>
Motivation: 现有特征重要性方法缺乏明确的基准进行比较，因此需要一种更可靠、有理论基础的评估方式来理解机器学习模型的决策过程。

Method: 生成正负反事实样本，利用核密度估计建模其分布，采用分布差异度量对特征重要性进行排序。

Result: 相比已有方法，该方法在忠实性（全面性和充分性）方面表现更优，提供互补视角并提升解释可靠性。

Conclusion: CID是一种具有坚实理论基础且性能优越的特征重要性分析工具，具有重要的应用潜力。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [140] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%的核心参数来在不访问历史数据或增加参数量的情况下，有效保留基础模型的通用能力并学习领域知识。该方法利用基于Fisher信息的PIECE-F和结合梯度与曲率信息的PIECE-S两个重要性估计器，实现高效且精准的参数更新。实验表明，PIECE在多个语言模型和多模态模型上均保持了优秀的通用能力，并在各类下游任务中达到当前最优的持续学习性能，为可扩展、动态适应的大型基础模型提供了可行路径。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定微调导致灾难性遗忘的问题，使基础模型在保持通用推理能力的同时，能够高效适应新领域知识，克服传统持续学习方法在性能、数据依赖或参数开销上的局限。

Method: 提出PIECE方法，通过两个重要性估计器（PIECE-F基于Fisher信息，PIECE-S结合梯度与曲率信息）识别对新任务最相关的0.1%核心参数，并仅更新这些参数，从而在不增加额外参数或依赖历史数据的前提下实现高效持续学习。

Result: 在三种语言模型和两种多模态模型上的实验表明，PIECE能有效维持模型的通用能力，同时在多种下游任务中达到最先进的持续学习性能，显著减少遗忘并提升适应性。

Conclusion: PIECE提供了一种无需访问历史数据、无额外参数开销的高效持续学习方案，为构建可动态适应真实世界环境的可扩展基础模型提供了实用路径。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [141] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: EVA-Net is a novel, interpretable framework that uses a sparsified-attention Transformer and Variational Information Bottleneck to model EEG data for brain age prediction, enabling robust anomaly detection in imperfect medical data. It learns a normative healthy aging manifold via a continuous prototype network, achieving state-of-the-art performance and effectively identifying deviations in MCI and AD patients.


<details>
  <summary>Details</summary>
Motivation: Existing models for brain age prediction using EEG struggle with imperfect medical data, such as weakly supervised healthy-only cohorts, and lack interpretability, making it difficult to detect pathological deviations. There is a need for a robust, interpretable anomaly detection framework that can identify disease from noisy, real-world data.

Method: EVA-Net combines a sparsified-attention Transformer for modeling long EEG sequences, a Variational Information Bottleneck for learning robust, compressed representations, and a continuous prototype network to align representations with the normative healthy aging manifold, enabling interpretable anomaly detection.

Result: EVA-Net achieves state-of-the-art accuracy on healthy subjects (n=1297) and demonstrates strong anomaly detection performance on an unseen cohort of 27 MCI and AD patients, showing significantly higher brain-age gaps and a new metric—Prototype Alignment Error—indicating deviation from healthy aging.

Conclusion: EVA-Net provides a powerful, interpretable framework for healthcare intelligence by leveraging imperfect EEG data to detect pathological brain aging, offering both high performance and actionable insights into disease progression.

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [142] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 本文提出了一类用于非线性、非高斯状态空间模型的状态估计算法，基于变分拉格朗日框架，将贝叶斯推断转化为受动态约束的熵信任域更新。该框架导出一系列前向-后向算法，其结构由变分后验的分解方式决定。通过聚焦高斯-马尔可夫近似，推导出计算复杂度较低的递归方案，并在一般非线性、非高斯模型中利用广义统计线性回归和傅里叶-埃米特矩匹配闭合递归。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计方法在处理非线性、非高斯系统时面临计算复杂性和近似误差的挑战，亟需一种既能保持精度又具备高效计算能力的新方法。

Method: 采用变分拉格朗日框架，将贝叶斯推断形式化为带有动态约束的熵信任域优化问题；通过选择不同的变分后验因子分解方式构造前向-后向算法；利用高斯-马尔可夫近似降低复杂度，并结合广义统计线性回归与傅里叶-埃米特矩匹配实现递归闭合。

Result: 所提算法在非线性、非高斯模型下表现出良好的估计精度与可扩展性，且具有较低的计算开销，适用于实际应用中的实时状态估计任务。

Conclusion: 本研究提出的变分拉格朗日框架为非线性、非高斯状态估计提供了一个统一且高效的解决方案，其递归结构灵活、计算高效，具备广泛的应用前景。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [143] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 本文研究了表格上下文学习（ICL）模型中各层对预测的贡献，发现仅部分层共享共同的表示语言，表明存在结构冗余，为模型压缩和可解释性提升提供了机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型与大语言模型（LLMs）在架构上相似，但对其各层如何贡献于预测的理解仍不充分，亟需揭示其潜在冗余和表示演化机制。

Method: 采用'层如画家'视角分析TabPFN和TabICL模型，通过考察隐空间在不同层间的演化过程，识别共有的表示语言及冗余层。

Result: 发现只有部分层共享一致的表示语言，表明存在结构性冗余；这一发现支持模型压缩与增强可解释性的可能性。

Conclusion: 表格ICL模型中存在层间表示冗余，可通过剪枝或重构实现模型压缩，并提升对模型决策过程的理解。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [144] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 本文提出一种基于上下文学习的时间序列基础模型（TSFM）分类方法，无需微调即可对未在训练数据中的新数据进行分类。通过在提示中引入目标标签和协变量数据，将频率域参考信号转换为伪时间序列，生成对齐的协变量与目标信号，利用TSFM预测数据对应预定义标签的概率。该方法在伺服压力机轴承健康状态评估中验证了有效性，展示了跨多种工况的可扩展性，推动了从定制化窄AI向更广泛、通用的AI驱动维护系统的进展。


<details>
  <summary>Details</summary>
Motivation: 现有维护系统依赖于针对特定任务的定制化人工智能模型，缺乏通用性和可扩展性。为实现更广泛的智能维护，需要一种无需微调即可适应新数据的通用分类方法。

Method: 将频率域参考信号转换为伪时间序列，构建对齐的协变量与目标信号，利用时间序列基础模型（TSFM）在提示中通过上下文学习进行分类，无需微调模型。

Result: 该方法在不同工况下对轴承健康状态分类表现出良好性能，能够有效识别未知数据模式，且无需微调，展现了强泛化能力与可扩展性。

Conclusion: 基于上下文学习的分类方法在时间序列基础模型中实现了无需微调的高效分类，为构建通用、可扩展的智能维护系统提供了新路径，标志着从窄AI向广义AI驱动维护的重要进步。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [145] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy is a fairness-aware energy minimization framework for federated learning in wireless edge systems, optimizing device selection, bandwidth allocation, and compression level to balance energy efficiency, fairness, and model accuracy. It uses a contribution score based on update magnitude and compression ratio, and solves the complex optimization problem via relaxation and Lagrangian decomposition.


<details>
  <summary>Details</summary>
Motivation: Federated learning in wireless edge systems faces challenges in balancing energy efficiency, fair client participation, and high model accuracy due to heterogeneous resources, unequal contributions, and limited communication capacity.

Method: FairEnergy integrates a contribution score into joint optimization of device selection, bandwidth allocation, and compression level. The mixed-integer non-convex problem is solved by relaxing binary variables and applying Lagrangian decomposition to manage global bandwidth constraints, followed by per-device subproblem optimization.

Result: Experiments on non-IID data demonstrate that FairEnergy achieves higher model accuracy while reducing energy consumption by up to 79% compared to baseline methods.

Conclusion: FairEnergy effectively balances energy efficiency, fairness, and model accuracy in federated learning by jointly optimizing key system parameters, making it suitable for resource-constrained wireless edge environments.

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [146] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: NINT提出一种基于NTK的动态坐标选择策略，通过评估损失梯度的NTK加权范数来选取对全局函数更新贡献最大的坐标，从而加速隐式神经表示（INRs）的训练。该方法同时考虑拟合误差和坐标间的异质影响（自影响与耦合），显著减少训练时间近一半，且保持或提升表示质量，优于现有采样策略。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示（INRs）在高分辨率信号建模中需优化数百万个坐标点，计算成本极高。为降低训练开销，亟需高效采样策略以减少冗余计算，提升训练效率。

Method: 提出NTK-Guided Implicit Neural Teaching（NINT），利用神经正切核（NTK）对每个坐标的损失梯度进行加权，根据其加权梯度范数动态选择最具影响力的样本坐标，实现高效、有导向的训练过程。

Result: 实验表明，NINT可将训练时间减少约50%，同时保持甚至提升模型表示质量，在同类采样策略中达到最新技术水平。

Conclusion: NINT通过引入NTK指导的动态采样机制，有效解决了INRs在高分辨率建模中的计算瓶颈，是一种高效且鲁棒的训练加速方法。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [147] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了在受限轮数内自适应采样k个分布时，样本复杂度与轮复杂度之间的权衡。在可实现的多分布学习（MDL）设置中，证明r轮算法的最优样本复杂度约为dk^{Θ(1/r)} / ε；在一般非可实现情况下，提出一个在˜O(√k)轮内达到近似最优样本复杂度˜O((d + k)/ε²)的算法。引入新的框架OODS，抽象样本自适应性权衡，并捕捉大多数现有MDL算法。建立了OODS设置下近乎紧的轮复杂度界，上界直接导出非可实现MDL的˜O(√k)轮算法，下界表明要实现亚多项式轮复杂度需根本性新方法以绕过OODS的固有困难。


<details>
  <summary>Details</summary>
Motivation: 探索在有限轮数内自适应采样时，如何在样本复杂度和轮复杂度之间取得最优平衡，尤其针对多分布学习中的实际应用需求。

Method: 提出优化通过按需采样（OODS）的新框架，分析其在不同设置下的轮复杂度界限，设计高效算法并建立理论下界。

Result: 在可实现设置中，得到样本复杂度为dk^{Θ(1/r)} / ε的最优结果；在非可实现设置中，实现˜O((d + k)/ε²)的样本复杂度和˜O(√k)的轮数；证明了在现有技术下无法实现亚多项式轮复杂度。

Conclusion: 该研究揭示了样本自适应性与轮复杂度之间的深层权衡，提出的新框架和算法为多分布学习提供了理论支撑，同时指出突破当前轮复杂度下限需要全新的技术路径。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>
