<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统研究了强化学习（RL）在文本到3D自回归生成中的应用，针对3D生成的高空间复杂性挑战，提出了多维度优化方案：包括奖励设计、RL算法选择、新基准MME-3DR的构建以及层次化训练框架Hi-GRPO。基于这些工作，提出了首个增强型文本到3D生成模型AR3D-R1，实现从粗略形状到纹理细节的渐进式优化。


<details>
  <summary>Details</summary>
Motivation: 3D生成因几何全局一致性和局部纹理精细度要求高，对奖励设计和RL算法极为敏感，而此前缺乏对强化学习在3D生成中系统性探索，因此亟需建立有效的RL驱动框架以提升生成质量与推理能力。

Method: 通过评估不同奖励维度与模型选择，研究GRPO变体及训练数据与迭代规模的影响，构建新基准MME-3DR以衡量隐式推理能力，并提出层级化强化学习方法Hi-GRPO，实现从全局到局部的分阶段优化。

Result: 成功开发出AR3D-R1模型，显著提升文本到3D生成的质量，尤其在结构一致性与细节丰富度方面表现优异；新基准与方法为未来3D生成研究提供了有效工具与范式。

Conclusion: 本研究首次系统推进了强化学习在文本到3D生成中的应用，揭示了奖励对齐、层次化优化与高质量数据的重要性，为后续3D生成模型的智能推理与可控生成提供了重要基础。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本文提出MMSI-Video-Bench，一个全面的人工标注视频空间智能基准，涵盖感知、规划、预测和跨视频推理四个层次，包含1,106个问题和1,278段来自25个数据集及自建视频的片段。该基准支持三个领域子基准，用于针对性评估。评估25个主流多模态大模型发现显著的人机差距，最佳模型仍落后人类近60%。研究揭示几何推理、运动定位、长时序预测和跨视频对应等系统性失败，且常见帧采样策略、3D空间线索和链式思维提示均未带来明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在连续视觉输入下的空间理解能力缺乏全面评估基准，难以推动其在物理环境中的通用助手演进。

Method: 构建四层框架（感知、规划、预测、跨视频推理），通过人工标注设计1,106个问题，覆盖1,278段视频，由3DV专家审核确保精确性，并支持三个领域子基准进行定向评估。

Result: 25个模型表现普遍不佳，多数接近随机水平，最优模型仍落后人类近60%；空间微调模型泛化能力差；存在几何推理、运动定位、长时预测和跨视频对应等系统性错误；帧采样策略、3D线索与链式思维提示效果有限。

Conclusion: MMSI-Video-Bench为视频空间智能提供了可靠的评估平台，有助于推动多模态大模型在复杂物理场景中空间理解能力的发展。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: BabyVLM-V2 是一个基于儿童发展理论的视觉语言建模框架，通过纵向、多方面的预训练数据集和名为 DevCV Toolbox 的认知评估工具包，显著提升了 BabyVLM-V1。该框架构建了覆盖广泛的婴儿中心视听语料库，生成视频-话语、图像-话语及多轮对话数据，并将 NIH Baby Toolbox 中的视觉相关评估指标转化为涵盖空间推理、记忆和词汇理解等能力的十项多模态任务。实验表明，从零开始预训练的小型模型在 DevCV Toolboox 上表现优异，甚至在某些任务上超越 GPT-4o，推动了发展性合理的视觉基础模型预训练研究。


<details>
  <summary>Details</summary>
Motivation: 早期儿童的发展轨迹为视觉基础模型的样本高效预训练提供了自然目标。现有方法缺乏与婴幼儿认知发展相一致的系统性框架，亟需一种能反映婴儿真实学习经验并支持认知评估的多模态预训练体系。

Method: 引入纵向、多维度的婴儿中心音频视频预训练数据集；设计灵活可扩展的模型架构；开发 DevCV Toolbox，将 NIH Baby Toolbox 的视觉认知测量方法转化为十项多模态任务基准，用于评估模型的认知能力。

Result: 小型模型在未微调情况下即可在 DevCV Toolbox 上取得具有竞争力的表现，部分任务性能优于 GPT-4o，验证了该框架在发展性合理预训练中的有效性。

Conclusion: BabyVLM-V2 提供了一个原则性强、统一的框架，有望加速发展性合理视觉基础模型的预训练研究，推动人工智能向更贴近人类认知发展的方向演进。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [4] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D 是一种可扩展的多视角变换器，用于度量尺度、密集的前馈4D重建。它能直接生成N帧的每像素运动和几何预测，相比以往仅关注2视图密集场景流或稀疏3D点跟踪的方法具有更广的应用范围。此外，与现有单目RGB视频4D重建方法不同，Any4D可处理额外模态和传感器数据（如RGB-D、IMU姿态、雷达多普勒）。其关键创新在于采用模块化4D场景表示：以局部相机坐标系表示各视角的深度图和相机内参等自我中心因素，以全局世界坐标系表示相机外参和场景流等他心因素。该方法在多种设置下均表现出色，精度提升2-3倍，计算效率提高15倍，为下游应用开辟了新路径。


<details>
  <summary>Details</summary>
Motivation: 现有4D重建方法在精度、效率及多模态支持方面存在局限，难以实现高精度、高效且灵活的多视角密集重建。因此需要一种能够统一处理多帧、多模态输入，并兼顾性能与效率的新框架。

Method: 提出模块化4D场景表示，将每视角的4D预测分解为局部坐标系下的自我中心因素（如深度图、相机内参）和全局坐标系下的他心因素（如相机外参、场景流），并基于此构建可扩展的多视角变换器架构，支持多模态输入（如RGB-D、IMU、雷达），实现前馈式密集4D重建。

Result: 在多种设置下，Any4D实现了2-3倍的误差降低和15倍的计算速度提升，显著优于现有方法，在精度和效率上均表现优异。

Conclusion: Any4D通过模块化4D场景表示和多模态融合能力，实现了高效、高精度的前馈式4D重建，具备广泛的应用潜力，是推动动态场景建模迈向实用的重要进展。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [5] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView 是一个统一的框架，能够泛化到多种4D一致性任务，通过分别表示空间、时间和视角条件，实现灵活组合输入。它可从静态、动态和多视角输入中合成新视图，外推时间轨迹，并根据文本或图像提示生成带相机控制的视频。在多个基准测试中表现优异，相比现有方法在图像质量上提升显著，且大幅降低相机轨迹误差，展示了通用4D视频模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于特定4D一致性任务，训练数据分散，缺乏统一框架，难以实现跨任务泛化。为解决这一问题，需要一个能统一处理多种4D任务的通用模型。

Method: OmniView 采用分离表示空间、时间与视角条件的方法，支持灵活组合输入，通过统一架构实现多任务泛化，适用于多视角、动态场景、文本/图像到视频生成及相机轨迹控制等任务。

Result: OmniView 在多项基准测试中表现卓越：在多视角新视图合成（LLFF）上图像质量提升33%，动态新视图合成（Neural 3D Video）提升60%，静态相机控制（RE-10K）提升20%，文本条件视频生成中相机轨迹误差减少4倍。

Conclusion: OmniView 证明了单一模型实现通用4D视频生成的可行性，具备强大的跨任务泛化能力，为构建通用视觉生成系统提供了新方向。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [6] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens 是一种模态无关的潜在标记，通过预训练在图像或文本模态中保持中间信息，实现自由形式的多模态推理。该方法避免了对专用工具、昂贵图像生成或手工设计推理数据的依赖，仅通过交替的文本-图像轨迹监督训练，并在无监督条件下微调最终答案。在四个空间推理基准测试中，其平均性能提升3%，在拼图任务上最高提升16%，显著优于现有文本或图文交替推理基线。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态模型在推理时依赖专用工具、高成本图像生成或人工标注的推理数据，导致系统脆弱且难以扩展。本文旨在提出一种更简单、可扩展的多模态推理机制，使模型能够抽象地在多种模态间自由思考，从而解决语言与视觉信息融合中的根本性挑战。

Method: 首先使用交错的文本-图像轨迹进行有监督训练以初始化 Mull-Tokens；随后在无需额外标注的情况下，仅基于最终答案进行无监督微调。该方法借鉴了隐式推理框架的设计理念，使模型能在图像和文本之间灵活切换并保留中间推理状态。

Result: 在四个空间推理基准测试中，包括解谜和视角转换等任务，Mull-Tokens 实现了平均 +3% 的性能提升，尤其在推理密集型的拼图子集上达到 +16% 的显著增益，优于当前最强的文本推理及图文交替推理基线。

Conclusion: Mull-Tokens 提供了一种简洁而有效的多模态推理方案，能够在不依赖复杂外部工具或人工标注的前提下，实现跨模态的自由思维。这一方法为理解如何在真实世界中融合语言与视觉推理提供了新的思路，具有良好的可扩展性和通用性。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [7] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT提出了一种统一框架，通过显式的时间戳条件控制，实现对主体驱动视频生成中主体出现与消失的细粒度时间控制。该方法引入新颖的位置编码机制以编码时间区间，并结合预训练视频生成模型的位置嵌入；同时加入主体描述性文本标记以增强视觉身份与视频描述之间的绑定，避免生成歧义。通过标记级连接，AlcheMinT无需额外交叉注意力模块，参数开销极小。实验表明，AlcheMinT在保持与当前先进视频个性化方法相当的视觉质量的同时，首次实现了多主体视频生成中的精确时间控制。


<details>
  <summary>Details</summary>
Motivation: 现有主体驱动视频生成方法缺乏对主体出现和消失的细粒度时间控制，限制了其在组合视频合成、故事板设计和可控动画等应用中的使用。

Method: 提出AlcheMinT框架，采用显式时间戳条件，引入新型位置编码以表示时间区间，融合预训练模型的位置嵌入；结合主体描述性文本标记强化视觉身份与文本描述的关联；通过标记级拼接实现高效集成，避免额外交叉注意力模块，降低参数开销。

Result: AlcheMinT在视频保真度和主体身份保留方面达到当前顶尖水平，同时首次实现对多主体视频生成中主体出现/消失的精确时间控制，在多个基准测试中表现优异。

Conclusion: AlcheMinT成功实现了主体驱动视频生成中对主体时间行为的精细控制，为复杂视频内容创作提供了新范式，具有良好的可扩展性和实用性。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SceneMaker的解耦3D场景生成框架，旨在解决现有方法在严重遮挡和开放集设置下难以同时生成高质量几何结构和准确姿态的问题。通过将去遮挡模型与3D物体生成解耦，并利用图像数据集和收集的去遮挡数据集增强多样性，提升开放集遮挡模式的表现；同时提出一个统一的姿态估计模型，结合全局与局部机制，优化自注意力和交叉注意力以提高精度；此外，构建了一个开放集3D场景数据集以增强姿态估计模型的泛化能力。大量实验表明该框架在室内和开放集场景中均具有优越性能。代码与数据集已公开。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡和开放集条件下，由于缺乏足够的开放集去遮挡和姿态估计先验知识，难以同时实现高质量几何生成和精确姿态估计。因此需要一种更鲁棒且可扩展的框架来应对复杂遮挡下的3D场景生成挑战。

Method: 提出解耦的3D场景生成框架SceneMaker：1）将去遮挡模型与3D物体生成分离，利用图像数据集和专门收集的去遮挡数据集提升对多样化开放集遮挡模式的建模能力；2）设计统一的姿态估计模型，融合全局与局部机制，改进自注意力和交叉注意力结构以增强姿态预测精度；3）构建开放集3D场景数据集，用于训练和评估模型的泛化性能。

Result: 在室内和开放集场景中，所提框架在几何质量与姿态准确性方面均优于现有方法。实验结果验证了其在复杂遮挡条件下的强泛化能力和有效性。

Conclusion: SceneMaker通过解耦去遮挡与3D生成流程、引入增强的多尺度注意力机制以及构建专用开放集数据集，显著提升了3D场景生成在严重遮挡和开放集设定下的表现，为未来开放世界3D理解提供了有效解决方案。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 本研究利用计算文本分析方法，通过正则表达式提取《霍比特人》中的对话，并使用NRC-VAD词典量化情感维度。结果显示，对话整体呈现积极（高愉悦度）和平静（低唤醒度）的情感基调，且随着故事推进，主导性逐渐增强。这种情感节奏体现了小说中紧张与放松、危险与幽默、冒险与友情的交替平衡。可视化手段如情感轨迹图和词云进一步揭示了托尔金语言中张力与慰藉的循环模式。研究展示了数字方法如何揭示文学作品中微妙的情感结构，凸显《霍比特人》叙事中稳定的情感韵律与调节机制。


<details>
  <summary>Details</summary>
Motivation: 探究《霍比特人》中对话的情感结构，揭示其内在情感节奏与叙事策略，验证计算文本分析在文学研究中的适用性与价值。

Method: 使用正则表达式提取对话，进行预处理后，采用NRC-VAD词典对情感维度（愉悦度、唤醒度、主导性）进行量化分析，并结合可视化工具呈现情感变化趋势。

Result: 对话整体保持高愉悦度与低唤醒度，主导性随情节发展逐步上升；情感呈现紧张与安慰交替的周期性模式，反映小说独特的叙事节奏。

Conclusion: 计算文本分析能够有效揭示《霍比特人》中隐藏的情感结构，展现托尔金通过情感调节构建叙事韵律的创作智慧，证明数字人文方法在文学研究中的有效性与潜力。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [10] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 该研究评估了多模态大语言模型（mLLMs）在视频情感唤醒分析中的表现，发现其在理想条件下表现可靠且无显著人口统计偏差，但在真实议会辩论视频中表现不佳，可能影响后续统计推断。研究强调需持续评估生成式AI在政治分析中的应用，并提供可复制的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前政治传播研究日益依赖音视频材料分析情绪表现，而多模态生成式AI虽具潜力，但缺乏对其在情绪分析中有效性的实证证据，亟需系统评估。

Method: 通过两个互补的人类标注视频数据集，对比mLLMs在理想条件与真实议会辩论场景下的情感唤醒评分表现，评估其可靠性与潜在偏见。

Result: 在理想条件下，mLLMs的情感唤醒评分高度可靠且无明显人口统计偏差；但在真实议会辩论视频中，其表现未能达到预期，可能导致下游分析出现偏差。

Conclusion: 应持续、深入评估新兴生成式AI方法在政治分析中的适用性，本研究提供了可复现的评估框架，以推动更可靠的自动化情绪分析。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）如何提升大型用户设施中的提案选择流程，提出了一种基于成对偏好、可扩展且成本低廉的替代方案。传统人工评审存在评分不一致和偏见问题，而成对比较虽更优但工作量大。利用来自橡树岭国家实验室散裂中子源三个光束线的高质量提案与发表记录，研究发现LLM排名与人工排名高度相关（斯皮尔曼等级相关系数ρ≈0.2–0.8，剔除10%异常值后≥0.5），且在识别高发表潜力提案方面表现不劣于人类，同时成本降低超过两个数量级。此外，LLM还支持嵌入模型下的提案相似性量化分析，为评审委员会提供关键信息。


<details>
  <summary>Details</summary>
Motivation: 传统人工提案评审存在评分一致性差、受主观偏见影响等问题，而成对偏好方法虽逻辑更优但因二次复杂度难以由人力实施，亟需一种高效、一致、低成本的替代方案。

Method: 采用大型语言模型（LLMs）处理提案评估任务，基于三个光束线的历史提案与发表数据进行训练与验证；通过成对偏好建模实现提案排序，并结合嵌入模型进行提案相似性分析。

Result: LLM生成的排名与人工排名具有强相关性（Spearman ρ ≈ 0.2–0.8，剔除10%异常值后 ≥ 0.5）；在识别高发表潜力提案上性能不低于人类；成本低于人工评审两个数量级以上；并可实现人类难以完成的提案相似性量化分析。

Conclusion: LLMs可有效替代或辅助人工评审，显著提升提案选择的效率、一致性与可扩展性，同时拓展了评审过程中的数据分析能力，为大型科研设施的资源分配提供可靠技术支持。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [12] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 本文提出一种节点级剪枝框架，用于大语言模型中的电路发现，解决了现有方法在计算效率和粒度上的局限性。通过引入多层级可学习掩码和特定粒度的稀疏性惩罚，在单一微调过程中实现全面压缩，显著降低内存占用（5-10倍），并识别出更小规模、更精确的神经元级电路。实验表明，许多被粗粒度方法认为重要的神经元其实无关紧要，但任务性能得以保持。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法依赖迭代边剪枝，计算成本高且仅支持粗粒度单元（如注意力头或MLP块），忽略细粒度结构（如单个神经元），限制了对模型行为机制的精准理解。

Method: 提出基于可学习掩码的节点级剪枝框架，支持从整个模块到单个神经元的多层级粒度统一优化；通过粒度特定的稀疏性惩罚引导剪枝过程，实现一次微调完成高效压缩。

Result: 所发现的电路在节点数量上更小，验证了粗粒度方法中部分关键神经元实为冗余；同时内存使用减少5-10倍，无需存储中间激活值，显著提升可扩展性。

Conclusion: 该框架实现了高效、细粒度的电路发现，提升了对大语言模型内部工作机制的理解精度，并具备实际部署优势。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 本文提出一种无需额外训练的方法，利用旋转位置编码的特性，使具备推理能力的大型语言模型（LLM）能够在实时交互中同时思考、听觉输入和生成输出，从而显著提升响应速度与交互性。该方法将首次非思考型标记的延迟从数分钟减少到5秒以内，整体实时延迟降低6-11倍。


<details>
  <summary>Details</summary>
Motivation: 现有先进LLM在回答前需完成完整推理过程，导致响应延迟高，不适用于语音或嵌入式助手等需要实时交互的应用场景。人类在听、思、答过程中可异步进行，而当前模型无法模拟这种能力。因此，亟需一种无需重新训练即可实现异步多任务处理的方法。

Method: 利用旋转位置编码（Rotary Embeddings）的数学性质，设计一种机制使模型在接收输入的同时即可开始推理并逐步生成输出，实现思考、监听与生成三者的并行执行，从而打破传统序列化推理的限制。

Result: 在数学、常识及安全推理任务上均验证了该方法的有效性，能够实现实时生成带有推理过程的答案，首次非思考令牌延迟从分钟级降至5秒内，整体延迟降低6至11倍。

Conclusion: 本方法成功实现了无需额外训练的异步推理能力，使大型语言模型更适用于真实世界中的低延迟、高交互性应用场景。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [14] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文研究了点对点函数的内在特性对训练和性能的影响，通过大规模搜索发现一种新的函数设计Derf(x) = erf(αx + s)，其在视觉、语音表示和DNA序列建模等多个领域均优于LayerNorm、RMSNorm和DyT，性能提升主要源于更好的泛化能力而非更强的拟合能力。


<details>
  <summary>Details</summary>
Motivation: 探索替代传统归一化层的新函数设计，以突破现有方法如Dynamic Tanh（DyT）的性能瓶颈，实现更高效且无需归一化的Transformer架构。

Method: 系统分析点对点函数的性质，进行大规模函数搜索，最终提出并验证Derf函数，结合实验评估其在多种任务中的表现。

Result: Derf在图像识别、图像生成、语音表示和DNA序列建模等多个任务中均显著超越LayerNorm、RMSNorm和DyT，展现出更强的泛化能力与实用性。

Conclusion: Derf函数因其简洁性和优越性能，成为一种极具潜力的归一化替代方案，适用于无需归一化的Transformer模型设计。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [15] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 在仅使用室内温度历史值进行长期温度预测的挑战性单变量设置中，线性模型（Linear, NLinear, DLinear）表现优于复杂的Transformer家族架构，其中DLinear表现最佳，表明精心设计的线性模型在无外部变量的复杂时间序列预测任务中仍是强有力的基线。


<details>
  <summary>Details</summary>
Motivation: 研究在仅依赖历史温度数据的长期预测场景下，不同模型的性能表现，以评估复杂模型是否在该设定中仍具有优势。

Method: 在标准化的训练、验证和测试划分下，对比Linear、NLinear、DLinear、Transformer、Informer、Autoformer等模型在长周期温度预测任务中的表现。

Result: 线性基线模型（尤其是DLinear）在所有数据分割上均显著优于Transformer家族模型，展现出更高的预测准确性。

Conclusion: 在无外部变量的复杂时间序列预测任务中，精心设计的线性模型依然保持强大竞争力，应作为基准方法优先考虑。

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [16] [Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes](https://arxiv.org/abs/2512.10878)
*Xuan Zhao,Zhuo Cao,Arya Bangun,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: This paper improves model reconstruction by integrating counterfactuals with original data via Wasserstein barycenter-based class prototypes, enhancing surrogate model fidelity and mitigating decision boundary shift.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are valuable for interpretability and can aid in model reconstruction, but their use as training data often leads to decision boundary shifts due to their proximity to the boundary. This work addresses the challenge of limited labeled data by leveraging counterfactuals more effectively.

Method: The proposed method integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, preserving distributional structure and reducing boundary shift.

Result: Empirical results across multiple datasets demonstrate improved fidelity between surrogate and target models, confirming the effectiveness of the approach.

Conclusion: By treating counterfactuals as informative yet non-representative samples and using Wasserstein barycenter-based prototype approximation, model reconstruction is significantly enhanced, especially under low-data regimes.

Abstract: Counterfactual explanations provide actionable insights by identifying minimal input changes required to achieve a desired model prediction. Beyond their interpretability benefits, counterfactuals can also be leveraged for model reconstruction, where a surrogate model is trained to replicate the behavior of a target model. In this work, we demonstrate that model reconstruction can be significantly improved by recognizing that counterfactuals, which typically lie close to the decision boundary, can serve as informative though less representative samples for both classes. This is particularly beneficial in settings with limited access to labeled data. We propose a method that integrates original data samples with counterfactuals to approximate class prototypes using the Wasserstein barycenter, thereby preserving the underlying distributional structure of each class. This approach enhances the quality of the surrogate model and mitigates the issue of decision boundary shift, which commonly arises when counterfactuals are naively treated as ordinary training instances. Empirical results across multiple datasets show that our method improves fidelity between the surrogate and target models, validating its effectiveness.

</details>


### [17] [Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields](https://arxiv.org/abs/2512.10886)
*Stefan Matthes,Markus Schramm*

Main category: cs.LG

TL;DR: 本文提出一种物理信息学习框架，利用夜间均质化时段的运行数据，从常规操作数据中推断出回路级质量流量比和随时间变化的接收器传热系数。该方法结合可微分共轭传热模型与历史数据优化，在安达索尔3号50兆瓦太阳能场验证中实现了低于2℃的温度重建误差，并准确识别出高损失接收器区域，与无人机红外热成像结果高度一致，证明了真实世界数据在恰当建模和可微优化下可恢复隐含物理参数。


<details>
  <summary>Details</summary>
Motivation: 传统监控工具无法诊断液压不平衡或接收器退化，因回路质量流量和热损失参数不可观测；需从现有运行数据中提取隐藏的物理信息以提升系统性能与维护效率。

Method: 构建可微分共轭传热模型，将其离散化并嵌入端到端学习管道，利用夜间无辐照条件下的均质化时段分离水力与热损失效应，基于历史数据进行优化。

Result: 模型实现回路温度重建的均方根误差小于2℃，成功估计出回路不平衡和接收器热损失，与无人机红外热成像结果高度一致，能准确识别高损失区域。

Conclusion: 即使在噪声较大的实际运行数据中，结合物理模型与可微优化仍可有效恢复隐含的物理参数，为CSP电站的故障诊断与性能监控提供了新范式。

Abstract: Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.
  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.
  The model accurately reconstructs loop temperatures (RMSE $<2^\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.

</details>


### [18] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本文提出了一种高效且可扩展的1-swap算法，用于大型语言模型（LLM）中的层内掩码选择问题。通过强制每行具有相等的稀疏度，作者推导出可高效计算的最优1-swap操作，利用校准数据的格拉姆矩阵实现快速优化。该方法无需复杂超参数调优，可在GPU上高效运行，显著降低每层剪枝误差，相比Wanda方法最多减少60%的误差，并在多个GPT架构上提升困惑度和零样本准确率。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法在大型语言模型中面临性能下降且全量重训练成本过高，而现有层内掩码选择方法因组合爆炸难以精确求解，亟需一种高效、可扩展且低超参数依赖的解决方案。

Method: 通过强制每行保持相同稀疏度，将掩码选择问题分解为可高效求解的1-swap操作；利用校准数据的格拉姆矩阵快速计算最优交换对，提出一种基于热启动的轻量级1-swap算法，适用于大规模模型的剪枝优化。

Result: 相比Wanda方法，本方法在层内剪枝误差上最多降低60%，并在多种主流GPT架构上持续提升困惑度与零样本准确率，验证了其在大模型场景下的有效性与实用性。

Conclusion: 本文提出的1-swap算法在保持极简设计的同时，实现了对大规模语言模型剪枝掩码选择问题的有效求解，具备高效率、低依赖性和强泛化能力，为大模型压缩提供了实用新范式。

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [19] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文提出一种基于近端策略优化（PPO）的深度强化学习方法，用于水下机器人BlueROV2的自主导航。通过结合目标导向信息、虚拟占用网格和边界射线扫描构建观察空间，在复杂障碍环境中显著优于传统动态窗口法（DWA），且具备良好的仿真到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 水下环境缺乏GPS信号、能见度低且存在大量隐蔽障碍物，导致自主导航困难。现有方法如动态窗口法（DWA）在复杂场景中适应性不足，亟需更智能的自主决策机制。

Method: 采用PPO算法，设计融合目标导向信息、虚拟占用网格与边界射线扫描的多模态观察空间，训练深度强化学习策略以实现水下自主避障与导航。

Result: 在高密度障碍物仿真环境中，PPO策略表现优于DWA，具有更强的局部适应能力与更低的碰撞率；物理平台上的数字孪生验证进一步证明了其从仿真到现实的可迁移性。

Conclusion: 深度强化学习结合合理的观察设计，可有效提升水下机器人在复杂环境中的自主导航性能，且具备实际部署潜力。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [20] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出一种新算法，通过将批评家（critic）的块长度与策略（policy）的块长度解耦，使策略能够以较短的动作块进行操作，从而避免长动作块带来的建模困难和开环子最优问题。该方法利用从原始分块批评家乐观回溯来近似部分动作块扩展为完整动作块时的最大值，构建一个提炼后的批评家用于策略优化。实验表明，在具有挑战性的长时域离线目标条件任务中，该方法显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统时序差分（TD）方法因自举机制易产生偏差，而分块批评家虽能加速价值传播，但其策略需输出完整动作块，导致开环子最优且难以建模，尤其在长块长度下。因此，需要一种既能保留多步价值传播优势，又能避免上述问题的方法。

Method: 提出一种新算法，通过构建一个基于乐观回溯的提炼批评家，用于优化策略在部分动作块上的表现；策略仅需处理较短动作块，而批评家仍可利用长块信息进行价值估计，实现块长度解耦。

Result: 在多个复杂、长时域的离线目标条件任务中，所提方法表现出更优性能，稳定超越现有基准方法，验证了其有效性与鲁棒性。

Conclusion: 通过解耦策略与批评家的块长度，该方法有效缓解了长动作块带来的建模难题和开环子最优问题，同时保留了多步价值传播的优势，适用于高难度、长期依赖的强化学习任务。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [21] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: 本文提出了一种名为DaSH的新型数据集选择方法，旨在从异构数据池中选择最相关的完整数据集以提升下游任务性能。该方法通过建模数据集及分组（如机构、集合）层级的效用，实现高效泛化，显著优于现有基线方法，在两个公开基准上最高提升26.2%准确率，且探索步骤更少。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法多关注单个样本，忽视数据集及其来源之间的差异，导致在真实场景下无法有效利用不同质量与相关性的数据集。因此需要一种能综合考虑数据集整体价值的方法。

Method: 提出Dataset Selection via Hierarchies (DaSH)，通过层次化建模数据集和其所属分组（如机构、集合）的效用，实现对有限观测下的高效泛化。

Result: 在Digit-Five和DomainNet两个基准上，DaSH相比现有最优基线方法在准确率上提升高达26.2%，同时大幅减少探索步数；实验表明其在低资源和缺乏相关数据集情况下仍具鲁棒性。

Conclusion: DaSH是一种适用于实际多源学习工作流的可扩展、自适应的数据集选择方法，能够有效应对真实世界中数据异质性和资源限制的问题。

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>
