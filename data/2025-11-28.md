<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 10]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: G$^2$VLM 是一种基于几何的视觉语言模型，旨在提升视觉语言模型在空间智能方面的表现。它通过从2D图像中重建3D空间，融合3D视觉几何特征来增强空间理解与推理能力，利用多视角图像和视频数据进行训练，并在不依赖稀有标注的情况下引入3D先验知识。实验表明，该模型在3D重建和空间理解任务上均达到先进水平，具备良好的可扩展性，有望成为未来3D场景编辑等应用的基准模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在空间理解与推理任务中表现不佳，主要由于缺乏从2D图像重建3D空间的视觉几何学习过程。为弥补这一缺陷，需要引入3D几何信息以提升模型的空间智能。

Method: G$^2$VLM 采用统一架构，联合训练多视角图像与视频数据，学习3D视觉几何特征，通过上下文学习与交错推理机制，直接预测3D属性并增强空间推理能力，同时利用3D先验信息提升性能。

Result: G$^2$VLM 在3D重建任务上达到与现有前沿模型相当的性能，在多种空间理解与推理任务中表现更优或具有竞争力，且无需依赖稀缺标注即可实现高效训练。

Conclusion: 通过将语义强大的视觉语言模型与低层3D视觉任务相结合，G$^2$VLM 实现了对空间智能的统一建模，不仅提升了性能，还为未来如3D场景编辑等应用提供了坚实基础，可作为社区的重要基准模型。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [2] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN 是一种以运动为中心的后训练框架，通过引入基于 DiT 的光流判别器和分布匹配正则化，在不依赖奖励模型或人类偏好数据的情况下显著提升视频生成的运动真实感。该方法在多个基准测试中均表现出色，如在 VBench 和 VideoJAM-Bench 上大幅提高运动评分，并在人类评估中获得更高偏好，同时保持或提升视觉质量与效率。


<details>
  <summary>Details</summary>
Motivation: 标准去噪 MSE 损失无法直接监督时间一致性，导致视频扩散模型虽有低损失但运动表现差，如抖动、鬼影或不合理的动态。因此需要一种无需人工标注或奖励模型的方法来增强运动连贯性与真实性。

Method: 基于三步蒸馏视频扩散模型，训练一个基于 DiT 架构的光流判别器以区分真实与生成运动，并结合分布匹配正则化项以维持视觉保真度，实现运动质量的提升。

Result: 在 Wan2.1-T2V-1.3B 模型上，MoGAN 在 VBench 上相比 50 步教师模型提升 +7.3%，相比 3 步 DMD 模型提升 +13.3%；在 VideoJAM-Bench 上分别提升 +7.4% 和 +8.8%。人类评估显示其在运动质量上更受青睐（52% 对 38% 教师，56% 对 29% DMD），且图像质量与美学表现相当或更优。

Conclusion: MoGAN 能有效提升视频生成的运动真实感，同时不牺牲视觉质量或推理效率，为快速高质量视频生成提供了一条实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [3] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 本文提出一种自提示、点监督的框架，用于将SAM模型适配到遥感图像（RSI），仅需稀疏点标注。通过Refine-Requery-Reinforce循环，逐步优化伪掩码、构建自建框提示并对齐嵌入以减少确认偏差，实现无需完整掩码监督的自引导提示适应。在WHU、HRSID和NWPU VHR-10三个遥感基准数据集上验证，性能优于预训练SAM及现有点监督方法，证明了自提示与语义对齐在可扩展点级适配中的有效性。


<details>
  <summary>Details</summary>
Motivation: 遥感图像存在严重域偏移且密集标注稀缺，导致SAM等交互分割模型在遥感图像上表现不佳。亟需一种仅依赖稀疏点标注即可有效提升模型泛化能力的方法。

Method: 提出Refine-Requery-Reinforce循环：首先基于初始点生成粗略伪掩码（Refine）；其次利用自构建框提示改进分割结果（Requery）；最后通过跨迭代嵌入对齐减少确认偏差（Reinforce），实现自引导提示适应。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感数据集上，所提方法显著优于预训练SAM及现有点监督分割方法，验证了其在提升分割质量与域鲁棒性方面的有效性。

Conclusion: 自提示与语义对齐为大规模、点级适配基础分割模型至遥感应用提供了高效路径，具有良好的可扩展性和实用价值。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [4] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是目前Qwen系列中能力最强的视觉语言模型，支持高达256K tokens的交错文本、图像和视频输入，具备强大的纯文本理解、长上下文处理和多模态推理能力。其架构引入了增强的交错-MRoPE、DeepStack融合与基于文本的时间对齐技术，在密集型和混合专家（MoE）模型中均实现领先性能，适用于图像引导推理、智能体决策和多模态代码智能等实际应用。


<details>
  <summary>Details</summary>
Motivation: 提升视觉语言模型在多模态理解、长上下文处理和复杂推理任务中的表现，满足真实世界中对高精度、高效率多模态交互的需求。

Method: 采用增强的交错-MRoPE进行时空建模；引入DeepStack以整合多层级ViT特征，强化视觉-语言对齐；使用基于文本的时间戳对齐替代T-RoPE，实现更精确的视频时序定位；同时提供多种规模的密集模型与混合专家模型以适应不同延迟-质量权衡需求。

Result: Qwen3-VL在多个主流多模态基准测试中表现优异，包括MMMU、MathVista和MathVision等，在纯文本理解、长文档处理及多模态推理方面超越现有模型，尤其在大上下文长度下展现出更强的忠实性与跨参考能力。

Conclusion: Qwen3-VL作为新一代多模态基础模型，具备强大的多模态理解与推理能力，可作为图像引导推理、智能体决策和多模态代码智能的核心引擎，推动多模态AI在现实场景中的深度应用。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [5] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 本文提出一种新型系统，使用户可通过少量样本纠正AI误分类，适用于资源受限设备。系统结合服务器端知识蒸馏与设备端基于原型的分类，实现高效错误修正，无需重新训练模型。在图像分类和目标检测任务中，该系统在单次修正场景下对Food-101和Flowers-102数据集的错误纠正率超过50%，遗忘率低于0.02%，计算开销可忽略不计，并通过Android应用验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在日常设备中普遍存在预测错误问题，影响用户体验。尽管已有方法关注错误检测，但缺乏高效的纠错机制，尤其在资源受限设备上难以实现。因此亟需一种轻量级、可快速适应的纠错方案。

Method: 系统采用两阶段设计：(1) 服务器端通过知识蒸馏将大模型的鲁棒特征表示迁移至轻量级设备模型；(2) 设备端利用原型更新机制进行超高效错误修正，仅通过少量样本调整原型即可完成适应，避免模型重训练。

Result: 在Image Classification和Object Detection任务中，系统在单次修正场景下对Food-101和Flowers-102数据集的错误纠正率超过50%，遗忘率低于0.02%，计算开销极低。通过实际Android应用验证，系统具备良好的实用性。

Conclusion: 所提出的系统为资源受限设备上的AI错误修正提供了一种高效、轻量且实用的解决方案，支持用户以极少成本完成模型自适应，显著提升AI系统的可用性和用户体验。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [6] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow提出一种统一框架，结合反事实去混淆与双向时间条件流，用于长时行动质量评估（AQA）。通过自监督的因果反事实正则化（CCR）模块分离因果与混淆特征，并利用反事实干预增强因果鲁棒性；双向时间流（BiT-Flow）模块通过循环一致性约束建模前向和后向动态，生成更平滑、连贯的表示。在多个长期AQA基准上，CaFlow表现优于现有方法，达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 长期动作质量评估（AQA）面临建模长时间动态和应对上下文混杂因素的挑战。现有方法依赖昂贵标注或单向时间建模，易受虚假相关性和不稳定长期表征影响。

Method: 提出CaFlow框架，包含两个核心模块：1）因果反事实正则化（CCR）模块，通过自监督方式解耦因果与混杂特征，并利用反事实干预强化因果鲁棒性；2）双向时间条件流（BiT-Flow）模块，采用循环一致性约束联合建模前向与后向动态，提升表示的平滑性与一致性。

Result: 在多个长期动作质量评估基准上，CaFlow均取得当前最优性能，验证了其有效性与优越性。

Conclusion: CaFlow通过融合反事实去混淆与双向时间建模，有效提升了长期动作质量评估中对复杂时空动态与混杂因素的建模能力，为该任务提供了新的解决方案。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [7] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: 本研究提出Multi-Crit基准，用于评估大模型在遵循多样化、细粒度评价标准方面的能力。通过严谨的数据筛选流程构建涵盖开放生成与可验证推理任务的挑战性样本对，并引入三项新指标衡量模型在多标准遵从性、标准切换灵活性及识别标准间偏好冲突方面的表现。对25个LMM的分析发现：1）专有模型在开放性评价中仍难以保持多标准一致性；2）开源模型在灵活应对不同标准方面更落后；3）基于整体判断信号的批评者微调虽提升视觉理解能力，但无法推广至多标准层级判断。进一步分析揭示了当前多模态评估模型的局限性，为构建可信赖且可调控的多模态AI评估体系奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在作为多模态评估裁判时，其对多样、细粒度评价标准的遵循能力尚未充分探索，亟需一个系统性评估框架来检验其在复杂评价场景下的表现。

Method: 构建Multi-Crit基准，采用严格数据筛选流程生成具有多标准人类标注的挑战性响应对，并设计三项新指标（多标准遵从性、标准切换灵活性、偏好冲突识别能力）进行量化评估。同时对25个主流大模型进行系统测试，结合推理微调、测试时扩展和边界一致性分析，深入探究模型性能极限。

Result: 1）专有模型在开放生成任务中对多标准的一致性表现不佳；2）开源模型在灵活适应不同标准方面显著落后；3）批评者微调虽增强视觉定位能力，但无法泛化到多标准层级判断；4）测试时扩展与微调策略对模型表现存在差异影响，凸显当前多模态裁判的内在局限。

Conclusion: Multi-Crit是首个系统评估多模态裁判在多标准情境下表现的基准，揭示了当前大模型在准则灵活性与一致性上的不足，推动建立更可靠、可控制的多模态评估体系。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [8] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: ADVLA 是一种针对视觉-语言-动作（VLA）模型的高效对抗攻击框架，通过在视觉编码器投影到文本特征空间的特征上直接施加对抗扰动，实现低幅度、局部稀疏且几乎不可察觉的攻击。该方法无需端到端训练，仅需单步迭代（约0.06秒），在 $L_{\infty}=4/255$ 约束下，通过Top-K掩码策略使扰动覆盖少于10%的图像块，攻击成功率接近100%，且集中在关键区域。相比传统基于补丁的攻击，ADVLA显著降低计算成本与可见性，具备更强的实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在VLA模型中存在训练成本高、生成明显扰动补丁等问题，限制了其在实际场景中的应用。因此亟需一种高效、隐蔽且无需复杂训练的攻击方式，以更真实地评估VLA模型的安全性。

Method: ADVLA 采用直接在视觉编码器输出投影至文本空间的特征上施加对抗扰动的方法，结合注意力引导机制，利用三种策略增强敏感性、强制稀疏性并集中扰动分布，实现精准而隐蔽的攻击。

Result: 在 $L_{\infty}=4/255$ 的约束下，ADVLA配合Top-K掩码可在少于10%的图像块上实现近100%的攻击成功率，扰动高度集中、几乎不可察觉，单次迭代仅需0.06秒，显著优于传统补丁攻击方法。

Conclusion: ADVLA 有效实现了对VLA模型下游动作预测的高效、低可见性攻击，在保持极低扰动幅度和局部稀疏性的前提下，避免了高昂的训练开销，展现出卓越的攻击性能与实际应用价值，为评估VLA模型安全性提供了新思路。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [9] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 该论文首次系统性地探究了仅通过相机轨迹（而非像素）感知视频内容的可能性。提出了一种对比学习框架来训练CamFormer，将相机姿态轨迹映射到联合嵌入空间，并与自然语言对齐。研究发现，尽管看似简单，相机轨迹是揭示视频内容的极有信息量的信号，即“你如何移动”可以反映“你在做什么”（第一人称）或“你在观察什么”（第三人称）。所学的CamFormer嵌入在多种下游任务中表现出色，包括跨模态对齐、分类和时间分析。其表示对不同的相机位姿估计方法具有鲁棒性，涵盖高保真多传感器和标准仅RGB估计器。研究确立了相机轨迹作为一种轻量、鲁棒且多功能的视频内容感知模态。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过相机轨迹而非视频像素来感知视频内容，验证相机运动轨迹作为视觉信息源的潜力。

Method: 提出基于对比学习的CamFormer框架，将相机姿态轨迹编码为与自然语言对齐的嵌入表示。

Result: 相机轨迹能有效捕捉视频内容，支持跨模态对齐、分类和时间分析等多种任务，且对不同位姿估计方法具有鲁棒性。

Conclusion: 相机轨迹是一种轻量、鲁棒且多功能的视频内容感知模态，能够通过运动方式揭示视频中的行为与观察内容。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [10] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: Canvas-to-Image 提出一种统一框架，将文本提示、主体参考、空间布局、姿态约束和版式标注等异构控制信号编码为单一画布图像，实现高保真组合与多模态控制。通过多任务画布训练策略，模型在统一学习范式下联合理解多种控制，显著提升生成质量与控制遵循能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂多模态控制（如文本、姿态、布局等）下仍难以精准生成符合用户意图的图像，缺乏统一的集成控制机制。

Method: 将多种控制信号编码为复合画布图像，设计多任务画布训练策略，使扩散模型在统一框架下进行跨模态视觉-空间推理。

Result: 在多人组合、姿态控制、布局约束及多控制生成等挑战性基准上，该方法在身份保持和控制遵循方面显著优于当前最优方法。

Conclusion: Canvas-to-Image 通过统一画布接口与多任务联合训练，实现了对复杂多模态控制的高效整合，为高保真图像生成提供了新范式。

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [11] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文提出一种简单、轻量且通用的方法，用于识别编码特定技能的神经元。该方法基于先前通过软提示训练在分类任务中识别'技能神经元'的工作，扩展至包含多个技能的复杂场景。通过将神经元激活与外部标签和模型自身置信度等辅助指标相关联，无需手动标记词元即可揭示可解释的任务特异性行为。在开放式文本生成和自然语言推理等任务上进行实证验证，结果表明该方法不仅能检测已知技能的神经元，还能发现大基准（BigBench）中算术推理的未被识别捷径。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在多种任务中表现出色，但其内部机制仍不透明。现有方法在识别特定技能神经元方面存在局限，尤其在多技能复杂场景中缺乏有效手段。因此，亟需一种更高效、可扩展且无需人工干预的分析方法，以揭示模型内部可解释的行为模式。

Method: 提出一种基于神经元激活与外部标签及模型置信度相关性的分析方法。通过关联这些辅助指标，自动识别与特定任务或技能相关的神经元，避免了传统依赖人工聚类或词元标注的繁琐过程。该方法适用于开放生成和推理等多样化任务。

Result: 在开放文本生成和自然语言推理任务中，该方法成功识别出驱动已知技能的神经元；更重要的是，在BigBench的算术推理任务中发现了此前未被察觉的模型捷径，证明其在发现隐含行为方面的有效性与潜力。

Conclusion: 本研究提出的方法为理解大语言模型内部机制提供了新的可解释性工具，具有轻量、通用、无需人工标注的优势，能够有效识别多技能场景下的关键神经元，并揭示潜在的推理捷径，对模型调试、安全性和可控性提升具有重要意义。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [12] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文研究了在大语言模型预训练中引入多种元数据的效果，发现细粒度的文档质量指标等元数据能有效加速训练。有效的元数据具有细粒度特征，且通过元数据追加和可学习元标记（meta-tokens）可进一步提升训练效率。研究还通过探针分析揭示了元数据如何影响模型表征学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注URL等单一元数据信号，未充分探索其他元数据类型对预训练效率的潜在提升作用，因此亟需系统评估更多元数据形式的价值。

Method: 实验考察多种元数据类型，提出元数据追加策略，引入可学习元标记并采用掩码损失进行训练，结合探针分析理解模型内部表示变化。

Result: 细粒度元数据显著加速预训练；元数据追加与可学习元标记能恢复部分速度优势；探针分析显示元数据引导了更高质量的隐层结构学习。

Conclusion: 元数据的细粒度特性是其有效性关键，合理设计元数据注入方式可显著提升大语言模型预训练的效率与效果，为未来高效预训练提供实用指导。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [13] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: 本研究探讨了捷克语AI生成诗歌与人类创作诗歌在感知上的差异，发现捷克母语者难以区分两者，且对AI诗歌存在明显的审美偏见。尽管AI诗歌在评价上并不逊色，但当人们认为是AI所作时，评分更低。作者指出，人们对诗歌的喜爱程度越高，越难准确判断其作者身份。


<details>
  <summary>Details</summary>
Motivation: 当前AI诗歌研究多集中于英语，而本文旨在探索在低资源、形态复杂的斯拉夫语言——捷克语中，AI生成诗歌的可辨识性及读者对其的审美评价，以揭示跨语言、跨文化背景下人工智能创作的艺术接受度。

Method: 通过实验让捷克母语者评估由AI和人类创作的诗歌，记录其对作者身份的判断准确性及美学评分，并使用逻辑回归模型分析喜爱程度与识别准确率之间的关系。

Result: 参与者在判断作者身份时表现接近随机（平均正确率为45.8%），表明捷克语AI诗歌与人类作品难以区分；尽管如此，当被误认为是AI创作时，诗歌获得更低的审美评分，显示出显著的作者身份偏见；喜爱程度越高，识别准确率越低。

Conclusion: AI已能高质量生成捷克语诗歌，在形态复杂、训练数据较少的语言中同样具备高度逼真性。读者对作者身份的认知深刻影响其审美判断，说明审美评价并非完全基于内容本身，而是受认知偏见驱动。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [14] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 本文提出ToolOrchestra方法，通过小规模协调模型（orchestrator）高效调度多种智能工具，利用强化学习结合结果、效率和用户偏好奖励，训练出一个8B参数的Orchestrator模型。该模型在HLE、tau2-Bench和FRAMES等任务上超越GPT-5，准确率更高且成本降低约70%，展现出更优的性能-成本权衡与对未见工具的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在解决复杂问题时存在计算成本高、效率低的问题，而单纯依赖大型模型难以实现高效与可扩展的工具使用。因此需要一种轻量级但高效的协调机制，以提升智能体在复杂任务中的表现和资源利用效率。

Method: 提出ToolOrchestra方法，采用强化学习框架，设计包含结果奖励、效率奖励和用户偏好奖励的多目标奖励函数，训练小型协调模型来动态选择并组合工具完成任务。

Result: Orchestrator在HLE上达到37.1%得分，优于GPT-5的35.1%，同时效率提升2.5倍；在tau2-Bench和FRAMES上显著超越GPT-5，成本仅为其30%左右。模型具备良好的性能-成本平衡及对新工具的泛化能力。

Conclusion: 通过轻量级协调模型与多样化工具的组合，能够实现比传统大模型方法更高效、更有效的智能推理系统，为构建实用、可扩展的工具增强型智能体提供了新路径。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [15] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型（LLMs）在不同任务难度下的泛化能力，通过使用大量LLM输出和项目反应理论（IRT）对六个数据集中的例子进行难度排序，避免了人类主观判断。研究发现，无论是在简单还是困难数据上训练，都难以在全范围难度上实现一致的性能提升，表明训练和评估数据应包含多样化的难度水平，简化难度选择存在风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究对在简单或困难数据上训练是否能带来更好效果存在分歧，且不清楚这些提升是否适用于简单或困难的测试数据。因此需要更客观、大规模、细粒度地分析LLMs在不同难度间的泛化能力。

Method: 采用大量不同LLM的输出与项目反应理论（IRT）对六大数据集中的示例进行难度评分，以非人为方式量化难度，进而系统评估模型在不同难度上的泛化表现。

Result: 跨难度泛化能力通常有限；在简单或困难数据上训练均无法在所有难度级别上持续提升性能。

Conclusion: 为了有效训练和评估大语言模型，必须在数据中保留多样化的难度层次，避免仅依赖单一难度的数据，否则可能带来不可预测的性能下降。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 本文通过降维技术提取、处理并可视化基于Transformer的语言模型中的潜在状态几何结构，利用PCA和UMAP对GPT-2和LLaMa模型的层间激活进行系统分析，揭示了注意力与MLP组件输出在中间层的明显分离、初始序列位置的高范数潜态、以及GPT-2位置嵌入的高维螺旋结构等几何模式。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（如GPT-2和LLaMa）内部机制的可解释性问题，特别是其潜在空间中激活状态的几何特性，以支持更系统的模型内部分析和可复现的可解释性研究。

Method: 采用主成分分析（PCA）和均匀流形近似（UMAP）对Transformer块中多个时间点的层间激活进行降维与可视化，系统分析模型内部潜在状态的几何演化。

Result: 发现了注意力与MLP组件输出在中间层的显著分离、初始位置潜态的高范数特征、GPT-2位置嵌入的高维螺旋结构、以及LLaMa中序列级几何模式；通过重复标记序列实验进一步验证了这些模式的稳定性。

Conclusion: 本研究为理解Transformer模型内部动态提供了新的几何视角，所提出的方法有助于推动可解释性研究的系统化与可复现性，代码已开源以支持后续研究。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [17] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 算法在2012至2023年间使AI训练的浮点运算效率提升了22,000倍，但小规模实验仅能解释不到10倍的提升；结合文献分析，额外创新最多贡献不足10倍，总计不足100倍。通过扩展实验发现，大部分效率提升源于具有规模依赖性的算法改进，特别是从LSTM到Transformer的转变，其计算最优缩放规律存在指数差异，而其他创新的缩放效应相近。基于实验外推和文献估计，可解释6,930倍的效率提升，其中多数由算法规模依赖性驱动。研究表明，小模型的算法进步远慢于以往认知，且算法效率衡量高度依赖参考基准。


<details>
  <summary>Details</summary>
Motivation: 评估过去十余年中算法对AI训练效率的实际贡献，揭示现有估算可能高估了小规模模型的算法进步，同时探索算法效率与计算规模之间的关系。

Method: 通过小规模消融实验、文献调研与大规模缩放实验，对比LSTM与Transformer等算法在不同计算规模下的性能表现，分析其缩放特性，并利用实验外推法估算整体效率提升。

Result: 发现算法效率提升显著依赖计算规模，尤其是从LSTM到Transformer的过渡带来主要效率增益，实际可解释6,930倍的效率提升，远超传统估算。

Conclusion: 算法效率的提升并非普遍适用于所有模型规模，其进展在小模型上被严重低估，且效率评价结果强烈依赖于所参照的模型规模与基准，强调了在评估算法进步时需考虑规模依赖性。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [18] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 深度视觉变换器（ViT）在加深后性能反而下降，本文通过系统分析ViT-S/B/L在ImageNet上的表现，发现表征演化遵循‘悬崖-平台-攀登’三阶段模式。性能提升与[CLS] token被分散的patch token共识取代有关。使用信息打乱指数量化信息混合，发现大模型中信息-任务权衡出现较晚，额外层数主要促进信息扩散而非任务性能提升。因此，未来设计应关注精确控制深度以实现清晰的相变，而非盲目增加参数。该指数可作为模型诊断和设计目标。


<details>
  <summary>Details</summary>
Motivation: 深层视觉变换器（ViT）在实践中往往表现不如浅层模型，挑战了常规的扩展假设。研究旨在揭示深层模型性能下降的根本原因，并探索如何更有效地扩展模型深度。

Method: 通过对ViT-S、ViT-B和ViT-L在ImageNet上的系统性实证分析，识别表征随深度演化的三阶段模式；引入信息打乱指数（Information Scrambling Index）量化信息混合程度；分析[CLS] token与patch token的信息分布变化。

Result: 发现深层模型存在‘悬崖-平台-攀登’三阶段演化模式；性能提升与[CLS] token被分布式共识取代相关；大模型中信息-任务权衡延迟约10层，额外层数主要带来信息扩散而非性能提升。

Conclusion: Transformer架构在当前深度范围内，应通过精心校准的深度实现清晰的相变，而非单纯增加参数量。信息打乱指数可作为现有模型的诊断工具，并为未来架构设计提供参考。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [19] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 该研究通过8数码谜题评估大语言模型（LLMs）在无外部工具支持下的规划与状态推理能力。尽管使用了多种提示策略（如零样本、思维链、算法链）和分层反馈，部分模型表现有所提升，但整体仍存在严重缺陷：内部状态表示脆弱，常产生无效动作；启发式规划能力弱，易陷入循环或选择非目标导向的动作。即使引入外部移动验证器以确保仅生成合法动作，所有模型均无法解决任何谜题。研究结论表明，当前LLMs在缺乏代码解释器等外部工具时，在复杂规划任务中存在显著局限，未来进展需依赖显式状态维护和结构化搜索机制。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在无外部工具支持下进行规划与状态推理的能力，明确其在复杂任务中的局限性，并为后续改进提供方向。

Method: 采用经典的8数码谜题作为测试任务，评估四种大语言模型在不同提示策略（零样本、思维链、算法链）及分层反馈条件下的表现；进一步引入外部移动验证器以排除无效动作干扰，通过定量与定性分析揭示模型的内在缺陷。

Result: 尽管反馈机制提升了部分模型的成功率，但多数成功路径冗长且计算成本高；即使在外部移动验证器辅助下，所有模型仍无法解决任何谜题；主要问题包括内部状态表示不稳健和启发式规划能力不足，导致频繁无效动作和循环行为。

Conclusion: 当前大语言模型在缺乏外部工具的情况下，规划与状态推理能力存在显著局限，未来的发展需要引入显式状态维护和结构化搜索机制以突破现有瓶颈。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [20] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文将系统动力学与结构方程建模整合到一个共同的数学框架中，以生成系统、开发方法并比较结果，从而为数据科学和人工智能/机器学习应用中的系统动力学基础认识论提供信息。


<details>
  <summary>Details</summary>
Motivation: 当前负责任的人工智能/机器学习发展面临挑战，主要由于不同方法基于不同假设难以融合，特别是丹娜·梅多的‘不可避免的先验’问题。本文旨在解决这一障碍。

Method: 通过构建一个统一的数学框架，将系统动力学与结构方程建模相结合，实现系统生成、方法开发和结果对比。

Result: 成功建立了一个可操作的跨方法框架，有助于推动负责任AI/ML的发展，并深化对系统动力学在数据科学中应用的理解。

Conclusion: 该框架为整合不同理论范式提供了可行路径，有助于提升人工智能系统在复杂社会系统中的可靠性与责任性。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [21] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: ViLoMem提出一种双流记忆框架，通过分离视觉干扰与逻辑错误，实现多模态语义记忆的累积与更新，显著提升MLLM在跨域任务中的表现并减少重复错误。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的记忆方法存在简略偏差，且仅记录单模态行为，无法有效保留视觉注意力与逻辑推理的协同作用，与人类多模态整合记忆机制不一致。

Method: 设计双流记忆架构，分别编码视觉干扰模式和逻辑推理错误，采用生长-精炼原则逐步积累并更新多模态知识，支持稳定、可泛化的策略学习。

Result: 在六个多模态基准上，ViLoMem显著提升通过率（pass@1），大幅减少重复的视觉与逻辑错误；消融实验证明双流结构与显式分离干扰-幻觉的重要性。

Conclusion: ViLoMem通过误差感知的多模态记忆机制，为持续学习和跨领域智能体学习提供了有效路径，推动了多模态大模型向更接近人类认知的方向发展。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>
