<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 49]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.LG](#cs.LG) [Total: 35]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis](https://arxiv.org/abs/2601.02409)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh*

Main category: cs.CV

TL;DR: 本文提出双框架解决方案EGxFSL和xGAL，以应对医学图像分析中标签数据稀缺与模型可解释性不足的问题。EGxFSL通过放射科医生定义的感兴趣区域结合Grad-CAM-based Dice损失实现可解释的少样本学习；xGAL则基于预测不确定性和注意力不一致性的迭代样本选择，构建可解释性驱动的主动学习闭环。在BraTS、VinDr-CXR和SIIM-COVID-19数据集上表现优异，尤其在极低数据量下显著优于随机采样，且可视化验证了模型聚焦于诊断相关区域，具备跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析面临标注数据稀缺与模型缺乏可解释性两大挑战，限制了临床AI的应用。现有少样本学习（FSL）虽缓解数据问题但缺乏透明度，主动学习（AL）虽优化数据采集却忽视样本的可解释性，亟需一种兼顾数据效率与可解释性的新方法。

Method: 提出EGxFSL框架，利用放射科医生定义的ROI作为空间监督信号，结合Grad-CAM生成的Dice损失与原型分类联合优化，提升少样本学习的可解释性；设计xGAL框架，通过同时考虑预测不确定性与注意力图的不一致性来指导样本选择，形成可解释性与训练过程协同演进的闭环系统。

Result: 在BraTS（MRI）、VinDr-CXR（胸片）和SIIM-COVID-19（胸片）数据集上分别达到92%、76%和62%的准确率，全面超越非引导基线。在极端数据受限条件下，xGAL仅用680个样本即达76%准确率，远超随机采样的57%。Grad-CAM可视化显示模型聚焦于诊断相关区域，且在乳腺超声数据上验证了跨模态泛化能力。

Conclusion: EGxFSL与xGAL共同构建了一个兼具数据高效性与可解释性的医学图像分析框架，通过专家引导与可解释性反馈实现智能模型的可信部署，为临床AI落地提供了可行路径。

Abstract: Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\%, 76\%, and 62\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\% accuracy with only 680 samples versus 57\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.

</details>


### [2] [Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning](https://arxiv.org/abs/2601.02422)
*Wenting Lu,Didi Zhu,Tao Shen,Donglin Zhu,Ayong Ye,Chao Wu*

Main category: cs.CV

TL;DR: 提出CoCoT框架，通过动态多区域定位和关系感知推理解决视觉语言推理中对单一粗粒度图像区域的依赖及推理步骤间的语义碎片化问题。构建了包含74,691个高质量样本的CoCoT-70K数据集，并在六个基准上实现平均准确率提升15.4%（LLaVA-1.5）和4.0%（Qwen2-VL）。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought方法在跨模态场景中存在对单一粗粒度图像区域的过度依赖以及推理步骤间语义碎片化的缺陷，限制了复杂视觉推理能力。

Method: 提出CoCoT框架，包含两项核心创新：(a) 动态多区域定位，根据问题自适应检测最相关图像区域；(b) 关系感知推理，通过迭代对齐视觉线索促进多区域协作，形成连贯逻辑的思维链。

Result: 在六个挑战性基准上，CoCoT显著提升复杂视觉推理性能，相较于LLaVA-1.5平均准确率提升15.4%，相较于Qwen2-VL提升4.0%。

Conclusion: CoCoT框架有效克服了现有方法在多模态推理中的关键瓶颈，通过动态区域选择与多区域协同推理，实现了更精准、连贯的跨模态思维链生成，为复杂视觉理解提供了新范式。

Abstract: Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) frame- work, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively align- ing visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual rea- soning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.

</details>


### [3] [NitroGen: An Open Foundation Model for Generalist Gaming Agents](https://arxiv.org/abs/2601.02427)
*Loïc Magne,Anas Awadalla,Guanzhi Wang,Yinzhen Xu,Joshua Belofsky,Fengyuan Hu,Joohwan Kim,Ludwig Schmidt,Georgia Gkioxari,Jan Kautz,Yisong Yue,Yejin Choi,Yuke Zhu,Linxi "Jim" Fan*

Main category: cs.CV

TL;DR: NitroGen is a vision-action foundation model trained on 40,000 hours of gameplay across 1,000+ games, using a large-scale video-action dataset, multi-game benchmark, and behavior cloning. It shows strong generalization in combat, precision control, and exploration, outperforming models trained from scratch by up to 52% in task success. The dataset, evaluation suite, and model are released.


<details>
  <summary>Details</summary>
Motivation: To develop a generalist gaming agent capable of transferring knowledge across diverse games, overcoming the limitations of task-specific or game-specific models.

Method: Constructing an internet-scale video-action dataset via automatic extraction of player actions from public gameplay videos; designing a multi-game benchmark for cross-game evaluation; training a unified vision-action model using large-scale behavior cloning.

Result: NitroGen demonstrates strong performance across various game types, including 3D combat, 2D platformers, and procedurally generated worlds, with significant transfer gains (up to 52% improvement) on unseen games.

Conclusion: NitroGen establishes a new benchmark for generalist gaming agents through scalable training and effective cross-game generalization, enabling future research through open-sourced resources.

Abstract: We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.

</details>


### [4] [Understanding Pure Textual Reasoning for Blind Image Quality Assessment](https://arxiv.org/abs/2601.02441)
*Yuan Li,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文从信息流视角分析了文本信息在盲图像质量评估（BIQA）中的作用，通过对比现有模型与三种学习图像-文本-分数关系的范式（思维链、自一致性、自编码器），发现仅使用文本信息时现有模型性能显著下降；自一致性范式显著缩小了图像与文本条件预测之间的差距，而思维链和自编码器范式效果有限，但为后续优化提供了方向。


<details>
  <summary>Details</summary>
Motivation: 探究文本信息在盲图像质量评估（BIQA）中的贡献机制，明确文本对评分相关图像内容的表征能力，以改进文本推理在高阶视觉任务中的应用。

Method: 采用三种新范式——思维链（Chain-of-Thought）、自一致性（Self-Consistency）和自编码器（Autoencoder）——来建模图像-文本-分数之间的关系，并与现有BIQA模型进行对比实验，从信息流角度分析文本的作用。

Result: 仅依赖文本信息时，现有模型性能大幅下降；自一致性范式显著缩小图像与文本条件预测间的差距（PLCC/SRCC差异降至0.02/0.03）；思维链提升有限，自编码器虽效果不佳但揭示了优化方向。

Conclusion: 文本信息在BIQA中具有重要但受限的作用，自一致性范式能有效增强文本推理能力，未来应聚焦于提升文本对图像质量内容的表征能力，以推动高阶视觉任务中的文本推理发展。

Abstract: Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.

</details>


### [5] [Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative](https://arxiv.org/abs/2601.02443)
*Li Wang,Xi Chen,XiangWen Deng,HuaHui Yi,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 该研究评估了多模态大语言模型（MLLM）在膝骨关节炎（OA）X光片分类任务中的表现，发现单独的视觉编码器在分类准确率上优于完整的MLLM流程，且微调大语言模型（LLM）并未带来显著提升。在小而平衡的数据集上使用LoRA微调效果优于大规模但不平衡的数据集，表明数据质量与平衡性比数据量更重要。研究建议：在特定医学分类任务中，应优先优化视觉编码器并精心设计数据集，而非依赖MLLM架构进行高置信度诊断分类。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态大语言模型在医学视觉问答和报告生成方面表现良好，但在疾病特异性分类任务中表现不佳，尤其膝骨关节炎（OA）影像分类在当前基准中被严重忽视。因此，需要系统评估MLLM在该任务中的有效性，并探索其关键组件的作用。

Method: 通过系统消融实验，调整视觉编码器、连接模块和大语言模型（LLM）的不同组合，结合多种训练策略，在膝骨关节炎放射影像分类任务上评估各组件对诊断准确率的影响。比较不同数据集规模与分布（500张平衡数据 vs. 5778张不平衡数据）下的微调效果。

Result: 视觉编码器单独使用时分类性能优于完整MLLM；微调LLM未显著提升性能；在小而平衡的数据集上使用LoRA微调效果更佳；数据平衡性和质量比数据规模更为重要。

Conclusion: 对于需要高置信度的特定医学图像分类任务，多模态大语言模型架构并不适用，其核心作用应为解释和报告生成，而非主分类器。未来临床系统开发应聚焦于视觉编码器优化和高质量、类平衡的数据集构建。

Abstract: Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.

</details>


### [6] [MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark](https://arxiv.org/abs/2601.02536)
*Shaden Shaar,Bradon Thymes,Sirawut Chaixanien,Claire Cardie,Bharath Hariharan*

Main category: cs.CV

TL;DR: 本文提出一个新型开放式多模态视频问答基准MovieRecapsQA，基于电影回顾视频（recap videos）构建，该视频通过同步的视觉（回顾视频）和文本（回顾摘要）模态总结电影关键事件。研究生成约8.2万个与电影字幕对齐的问答对，并提供用于验证答案的“事实”信息，实现无参考评估。这是首个为输入（视频和/或文本）提供明确文本上下文的开放式视频问答基准，支持多长度视频片段和问题分类，便于细粒度分析。实验评估了七种先进多模态大模型，发现：1）仅依赖视觉的问题仍最困难；2）模型在有文本输入时倾向于依赖文本；3）从视频中准确提取事实信息仍是挑战；4）专有与开源模型在视频相关问题上表现相当。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准难以捕捉真实世界视频中视觉与对话线索的多模态推理，且大多非开放式，因自由形式回答评估困难。为解决此问题，需构建一个能支持开放问答、提供明确文本上下文并可进行无参考评估的多模态视频问答基准。

Method: 利用电影回顾视频（一种YouTube内容类型）作为数据源，结合其同步的视觉内容和文本摘要，生成与电影字幕对齐的约8.2万组问答对。每个问题配有可验证的答案所需的事实信息，实现无需参考的评估方式。提供不同长度的视频片段（如回顾片段、电影片段）及按模态和类型分类的问题，支持细致分析。

Result: 评估七种先进多模态大模型表明：1）仅依赖视觉的问题仍最具挑战性；2）当存在文本输入时，模型倾向于依赖文本；3）从视频中准确提取事实信息仍非常困难；4）专有模型与开源模型在视频依赖型问题上的表现相近。

Conclusion: MovieRecapsQA是首个提供显式文本上下文并支持开放式问答的多模态视频理解基准，有效推动了复杂视频理解任务的研究。实验揭示当前模型在跨模态推理与视频事实提取方面仍有显著局限，尤其在纯视觉理解上表现不佳。

Abstract: Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary "facts" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.

</details>


### [7] [DreamLoop: Controllable Cinemagraph Generation from a Single Photograph](https://arxiv.org/abs/2601.02646)
*Aniruddha Mahapatra,Long Mai,Cusuh Ham,Feng Liu*

Main category: cs.CV

TL;DR: DreamLoop 是一种可控视频合成框架，旨在从单张照片生成高质量、复杂的 cinemagraphs（动态图像），无需专门的 cinemagraph 训练数据。通过在通用视频扩散模型上引入时间桥接和运动条件化两个训练目标，实现对动画轨迹和时序的灵活控制。推理阶段利用输入图像作为首尾帧以保证无缝循环，并通过静态轨迹保持背景静止，同时允许用户指定目标物体的运动路径，从而实现直观的动画控制。该方法首次实现了对一般场景的高自由度 cinemagraph 生成，在质量与用户意图契合度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像动画技术仅适用于简单、低频运动，且局限于重复纹理如水和烟雾等特定领域；而大规模视频扩散模型缺乏针对 cinemagraph 的定制化设计与数据支持，难以生成符合要求的无缝、可控循环动画。因此需要一种能够从单图生成复杂、可控 cinemagraph 的新方法。

Method: 采用通用视频扩散模型，通过引入两个训练目标：时间桥接（temporal bridging）和运动条件化（motion conditioning）。在推理阶段，将输入图像同时作为第一帧和最后一帧，强制形成无缝循环；通过静态轨迹约束背景不变；通过用户定义的目标物体运动路径实现对动画轨迹与时间的精确控制。

Result: DreamLoop 能够生成高质量、复杂且符合用户意图的 cinemagraphs，具备良好的视觉连贯性与控制灵活性，在多个评估指标上超越现有方法，首次实现了对一般场景的可控 cinemagraph 生成。

Conclusion: DreamLoop 是首个无需专用训练数据即可实现通用场景下可控 cinemagraph 生成的方法，其通过创新的训练策略与推理机制，成功解决了传统方法在控制性、泛化性和复杂性方面的局限，为动态图像创作提供了新范式。

Abstract: Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.

</details>


### [8] [GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images](https://arxiv.org/abs/2601.02709)
*Shuman He,Xiehua Li,Xioaju Yang,Yang Xiong,Keqin Li*

Main category: cs.CV

TL;DR: 提出一种基于通道移除重建的新型检测范式GRRE，利用真实图像与AI生成图像在移除绿色通道后的重建误差差异，实现对生成图像的高精度检测。该方法在多种生成模型上表现出色，具备良好的跨模型泛化能力和对各类扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型（如扩散模型和GAN）生成的图像越来越难以与真实图像区分，而现有检测方法在面对未见过的生成模型时性能下降，亟需一种具有强泛化能力的检测方法。

Method: 通过移除图像的绿色（G）通道并进行重建，计算重建误差，利用真实图像与生成图像在此误差上的显著差异进行检测，提出G-channel Removed Reconstruction Error（GRRE）方法。

Result: GRRE在多个生成模型上均实现了高检测准确率，尤其在未见过的模型上表现优异，对各种后处理操作和扰动也表现出强鲁棒性，优于现有方法。

Conclusion: 基于通道移除的重建误差是一种有效的图像真实性检测手段，为应对生成式AI带来的伪造威胁提供了强有力的工具。

Abstract: The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.

</details>


### [9] [CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos](https://arxiv.org/abs/2601.02716)
*Taeyeon Kim,Youngju Na,Jumin Lee,Minhyuk Sung,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: CAMO是一种无类别依赖的运动迁移框架，能够直接从单目2D视频中将动作迁移到多样化的3D网格上，无需预定义模板或显式的3D监督。其核心是结合形态参数化关节式3D高斯点云模型与密集语义对应关系，通过优化联合调整形状与姿态，有效缓解了形状-姿态模糊问题，实现了在多种物体类别和非正式视频场景下的高质量运动迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将2D视频中的运动迁移到3D资产时，常受限于类别特定的参数化模板以及形状-姿态之间的固有歧义，难以处理多样化目标形状和复杂场景。因此需要一种无需依赖预设模板、可适应多种物体类别的通用运动迁移方法。

Method: 提出基于形态参数化关节式3D高斯点云模型的框架，结合密集语义对应关系，通过优化过程同步调整目标形状与姿态，实现从单目2D视频到任意3D网格的直接运动迁移。

Result: 实验表明，CAMO在运动精度、计算效率和视觉一致性方面均优于现有方法，显著提升了在不同物体类别及非正式视频场景下的运动迁移性能。

Conclusion: CAMO成功实现了无需类别先验和3D监督的通用运动迁移，为多样化3D资产的动作生成提供了高效且视觉逼真的解决方案。

Abstract: Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.

</details>


### [10] [Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing](https://arxiv.org/abs/2601.02721)
*Guoquan Zheng,Jie Hao,Huiyu Duan,Yongming Han,Liang Yuan,Dong Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D网格显著性标注框架，通过视锥采样（VCS）和混合流形-欧几里得约束扩散（HCD）算法，解决传统方法在拓扑结构建模上的不足，提升复杂几何下的注意力传播准确性，更符合人类自然感知。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格显著性标注方法沿用2D图像方法，忽略了3D几何拓扑与2D图像阵列的本质差异；同时，当前VR眼动追踪依赖单射线采样和欧几里得平滑，导致纹理注意力误判和信号跨间隙泄漏，影响标注可靠性。

Method: 提出视锥采样（VCS）策略，模拟人眼中央凹感受野，使用高斯分布射线束提升复杂拓扑下的采样鲁棒性；设计混合流形-欧几里得约束扩散（HCD）算法，融合流形测地线约束与欧几里得尺度，实现拓扑一致的显著性传播，避免拓扑短路和混叠现象。

Result: 所提框架能够有效减少信号泄漏，提升显著性传播的拓扑一致性，获得更贴近人类感知的高保真3D注意力标注，为3D网格显著性研究提供更准确、鲁棒的基准数据。

Conclusion: 该框架通过创新的采样与扩散机制，实现了对3D几何结构特性的精准建模，显著提升了3D mesh saliency GT的质量，为虚拟现实中的视觉建模提供了可靠基础。

Abstract: Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating "topological short-circuits" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.

</details>


### [11] [Foreground-Aware Dataset Distillation via Dynamic Patch Selection](https://arxiv.org/abs/2601.02727)
*Longzhen Li,Guang Li,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出一种前景感知的数据集蒸馏方法，通过内容自适应的补丁选择策略提升蒸馏效果。针对传统优化方法计算成本高、生成伪图像等问题，以及非优化方法中固定补丁选择策略可能丢失关键对象信息的缺陷，该方法利用Grounded SAM2识别前景并计算每张图像的前景占据率，进而设定类别级补丁选择阈值。基于此阈值，设计动态补丁选择机制：当前景占主导时直接使用整图，否则从候选补丁中选取最信息量的补丁，从而保留主要物体的关键信息并减少冗余背景。实验表明，该方法在多个基准上均优于现有方法，提升了蒸馏性能与模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统优化型数据集蒸馏方法存在计算开销大、内存压力高、生成图像不真实等问题；而现有非优化方法虽减轻了部分负担，但其固定的补丁选择策略可能忽略主对象的重要信息，导致知识损失。因此需要一种既能降低计算成本又能有效保留关键语义信息的自适应补丁选择机制。

Method: 1. 使用Grounded SAM2检测图像中的前景区域并计算每张图像的前景占据率；2. 根据前景占据率建立类别级别的补丁选择阈值；3. 设计双路径动态补丁选择策略：若前景占主导，则直接使用整图；否则从多个候选补丁中选择最具信息量的补丁；4. 以真实图像补丁构建紧凑的合成数据集。

Result: 在多个公开图像分类和目标检测基准上，所提方法显著优于现有数据集蒸馏方法，在不同架构和图像组成下均表现出更强的泛化能力与鲁棒性，生成的蒸馏数据更具代表性与信息量。

Conclusion: 本文提出的前景感知数据集蒸馏方法通过内容自适应的补丁选择机制，有效平衡了信息保留与计算效率，克服了传统方法中因固定选择策略导致的关键信息丢失问题，为高效、高质量的数据集蒸馏提供了新思路。

Abstract: In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.

</details>


### [12] [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/abs/2601.02737)
*Zanting Ye,Xiaolong Niu,Xuanbin Wu,Xu Han,Shengyuan Liu,Jing Hao,Zhihao Peng,Hao Sun,Jieqin Lv,Fanghu Wang,Yanchao Huang,Hubing Wu,Yixuan Yuan,Habib Zaidi,Arman Rahmim,Yefeng Zheng,Lijun Lu*

Main category: cs.CV

TL;DR: 本文提出PET-Bench，首个大规模功能性影像基准，涵盖52,308个层级问答对，用于评估多模态大模型在功能成像中的表现。研究发现当前视觉编码器无法独立于解剖先验解析功能示踪剂分布，导致链式思维（CoT）提示引发严重幻觉，产生看似合理但不准确的诊断。为此，提出原子视觉对齐（AVA）策略，通过微调强化低层次功能感知，再进行高层次推理，显著提升诊断准确率（最高达14.83%），并使CoT从幻觉源转变为可靠推理工具。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在解剖影像任务中表现优异，但在功能影像如PET方面能力不足，尤其缺乏对功能示踪剂分布的独立感知能力，亟需系统评估与改进。

Method: 构建PET-Bench基准数据集，评估19种主流多模态大模型；发现CoT提示导致视觉-语言脱钩；提出原子视觉对齐（AVA）微调策略，强制模型先掌握低级功能感知，再进行高级推理。

Result: AVP有效弥合了功能感知鸿沟，将原本易产生幻觉的CoT转化为稳健推理工具，诊断准确率提升最高达14.83%。

Conclusion: 功能影像理解存在深层感知差距，仅靠提示工程无法解决；通过引入基于低级感知优先的训练策略（AVA），可显著提升模型在功能影像上的可靠性与准确性。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.

</details>


### [13] [D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2601.02747)
*Zixiao Wen,Zhen Yang,Xianjie Bao,Lei Zhang,Xiantai Xiang,Wenshuai Li,Yuhan Liu*

Main category: cs.CV

TL;DR: 提出D$^3$R-DETR，一种基于双域密度精炼的DETR检测器，通过融合空间与频域信息，提升小目标检测性能，在AI-TOD-v2数据集上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 主流Transformer-based检测器在小目标检测中存在收敛慢和查询-对象匹配不准确的问题，主要由于像素信息极少且目标密度变化大。

Method: 融合空间域与频率域信息，对低层特征图进行精炼，生成更精确的目标密度图，以指导模型精确定位小目标。

Result: 在AI-TOD-v2数据集上，D$^3$R-DETR显著优于现有最先进的小目标检测器。

Conclusion: D$^3$R-DETR通过双域密度精炼有效提升了小目标检测的精度与收敛速度，为遥感智能解译提供了有力支持。

Abstract: Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.

</details>


### [14] [Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups](https://arxiv.org/abs/2601.02759)
*Hyungtae Lim,Minkyun Seo,Luca Carlone,Jaesik Park*

Main category: cs.CV

TL;DR: BUFFER-X 是一种无需训练的点云配准框架，通过几何自举、分布感知的最远点采样和局部坐标归一化，实现了零样本泛化，解决了固定超参数、关键点检测器跨域迁移差和绝对坐标导致尺度不匹配的问题。其多尺度匹配机制在不同场景下表现鲁棒，并推出了轻量版 BUFFER-X-Lite 以提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的点云配准方法在零样本泛化方面存在局限，需要针对新环境进行超参数调优或重新训练，主要受限于固定超参数、可迁移性差的关键点检测器以及绝对坐标带来的尺度不一致问题。

Method: 提出缓冲区-扩展（BUFFER-X）框架：(a) 使用几何自举自动估计超参数；(b) 采用分布感知的最远点采样替代学习型关键点检测器；(c) 通过局部坐标归一化实现尺度一致性；结合层次化多尺度匹配提取跨局部、中程和全局感受野的对应关系。同时设计 BUFFER-X-Lite，利用早期退出和快速位姿求解器减少计算时间。

Result: 在包含12个数据集的综合基准上评估，涵盖物体级、室内与室外场景，以及异构激光雷达间的跨传感器配准，结果表明该方法无需人工调参或先验知识即可有效泛化，且 BUFFER-X-Lite 相比原版计算时间减少43%而保持精度。

Conclusion: BUFFER-X 及其轻量版本实现了真正意义上的零样本点云配准，在多种复杂环境中表现出强泛化能力与高效率，为实际部署提供了可行方案。

Abstract: Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.

</details>


### [15] [AnyDepth: Depth Estimation Made Easy](https://arxiv.org/abs/2601.02760)
*Zeyu Ren,Zeyu Zhang,Wukai Li,Qingxiang Liu,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出一种轻量级、数据驱动的零样本单目深度估计框架，采用DINOv3作为视觉编码器获取高质量密集特征，并设计了结构简单的SDT解码器，相比DPT减少85%-89%参数量且提升精度。同时引入基于质量的样本过滤策略，优化数据集质量并缩小规模。在五个基准测试上表现优于DPT，强调模型设计与数据质量平衡对高效泛化的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法依赖大规模数据集和复杂解码器，导致效率低、泛化能力差。需要更轻量、高效且泛化能力强的零样本方法。

Method: 使用DINOv3作为视觉编码器；设计简单深度变压器（SDT）解码器，采用单路径特征融合与上采样以降低计算开销；提出基于质量的样本过滤策略，筛选高质量训练样本。

Result: 在五个基准测试中，所提框架在精度上超过DPT，参数量减少85%-89%，计算效率更高，数据利用更高效。

Conclusion: 通过合理平衡模型设计与数据质量，可实现高效且泛化的零样本单目深度估计，为未来研究提供新方向。

Abstract: Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.

</details>


### [16] [ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration](https://arxiv.org/abs/2601.02763)
*Xu Zhang,Huan Zhang,Guoli Wang,Qian Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: ClearAIR 是一种受人类视觉感知启发的新型全功能图像修复框架，采用分层、由粗到细的修复策略。它首先利用多模态大语言模型进行全局图像质量评估，以更准确地识别复杂退化；接着通过语义交叉注意力生成粗粒度语义提示，并结合退化感知模块实现区域级退化特征捕捉；最后引入自监督的内部线索重用机制，挖掘图像自身内在信息以增强细节恢复。实验表明，ClearAIR 在多种合成与真实世界数据集上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有全功能图像修复方法依赖特定退化表征，常导致过度平滑和伪影问题。为解决此问题，本文提出受人类视觉感知启发的新框架，旨在提升对复杂退化场景的适应性与修复精度。

Method: 提出 ClearAIR 框架，包含三部分：1）基于 MLLM 的跨模态图像质量评估，实现全局退化理解；2）语义交叉注意力与退化感知模块，实现区域感知与任务识别；3）自监督内部线索重用机制，用于精细细节恢复。

Result: 在多个合成及真实世界图像修复数据集上，ClearAIR 均取得领先性能，有效缓解了过度平滑与伪影问题，显著提升了修复质量。

Conclusion: ClearAIR 通过融合人类视觉感知机制与先进的深度学习技术，实现了对复杂退化的高效建模与精准修复，为全功能图像修复提供了新思路。

Abstract: All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.

</details>


### [17] [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/abs/2601.02771)
*Boyu Chang,Qi Wang,Xi Guo,Zhixiong Nan,Yazhou Yao,Tianfei Zhou*

Main category: cs.CV

TL;DR: AbductiveMLLM 提出一种双模态推理框架，通过结合语言与图像生成来增强多模态大模型在视觉归纳推理（VAR）中的表现。该模型包含两个协同组件：REASONER 负责在语言层面探索并筛选合理的解释，IMAGINER 则利用文本到图像扩散模型生成与解释一致的视觉场景，从而增强模型的上下文理解能力。两者联合训练，在标准 VAR 基准上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在视觉归纳推理任务中表现不佳，难以像人类一样基于不完整视觉信息进行合理推断。人类在推理时常结合语言和图像思维，因此本文旨在模仿这种双模态认知机制，提升模型的归纳推理能力。

Method: 提出 AbductiveMLLM 框架，包含两个组件：1）REASONER 通过盲式大语言模型生成多种可能解释，并基于跨模态因果对齐剔除视觉不一致假设；2）IMAGINER 使用文本到图像扩散模型，以输入视频和 REASONER 输出为条件生成符合语义的视觉场景，增强模型上下文感知。两模块端到端联合训练。

Result: 在多个标准视觉归纳推理基准测试中，AbductiveMLLM 显著优于传统方法和现有先进 MLLMs，实现当前最佳性能。

Conclusion: 通过模仿人类语言与图像双重推理机制，AbductiveMLLM 有效提升了多模态模型在不完整视觉观察下的合理解释能力，为复杂视觉推理任务提供了新范式。

Abstract: Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to "imagine" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.

</details>


### [18] [DreamStyle: A Unified Framework for Video Stylization](https://arxiv.org/abs/2601.02785)
*Mengtian Li,Jinshu Chen,Songtao Zhao,Wanquan Feng,Pengqi Tu,Qian He*

Main category: cs.CV

TL;DR: DreamStyle 是一个统一的视频风格化框架，支持文本引导、风格图像引导和首帧引导三种方式，并通过精心设计的数据清洗流程获取高质量配对视频数据。该框架基于基础的图像到视频模型，采用特定标记的低秩适配（LoRA）方法减少不同条件标记之间的混淆，实验证明其在风格一致性与视频质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法通常仅支持单一风格条件输入，限制了应用范围；同时缺乏高质量数据集导致风格不一致和时间闪烁问题。因此需要一种统一且高效的方法来支持多种风格条件并提升生成质量。

Method: 基于基础图像到视频（I2V）模型，采用低秩适配（LoRA）结合标记特异性上采样矩阵，以降低不同风格条件标记间的混淆；配合一套专门设计的数据清洗流程，构建高质量配对视频数据集。

Result: DreamStyle 在三种风格化任务中均表现良好，尤其在风格一致性和视频质量方面显著优于现有方法，证明其有效性与通用性。

Conclusion: DreamStyle 提供了一个统一、高效且高质量的视频风格化解决方案，支持多条件输入并有效缓解风格不一致与时间闪烁问题，为视频风格化研究提供了新范式。

Abstract: Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.

</details>


### [19] [StableDPT: Temporal Stable Monocular Video Depth Estimation](https://arxiv.org/abs/2601.02793)
*Ivan Sobko,Hayko Riemenschneider,Markus Gross,Christopher Schroers*

Main category: cs.CV

TL;DR: 提出一种新方法StableDPT，通过引入可训练的时序模块，将单图像深度估计模型适配到视频处理中，利用跨注意力机制融合全局上下文和帧间关系，实现更准确、稳定的深度预测。该方法在多个基准数据集上表现优异，具有更高的时间一致性、竞争性性能和2倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 解决单图像深度估计模型应用于视频序列时出现的时间不稳定性与闪烁伪影问题。

Method: 基于预训练的视觉变换器（ViT）编码器，增强密集预测变换器（DPT）头，引入高效的跨注意力机制构建时序层，以整合整个视频序列的关键帧信息，同时提出一种新型推理策略，避免重叠窗口带来的尺度错位和冗余计算。

Result: 在多个基准数据集上实现了更好的时间一致性、先进的性能表现，并在真实场景中达到2倍的处理速度提升。

Conclusion: StableDPT是一种高效且通用的视频深度估计框架，能够显著改善深度预测的时间稳定性，适用于任意长度视频处理，具备实际应用潜力。

Abstract: Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.

</details>


### [20] [Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining](https://arxiv.org/abs/2601.02806)
*Mingzhou Jiang,Jiaying Zhou,Nan Zeng,Mickael Li,Qijie Tang,Chao He,Huazhu Fu,Honghui He*

Main category: cs.CV

TL;DR: 本文提出了一种新型拓扑感知框架，用于从H&E图像生成IHC图像，以解决虚拟染色中因空间错位和局部形变导致的弱配对数据问题。通过引入拓扑感知一致性匹配（TACM）机制和拓扑约束病理匹配（TCPM）机制，该方法在保持结构和病理一致性方面表现优异，在多个基准测试中优于现有先进方法，具有更高的临床相关性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统免疫组化（IHC）染色过程复杂、耗时且成本高，限制了其在临床中的广泛应用。虚拟染色可将H&E图像转换为IHC图像，提供一种经济高效的替代方案，但因使用相邻切片作为真实标签，常出现空间错位和局部形变，导致监督学习困难。

Method: 提出拓扑感知一致性匹配（TACM）机制，利用图对比学习和拓扑扰动来学习鲁棒的匹配模式；设计拓扑约束病理匹配（TCPM）机制，基于节点重要性对病理阳性区域进行对齐，增强病理一致性。

Result: 在两个基准数据集上进行的四项染色任务实验表明，所提方法显著优于现有最先进的方法，生成图像在视觉质量和临床相关性方面均表现更优。

Conclusion: 本研究提出的拓扑感知虚拟染色框架有效解决了弱配对数据下的空间错位与病理不一致问题，具备良好的临床应用潜力，为数字病理学中的低成本、高效率分子表征提供了新思路。

Abstract: Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.

</details>


### [21] [SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models](https://arxiv.org/abs/2601.02825)
*Ruiyang Zhang,Dongzhan Zhou,Zhedong Zheng*

Main category: cs.CV

TL;DR: SketchThinker-R1通过三阶段方法提升多模态大模型的推理效率：先将长推理转为简略式推理进行微调，再训练评估模型（SketchJudge）以奖励简略推理，最后通过强化学习进一步优化。实验显示其推理令牌成本降低64%以上，且不牺牲答案准确率，同时更聚焦关键线索。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型虽在复杂推理中表现良好，但长推理过程带来高计算开销，影响推理效率；人类则倾向于使用简洁、目标导向的‘草图式’思维，启发研究者设计更高效的推理机制。

Method: 提出SketchThinker-R1框架，包含三个阶段：1）Sketch-Mode Cold Start，将标准长推理转化为草图式推理并微调基础模型；2）训练SketchJudge奖励模型，专门评估并给予草图式推理更高评分；3）基于SketchJudge的监督，进行草图式推理强化学习，提升泛化能力。

Result: 在四个基准测试中，SketchThinker-R1实现超过64%的推理令牌成本降低，同时保持最终答案准确性；定性分析表明该方法更关注问题解决中的关键线索。

Conclusion: SketchThinker-R1成功实现了高效且准确的草图式推理，在不损失性能的前提下显著降低了计算开销，为多模态大模型的推理效率提供了新范式。

Abstract: Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.

</details>


### [22] [DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection](https://arxiv.org/abs/2601.02831)
*Yuetong Li,Qing Zhang,Yilin Zhao,Gongyang Li,Zeming Liu*

Main category: cs.CV

TL;DR: DGA-Net提出一种新的深度提示范式，通过跨模态图增强（CGE）和锚点引导精炼（AGR）模块，利用密集深度提示提升伪装目标检测性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有伪装目标检测方法主要依赖稀疏提示（如点或框），难以充分挖掘深度线索；为更有效利用深度信息，需设计一种能够整合多模态信息并保持特征一致性的新机制。

Method: 提出跨模态图增强（CGE）模块，融合RGB语义与深度几何信息形成统一引导信号；设计锚点引导精炼（AGR）模块，通过全局锚点建立非局部路径，将深层指导信息传播至浅层，缓解特征层次中的信息衰减问题。

Result: 在定量和定性实验中，DGA-Net均优于当前最先进的COD方法，证明其在伪装目标检测任务中的有效性与优越性。

Conclusion: DGA-Net通过引入密集深度提示与双模块设计，实现了对深度线索的高效利用，在伪装目标检测中展现出卓越性能，为该领域提供了新思路。

Abstract: To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.

</details>


### [23] [Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection](https://arxiv.org/abs/2601.02837)
*Yuteng Liu,Duanni Meng,Maoxun Yuan,Xingxing Wei*

Main category: cs.CV

TL;DR: SEF-DETR提出一种新框架解决红外小目标检测中因自注意力机制导致目标特征被背景淹没的问题，通过频率引导的补丁筛选、动态嵌入增强和可靠性一致性融合三个模块，显著提升查询初始化质量与目标定位精度，在多个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的红外小目标检测方法因自注意力机制导致目标相关特征被强背景特征掩盖，造成查询初始化不可靠和目标定位不准，亟需改进。

Method: SEF-DETR包含三个核心组件：频率引导的补丁筛选（FPS）利用局部块的傅里叶谱构建目标相关密度图以抑制背景；动态嵌入增强（DEE）以目标感知方式加强多尺度表示；可靠性一致性感知融合（RCF）通过空间-频率一致性和可靠性约束进一步优化对象查询。

Result: 在三个公开红外小目标检测数据集上的实验表明，SEF-DETR显著优于当前最先进的方法，具备更强的鲁棒性和效率。

Conclusion: SEF-DETR通过重构查询初始化机制，有效缓解了红外小目标检测中背景干扰问题，为该任务提供了一种高效可靠的解决方案。

Abstract: Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.

</details>


### [24] [Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion](https://arxiv.org/abs/2601.02881)
*Jakob Lønborg Christensen,Morten Rieger Hannemose,Anders Bjorholm Dahl,Vedrana Andersen Dahl*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的通用图像分割框架，实现了无需依赖掩码的全局式分割。通过引入位置感知的灰度码排序调色板、最终的tanh激活函数以及sigmoid损失加权策略，显著提升了离散设置下的性能。尽管当前模型尚未超越主流掩码基架构，但缩小了性能差距，并具备模糊性建模等独特优势。所有模型均从头训练，未来结合大规模预训练或提示条件化有望达到竞争力水平。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法多依赖掩码基框架，难以实现真正意义上的通用分割。本文旨在探索一种不依赖掩码、以整体方式预测完整分割的新型范式，从而实现更灵活、更具表达力的分割能力。

Method: 提出基于扩散模型的通用图像分割框架，采用2D灰度码位置编码增强空间感知，引入tanh激活函数处理离散数据，使用sigmoid损失加权和x-prediction策略优化训练过程。

Result: 所提方法在保持全局分割能力的同时，展现出对不确定性的合理建模能力；虽未超越顶尖掩码基模型，但性能差距明显缩小，且具备独特的灵活性与可解释性优势。

Conclusion: 该框架为通用图像分割提供了新思路，其核心组件（如位置感知调色板、tanh激活、sigmoid损失）对离散扩散模型至关重要。未来通过大规模预训练或提示引导，有望发展出具有竞争力的分割系统。

Abstract: This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.

</details>


### [25] [TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors](https://arxiv.org/abs/2601.02908)
*Wei-Yuan Cheng,Kai-Po Chang,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本文提出TA-Prompting方法，通过引入时间锚点（Temporal Anchors）增强视频大模型（VideoLLMs）在未剪辑视频中精确定位事件边界的能力，并结合事件连贯采样策略生成具有时间一致性和跨模态相似性的描述。该方法显著提升了密集视频字幕生成、时刻检索和时序问答等任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型在未剪辑视频中难以精确定位事件边界，导致生成的字幕缺乏时空准确性，影响下游任务表现。

Method: 提出TA-Prompting框架，利用时间锚点学习事件定位，并通过提示工程引导模型进行时序感知的理解；在推理阶段采用事件连贯采样策略，筛选出时间上连贯且与视频内容匹配度高的字幕序列。

Result: 在多个基准数据集上的实验表明，TA-Prompting优于当前最先进的VideoLLMs，在密集视频字幕生成、时刻检索和时序问答任务中均取得更优性能。

Conclusion: TA-Prompting通过时间锚点和连贯采样策略有效提升了视频大模型在事件定位与描述方面的精准性与一致性，为密集视频理解提供了新的有效范式。

Abstract: Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.

</details>


### [26] [Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning](https://arxiv.org/abs/2601.02918)
*Guoqiang Liang,Jianyi Wang,Zhonghua Wu,Shangchen Zhou*

Main category: cs.CV

TL;DR: Zoom-IQA 是一种基于视觉语言模型（VLM）的图像质量评估方法，通过模拟不确定性感知、区域推理和迭代优化等认知行为，提升评估的可解释性与鲁棒性。该方法采用两阶段训练：首先在自建的GR-IQA数据集上进行监督微调以实现关键区域定位；其次通过强化学习结合KL-Coverage正则化和渐进重采样策略，防止推理多样性崩溃并缓解标注偏差。实验表明，Zoom-IQA 在准确性、可解释性和泛化能力方面均优于现有方法，并在图像修复等下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的图像质量评估方法在推理过程中缺乏可靠性，难以有效融合视觉与文本信息，导致评估结果不可靠且解释性不足。因此需要一种能够模拟人类认知行为的模型，提升评估的准确性和可解释性。

Method: 提出Zoom-IQA模型，采用两阶段训练：1）在Grounded-Rationale-IQA（GR-IQA）数据集上进行监督微调（SFT），使模型能将评估结果锚定在关键视觉区域；2）通过强化学习（RL）进行动态策略探索，引入KL-Coverage正则化防止推理与评分多样性崩溃，并结合渐进重采样策略减轻标注偏差。

Result: Zoom-IQA在多个基准测试中表现出更高的鲁棒性、可解释性和泛化能力；在图像修复等下游任务中也展现出显著优势，验证了其实际应用价值。

Conclusion: Zoom-IQA通过模拟人类认知过程，实现了更可靠、可解释且通用的图像质量评估，为视觉语言模型在复杂视觉理解任务中的应用提供了新范式。

Abstract: Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.

</details>


### [27] [DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification](https://arxiv.org/abs/2601.02924)
*Aihua Zheng,Ya Gao,Shihao Li,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出DCG-ReID方法，通过动态置信度解耦加权机制（DCDW）分离不同质量分布的多模态数据，并设计针对平衡与非平衡质量分布的两种融合策略：协作融合模块（CFM）增强类内一致性，引导融合模块（GFM）强化主导模态优势并缓解模态间差异，显著提升多模态车辆重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将所有多模态数据统一处理，忽视了平衡与非平衡质量分布下不同的融合需求，导致类内一致性与模态异质性之间的冲突难以解耦。

Method: 提出DCDW机制实现模态贡献的动态重加权，构建解耦融合框架；设计CFM用于平衡情况下的共识特征挖掘，GFM用于非平衡情况下的差异放大与互补信息引导。

Result: 在WMVeID863、MSVR310、RGBNT100三个基准上均取得优异表现，验证了方法的有效性。

Conclusion: DCG-ReID通过解耦协同与引导融合机制，有效应对多模态车辆重识别中因模态质量分布不均衡带来的挑战，提升了跨模态联合决策能力。

Abstract: Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.

</details>


### [28] [PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding](https://arxiv.org/abs/2601.02927)
*Iñaki Erregue,Kamal Nasrollahi,Sergio Escalera*

Main category: cs.CV

TL;DR: PrismVAU 是一种轻量级、高效的视频异常理解系统，通过单一现成的多模态大语言模型（MLLM）实现异常评分、解释和提示优化，无需指令微调、帧级标注或外部模块，支持实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常理解方法依赖于微调的多模态大模型或外部模块，导致标注成本高、训练复杂且推理开销大。为解决这些问题，需要一种更高效、轻量且无需复杂依赖的方案。

Method: PrismVAU 采用两阶段框架：第一阶段为粗粒度异常评分模块，通过帧与文本锚点的相似性计算异常得分；第二阶段为基于 MLLM 的精炼模块，利用系统和用户提示对异常进行上下文化解释。文本锚点和提示通过弱监督自动提示工程（APE）框架优化。

Result: 在标准视频异常检测基准上，PrismVAU 实现了具有竞争力的检测性能，并生成可解释的异常说明，同时避免了指令微调、帧级标注、外部模块和密集处理，显著提升效率与实用性。

Conclusion: PrismVAU 提供了一种高效、实用且可扩展的视频异常理解解决方案，适用于真实场景下的实时应用，展示了单个现成 MLLM 在复杂任务中的强大潜力。

Abstract: Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.

</details>


### [29] [VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on](https://arxiv.org/abs/2601.02945)
*Xinyi Wei,Sijing Wu,Zitong Xu,Yunhao Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文构建了首个针对图像虚拟试穿（VTON）的多维度质量评估数据集VTONQA，包含8,132张由11个代表性VTON模型生成的图像及24,396个主观评分，涵盖服装贴合度、身体匹配度和整体质量三个维度。基于该数据集，作者对VTON模型和多种图像质量评估指标进行了基准测试，揭示了现有方法的局限性，并验证了VTONQA的价值。该数据集将为感知一致的质量评估提供坚实基础，推动高质量评估方法与VTON技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有VTON模型常出现服装扭曲、身体不一致等伪影，缺乏可靠的视觉质量评估手段，亟需一个专门针对VTON任务的多维度质量评估数据集来支持模型性能评测与优化。

Method: 构建VTONQA数据集，包含11个主流VTON模型生成的8,132张图像，通过大规模主观评分（MOS）收集三维度（服装贴合度、身体匹配度、整体质量）的评价数据，并基于此对VTON模型和多种图像质量评估（IQA）指标进行系统性基准测试。

Result: VTONQA数据集成功揭示了现有VTON模型在真实感与一致性方面的不足，同时暴露出传统IQA指标在评估VTON图像时的局限性，证明了该数据集在推动感知对齐评估方面的重要价值。

Conclusion: VTONQA是首个面向虚拟试穿任务的多维度质量评估数据集，其建立为高质量评估方法的研发和VTON模型的持续改进提供了重要支撑，具有显著的学术与应用意义。

Abstract: With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.

</details>


### [30] [LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing](https://arxiv.org/abs/2601.02987)
*Wingwa Fu,Takayuki Okatani*

Main category: cs.CV

TL;DR: LAMS-Edit通过在图像生成过程中融合反演过程中的潜在表示和注意力图，利用调度器控制加权插值，实现内容保留与编辑应用的平衡。该方法结合了Prompt-to-Prompt（P2P），支持区域掩码精确编辑和通过LoRA实现风格迁移，实验表明其在真实图像编辑中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像编辑方法在真实图像编辑中难以平衡内容保留与编辑效果，且处理复杂编辑任务时能力有限。

Method: 提出LAMS-Edit框架，利用反演过程中的中间状态（潜在表示和注意力图），通过调度器控制的加权插值进行融合，结合Prompt-to-Prompt实现精细编辑与风格迁移。

Result: 实验表明，LAMS-Edit能有效平衡内容保留与编辑应用，在区域掩码编辑和风格迁移任务中表现良好。

Conclusion: LAMS-Edit是一种可扩展的文本到图像编辑框架，能够高效处理真实图像编辑任务，兼顾内容一致性与编辑精度。

Abstract: Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.

</details>


### [31] [ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation](https://arxiv.org/abs/2601.02988)
*Rianne Weber,Niels Rocholl,Max de Grauw,Mathias Prokop,Ewoud Smit,Alessa Hering*

Main category: cs.CV

TL;DR: ULS+ is an enhanced version of the ULS model for whole-body lesion segmentation in CT scans, leveraging new public datasets and smaller input sizes to achieve higher accuracy and faster inference. It outperforms ULS on Dice score and robustness to click-point location, ranking first on the ULS23 Challenge leaderboard.


<details>
  <summary>Details</summary>
Motivation: The original ULS model, while effective, can be improved by incorporating newly available public datasets and optimizing input size for better performance and efficiency.

Method: ULS+ integrates additional public datasets and uses smaller input image sizes compared to the original ULS model, enabling improved accuracy and faster inference while maintaining robustness.

Result: ULS+ significantly outperforms ULS on both Dice score and robustness to click-point location across multiple datasets, including ULS23 Challenge test data and a subset of Longitudinal-CT. It also ranks first on the ULS23 Challenge test-phase leaderboard.

Conclusion: ULS+ establishes a strong foundation for future development of robust, clinically relevant lesion segmentation models through continuous data-driven updates and clinical validation.

Abstract: In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference.
  We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.

</details>


### [32] [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/abs/2601.02991)
*Chengcheng Feng,Haojie Yin,Yucheng Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 本文针对漫画视觉问答（CVQA）中多模态大模型（MLLMs）面临的符号抽象、叙事逻辑和幽默理解等挑战，提出了一种新型漫画推理框架。尽管链式思维（CoT）提示广泛用于提升模型推理能力，但在小规模模型上直接应用时反而降低性能，主要由于状态纠缠、虚假转移和探索效率低下等问题。为此，作者结合模块化CoT生成、基于GRPO的强化微调及新颖结构化奖励机制，构建了更忠实且可迁移的推理链。该方法在五个基准测试中表现优异，3B模型超越现有方法，并在不同模型上的插件实验平均提升12.1%。


<details>
  <summary>Details</summary>
Motivation: 传统链式思维（CoT）提示在漫画视觉问答（CVQA）任务中表现不佳，尤其在小规模模型上存在状态纠缠、虚假转移和探索效率低的问题，亟需一种更有效、可迁移的推理框架以提升模型对抽象、叙事和幽默的理解能力。

Method: 提出一种结合模块化CoT生成、基于GRPO的强化微调以及新型结构化奖励的漫画推理框架，通过解耦推理过程、优化策略学习和增强语义一致性来缓解小模型中的推理缺陷。

Result: 在五个挑战性基准测试中，所提出的3B模型显著优于现有方法；插件式部署实验在多种不同规模的MLLM上带来平均12.1%的性能提升。

Conclusion: 本研究揭示了标准CoT在CVQA中的内在局限性，并提出了一个高效、可迁移的推理框架，为小规模模型在抽象与幽默视觉理解任务中的性能突破提供了新路径。

Abstract: Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\mathbf{12.1\%}$ across different MLLMs.

</details>


### [33] [Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection](https://arxiv.org/abs/2601.03001)
*Li Wang,Boqi Li,Hang Chen,Xingjian Wu,Yichen Wang,Jiewen Tan,Xinyu Zhang,Huaping Liu*

Main category: cs.CV

TL;DR: 提出RiSe框架，通过风险-意图感知机制，实现车辆-基础设施协同感知中通信带宽与特征冗余的平衡。利用潜在场-轨迹相关模型（PTCM）量化运动风险，结合意图驱动的区域预测模块（IDAPM）预判关键鸟瞰图区域，仅传输高交互区域的高质量特征，显著降低通信开销至全特征共享的0.71%，同时保持领先的检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知框架在通信带宽与特征冗余之间存在瓶颈，尤其依赖空间压缩或静态置信图，无法高效处理非关键背景区域的冗余特征。为提升效率并聚焦关键区域，需从可见性识别转向风险关键区域优先。

Method: 引入潜在场-轨迹相关模型（PTCM）基于势场理论量化动态风险；设计意图驱动的区域预测模块（IDAPM），利用自身运动先验主动预测决策关键的BEV区域；构建语义选择性融合机制，仅传输高交互区域的高保真特征，实现特征去噪。

Result: 在DeepAccident数据集上，RiSe将通信量降至全特征共享的0.71%，同时达到当前最优的检测精度，建立带宽效率与感知性能之间的优越权衡关系。

Conclusion: RiSe通过风险-意图双重视角重构协同感知中的特征传输策略，有效解决了通信开销与感知质量间的矛盾，为未来智能交通系统提供了高效、精准的协同感知范式。

Abstract: Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.

</details>


### [34] [SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection](https://arxiv.org/abs/2601.03024)
*Kim Jun-Seong,Tae-Hyun Oh,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: SA-ResGS提出一种新型框架，通过自增强点云生成和残差学习策略，提升3D高斯点阵在主动场景重建中的不确定性量化与监督效果。该方法结合物理引导的视图选择与不确定性感知的残差监督，有效改善稀疏广基线视角下的训练稳定性，并实现更均匀的场景覆盖。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D高斯点阵在主动视图选择中因稀疏广基线视角导致的不确定性估计不可靠、监督不足的问题，提升重建质量与视图选择鲁棒性。

Method: 通过训练视图与渲染外推视图间的三角化生成自增强点云（SA-Points），实现高效场景覆盖估计；引入专为3D高斯点阵设计的残差学习策略，结合不确定性过滤与丢弃/硬负样本挖掘采样方式，增强高不确定性高斯点的梯度传播。

Result: 实验表明，SA-ResGS在重建质量与视图选择鲁棒性方面均优于现有最先进方法，显著提升了不确定性估计的可靠性与训练稳定性。

Conclusion: SA-ResGS通过物理引导的视图选择与不确定性感知的残差监督机制，实现了更准确的不确定性量化与更有效的训练信号传递，为复杂相机分布下的主动场景重建提供了新范式。

Abstract: We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.

</details>


### [35] [Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion](https://arxiv.org/abs/2601.03046)
*Han Zhang,Yanwei Wang,Fang Li,Hongjun Wang*

Main category: cs.CV

TL;DR: 提出DFRCP，一种用于YOLOv11的动态模糊鲁棒卷积金字塔，通过结合多尺度特征并引入动态鲁棒开关单元，自适应注入模糊特征以增强抖动下的全局感知能力。利用旋转与非线性插值合成模糊特征，并通过透明卷积学习内容自适应权衡原始与模糊线索。开发了CUDA并行旋转与插值核，实现超过400倍加速，适合边缘部署。在约3500张图像的私有小麦害虫损伤数据集上训练，采用两种模糊模式增强，测试集上相比基线提升约10.4%精度，显著减少数据采集后的人工过滤需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将运动模糊当作噪声抑制导致结构信息丢失，要么进行全图恢复增加延迟，难以在资源受限设备上部署。需要一种既能保留关键结构又能有效应对模糊的轻量级解决方案。

Method: 提出DFRCP模块，融合大尺度与中尺度特征，保留原始表示；引入动态鲁棒开关单元，自适应注入由旋转和非线性插值生成的模糊特征；使用透明卷积学习原始与模糊特征之间的内容自适应权衡；设计CUDA并行旋转与插值核以避免边界溢出并实现高速计算。

Result: 在模糊测试集上，YOLOv11+DFRCP相比基线提升约10.4%精度；训练时间开销小；实现超过400倍的速度提升，具备边缘部署可行性；减少数据收集后的手动过滤需求。

Conclusion: DFRCP作为YOLOv11的即插即用升级，有效提升了运动模糊条件下的目标检测性能，兼具高精度与低延迟，在资源受限场景下具有良好的实用价值。

Abstract: Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.

</details>


### [36] [On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning](https://arxiv.org/abs/2601.03048)
*Siyi Lyu,Quan Liu,Feng Yan*

Main category: cs.CV

TL;DR: 该论文指出，视觉变换器（ViT）在空间推理任务（如心理旋转）中表现不佳，其根本原因并非数据量不足，而是架构本身的电路复杂性限制。作者将空间理解形式化为保持变换群代数结构的群同态学习，证明对于非可解群（如3D旋转群SO(3)），该任务的计算复杂度下界为NC¹-完全，而常深ViT仅能实现TC⁰能力。基于TC⁰ ⊆ NC¹的假设，表明常深ViT无法高效捕捉非可解空间结构。实验通过潜空间探测验证了这一复杂性差距，发现随着组合深度增加，ViT表示出现结构性坍塌。


<details>
  <summary>Details</summary>
Motivation: 揭示视觉变换器在空间推理任务中的系统性失败根源，超越传统对数据规模的归因，从计算复杂性角度提供理论解释。

Method: 将空间理解建模为群同态学习；利用计算复杂性理论分析常深ViT与非可解群结构间的匹配能力；通过潜空间探测进行实证验证。

Result: 证明常深ViT受限于TC⁰能力，无法高效处理非可解空间结构；实验显示其潜空间在复杂任务中发生结构性坍塌，支持理论预测。

Conclusion: 视觉变换器的架构内在复杂性使其难以有效建模非可解空间变换，这是其在高级空间推理任务中表现不佳的根本原因。

Abstract: Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\mathsf{TC^0}$. Under the conjecture $\mathsf{TC^0} \subsetneq \mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.

</details>


### [37] [Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding](https://arxiv.org/abs/2601.03056)
*Zhen Wang,Jiaojiao Zhao,Qilong Wang,Yongfeng Dong,Wenlong Yu*

Main category: cs.CV

TL;DR: 提出CFSG模型，通过分离概念和特征空间为共性、特异性和混淆三部分，结合自适应机制与显式权重分配，在细粒度领域泛化任务中显著提升性能，平均优于基线9.87%，优于现有SOTA 3.08%。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在细粒度识别中对领域变化敏感，难以有效利用人类分类时的共性与特异性属性，导致性能下降。

Method: 提出概念-特征结构化泛化（CFSG），将概念与特征空间解耦为共性、特定和混淆三个结构化组件，引入自适应机制动态调整各组件比例，并在预测阶段赋予显式权重。

Result: 在三个单源基准数据集上，CFSG平均性能比基线提升9.87%，比现有最先进方法平均提升3.08%；可解释性分析验证了多粒度结构知识的有效整合及特征结构化促进概念结构化的有效性。

Conclusion: CFSG通过模拟人类分类机制，实现细粒度领域泛化的有效突破，为未来模型设计提供了新的结构化思路。

Abstract: Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.

</details>


### [38] [Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA](https://arxiv.org/abs/2601.03073)
*Tong Wu,Thanet Markchom*

Main category: cs.CV

TL;DR: 提出一个针对卡通图像视觉问答（VQA）的多智能体大语言模型框架，包含视觉、语言和批判三个专门智能体，通过协同推理整合视觉线索与叙事上下文，在Pororo和Simpsons两个卡通数据集上进行评估，揭示各智能体对最终预测的贡献，深化对基于LLM的多智能体在卡通VQA中行为的理解。


<details>
  <summary>Details</summary>
Motivation: 标准大语言模型在处理卡通图像中的夸张视觉抽象和叙事性上下文时表现不足，需要专门设计的多智能体框架以提升对卡通内容的理解能力。

Method: 设计并实现一个由视觉、语言和批判三个智能体组成的多智能体框架，通过协作机制进行结构化推理，融合视觉信息与叙事背景，支持更准确的视觉问答。

Result: 在Pororo和Simpsons数据集上的实验表明，该框架显著提升了卡通图像VQA的性能，且通过分析各智能体的贡献，揭示了多智能体在复杂语境下的协同推理机制。

Conclusion: 所提出的多智能体框架有效应对了卡通图像中视觉抽象与叙事上下文带来的挑战，为未来面向非自然图像的多模态推理提供了可扩展的解决方案。

Abstract: Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.

</details>


### [39] [LesionTABE: Equitable AI for Skin Lesion Detection](https://arxiv.org/abs/2601.03090)
*Rocio Mexia Diaz,Yasmin Greenway,Petru Manescu*

Main category: cs.CV

TL;DR: LesionTABE是一种以公平性为中心的框架，结合对抗去偏与皮肤病学特定的基础模型嵌入，显著提升了AI在不同肤色患者中的诊断公平性，相比ResNet-152基准提升超过25%的公平性指标，并同时提高整体诊断准确率。


<details>
  <summary>Details</summary>
Motivation: 当前AI在皮肤病学中的临床应用受限于偏见问题，尤其在深色皮肤人群中的表现较差，亟需提升模型的公平性以实现更广泛的应用。

Method: 采用对抗去偏技术与专用于皮肤病学的基础模型嵌入相结合的方法，构建公平性导向的诊断框架。

Result: 在多个涵盖恶性与炎症性病变的数据集上评估，LesionTABE在公平性指标上优于现有去偏方法超过25%，且整体诊断准确率也得到提升。

Conclusion: 基础模型去偏策略具有推动公平临床AI应用的巨大潜力，为实现更包容的AI辅助诊断系统提供了可行路径。

Abstract: Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.

</details>


### [40] [LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition](https://arxiv.org/abs/2601.03124)
*B. M. Shahria Alam,Md. Nasim Ahmed*

Main category: cs.CV

TL;DR: 该研究针对葡萄叶病害检测，利用9,032张图像数据集（包括三种病害类和健康叶类），通过70%训练、20%验证、10%测试的划分方式，采用InceptionV3和Xception两个预训练模型进行分类。其中Xception模型表现更优，达到96.23%的准确率。研究引入对抗训练提升模型鲁棒性，并结合Grad-CAM生成热力图以增强可解释性。最终基于Streamlit开发了可视化网页应用，实现病害分类预测及置信度展示。


<details>
  <summary>Details</summary>
Motivation: 葡萄叶病害严重影响作物产量与品质，及时诊断对农业管理至关重要。现有方法在准确性与可解释性方面存在不足，亟需高效、透明的智能诊断系统支持农业生产决策。

Method: 采用数据预处理、划分训练/验证/测试集，部署InceptionV3与Xception两个预训练模型；使用对抗训练提升模型鲁棒性；结合Grad-CAM生成热力图以可视化关键特征区域；最终构建基于Streamlit的Web应用实现交互式预测与结果展示。

Result: Xception模型在测试集上取得96.23%的准确率，显著优于InceptionV3；Grad-CAM有效识别病害区域，增强了模型决策的可解释性；部署的Web应用具备热力图可视化与置信度输出功能，具备实际应用潜力。

Conclusion: 本研究提出了一种高效且可解释的葡萄叶病害分类方法，通过先进的深度学习模型与可视化技术，实现了高精度病害识别，并成功构建了实用的Web诊断工具，为智慧农业提供技术支持。

Abstract: Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.

</details>


### [41] [Unified Thinker: A General Reasoning Modular Core for Image Generation](https://arxiv.org/abs/2601.03127)
*Sashuai Zhou,Qiang Zhou,Jijin Hu,Hanqing Yang,Yue Cao,Junpeng Ma,Yinchao Ma,Jun Song,Tiezheng Ge,Cheng Yu,Bo Zheng,Zhou Zhao*

Main category: cs.CV

TL;DR: Unified Thinker 提出一种任务无关的推理架构，通过解耦思考者（Thinker）与图像生成器，实现可模块化升级的执行推理，显著提升图像生成中的逻辑遵循能力。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在遵循逻辑密集型指令时仍存在推理-执行差距，尤其与闭源系统相比，开放源代码模型表现不足，亟需可执行的推理机制来弥补这一差距。

Method: 提出 Unified Thinker 架构，将思考者与生成器解耦，采用两阶段训练：先构建结构化规划接口，再通过强化学习结合像素级反馈，优化视觉正确性而非仅文本合理性。

Result: 在文本到图像生成和图像编辑任务中，Unified Thinker 显著提升了图像生成质量与推理能力，有效缩小了与闭源系统的差距。

Conclusion: 通过引入可执行的、模块化的推理机制，Unified Thinker 为开放源代码图像生成模型提供了通往更强逻辑遵循能力的新路径。

Abstract: Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.

</details>


### [42] [LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images](https://arxiv.org/abs/2601.03163)
*Matěj Pekár,Vít Musil,Rudolf Nenutil,Petr Holub,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LSP-DETR 是一种用于细胞核实例分割的端到端框架，采用轻量级线性复杂度的Transformer处理大图像，通过星形凸多边形表示和新型径向距离损失函数实现重叠核的自然分割，无需额外后处理，显著提升效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分块处理和昂贵的后处理，牺牲了上下文信息并影响效率，难以应对全幻灯片图像的大规模计算挑战。

Method: 提出LSP-DETR框架，使用轻量级Transformer以线性复杂度处理大图像；将细胞核建模为星形凸多边形，并引入径向距离损失函数，实现重叠核的自动分割。

Result: 在PanNuke和MoNuSeg数据集上表现出卓越的泛化能力与最先进的效率，速度超过次快方法五倍以上。

Conclusion: LSP-DETR提供了一种高效、准确且可扩展的细胞核实例分割方案，适用于大规模病理图像分析，具备实际应用潜力。

Abstract: Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.

</details>


### [43] [DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation](https://arxiv.org/abs/2601.03178)
*Jiajun jiao,Haowei Zhu,Puyuan Yang,Jianghui Wang,Ji Liu,Ziqiong Liu,Dong Li,Yuejian Fang,Junhai Yong,Bin Wang,Emad Barsoum*

Main category: cs.CV

TL;DR: 本文提出了一种基于大语言模型（LLM）的自动化加速代码生成与评估框架，用于解决扩散模型加速中多技术组合的挑战。首先构建了DiffBench基准，实现跨多种扩散架构、优化组合和部署场景的三阶段自动化评估；其次设计了DiffAgent智能体，通过闭环工作流结合规划、调试与遗传算法反馈，持续优化加速策略与代码生成。实验表明，该框架能有效评估生成代码，且DiffAgent在生成高效加速策略方面显著优于现有LLM。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在图像和视频生成中表现优异，但其多步推理过程带来巨大计算开销，限制实际部署。如何有效组合多种加速技术仍是关键挑战，亟需自动化方法支持。

Method: 提出DiffBench基准与DiffAgent智能体，采用闭环工作流：规划组件制定策略，代码生成组件编写加速代码，调试组件修正错误，并结合遗传算法从执行环境获取性能反馈以指导迭代优化。

Result: DiffBench实现了对加速代码的全面评估；DiffAgent在生成有效加速策略方面显著优于现有大语言模型，具备更强的自动化与优化能力。

Conclusion: 本研究构建的LLM驱动框架为扩散模型加速提供了高效、可扩展的自动化解决方案，推动了复杂模型优化的智能化发展。

Abstract: Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.

</details>


### [44] [AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2601.03191)
*Anees Ur Rehman Hashmi,Numan Saeed,Christoph Lippert*

Main category: cs.CV

TL;DR: AnatomiX is a multitask multimodal large language model designed for anatomically grounded chest X-ray interpretation, addressing spatial reasoning and anatomical understanding challenges in medical imaging. It follows a two-stage radiological workflow: first identifying anatomical structures and extracting features, then using a large language model for downstream tasks like phrase grounding, report generation, visual question answering, and image understanding. Experiments show over 25% improvement in anatomy grounding, phrase grounding, grounded diagnosis, and captioning compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal medical LLMs struggle with accurate anatomical correspondence in chest X-ray interpretation, leading to incorrect anatomical understanding despite improvements from current grounding techniques. There is a need for models that explicitly incorporate anatomical reasoning.

Method: AnatomiX uses a two-stage approach: (1) anatomical structure identification and feature extraction; (2) leveraging a large language model for diverse downstream tasks including phrase grounding, report generation, visual question answering, and image understanding, all grounded in anatomical context.

Result: AnatomiX achieves over 25% performance improvement across multiple benchmarks on anatomy grounding, phrase grounding, grounded diagnosis, and grounded captioning tasks, demonstrating superior anatomical reasoning capabilities.

Conclusion: AnatomiX effectively addresses the limitations of current multimodal medical LLMs in anatomical understanding by integrating radiological workflow principles, significantly enhancing spatial and anatomical reasoning in chest X-ray interpretation.

Abstract: Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix

</details>


### [45] [UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision](https://arxiv.org/abs/2601.03193)
*Ruiyan Han,Zhen Fang,XinYu Sun,Yuchen Ma,Ziheng Wang,Yu Zeng,Zehui Chen,Lin Chen,Wenxuan Huang,Wei-Jie Xu,Yi Cao,Feng Zhao*

Main category: cs.CV

TL;DR: UniCorn提出了一种无需外部数据或教师监督的自提升框架，通过将单一统一多模态模型（UMM）分为提议者、求解者和裁判三个协作角色，利用自对弈生成高质量交互，并通过认知模式重构将隐含理解提炼为显式生成信号。为验证多模态一致性恢复，引入基于文本-图像-文本循环重建的UniCycle基准。实验表明，UniCorn在六个通用图像生成基准上显著优于基线模型，尤其在TIIF、DPG、CompBench和UniCycle上达到新纪录，并在WISE和OneIG上分别提升5.0和6.5，证明其在提升文本到图像生成质量的同时保持强理解能力，展示了完全自监督优化在统一多模态智能中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在跨模态理解方面表现优异，但在将内部理解转化为高质量、可控生成方面存在明显不足，这种现象被定义为‘传导性失语’。为解决该问题，需要一种不依赖外部数据或教师监督的自提升机制，以增强模型的生成能力并保持多模态一致性。

Method: 提出UniCorn框架，将单个UMM分解为三个角色：Proposer（提议者）、Solver（求解者）和Judge（裁判），通过自对弈方式生成互动；采用认知模式重构技术，将模型内部隐含知识提炼为显式的生成信号；设计UniCycle基准，基于文本→图像→文本的循环一致性评估多模态生成的一致性与连贯性。

Result: UniCorn在六个通用图像生成基准上均取得显著提升，其中在TIIF（73.8）、DPG（86.8）、CompBench（88.5）和UniCycle上达到当前最佳性能；在WISE和OneIG上分别获得+5.0和+6.5的显著增益；整体表现出更强的生成质量与多模态理解能力，验证了全自监督优化的有效性与可扩展性。

Conclusion: UniCorn通过自对弈与认知重构实现了统一多模态模型的自我提升，显著增强了文本到图像生成的质量与可控性，同时保持了强大的跨模态理解能力，为构建真正具备生成与理解双重能力的统一多模态智能系统提供了可行路径。

Abstract: While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.

</details>


### [46] [LTX-2: Efficient Joint Audio-Visual Foundation Model](https://arxiv.org/abs/2601.03233)
*Yoav HaCohen,Benny Brazowski,Nisan Chiprut,Yaki Bitterman,Andrew Kvochko,Avishai Berkowitz,Daniel Shalem,Daphna Lifschitz,Dudu Moshe,Eitan Porat,Eitan Richardson,Guy Shiran,Itay Chachy,Jonathan Chetboun,Michael Finkelson,Michael Kupchick,Nir Zabari,Nitzan Guetta,Noa Kotler,Ofir Bibi,Ori Gordon,Poriya Panet,Roi Benita,Shahar Armon,Victor Kulikov,Yaron Inger,Yonatan Shiftan,Zeev Melumian,Zeev Farbman*

Main category: cs.CV

TL;DR: LTX-2 is an open-source foundational model that generates high-quality, temporally synchronized audiovisual content using a dual-stream transformer with 14B-parameter video and 5B-parameter audio streams. It uses bidirectional cross-attention, temporal positional embeddings, and modality-aware classifier-free guidance to improve alignment and controllability. The model excels in generating coherent audio tracks matching scene context and achieves state-of-the-art performance among open-source systems with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video diffusion models lack audio, missing crucial semantic, emotional, and atmospheric cues. There is a need for a unified, efficient, and high-quality audiovisual generation model that can be used openly and effectively.

Method: LTX-2 employs an asymmetric dual-stream transformer with separate video (14B) and audio (5B) streams connected via bidirectional audio-video cross-attention layers. It incorporates temporal positional embeddings, cross-modality AdaLN for shared timestep conditioning, a multilingual text encoder, and modality-aware classifier-free guidance (modality-CFG) to enhance audiovisual alignment and control.

Result: LTX-2 achieves state-of-the-art audiovisual quality and prompt adherence among open-source models, with performance comparable to proprietary models but at significantly reduced computational cost and inference time. It generates rich, coherent audio including background and foley elements that align with scene characters, environment, style, and emotion.

Conclusion: LTX-2 demonstrates a scalable and efficient framework for unified audiovisual generation, offering strong performance, open accessibility, and practical deployment advantages over existing solutions.

Abstract: Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.

</details>


### [47] [A Versatile Multimodal Agent for Multimedia Content Generation](https://arxiv.org/abs/2601.03250)
*Daoan Zhang,Wenlin Yao,Xiaoyang Wang,Yebowen Hu,Jiebo Luo,Dong Yu*

Main category: cs.CV

TL;DR: 本文提出一种名为MultiMedia-Agent的多模态内容生成智能体，旨在解决当前AIGC模型在真实场景中难以端到端完成复杂多媒体任务的问题。该系统包含数据生成管道、内容创作工具库及偏好对齐评估指标，并引入技能习得理论优化训练数据与模型训练过程。通过两阶段相关性策略（自相关与模型偏好相关）进行计划优化，并采用三阶段训练方法（基础/成功计划微调与偏好优化），显著提升了生成内容的质量与多模态整合能力。实验表明，该方法优于现有模型，能生成更高质量的多媒体内容。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC模型大多局限于特定应用场景，无法在真实世界中实现跨模态、端到端的内容生成。编辑专家常需处理多种图像和视频输入，生成包含音频、文本等元素的复合输出，而现有模型难以有效整合多模态信息，限制了其实际应用。因此亟需一种能够协同处理多模态任务的智能体系统。

Method: 提出MultiMedia-Agent系统，结合数据生成管道、工具库与评估指标；引入技能习得理论用于训练数据构建与模型训练；设计两阶段相关性策略（自相关与模型偏好相关）优化任务规划；采用三阶段训练流程（基础微调、成功计划微调、偏好优化）提升模型性能。

Result: 实验结果显示，所提方法在生成多媒体内容方面表现优异，相比新模型具有更强的生成能力与更好的多模态融合效果，验证了系统的有效性与先进性。

Conclusion: MultiMedia-Agent通过系统化的设计与创新的训练机制，实现了复杂多模态内容的自动化生成，为未来通用型AIGC系统的发展提供了可行路径。

Abstract: With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.

</details>


### [48] [InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields](https://arxiv.org/abs/2601.03252)
*Hao Yu,Haotong Lin,Jiawei Wang,Jiaxin Li,Yida Wang,Xueyang Zhang,Yue Wang,Xiaowei Zhou,Ruizhen Hu,Sida Peng*

Main category: cs.CV

TL;DR: InfiniDepth提出了一种基于神经隐式场的连续深度估计方法，突破了传统离散网格表示的限制，支持任意分辨率和细粒度深度预测。通过构建4K合成基准测试，实验表明该方法在合成与真实世界数据上均达到先进水平，尤其在细节区域表现优异，并提升了大视角变化下的新视图合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计方法受限于离散图像网格表示，难以实现任意分辨率输出且不利于几何细节恢复。因此需要一种能够支持连续坐标查询、高分辨率和精细结构重建的深度表示方式。

Method: 引入神经隐式场表示深度，设计简单有效的局部隐式解码器，可对任意2D连续坐标进行深度查询，实现高分辨率、细粒度的深度估计。

Result: InfiniDepth在合成与真实世界基准上均取得领先性能，尤其在细粒度区域表现突出；同时显著提升大视角变化下的新视图合成质量，减少孔洞与伪影。

Conclusion: InfiniDepth通过神经隐式场实现了连续、任意分辨率的深度估计，有效解决了传统方法在分辨率扩展与细节恢复上的瓶颈，在多种任务中展现出卓越性能。

Abstract: Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.

</details>


### [49] [Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training](https://arxiv.org/abs/2601.03256)
*Hexiao Lu,Xiaokun Sun,Zeyu Cai,Hao Guo,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: Muses 是首个无需训练的 3D 奇幻生物生成方法，采用前馈范式。它利用 3D 骨骼作为生物形态的基础表示，通过图约束推理构建合理布局的骨骼，再在结构化潜在空间中进行体素级组装，并结合图像引导的外观建模生成风格一致的纹理。该方法避免了传统方法中的部分级操控难题，实现了高质量、符合文本描述的 3D 生成与灵活编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成复杂 3D 生物时存在现实感差、结构不连贯的问题，主要受限于部分级操作困难和跨域生成能力不足。因此需要一种无需训练、能有效整合结构与外观的新方法。

Method: Muses 采用三阶段流程：1）基于图约束推理生成创意且结构合理的 3D 骨骼；2）在结构化潜在空间中以体素方式组装来自不同对象的区域；3）在骨骼条件下进行图像引导的外观建模，生成协调一致的纹理。

Result: 实验表明，Muses 在视觉保真度和文本对齐方面达到当前最佳水平，具备灵活的 3D 物体编辑潜力，生成结果自然且风格统一。

Conclusion: Muses 为 3D 奇幻生物生成提供了一种高效、无需训练的前馈解决方案，其基于骨骼的结构化设计思路显著提升了生成质量与可控性，具有广泛的应用前景。

Abstract: We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [50] [WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables](https://arxiv.org/abs/2601.02391)
*Zhaojiang Lin,Yong Xu,Kai Sun,Jing Zheng,Yin Huang,Surya Teja Appini,Krish Narang,Renjie Tao,Ishan Kapil Jain,Siddhant Arora,Ruizhi Li,Yiteng Huang,Kaushik Patnaik,Wenfang Xu,Suwon Shon,Yue Liu,Ahmed A Aly,Anuj Kumar,Florian Metze,Xin Luna Dong*

Main category: cs.CL

TL;DR: WearVox是首个针对可穿戴设备中语音助手的基准测试，涵盖3,842个多通道、第一人称音频记录，覆盖多种任务和环境，评估真实场景下语音大模型表现。结果显示多数实时模型准确率仅29%-59%，尤其在嘈杂户外环境下性能显著下降；使用多通道音频能显著提升对背景噪声的鲁棒性及对设备定向语音的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略可穿戴设备中的实际挑战，如运动引起的音频畸变、噪声干扰、快速微交互以及区分设备定向语音与背景对话的需求，无法有效评估语音助手在真实场景下的表现。

Method: 构建WearVox基准数据集，包含多通道第一人称音频，覆盖五类任务（搜索基础问答、闭卷问答、侧聊拒绝、工具调用、语音翻译），并配有丰富元数据，用于评估主流开源与专有语音大模型的表现，同时对比单通道与多通道输入的模型效果。

Result: 大多数实时语音大模型在WearVox上的准确率仅为29%至59%，在嘈杂户外环境中性能明显下降；多通道音频输入显著提升模型对环境噪声的鲁棒性，并增强对设备定向语音的区分能力。

Conclusion: 空间音频线索对上下文感知语音助手至关重要，WearVox为推进可穿戴语音人工智能研究提供了全面可靠的测试平台。

Abstract: Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.

</details>


### [51] [PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models](https://arxiv.org/abs/2601.02404)
*Inpyo Song,Eunji Jeon,Jangwon Lee*

Main category: cs.CL

TL;DR: 本文提出了首个针对物理计算领域的自动评估基准PCEval，用于评估大语言模型（LLMs）在逻辑与物理层面的项目能力，无需人工参与。该框架在仿真环境中对13个主流模型进行测试，涵盖电路生成与代码兼容性评估，发现LLMs在代码生成和逻辑电路设计上表现良好，但在实际面包板布局中因引脚连接错误和电路问题而表现不佳。研究为硬件依赖型计算环境中的AI辅助提供了新理解，并为未来教育工具开发奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在软件开发等领域表现出色，但在物理计算场景下，尤其涉及软硬件交互时，其实际效果尚未充分探索。当前缺乏一个能自动评估模型在逻辑与物理层面综合能力的基准，因此亟需建立可重复、自动化验证的评估体系以填补这一空白。

Method: 提出PCEval框架，通过仿真环境自动评估大语言模型在生成电路与编写兼容代码方面的能力，覆盖不同复杂度的项目任务，实现对逻辑设计与物理布局的双重检验。

Result: 13个主流大语言模型在代码生成和逻辑电路设计上表现良好，但在物理面包板布局中存在显著缺陷，主要体现在引脚连接错误与电路故障上，表明模型对硬件实施约束的理解仍不充分。

Conclusion: PCEval是首个面向物理计算的自动化评估基准，揭示了大语言模型在硬件交互方面的局限性，推动了对智能辅助物理计算工具的研究与发展，为未来教育与工程应用提供重要参考。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.

</details>


### [52] [Losses that Cook: Topological Optimal Transport for Structured Recipe Generation](https://arxiv.org/abs/2601.02531)
*Mattia Ottoborgo,Daniele Rege Cambrin,Paolo Garza*

Main category: cs.CL

TL;DR: 本文研究了烹饪食谱生成中的复合目标函数，提出了一种新的拓扑损失，将食材列表表示为嵌入空间中的点云，以减少预测与真实食材之间的差异。实验表明，该损失显著提升了食材和动作级别的指标；Dice损失在时间和温度精度上表现优异，混合损失则在数量和时间上取得良好权衡。人类偏好分析显示，模型在62%的情况下更受青睐。


<details>
  <summary>Details</summary>
Motivation: 烹饪食谱生成不仅需要流畅的文本，还需准确的时间、温度和步骤连贯性，以及正确的食材组合。标准训练方法仅关注流畅性，忽略了这些关键要素。因此，亟需改进生成模型以更好地捕捉食谱的复杂性。

Method: 基于RECIPE-NLG框架，引入多种复合目标函数，提出一种新的拓扑损失，将食材列表映射为嵌入空间中的点云，通过最小化预测与真实食材间的分布差异来优化生成效果。同时结合Dice损失和混合损失进行对比实验。

Result: 新提出的拓扑损失显著提升食材和动作层面的性能；Dice损失在时间/温度精度方面表现最佳；混合损失在数量和时间之间实现良好平衡。人类评估结果显示，所提模型在62%的案例中更受偏好。

Conclusion: 通过引入拓扑损失及多种复合目标函数，可有效提升烹饪食谱生成的质量，尤其在食材准确性、时间温度控制和整体协调性方面表现突出，且获得人类用户的广泛认可。

Abstract: Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.

</details>


### [53] [ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation](https://arxiv.org/abs/2601.02535)
*Hyeong Kyu Choi,Sharon Li*

Main category: cs.CL

TL;DR: 提出一种无需评估器的Best-of-N选择框架ModeX，通过构建相似性图并递归应用谱聚类来识别生成文本中的主导语义共识（模态输出），实现高效且无需额外模型的高质量输出选择。进一步提出轻量级版本ModeX--Lite，通过早期剪枝提升效率。在文本摘要、代码生成和数学推理等开放任务中，该方法持续优于标准单路径和多路径基线。


<details>
  <summary>Details</summary>
Motivation: 现有Best-of-N方法依赖外部评估器、奖励模型或精确字符串匹配投票，在开放任务中适用性和效率受限。需要一种无需额外模型或计算开销的高效、通用的高质量输出选择方法。

Method: 提出ModeX框架，通过构建候选生成文本的相似性图，并递归应用谱聚类以选出代表性的中心点（模态输出）；引入ModeX--Lite，通过早期剪枝优化效率。

Result: 在多个开放任务（如文本摘要、代码生成、数学推理）中，ModeX及其轻量版均显著优于现有基线方法，具备良好的性能与计算效率。

Conclusion: ModeX提供了一种高效、无需评估器的Best-of-N选择方案，适用于开放文本生成任务，为大语言模型的高质量输出选择提供了新思路。

Abstract: Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX--Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks--including text summarization, code generation, and mathematical reasoning--our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.

</details>


### [54] [Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency](https://arxiv.org/abs/2601.02574)
*Haoran Wang,Maryam Khalid,Qiong Wu,Jian Gao,Cheng Cao*

Main category: cs.CL

TL;DR: 本文提出PCC框架，通过联合建模概率确定性和推理一致性来估计大语言模型在事实判断上的置信度，实现自适应验证：高置信时直接回答，不确定或不一致时触发目标检索，高度模糊时升级至深度搜索。该方法显著提升效率与可靠性，在多个基准测试中优于现有基线，并具有跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查方法盲目检索外部证据，忽略模型内部知识，引入无关噪声，且缺乏针对模型推理中特定不确定性问题的精准应对机制。

Method: 提出PCC框架，通过联合建模大语言模型的概率确定性与推理一致性，生成置信度信号，驱动自适应验证策略，实现按需检索与决策。

Result: 在三个挑战性基准上，PCC在不确定性量化方面优于口头置信度，持续超越强基线；具备良好的跨模型泛化能力。

Conclusion: PCC通过模拟人类事实核查方式，实现了基于置信度的自适应验证，有效减少无效检索，提升大语言模型在需要事实准确性的应用中的可靠性与效率。

Abstract: Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.

</details>


### [55] [Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models](https://arxiv.org/abs/2601.02580)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 该研究提出一种基于大语言模型（LLM）微调的新方法，通过模拟不同能力水平学生对多项选择题的作答行为，隐式建模项目参数（如难度和区分度），无需昂贵的实地测试即可生成合成的项目特征曲线（ICC），从而估计项目反应理论（IRT）参数。实验在六年级英语语言艺术（ELA）题目和BEA 2024共享任务数据集上验证了该方法的有效性，表现优于或媲美基线方法，尤其在建模项目区分度方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统IRT参数校准依赖昂贵的实地测试收集学生作答数据，成本高且耗时；本研究旨在开发一种无需真实数据即可高效估计项目参数的替代方法，提升评估工具开发的效率与可扩展性。

Method: 采用Qwen-3密集模型系列，结合低秩适应（LoRA）技术，对大语言模型进行微调，使其根据离散的能力描述生成对应的学生作答响应；通过分析生成响应的正确率随能力变化的趋势，重建项目特征曲线（ICC），进而估计难度和区分度等IRT参数。

Result: 在Grade 6 ELA题目和BEA 2024共享任务数据集上的实验表明，该方法生成的IRT参数具有较高准确性，性能可媲美甚至超越现有基线方法，尤其在项目区分度建模方面表现突出。

Conclusion: 基于大语言模型的模拟方法为项目参数估计提供了一种高效、低成本的替代方案，具备良好的应用前景，尤其适用于需要快速构建或校准大量评估项目的场景。

Abstract: Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.

</details>


### [56] [FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions](https://arxiv.org/abs/2601.02589)
*Kris W Pan,Yongmin Yoo*

Main category: cs.CL

TL;DR: FlowPlan-G2P 是一种新颖的论文转专利框架，通过三阶段流程（概念图构建、段落与章节规划、图条件生成）模拟专家撰写逻辑，显著提升生成内容的逻辑连贯性和法律合规性，优于传统端到端大模型方法。


<details>
  <summary>Details</summary>
Motivation: 将科学论文转化为专利描述面临语篇风格差异和严格法律要求的挑战，现有黑箱式文本生成方法难以处理结构化推理和法律约束，亟需更符合人类专家思维流程的解决方案。

Method: 提出FlowPlan-G2P框架，包含三个阶段：(1) 概念图诱导，通过专家式推理提取技术实体及其关系构建有向图；(2) 段落与章节规划，将图结构重组为符合专利标准章节的逻辑集群；(3) 图条件生成，利用特定章节子图和定制提示生成合法合规的段落。

Result: 实验表明，FlowPlan-G2P在逻辑连贯性和法律合规性方面显著优于端到端大语言模型基线，为论文转专利生成建立了新范式，并推动了专业领域结构化文本生成的发展。

Conclusion: FlowPlan-G2P通过模拟专家认知流程，有效解决了论文到专利转换中的结构性与法律合规性难题，是面向专业化文本生成的重要进展。

Abstract: Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.

</details>


### [57] [Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs](https://arxiv.org/abs/2601.02604)
*Cesar Felipe Martínez Cisneros,Jesús Ulises Quiroz Bautista,Claudia Anahí Guzmán Solano,Bogdan Kaleb García Rivera,Iván García Pacheco,Yalbi Itzel Balderas Martínez,Kolawole John Adebayoc,Ignacio Arroyo Fernández*

Main category: cs.CL

TL;DR: 本研究提出了一种基于开放信息抽取（OpenIE）的肺癌知识库构建流程，通过整合MeSH术语、CC0许可的PubMed文献、OpenIE三元组提取及命名实体识别（NER）技术，生成大规模、低噪声、领域特定的结构化知识资源。该知识库用于微调T5模型，在监督语义微调下显著提升了语义连贯性与性能，验证了OpenIE方法在生物医学NLP中的可扩展性和低成本优势。


<details>
  <summary>Details</summary>
Motivation: 在肿瘤学领域，精准性和可解释性至关重要，而大语言模型（LLMs）的性能高度依赖训练数据的语义质量。现有方法缺乏高效、可扩展的结构化知识库构建手段，限制了LLMs在生物医学领域的应用。因此，亟需一种低成本、高效率的方法来构建高质量、领域特定的知识资源以支持模型微调。

Method: 1. 使用MeSH术语识别医学概念；2. 筛选具有CC0许可证的开放获取PubMed文献；3. 采用开放信息抽取（OpenIE）方法提取（主体，关系，客体）三元组；4. 利用命名实体识别（NER）对三元组进行生物医学相关性增强，确保数据质量与领域适配性。

Result: 构建的三元组集合具备领域专属性、大规模和噪声感知特性，作为训练数据显著提升了微调后T5模型的性能。在ROUGE和BERTScore评估中，模型表现出更高的语义一致性与生成质量，证明该方法可作为低成本、可扩展的生物医学NLP知识资源构建方案。

Conclusion: 基于OpenIE的自动化知识库构建流程为生物医学领域提供了高效、可扩展且低成本的知识资源生成路径，能够有效支持大语言模型的微调，提升其在肺癌等专业领域的推理能力与语义表现。

Abstract: The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.

</details>


### [58] [Improved Evidence Extraction for Document Inconsistency Detection with LLMs](https://arxiv.org/abs/2601.02627)
*Nelvin Tan,Yaowen Zhang,James Asikin Cheung,Fusheng Liu,Yu-Ching Shih,Dong Yang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的文档不一致检测方法，重点解决不一致句子的证据提取问题。通过引入新的综合证据提取指标和一种带约束过滤的重写-重试框架，显著提升了直接提示方法在不一致检测上的表现，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前针对大语言模型在文档不一致检测中的研究相对有限，尤其缺乏对不一致证据提取的有效方法，因此需要改进现有技术以提升检测精度与可解释性。

Method: 提出一种红笔-重试（redact-and-retry）框架，结合约束过滤机制，用于更准确地提取不一致证据，并设计了新的综合性评价指标来衡量证据质量。

Result: 实验结果表明，该方法在证据提取准确性与检测性能上均优于传统的直接提示方法，显著提升了文档不一致检测的效果。

Conclusion: 所提出的框架和指标能够有效增强大语言模型在文档不一致检测中的表现，为未来相关研究提供了可行的技术路径。

Abstract: Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.

</details>


### [59] [Empirical Comparison of Encoder-Based Language Models and Feature-Based Supervised Machine Learning Approaches to Automated Scoring of Long Essays](https://arxiv.org/abs/2601.02659)
*Kuo Wang,Haowei Hua,Pengfei Yan,Hong Jiao,Dan Song*

Main category: cs.CL

TL;DR: 该研究探讨了编码器类语言模型在处理长文本（如长篇作文）时的挑战，并比较了多种预训练模型及其集成模型在自动评分中的表现。实验使用17,307篇作文数据集，采用80%/10%/10%划分，评估指标为加权肯德尔协调系数（Quadratic Weighted Kappa）。结果表明，结合多个预训练模型嵌入表示并使用梯度提升分类器作为集成器的模型显著优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 长文本处理对编码器-only语言模型构成挑战，尤其在自动化作文评分任务中，亟需更有效的模型架构来提升评分准确性和鲁棒性。

Method: 训练并评估多种基于BERT的模型（BERT、RoBERTa、DistilBERT、DeBERTa）、多模型嵌入集成模型以及基于特征的监督学习模型（如GBDT、XGBoost、LightGBM），采用数据集划分和交叉验证方法进行性能对比。

Result: 集成嵌入模型（融合多个预训练语言模型表示+梯度提升分类器）在长作文自动评分任务中表现最佳，显著优于单个语言模型及其他集成方法。

Conclusion: 通过融合多模型嵌入并结合梯度提升分类器，可有效提升编码器模型在长文本自动评分中的性能，为后续研究提供了高效且可靠的解决方案。

Abstract: Long context may impose challenges for encoder-only language models in text processing, specifically for automated scoring of essays. This study trained several commonly used encoder-based language models for automated scoring of long essays. The performance of these trained models was evaluated and compared with the ensemble models built upon the base language models with a token limit of 512?. The experimented models include BERT-based models (BERT, RoBERTa, DistilBERT, and DeBERTa), ensemble models integrating embeddings from multiple encoder models, and ensemble models of feature-based supervised machine learning models, including Gradient-Boosted Decision Trees, eXtreme Gradient Boosting, and Light Gradient Boosting Machine. We trained, validated, and tested each model on a dataset of 17,307 essays, with an 80%/10%/10% split, and evaluated model performance using Quadratic Weighted Kappa. This study revealed that an ensemble-of-embeddings model that combines multiple pre-trained language model representations with gradient-boosting classifier as the ensemble model significantly outperforms individual language models at scoring long essays.

</details>


### [60] [Multi-Turn Jailbreaking of Aligned LLMs via Lexical Anchor Tree Search](https://arxiv.org/abs/2601.02670)
*Devang Kulshreshtha,Hang Su,Chinmay Hegde,Haohan Wang*

Main category: cs.CL

TL;DR: LATS是一种无需攻击者LLM的新型越狱方法，通过词汇锚点注入实现高效的多轮对话树搜索，仅需约6.4次查询即可在GPT、Claude和Llama等模型上达到97-100%的攻击成功率（ASR），显著优于传统方法所需的20+次查询。该方法利用对话结构作为攻击面，在保持高成功率的同时极大降低了资源消耗和查询成本。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法依赖攻击者LLM生成对抗性查询或需要大量查询预算，导致成本高且生成的查询缺乏可解释性，亟需一种高效、低成本且无需攻击者模型的新方法。

Method: LATS将越狱任务重构为基于多轮对话的广度优先树搜索，通过逐步向良性提示中注入目标攻击内容词（即词汇锚点）来实现攻击，完全不依赖攻击者LLM。

Result: 在AdvBench和HarmBench测试集上，LATS在最新GPT、Claude和Llama模型上实现了97-100%的攻击成功率，平均仅需约6.4次查询，远低于其他方法的20+次。

Conclusion: 对话结构是极具潜力但未被充分保护的攻击面；LATS展示了在高攻击成功率背景下，实现极低查询开销的可能性，具有良好的实用价值与可复现性。

Abstract: Most jailbreak methods achieve high attack success rates (ASR) but require attacker LLMs to craft adversarial queries and/or demand high query budgets. These resource limitations make jailbreaking expensive, and the queries generated by attacker LLMs often consist of non-interpretable random prefixes. This paper introduces Lexical Anchor Tree Search (), addressing these limitations through an attacker-LLM-free method that operates purely via lexical anchor injection. LATS reformulates jailbreaking as a breadth-first tree search over multi-turn dialogues, where each node incrementally injects missing content words from the attack goal into benign prompts. Evaluations on AdvBench and HarmBench demonstrate that LATS achieves 97-100% ASR on latest GPT, Claude, and Llama models with an average of only ~6.4 queries, compared to 20+ queries required by other methods. These results highlight conversational structure as a potent and under-protected attack surface, while demonstrating superior query efficiency in an era where high ASR is readily achievable. Our code will be released to support reproducibility.

</details>


### [61] [Extracting books from production language models](https://arxiv.org/abs/2601.02671)
*Ahmed Ahmed,A. Feder Cooper,Sanmi Koyejo,Percy Liang*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在生产环境中是否存在从训练数据中提取版权文本的风险。通过两阶段方法（初步探测与迭代续写提示），在Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro和Grok 3四款生产级模型上测试了文本提取可行性，并使用基于最长公共子串的nv-recall评分衡量成功率。结果显示，部分模型（如Gemini 2.5 Pro和Grok 3）无需越狱即可提取大量文本（如《哈利·波特》的nv-recall达76.8%和70.3%），而Claude 3.7 Sonnet在越狱后可近乎逐字复现整本书（nv-recall=95.8%）。GPT-4.1需更多尝试且最终拒绝输出。尽管存在安全机制，但生产级LLM仍面临训练数据泄露风险。


<details>
  <summary>Details</summary>
Motivation: 当前关于大模型与版权的法律争议核心在于模型是否记忆训练数据及其可被提取的可能性。尽管已有研究揭示开源模型存在此类风险，但生产级模型因具备更强的安全措施，其数据提取可行性尚不明确。本文旨在评估这些防护机制是否足以阻止版权内容被恢复。

Method: 采用两阶段提取流程：第一阶段为初步探测，部分使用Best-of-N越狱技术；第二阶段为迭代续写提示以逐步还原完整文本。对四个主流生产模型进行实验，并利用块级最长公共子串近似计算的nv-recall指标量化提取效果。

Result: Gemini 2.5 Pro和Grok 3在未越狱情况下即能提取高比例文本（如《哈利·波特》的nv-recall分别为76.8%和70.3%）；越狱后的Claude 3.7 Sonnet可近乎完整复现书籍（nv-recall=95.8%）；GPT-4.1虽经多次尝试（20倍于其他模型），但最终拒绝响应，仅获得极低提取率（nv-recall=4.0%）。

Conclusion: 即使在部署了模型级与系统级安全防护的情况下，生产级大模型仍存在从输出中提取受版权保护训练数据的风险，表明现有防护机制不足以完全消除该隐患。

Abstract: Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.

</details>


### [62] [Iterative Structured Pruning for Large Language Models with Multi-Domain Calibration](https://arxiv.org/abs/2601.02674)
*Guangxin Wu,Hao Zhang,Zhang Zhibin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出一种新的结构化剪枝框架，通过混合多域校准集和迭代校准策略，有效识别并移除冗余通道，实现显著压缩且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在自然语言处理任务中表现优异，但其不断增长的规模带来了计算开销、内存占用和推理延迟等实际部署障碍。现有非结构化剪枝方法常产生不规则稀疏模式，需要专用硬件或软件支持，限制了实用性。因此，亟需一种兼容标准硬件加速器的结构化剪枝方法。

Method: 提出一种基于混合多域校准集和迭代校准策略的结构化剪枝框架，通过系统性地识别和移除冗余通道，实现高效模型压缩。

Result: 在多种模型和下游任务上的大量实验表明，该方法能够在保持模型性能几乎不变的前提下，实现显著的模型压缩。

Conclusion: 所提出的结构化剪枝框架能够有效降低大型语言模型的资源消耗，同时保持高性能，具备良好的实用性和可扩展性，适用于广泛的实际部署场景。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide spectrum of natural language processing tasks. However, their ever-growing scale introduces significant barriers to real-world deployment, including substantial computational overhead, memory footprint, and inference latency. While model pruning presents a viable solution to these challenges, existing unstructured pruning techniques often yield irregular sparsity patterns that necessitate specialized hardware or software support. In this work, we explore structured pruning, which eliminates entire architectural components and maintains compatibility with standard hardware accelerators. We introduce a novel structured pruning framework that leverages a hybrid multi-domain calibration set and an iterative calibration strategy to effectively identify and remove redundant channels. Extensive experiments on various models across diverse downstream tasks show that our approach achieves significant compression with minimal performance degradation.

</details>


### [63] [Boosting Accuracy and Interpretability in Multilingual Hate Speech Detection Through Layer Freezing and Explainable AI](https://arxiv.org/abs/2601.02697)
*Meysam Shirdel Bilehsavar,Negin Mahmoudi,Mohammad Jalili Torkamani,Kiana Kiashemshaki*

Main category: cs.CL

TL;DR: 本研究评估了三种基于Transformer的模型（BERT-base-multilingual-cased、RoBERTa-base和XLM-RoBERTa-base，其中前八层冻结）在五种语言（英语、韩语、日语、汉语和法语）上的多语言情感分析与仇恨言论检测性能。通过准确率、精确率、召回率和F1分数等指标进行比较，并结合LIME解释框架提升模型可解释性，以增强系统透明度与有效性。


<details>
  <summary>Details</summary>
Motivation: 在线内容监管中，情感分析与仇恨言论检测对构建安全数字环境至关重要。现有模型在多语言场景下的表现与可解释性仍存在不足，亟需高效且透明的解决方案。

Method: 采用三种预训练Transformer模型，在五种语言上进行多语言情感分析与仇恨言论检测任务；使用标准评估指标对比性能；引入LIME框架分析各词语对模型决策的贡献，提升模型可解释性。

Result: 实验表明，不同模型在各语言上的表现存在差异，整体上XLM-RoBERTa-base表现更优；LIME有效揭示了关键词汇对预测的影响，增强了模型决策的透明度。

Conclusion: 结合先进的Transformer架构与可解释性技术，能够显著提升多语言情感分析与仇恨言论检测系统的性能与可信度，为未来内容安全治理提供有力支持。

Abstract: Sentiment analysis focuses on identifying the emotional polarity expressed in textual data, typically categorized as positive, negative, or neutral. Hate speech detection, on the other hand, aims to recognize content that incites violence, discrimination, or hostility toward individuals or groups based on attributes such as race, gender, sexual orientation, or religion. Both tasks play a critical role in online content moderation by enabling the detection and mitigation of harmful or offensive material, thereby contributing to safer digital environments. In this study, we examine the performance of three transformer-based models: BERT-base-multilingual-cased, RoBERTa-base, and XLM-RoBERTa-base with the first eight layers frozen, for multilingual sentiment analysis and hate speech detection. The evaluation is conducted across five languages: English, Korean, Japanese, Chinese, and French. The models are compared using standard performance metrics, including accuracy, precision, recall, and F1-score. To enhance model interpretability and provide deeper insight into prediction behavior, we integrate the Local Interpretable Model-agnostic Explanations (LIME) framework, which highlights the contribution of individual words to the models decisions. By combining state-of-the-art transformer architectures with explainability techniques, this work aims to improve both the effectiveness and transparency of multilingual sentiment analysis and hate speech detection systems.

</details>


### [64] [Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study](https://arxiv.org/abs/2601.02700)
*Agniv Roy Choudhury,Vignesh Ponselvan Rajasingh*

Main category: cs.CL

TL;DR: 本研究系统评估了Transformer模型在AddSent adversarial数据集上的对抗鲁棒性，通过多层级错误分析发现否定混淆和实体替换是主要失败模式。实验表明80%正常数据+20%对抗数据为最优微调比例；小模型存在容量瓶颈，而从ELECTRA-small扩展到ELECTRA-base可消除鲁棒性与准确率的权衡。采用基于命名实体识别的对比学习策略效果最佳，在AddSent上达到89.89% EM，SQuAD上达90.73% EM，实现对抗差距94.9%的缩小。这是首个结合语言学错误分析与NER引导对比学习的对抗问答工作，证明针对性缓解策略可实现清洁与对抗性能的近似对齐。


<details>
  <summary>Details</summary>
Motivation: 标准问答系统在如SQuAD等基准测试中表现优异，但对对抗样本仍脆弱。为提升模型在对抗攻击下的鲁棒性，亟需深入理解其失败原因并设计有效缓解策略。

Method: 采用多尺度模型（ELECTRA-small至ELECTRA-base）在AddSent数据集上进行系统实验，结合五种互补的分类方案开展多层次错误分析；通过调整对抗微调比例与数据增强探索优化策略；提出基于命名实体识别的对比学习方法以针对性提升鲁棒性。

Result: 80%正常数据+20%对抗数据为最优微调比例；模型规模扩大可消除鲁棒性-准确率权衡；实体感知对比学习在AddSent上实现89.89% EM、SQuAD上90.73% EM，对抗差距缩小94.9%。

Conclusion: 通过结合语言学错误分析与基于NER的对比学习，可有效提升问答模型在对抗攻击下的鲁棒性，实现清洁与对抗性能的接近对齐，为构建更稳健的QA系统提供了新路径。

Abstract: Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.

</details>


### [65] [Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning](https://arxiv.org/abs/2601.02739)
*Jinbo Hao,Kai Yang,Qingzhen Su,Yang Chen,Yifan Li,Chao Jiang*

Main category: cs.CL

TL;DR: 提出一种基于知识蒸馏链式模型并引入代码模块以缓解提示诱导型幻觉问题，通过代码引导知识图谱探索，增强模型推理的准确性与可验证性。在GPT-4和LLaMA-3.3上实验表明，该方法显著提升上下文理解能力，有效减少幻觉，各项指标（HIT@1/3/5）提升超过13%，部分场景下得分超95%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成过程中易受提示影响产生幻觉，尤其在缺乏外部结构化知识支持时表现更差，亟需一种能增强推理准确性和可验证性的方法来缓解此类问题。

Method: 构建基于知识蒸馏的链式模型，引入代码模块作为外部知识输入，指导知识图谱探索，并将代码融入思维链提示中，从而约束和优化大模型的推理过程。

Result: 在多个公开数据集上，该方法使HIT@1、HIT@3和HIT@5分别提升15.64%、13.38%和13.28%，部分设置下三项指标均超过95%，显著降低幻觉行为，提升模型输出的准确性与可信度。

Conclusion: 通过引入代码驱动的知识图谱探索机制，结合知识蒸馏链式模型，能够有效抑制提示诱导的幻觉，显著提升大语言模型在复杂推理任务中的准确性和可靠性。

Abstract: To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.

</details>


### [66] [Language Hierarchization Provides the Optimal Solution to Human Working Memory Limits](https://arxiv.org/abs/2601.02740)
*Luyao Chen,Weibo Gao,Junjie Wu,Jinshan Wu,Angela D. Friederici*

Main category: cs.CL

TL;DR: 该研究揭示了人类语言的层级结构如何最优地解决工作记忆容量有限的挑战。通过建立一个衡量语言处理机制中单位数量与人类工作记忆容量匹配程度的似然函数，发现最大似然估计值（theta_MLE）即为单位均值。计算模拟和自然语言验证表明，相较于线性处理，层级处理在序列/句子长度增加时更有效地控制theta_MLE值在人类工作记忆限制内，并呈现出与儿童工作记忆发展相一致的收敛模式。结果支持层级结构能优化语言输入的处理效率，同时满足记忆约束，从而真实解释人类语言普遍具有层级性的原因。


<details>
  <summary>Details</summary>
Motivation: 探究人类语言为何具有普遍的层级结构，特别是从工作记忆容量有限的角度出发，理解语言组织方式的适应性优势。

Method: 构建一个量化语言处理机制中单位数量与人类工作记忆容量匹配程度的似然函数，利用最大似然估计（MLE）分析其最优解，并通过符号序列的计算模拟及自然语言句子的验证分析，比较层级与线性处理在不同序列长度下的表现。

Result: 层级处理显著优于线性处理，在序列长度增加时仍能有效控制最大似然估计值（theta_MLE）在人类工作记忆容量范围内；且该模式与儿童工作记忆发展的趋势一致，表现出收敛特征。

Conclusion: 语言的层级结构是优化语言处理效率并适应人类有限工作记忆能力的进化解决方案，这为人类语言普遍具有层级性提供了根本性解释。

Abstract: Language is a uniquely human trait, conveying information efficiently by organizing word sequences in sentences into hierarchical structures. A central question persists: Why is human language hierarchical? In this study, we show that hierarchization optimally solves the challenge of our limited working memory capacity. We established a likelihood function that quantifies how well the average number of units according to the language processing mechanisms aligns with human working memory capacity (WMC) in a direct fashion. The maximum likelihood estimate (MLE) of this function, tehta_MLE, turns out to be the mean of units. Through computational simulations of symbol sequences and validation analyses of natural language sentences, we uncover that compared to linear processing, hierarchical processing far surpasses it in constraining the tehta_MLE values under the human WMC limit, along with the increase of sequence/sentence length successfully. It also shows a converging pattern related to children's WMC development. These results suggest that constructing hierarchical structures optimizes the processing efficiency of sequential language input while staying within memory constraints, genuinely explaining the universal hierarchical nature of human language.

</details>


### [67] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练、无标签的短文本聚类方法，可应用于任何现有嵌入模型。该方法基于迭代向量更新，通过大语言模型（LLM）指导，构建并优化稀疏向量以实现聚类。在无标注数据和未知聚类数量的商业场景下，该方法表现优于或相当於现有对比学习方法，且无需先验知识。实验表明其对不同嵌入模型、小规模LLM和多种聚类方法具有通用性，并具备良好的可扩展性，显著降低计算成本，更贴近真实应用场景。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，如客户聊天机器人，存在大量用户输入需要按意图聚类，但通常缺乏标注数据，且聚类数量未知。传统方法依赖标注数据或预设聚类数，难以适应真实场景。因此亟需一种无需标签、无需训练、灵活且高效的聚类方法。

Method: 提出一种基于迭代向量更新的方法：首先基于代表性文本生成稀疏向量，再通过大语言模型（LLM）引导进行迭代优化，实现聚类。整个过程不依赖训练或标签，适用于任意嵌入模型。

Result: 在多个数据集上，该方法达到与先进对比学习方法相当甚至更优的聚类效果；对不同嵌入模型、小型LLM和聚类算法均表现出良好兼容性；在大规模数据上具备高效性和低计算开销，显著提升实用性。

Conclusion: 所提方法为无监督短文本聚类提供了一种高效、灵活、低成本的解决方案，特别适用于现实世界中缺乏标签和先验信息的场景，具有高度实用价值和广泛适用性。

Abstract: In this paper, we propose a training-free and label-free method for short text clustering that can be used on top of any existing embedder. In the context of customer-facing chatbots, companies are dealing with large amounts of user utterances that need to be clustered according to their intent. In these commercial settings, no labeled data is typically available, and the number of clusters is not known. Our method is based on iterative vector updating: it constructs sparse vectors based on representative texts, and then iteratively refines them through LLM guidance. Our method achieves comparable or superior results to state-of-the-art methods that use contrastive learning, but without assuming prior knowledge of clusters or labels. Experiments on diverse datasets and smaller LLMs show that our method is model agnostic and can be applied to any embedder, with relatively small LLMs, and different clustering methods. We also show that our method scales to large datasets, reducing the computational cost of the LLM. These low-resource, adaptable settings and the scalability of our method make it more aligned with real-world scenarios than existing clustering methods.

</details>


### [68] [Window-based Membership Inference Attacks Against Fine-tuned Large Language Models](https://arxiv.org/abs/2601.02751)
*Yuetian Chen,Yuntao Du,Kaiyuan Zhang,Ashish Kundu,Charles Fleming,Bruno Ribeiro,Ninghui Li*

Main category: cs.CL

TL;DR: WBC（Window-Based Comparison）是一种针对大语言模型（LLM）的新型成员推断攻击方法，通过滑动窗口和基于符号的聚合来捕捉局部上下文中的记忆信号，相比传统的全局平均损失方法，显著提升了攻击效果。在11个数据集上的实验表明，WBC在高AUC值和低假阳性率下实现2-3倍的检测率提升，揭示了微调后LLM中的严重隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击主要依赖全局信号（如平均损失），但这些信号会稀释细微的局部记忆痕迹，导致攻击效果受限。本文旨在突破这一局限，探索更有效的局部化记忆信号利用方式。

Method: 提出WBC方法，采用滑动窗口机制，在不同大小的窗口内对目标模型与参考模型的损失进行比较，并根据符号差异生成二元投票；通过几何间隔分布的多尺度窗口集成投票结果，以捕获从词级到短语级的记忆模式。

Result: 在11个数据集上，WBC显著优于现有基线方法，实现了更高的AUC值，并在低假阳性率下检测率提升2-3倍，验证了局部证据聚合的有效性。

Conclusion: 局部化信号比全局平均信号更能有效揭示训练数据的成员身份，强调了微调大语言模型中存在的严重隐私风险，为未来模型安全设计提供了重要启示。

Abstract: Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.

</details>


### [69] [Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models](https://arxiv.org/abs/2601.02819)
*Junxiang Qiu,Shuo Wang,Zhengsu Chen,Hengheng Zhang,Jinda Lu,Changcheng Li,Qi Tian*

Main category: cs.CL

TL;DR: 本文提出了一种名为PHSA的可训练稀疏注意力框架，利用标点符号作为语义边界锚点，通过双分支聚合机制融合全局语义与标点增强的边界特征，在几乎不增加计算开销的情况下保留核心语义结构，并采用极端稀疏自适应训练与推理策略以稳定低激活率下的模型行为。实验表明，PHSA在长序列任务中显著优于密集注意力和现有稀疏注意力基线，尤其在32k输入长度下，97.3%稀疏度时信息损失减少10.8%。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法依赖粗粒度语义表示进行块选择，模糊了块内语义边界，导致关键信息丢失。为解决此问题，需一种能精确捕捉语义边界的稀疏注意力机制。

Method: 设计双分支聚合机制，融合全局语义与标点增强的边界特征；引入极端稀疏自适应训练与推理策略，提升低激活率下的稳定性。

Result: PHSA在通用基准和长序列评估中持续优于密集注意力及先进稀疏注意力基线（如InfLLM v2）；在0.6B参数、32k序列长度下，97.3%稀疏度时信息损失降低10.8%。

Conclusion: PHSA是一种高效且可训练的稀疏注意力框架，通过标点感知机制精准建模长序列语义边界，在保持低计算开销的同时显著提升长上下文建模能力。

Abstract: Attention serves as the fundamental mechanism for long-context modeling in large language models (LLMs), yet dense attention becomes structurally prohibitive for long sequences due to its quadratic complexity. Consequently, sparse attention has received increasing attention as a scalable alternative. However, existing sparse attention methods rely on coarse-grained semantic representations during block selection, which blur intra-block semantic boundaries and lead to the loss of critical information. To address this issue, we propose \textbf{P}unctuation-aware \textbf{H}ybrid \textbf{S}parse \textbf{A}ttention \textbf{(PHSA)}, a natively trainable sparse attention framework that leverages punctuation tokens as semantic boundary anchors. Specifically, (1) we design a dual-branch aggregation mechanism that fuses global semantic representations with punctuation-enhanced boundary features, preserving the core semantic structure while introducing almost no additional computational overhead; (2) we introduce an extreme-sparsity-adaptive training and inference strategy that stabilizes model behavior under very low token activation ratios; Extensive experiments on general benchmarks and long-context evaluations demonstrate that PHSA consistently outperforms dense attention and state-of-the-art sparse attention baselines, including InfLLM v2. Specifically, for the 0.6B-parameter model with 32k-token input sequences, PHSA can reduce the information loss by 10.8\% at a sparsity ratio of 97.3\%.

</details>


### [70] [The performances of the Chinese and U.S. Large Language Models on the Topic of Chinese Culture](https://arxiv.org/abs/2601.02830)
*Feiyan Liu,Chenxun Zhuo,Siyan Zhao,Bao Ge,Tianming Liu*

Main category: cs.CL

TL;DR: 本研究比较了中美开发的大语言模型在中文文化相关问题上的表现，发现中国开发的模型在理解中国传统文化（如历史、文学、诗歌）方面普遍优于美国开发的模型，尤其以DeepSeek-V3.2和Qwen3-Max为代表；而美国模型中Gemini 2.5Pro和GPT-5.1表现较好。性能差异可能源于训练数据分布、本地化策略及对中文文化内容的重视程度不同。


<details>
  <summary>Details</summary>
Motivation: 探究中美开发的大语言模型在中文文化理解上是否存在文化差异，揭示其背后的技术与数据因素。

Method: 采用直接提问法，评估GPT-5.1、DeepSeek-V3.2、Qwen3-Max和Gemini2.5Pro等模型在传统中国文化（历史、文学、诗歌等）相关问题上的表现，并进行中美模型间的对比分析。

Result: 中国开发的模型整体表现优于美国开发的模型；在美国模型中，Gemini 2.5Pro和GPT-5.1表现相对较高。

Conclusion: 大语言模型的文化表现差异可能源于训练数据分布、本地化策略以及对中文文化内容的重视程度，表明文化背景在模型设计与训练中具有重要影响。

Abstract: Cultural backgrounds shape individuals' perspectives and approaches to problem-solving. Since the emergence of GPT-1 in 2018, large language models (LLMs) have undergone rapid development. To date, the world's ten leading LLM developers are primarily based in China and the United States. To examine whether LLMs released by Chinese and U.S. developers exhibit cultural differences in Chinese-language settings, we evaluate their performance on questions about Chinese culture. This study adopts a direct-questioning paradigm to evaluate models such as GPT-5.1, DeepSeek-V3.2, Qwen3-Max, and Gemini2.5Pro. We assess their understanding of traditional Chinese culture, including history, literature, poetry, and related domains. Comparative analyses between LLMs developed in China and the U.S. indicate that Chinese models generally outperform their U.S. counterparts on these tasks. Among U.S.-developed models, Gemini 2.5Pro and GPT-5.1 achieve relatively higher accuracy. The observed performance differences may potentially arise from variations in training data distribution, localization strategies, and the degree of emphasis on Chinese cultural content during model development.

</details>


### [71] [To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs](https://arxiv.org/abs/2601.02858)
*Saurabh Kumar Pandey,Sougata Saha,Monojit Choudhury*

Main category: cs.CL

TL;DR: 本文提出逆向社会人口学提示（ISDP）以解决传统社会人口学提示（SDP）中因提示敏感性、解码参数和生成任务复杂性导致的偏差解释难题。通过Goodreads-CSI数据集，测试了四种LLM在真实与模拟用户行为下的表现，发现模型在真实行为上表现更好，但在个体层面两种行为下性能趋同，表明个性化存在局限。


<details>
  <summary>Details</summary>
Motivation: 传统SDP方法在评估LLMs文化适应能力时易受提示敏感性、解码参数及生成任务复杂性等干扰因素影响，难以区分性能下降是源于偏见还是任务设计问题，因此需要更可靠的评估方法。

Method: 采用逆向社会人口学提示（ISDP），即让LLMs根据实际和模拟的用户行为推断其社会人口属性，并基于Goodreads-CSI数据集对Aya-23、Gemma-2、GPT-4o和LLaMA-3.1四款模型进行测试。

Result: 模型在真实用户行为上的表现优于模拟行为；但当分析至个体层面时，两类行为下的性能趋于一致，表明个性化能力存在上限。

Conclusion: ISDP能更准确地评估LLMs的文化适应能力，揭示出当前模型在个性化推理中的局限性，提示未来需关注个体级行为理解而非仅依赖群体特征建模。

Abstract: Socio-demographic prompting (SDP) - prompting Large Language Models (LLMs) using demographic proxies to generate culturally aligned outputs - often shows LLM responses as stereotypical and biased. While effective in assessing LLMs' cultural competency, SDP is prone to confounding factors such as prompt sensitivity, decoding parameters, and the inherent difficulty of generation over discrimination tasks due to larger output spaces. These factors complicate interpretation, making it difficult to determine if the poor performance is due to bias or the task design. To address this, we use inverse socio-demographic prompting (ISDP), where we prompt LLMs to discriminate and predict the demographic proxy from actual and simulated user behavior from different users. We use the Goodreads-CSI dataset (Saha et al., 2025), which captures difficulty in understanding English book reviews for users from India, Mexico, and the USA, and test four LLMs: Aya-23, Gemma-2, GPT-4o, and LLaMA-3.1 with ISDP. Results show that models perform better with actual behaviors than simulated ones, contrary to what SDP suggests. However, performance with both behavior types diminishes and becomes nearly equal at the individual level, indicating limits to personalization.

</details>


### [72] [Training Language Models with homotokens Leads to Delayed Overfitting](https://arxiv.org/abs/2601.02867)
*Adrian Cosma,Stefan Ruseti,Emilian Radoi,Mihai Dascalu*

Main category: cs.CL

TL;DR: 该论文提出将同音子词（homotokens）——即同一词汇项的不同有效子词分段——作为保持语义不变的数据增强方式，通过引入轻量级训练架构，在不改变训练目标或词元接口的前提下，利用辅助因果编码器和块因果交叉注意力对采样的同音子词变体进行条件化，从而提升语言模型的泛化能力。在数据受限的预训练中，同音子词增强能有效延迟过拟合并改善跨多种评估数据集的性能；在多语言微调中，其效果取决于分词器质量：当标准词元高度压缩时增益最大，而分词器已过度碎片化输入时则效果减弱。整体上，同音子词提供了一种简单且模块化的机制，实现语言模型对词元化方式的不变性。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的子词分词存在非唯一性，即多个不同的词元序列可解码为相同表面形式并保留语义，但引发不同的内部计算。然而，当前训练通常仅使用单一标准最长前缀分词方式，忽略了这种多样性。因此，研究旨在探索如何利用这种非唯一性来增强模型鲁棒性和泛化能力。

Method: 提出同音子词概念，并设计一种轻量级训练架构，通过辅助因果编码器和块因果交叉注意力机制，使标准下一个词预测任务基于采样得到的同音子词变体进行条件化，从而引入对词元化方式的不变性。该方法无需修改训练目标或词元接口。

Result: 在数据受限的预训练中，同音子词增强显著延缓过拟合，提高模型在多样数据集上的泛化表现；在多语言微调中，其有效性依赖于分词器质量：当分词器压缩程度高时效果最佳，过度碎片化时则收益下降。

Conclusion: 同音子词是一种有效的、模块化的数据增强策略，能够引导语言模型具备对子词分词方式的不变性，尤其适用于资源有限或分词器质量较高的场景。

Abstract: Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.

</details>


### [73] [LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark](https://arxiv.org/abs/2601.02872)
*Ziyang Chen,Xing Wu,Junlong Jia,Chaochen Gao,Qi Fu,Debing Zhang,Songlin Hu*

Main category: cs.CL

TL;DR: LongBench Pro is a large-scale, bilingual benchmark with 1,500 real-world long-context samples in English and Chinese, covering 11 primary and 25 secondary tasks, and input lengths from 8k to 256k tokens. It introduces a multi-dimensional taxonomy for context requirements and uses a human-model collaborative pipeline to balance scalability and quality. Evaluation of 46 LLMs reveals that long-context optimization matters more than parameter scaling, actual effective context length is shorter than claimed, and reasoning paradigms benefit native-reasoning models most.


<details>
  <summary>Details</summary>
Motivation: Existing long-context benchmarks lack realism and scalability; synthetic tasks are too simplistic, while fully manual annotation is too costly for extreme lengths and diverse scenarios.

Method: Proposes LongBench Pro with a human-model collaborative construction pipeline: frontier LLMs generate draft questions, answers, and rationales, which experts then validate and refine. The benchmark includes fine-grained task-specific metrics and a taxonomy based on context dependency, length, and difficulty.

Result: Evaluations show that long-context optimization improves comprehension more than parameter scaling, effective context length is often shorter than advertised, and the 'thinking' paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a good trade-off.

Conclusion: LongBench Pro provides a robust, realistic, and scalable testbed for advancing long-context understanding in LLMs.

Abstract: The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the "thinking" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.

</details>


### [74] [Revisiting Data Compression with Language Modeling](https://arxiv.org/abs/2601.02875)
*Chen-Han Tsai*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在数据压缩任务中的应用潜力，提出了一种无需额外训练即可实现约18%调整后压缩率的新方法，在enwik9数据集上达到当前最优水平。研究还拓展至非英语文本、代码和字节流序列的压缩，发现尽管LLM在自然文本领域表现优异，但在合理配置下对非自然文本序列仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有数据压缩算法面临效率与通用性挑战，而大语言模型虽在文本和多模态数据压缩中展现潜力，但实际应用中仍存在若干关键问题，如压缩效率、通用性及部署成本，亟需探索如何有效利用LLM提升压缩性能并扩大适用范围。

Method: 采用无需额外训练的LLM进行数据压缩，通过优化提示工程与解码策略，降低调整后的压缩率；同时针对不同数据类型（非英语、代码、字节流）设计适配性配置，验证其压缩能力。

Result: 在enwik9数据集上实现了约18%的调整压缩率，优于先前方法且无需重新训练模型；在非自然文本序列压缩中，经适当配置后仍保持良好性能。

Conclusion: 大型语言模型在数据压缩中具有显著潜力，尤其在文本主导领域表现突出；通过合理配置可有效扩展至非自然文本序列，为未来基于LLM的通用压缩系统提供可行路径。

Abstract: In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.

</details>


### [75] [Linear Script Representations in Speech Foundation Models Enable Zero-Shot Transliteration](https://arxiv.org/abs/2601.02906)
*Ryan Soh-Eun Shim,Kwanghee Choi,Kalvin Chang,Ming-Hao Hsu,Florian Eichin,Zhizheng Wu,Alane Suhr,Michael A. Hedderich,David Harwath,David R. Mortensen,Barbara Plank*

Main category: cs.CL

TL;DR: 本文研究了多语言语音基础模型（如Whisper）在处理不同地区变体时因使用不同书写系统而导致的输出非确定性问题。研究发现，书写系统在模型激活空间中呈线性编码，并可通过在推理时修改激活值来直接控制输出书写系统。该方法能实现对语音识别输出书写的后验控制，即使在非常规的语言-书写系统组合（如意大利语用西里尔字母、日语用拉丁字母）下也有效，并在Whisper模型的所有规模上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 多语言语音模型在训练数据中包含同一语言的不同地区变体，这些变体常使用不同的书写系统，导致语音识别输出在书写系统上存在不确定性。为解决这一问题，需要一种方法在推理阶段灵活控制输出书写系统。

Method: 通过分析多语言语音模型的激活空间，发现书写系统信息以线性方式编码；提出在推理时向激活值添加特定的书写系统向量，从而实现对输出书写系统的直接控制。

Result: 该方法可在多种语言-书写系统组合中成功诱导书写系统变化，包括非常规配对，在所有尺寸的Whisper模型上均达到竞争性性能，验证了其有效性与通用性。

Conclusion: 书写系统在多语言语音模型的激活空间中具有可操作的线性表示，通过简单地在推理时注入脚本向量即可实现对语音识别输出书写的精确控制，为多语言语音处理提供了灵活的后验调控手段。

Abstract: Multilingual speech foundation models such as Whisper are trained on web-scale data, where data for each language consists of a myriad of regional varieties. However, different regional varieties often employ different scripts to write the same language, rendering speech recognition output also subject to non-determinism in the output script. To mitigate this problem, we show that script is linearly encoded in the activation space of multilingual speech models, and that modifying activations at inference time enables direct control over output script. We find the addition of such script vectors to activations at test time can induce script change even in unconventional language-script pairings (e.g. Italian in Cyrillic and Japanese in Latin script). We apply this approach to inducing post-hoc control over the script of speech recognition output, where we observe competitive performance across all model sizes of Whisper.

</details>


### [76] [Beyond the Black Box: Theory and Mechanism of Large Language Models](https://arxiv.org/abs/2601.02907)
*Zeyu Gan,Ruifeng Ren,Wei Yao,Xiaolin Hu,Gengze Xu,Chen Qian,Huayi Tang,Zixuan Gong,Xinhao Yao,Pengwei Tang,Zhenxing Dou,Yong Liu*

Main category: cs.CL

TL;DR: 该综述提出了一种基于生命周期的统一分类法，将大语言模型（LLMs）的研究划分为六个阶段：数据准备、模型准备、训练、对齐、推理和评估。通过系统梳理各阶段的基础理论与内在机制，分析了数据混合的数学依据、架构表征能力极限及对齐算法优化动态等核心问题，并指出了合成数据自提升、安全保证的数学边界以及涌现智能的机制起源等前沿挑战，旨在推动LLM发展从工程经验向科学范式转变。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽在实践中表现出色，但其理论理解仍严重滞后，导致系统被视为‘黑箱’，亟需建立系统的理论框架以弥合理论与实践之间的鸿沟。

Method: 提出一个六阶段生命周期分类法，涵盖数据准备、模型准备、训练、对齐、推理和评估，对各阶段的理论基础与机制进行系统性梳理与分析。

Result: 揭示了数据混合的数学合理性、模型架构的表征限制、对齐过程的优化特性等关键理论问题，并识别出合成数据自我改进的理论极限、安全保证的数学边界以及涌现智能的机制来源等前沿挑战。

Conclusion: 通过连接实证观察与严谨科学探究，本文为大语言模型的发展提供了一个结构化路线图，推动其从依赖工程直觉向具有原则性的科学学科演进。

Abstract: The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.

</details>


### [77] [Image, Word and Thought: A More Challenging Language Task for the Iterated Learning Model](https://arxiv.org/abs/2601.02911)
*Hyoyeon Lee,Seth Bullock,Conor Houghton*

Main category: cs.CL

TL;DR: 该研究利用改进的半监督迭代学习模型，成功在复杂意义空间（七段显示图像）中实现了可表达、组合性和稳定性的语言学习与传播。


<details>
  <summary>Details</summary>
Motivation: 探索语言传输中的约束如何促进语言结构的形成，特别是解决传统模型在处理复杂语义时的局限性。

Method: 采用基于自编码器架构的半监督迭代学习模型，结合监督与无监督学习，模拟多代语言传播过程。

Result: 模型成功学习并传播了一种语言：对128个字符使用不同代码（表达性），信号成分与意义成分一致映射（组合性），且语言在代际间保持不变（稳定性）。

Conclusion: 该模型验证了在有限信息输入下，通过迭代学习机制可以自发产生结构化语言，为理解语言演化提供了新的计算支持。

Abstract: The iterated learning model simulates the transmission of language from generation to generation in order to explore how the constraints imposed by language transmission facilitate the emergence of language structure. Despite each modelled language learner starting from a blank slate, the presence of a bottleneck limiting the number of utterances to which the learner is exposed can lead to the emergence of language that lacks ambiguity, is governed by grammatical rules, and is consistent over successive generations, that is, one that is expressive, compositional and stable. The recent introduction of a more computationally tractable and ecologically valid semi supervised iterated learning model, combining supervised and unsupervised learning within an autoencoder architecture, has enabled exploration of language transmission dynamics for much larger meaning-signal spaces. Here, for the first time, the model has been successfully applied to a language learning task involving the communication of much more complex meanings: seven-segment display images. Agents in this model are able to learn and transmit a language that is expressive: distinct codes are employed for all 128 glyphs; compositional: signal components consistently map to meaning components, and stable: the language does not change from generation to generation.

</details>


### [78] [RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems](https://arxiv.org/abs/2601.02917)
*Mengze Hong,Di Jiang,Jiangtao Wen,Zhiyang Su,Yawen Li,Yanjie Sun,Guan Wang,Chen Jason Zhang*

Main category: cs.CL

TL;DR: RAL2M 是一种新型框架，通过将大模型作为检索系统中的查询-响应匹配裁判，消除生成幻觉，利用查询自适应的潜在集成策略建模不同模型的能力与相互依赖关系，实现校准共识决策，在大规模基准测试中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 大模型在服务系统中存在幻觉问题，需要显式知识接地以保证合规性响应，因此需要一种替代纯生成方法的稳健方案。

Method: 提出 Retrieval-Augmented Learning-to-Match (RAL2M) 框架，将大模型作为查询-响应匹配裁判，并采用查询自适应的潜在集成策略，建模多模型间的异质能力与依赖关系，实现校准共识决策。

Result: 在大规模基准测试中，该方法有效利用了‘群体智慧’，显著优于多个强基线方法。

Conclusion: RAL2M 为减少大模型幻觉提供了可靠路径，未来可进一步探索潜在表示的利用潜力。

Abstract: Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.

</details>


### [79] [Memorization, Emergence, and Explaining Reversal Failures: A Controlled Study of Relational Semantics in LLMs](https://arxiv.org/abs/2601.02931)
*Yihua Zhu,Qianying Liu,Jiaxin Wang,Fei Cheng,Chaoran Liu,Akiko Aizawa,Sadao Kurohashi,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 该研究通过基于知识图谱的合成框架，探究自回归大模型在关系任务中的逻辑语义学习能力。发现仅需足够的逻辑相关监督即可在浅层模型中引发关系语义的突现，并且成功泛化与中间层信号的稳定性相关。反转失败主要由自回归顺序偏差引起，而非缺乏逆关系语义。


<details>
  <summary>Details</summary>
Motivation: 探究自回归大语言模型是否真正理解关系的逻辑语义（如对称性、逆关系），以及反转失败是源于语义缺失还是顺序偏差。

Method: 构建基于知识图谱的合成文本生成框架，使用对称/逆关系三元组训练从零开始的GPT风格自回归模型，并评估记忆、逻辑推理和上下文外推能力。

Result: 在足够逻辑监督下，即使浅层模型也出现关系语义的显著突现；成功泛化与中间层信号稳定性一致；顺序匹配的正反测试和扩散基线表明反转失败主要由自回归顺序偏差导致。

Conclusion: 自回归模型的反转失败主要源于其生成顺序的固有偏差，而非对逆关系逻辑的不理解；通过充分的逻辑监督，模型可在浅层结构中有效学习关系的逻辑语义。

Abstract: Autoregressive LLMs perform well on relational tasks that require linking entities via relational words (e.g., father/son, friend), but it is unclear whether they learn the logical semantics of such relations (e.g., symmetry and inversion logic) and, if so, whether reversal-type failures arise from missing relational semantics or left-to-right order bias. We propose a controlled Knowledge Graph-based synthetic framework that generates text from symmetric/inverse triples, train GPT-style autoregressive models from scratch, and evaluate memorization, logical inference, and in-context generalization to unseen entities to address these questions. We find a sharp phase transition in which relational semantics emerge with sufficient logic-bearing supervision, even in shallow (2-3 layer) models, and that successful generalization aligns with stable intermediate-layer signals. Finally, order-matched forward/reverse tests and a diffusion baseline indicate that reversal failures are primarily driven by autoregressive order bias rather than deficient inversion semantics.

</details>


### [80] [Pearmut: Human Evaluation of Translation Made Trivial](https://arxiv.org/abs/2601.02933)
*Vilém Zouhar,Tom Kocmi*

Main category: cs.CL

TL;DR: Pearmut is a lightweight, feature-rich platform that simplifies end-to-end human evaluation in multilingual NLP, particularly for machine translation. It removes barriers to human evaluation by supporting standard protocols (DA, ESA, MQM) and enabling extensibility for new ones. Key features include document-level context, absolute and contrastive evaluation, attention checks, pre-annotations, and adaptive assignment strategies. The platform aims to make human evaluation as practical and routine as automatic evaluation.


<details>
  <summary>Details</summary>
Motivation: Human evaluation is the gold standard in multilingual NLP but is often skipped due to high setup complexity and operational overhead with existing tools. There is a need for a more accessible and efficient solution.

Method: Pearmut is designed as a lightweight platform that integrates essential features for reliable human evaluation. It supports multiple evaluation protocols, offers document-level context, enables both static and active learning-based task assignment, includes attention checks, and supports pre-annotations via ESAAI. Its architecture allows extensibility for custom evaluation protocols.

Result: Pearmut successfully reduces the engineering and operational burden of human evaluation, making it feasible to integrate into routine model development and diagnosis workflows. It supports multilingual tasks effectively, especially machine translation, and provides flexibility through extensibility and advanced evaluation features.

Conclusion: Pearmut transforms human evaluation from an occasional, cumbersome process into a practical, routine component of NLP model development by significantly lowering entry barriers and offering rich, flexible functionality.

Abstract: Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.

</details>


### [81] [Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion](https://arxiv.org/abs/2601.02956)
*Jeonghyun Park,Byeongjeong Kim,Seojin Hwang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 该论文指出多语言检索增强生成（mRAG）系统存在对高资源语言（尤其是英语）的假性偏好，这种现象主要源于评估基准中的结构性偏差，如曝光偏差、黄金答案可用性偏差和文化偏见。为解决这些问题，作者提出DeLP（去偏语言偏好）度量方法，以校准这些混淆因素。分析表明，此前报告的英语偏好实为证据分布的结果而非模型固有偏见；相反，检索器更倾向于查询与文档语言的单语对齐。基于此，提出DELTA框架，通过利用单语对齐优化跨语言检索与生成。实验显示，DELTA在多种语言上均优于英语中转和现有mRAG基线。


<details>
  <summary>Details</summary>
Motivation: 现有mRAG系统在多语言任务中表现出对英语的偏好，但这一现象可能并非源于大语言模型本身的能力优势，而是由于评估基准中存在的结构性偏差，如英语资源集中导致的曝光偏差、黄金答案可得性差异以及话题文化局部性带来的文化偏见，从而扭曲了真实语言能力评估。因此，亟需一种能去除这些结构偏差的评估方法，并设计更公平有效的多语言系统。

Method: 提出DeLP（Debiased Language Preference）度量方法，用于识别并校正评估中的暴露偏差、黄金答案可用性偏差及文化偏见等结构性干扰因素；在此基础上，揭示检索器本质偏好为单语对齐，进而设计DELTA（DEbiased Language preference-guided Text Augmentation）框架，通过策略性地强化查询与文档间的单语一致性，提升跨语言检索与生成性能。

Result: 使用DeLP进行分析后发现，之前观察到的英语偏好主要由数据分布不均造成，而非模型内在偏见；而实际检索行为更偏向于单语对齐。DELTA框架在多个低资源语言上表现显著优于传统英语中转方法和现有mRAG基线，在不同语言间实现更均衡且高效的跨语言生成效果。

Conclusion: 多语言检索增强生成系统中的语言偏好很大程度上是评估基准结构性偏差所致，而非模型固有能力问题。通过引入去偏度量DeLP和基于单语对齐优化的DELTA框架，能够有效消除虚假偏好，实现更公平、高效的跨语言信息处理。

Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.

</details>


### [82] [LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation](https://arxiv.org/abs/2601.02957)
*Fabian Lukassen,Christoph Weisser,Michael Schlee,Manish Kumar,Anton Thielmann,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: 本文提出一种结合集成统计方法与大语言模型（LLMs）的新型变化点检测框架，旨在提升时间序列数据中制度变迁的检测准确性和可解释性。该框架通过整合十种不同的变化点检测算法，克服了单一方法在不同数据特征下的局限性，显著提升了性能和鲁棒性；同时，利用LLM生成上下文相关的解释，将检测到的变化点与真实世界历史事件关联起来，并针对私有或特定领域数据提供基于检索增强生成（RAG）的定制化解释。该开源Python框架已在金融、政治科学和环境科学等领域验证其实用性，有效将统计结果转化为分析师和决策者可用的洞察。


<details>
  <summary>Details</summary>
Motivation: 当前变化点检测存在两大瓶颈：一是单一检测方法在不同数据条件下表现差异大，方法选择困难且易导致次优结果；二是自动化、上下文相关的解释严重缺失，限制了结果的实际应用价值。

Method: 采用集成学习策略，融合十种不同的变化点检测算法以提高检测鲁棒性；设计基于LLM的解释生成管道，自动撰写与真实事件相关的上下文说明；对私有或领域特定数据，引入检索增强生成（RAG）机制，确保解释基于用户提供的文档内容。

Result: 相比单个检测方法，集成框架在多种数据场景下均表现出更高的检测准确率和稳定性；所生成的解释具有高度可读性和现实关联性，能有效连接技术发现与实际背景；开源框架在多个跨领域应用中展现出良好的实用性和扩展性。

Conclusion: 本研究构建了一个兼具高精度与强可解释性的变化点检测系统，通过融合统计方法与大语言模型，推动了时间序列分析从‘发现问题’向‘理解问题’的转变，为多领域数据分析提供了有力工具。

Abstract: This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.

</details>


### [83] [Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement](https://arxiv.org/abs/2601.02965)
*Phat Tran,Phuoc Pham,Hung Trinh,Tho Quan*

Main category: cs.CL

TL;DR: 本文针对越南、柬埔寨和老挝地区使用的少数民族语言Bahnar的数字化保存难题，提出一种结合表格与非表格检测技术及基于概率的后处理启发式方法的OCR优化方案，显著提升了识别准确率（从72.86%提升至79.26%），为该语言的保护提供了重要资源，并可推广至其他少数民族语言的数字化工作。


<details>
  <summary>Details</summary>
Motivation: Bahnar语言因研究和数据匮乏面临严重保存挑战，现有光学字符识别（OCR）技术在处理图像质量差的文档时易出错，影响信息检索，亟需提升识别准确性以支持语言数字化保存。

Method: 采用先进的表格与非表格检测算法改善输入数据质量，并结合概率性后处理启发式方法对OCR输出进行误差校正，形成一套完整的识别优化流程。

Result: 实验结果表明，该方法将OCR识别准确率从72.86%提升至79.26%，显著改善了低质量扫描文档的识别效果。

Conclusion: 本研究为Bahnar语言的数字化保存提供了有效技术路径和实用资源，其方法框架具有推广价值，适用于其他少数民族语言的文本数字化工作。

Abstract: Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.

</details>


### [84] [Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning](https://arxiv.org/abs/2601.02972)
*Nathanaël Carraz Rakotonirina,Ren Pang,Neha Anna John,Michael Bohlke-Schneider,Momchil Hardalov*

Main category: cs.CL

TL;DR: 本文提出一种多阶段高效推理方法，结合监督微调（通过拒绝采样或推理轨迹重格式化）与基于自适应长度惩罚的强化学习，设计轻量级奖励函数以惩罚首次正确答案后的生成token，仅在有益时鼓励自我验证。在七个多样化推理任务上的综合评估显示，该方法使8B和32B模型的响应长度平均减少28%和40%，性能仅轻微下降1.6和2.5点，且在过思考调整准确率曲线下面积（AUC_OAA）指标上达到76.6，优于基线模型5点，领先次优方法2.5点，展现出优越的准确率-长度权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的推理能力虽因测试时计算增加而提升，但链式思维（CoT）常导致响应过长，造成计算开销增加甚至性能下降，即‘过度思考’问题。如何在不牺牲准确率的前提下减少冗余推理，是提升效率的关键挑战。

Method: 提出一种多阶段高效推理框架：结合监督微调（使用拒绝采样或推理轨迹重格式化）与强化学习，引入自适应长度惩罚机制；设计轻量级奖励函数，对首次正确答案后的生成进行惩罚，仅在自我验证有帮助时才鼓励其发生。

Result: 在七项不同推理任务上的实验表明，该方法使8B模型响应长度平均减少28%，32B模型减少40%，准确率下降仅1.6和2.5点；在过思考调整准确率曲线下面积（AUC_OAA）指标上达到76.6，比基线高5点，比第二好方法高2.5点，表现显著优于现有复杂方法。

Conclusion: 尽管方法概念简单，但其在减少冗余推理、平衡准确率与响应长度方面表现出色，为高效推理提供了有效且可扩展的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy--response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.

</details>


### [85] [Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders](https://arxiv.org/abs/2601.02978)
*Ruikang Zhang,Shuo Wang,Qi Su*

Main category: cs.CL

TL;DR: 本文提出一种基于稀疏自编码器的框架，用于检索和操控与高级语言行为相关的语义可解释内部特征。通过对比语义对立的对比特征检索流程，结合统计激活分析与生成验证，从稀疏激活空间中提炼出单义功能特征。以五大性格特质为例，证明该方法能实现精确、双向的行为调控，并在稳定性和性能上优于现有方法如对比激活添加（CAA）。研究还发现‘功能忠实性’现象：干预特定内部特征会引发多个语言维度上与目标语义属性一致的可预测变化。表明LLMs内化了高阶概念的深层整合表示，为复杂AI行为调控提供了新颖且稳健的机制路径。


<details>
  <summary>Details</summary>
Motivation: 当前机制可解释性研究虽能识别和干预大语言模型中的内部特征，但仍难以将这些特征与语言生成中的复杂、行为层面的语义属性可靠关联。如何实现对高阶语言行为的精准控制是核心挑战。

Method: 提出基于稀疏自编码器的框架，采用基于受控语义对立的对比特征检索流程，融合统计激活分析与生成验证，从稀疏激活空间中提取单义功能特征。

Result: 在五大性格特质案例中，该方法实现了精确、双向的行为调节，相比对比激活添加（CAA）等方法具有更高的稳定性和性能；同时观察到‘功能忠实性’现象，即干预单一特征可引发多维度协同变化，体现模型对高阶概念的深度整合。

Conclusion: 大语言模型内化了高阶概念的深层集成表示。本研究提供了一种新的、稳健的机制路径，能够通过干预内部特征实现对复杂语言行为的有效调控，推动可解释性与可控性的发展。

Abstract: Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.

</details>


### [86] [P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist](https://arxiv.org/abs/2601.02986)
*Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本文提出P-Check，一种新型个性化奖励建模框架，通过生成动态评估清单来指导奖励预测。引入偏好对比准则加权策略，根据准则对个性化判断的区分能力分配显著性分数。实验表明，P-Check在奖励准确性、下游个性化生成以及分布外场景下均表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有个性化奖励建模方法将用户上下文视为静态或隐式条件信号，未能捕捉人类判断的动态性和多维度特性。

Method: 提出P-Check框架，包含可即插即用的清单生成器，并采用偏好对比准则加权策略，基于准则的判别能力动态分配权重。

Result: P-Check显著提升奖励预测准确性，增强个性化生成效果，并在分布外场景中保持鲁棒性。

Conclusion: P-Check通过动态生成个性化评估清单并优化准则权重，有效提升了个性化奖励建模的性能与泛化能力。

Abstract: Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.

</details>


### [87] [Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy](https://arxiv.org/abs/2601.02989)
*Hosein Hasani,Mohammadali Banayeeanzade,Ali Nafisi,Sadegh Mohammadian,Fatemeh Askari,Mobin Bagherian,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本文提出一种受人类系统2认知启发的测试阶段策略，将大型计数任务分解为多个独立的小任务，以克服Transformer架构在深度上的限制，从而提升大尺度计数任务的准确性。通过机制分析发现，模型在各部分的最终项表示中计算并存储隐式计数，通过专用注意力头传递至中间步骤，并在最后阶段聚合得到总数。实验表明该方法可突破模型架构限制，显著提升计数性能，为理解与改进大语言模型的推理行为提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂数学问题上表现良好，但在计数任务上存在系统性局限，主要源于Transformer架构的深度限制导致大计数问题精度下降。

Method: 提出一种测试阶段的分解策略，模仿人类系统2的认知过程，将大规模计数任务拆分为可信赖的小型子任务，并通过注意力机制实现中间状态的传递与最终聚合。

Result: 实验结果显示，该策略能有效突破模型架构的计数能力限制，在大规模计数任务上达到高准确率，且机制分析揭示了隐式计数在表示中的存储与传播路径。

Conclusion: 本研究揭示了大语言模型中类系统2计数的内在机制，提出了一种通用、可推广的方法，用于增强和理解模型的推理行为，尤其适用于受限于架构深度的任务。

Abstract: Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.

</details>


### [88] [Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2601.02993)
*Qianchi Zhang,Hainan Zhang,Liang Pang,Hongwei Zheng,Zhiming Zheng*

Main category: cs.CL

TL;DR: 本文提出Stable-RAG，一种应对检索文档顺序敏感性的问题的方法。通过在多种检索顺序下运行生成器，聚类隐藏状态并从中心表示中解码，以捕捉主导推理模式，从而减少因检索顺序变化导致的幻觉，提升答案准确性和推理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法虽关注低质量检索和位置偏差，但未解决检索顺序变化对模型行为的影响，而实验发现即使固定正确文档在首位，不同检索顺序仍会导致大范围答案差异，揭示了新的敏感性问题。

Method: Stable-RAG通过多轮检索顺序运行生成器，聚类隐藏状态，选取聚类中心作为解码基础，利用其代表的主流推理路径来校正幻觉输出，增强跨顺序的一致性与准确性。

Result: 在三个问答数据集上，Stable-RAG显著提升了答案准确率、推理一致性以及对不同数据集、检索器和输入长度的泛化能力，优于现有基线方法。

Conclusion: 检索顺序对LLM在RAG中的表现有显著影响，Stable-RAG通过建模和利用排列敏感性，有效缓解由此引发的幻觉问题，为构建更稳定可靠的RAG系统提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.

</details>


### [89] [Large Reasoning Models Are (Not Yet) Multilingual Latent Reasoners](https://arxiv.org/abs/2601.02996)
*Yihong Liu,Raoyuan Zhao,Hinrich Schütze,Michael A. Hedderich*

Main category: cs.CL

TL;DR: 本文系统研究了大型推理模型（LRMs）在11种语言中的多语言隐式推理现象。通过截断策略，发现模型在未完成完整推理步骤前即已得出正确答案，表明存在跨语言的隐式计算过程；资源丰富语言中表现较强，低资源语言较弱，难题上更难观测。尽管表面差异明显，表示分析显示不同语言间的内部预测演化高度一致，整体符合以英语为中心的隐式推理路径。


<details>
  <summary>Details</summary>
Motivation: 探究大型推理模型在多语言环境下的隐式推理行为，揭示其是否普遍存在于不同语言中，并理解其内部机制是否存在差异。

Method: 采用截断策略，逐步缩短推理文本输入，观察模型在不同阶段对答案的预测变化，结合表示分析考察内部状态演化模式。

Result: 发现多语言隐式推理普遍存在，但强度随语言资源丰富程度和任务难度变化；尽管表象不一，内部预测演化模式在各语言间高度一致，且与英语模式基本吻合。

Conclusion: 大型推理模型在多语言环境中普遍存在隐式推理能力，其内部机制具有跨语言一致性，呈现出以英语为中心的推理路径特征。

Abstract: Large reasoning models (LRMs) achieve strong performance on mathematical reasoning tasks, often attributed to their capability to generate explicit chain-of-thought (CoT) explanations. However, recent work shows that LRMs often arrive at the correct answer before completing these textual reasoning steps, indicating the presence of latent reasoning -- internal, non-verbal computation encoded in hidden states. While this phenomenon has been explored in English, its multilingual behavior remains largely unknown. In this paper, we conduct a systematic investigation of multilingual latent reasoning in LRMs across 11 languages. Using a truncation-based strategy, we examine how the correct answer emerges as the model is given only partial reasoning traces, allowing us to measure stepwise latent prediction formation. Our results reveal clear evidence of multilingual latent reasoning, though unevenly: strong in resource-rich languages, weaker in low-resource ones, and broadly less observable on harder benchmarks. To understand whether these differences reflect distinct internal mechanisms, we further perform representational analyses. Despite surface-level disparities, we find that the internal evolution of predictions is highly consistent across languages and broadly aligns with English -- a pattern suggesting an English-centered latent reasoning pathway.

</details>


### [90] [SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering](https://arxiv.org/abs/2601.03014)
*Junli Liang,Pengfei Zhou,Wangqiu Zhou,Wenjie Qing,Qi Zhao,Ziwen Wang,Qi Song,Xiangyang Li*

Main category: cs.CL

TL;DR: 提出SentGraph，一种基于句子级图的RAG框架，通过显式建模句子间的逻辑关系来提升多跳问答性能。利用修辞结构理论区分核心与卫星句，并构建跨文档实体连接的主题子图，在线检索时进行图引导的证据选择和路径扩展，显著改善了多跳推理中的证据链完整性和逻辑连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统基于段落的检索在多跳问答中常提供不相关且逻辑不连贯的上下文，导致证据链不完整和推理错误，因此需要更精细的句子级逻辑关系建模。

Method: 采用修辞结构理论识别核心与卫星句，构建层次化句子图；将句子组织为带跨文档实体桥接的主题子图；在线检索阶段实施图引导的证据选择与路径扩展策略。

Result: 在四个多跳问答基准测试上实验验证了SentGraph的有效性，证明显式建模句子级逻辑依赖对多跳推理至关重要。

Conclusion: SentGraph通过句子级图结构显式建模逻辑关系，有效提升了多跳问答中证据链的完整性与推理准确性，为复杂问答任务提供了新的解决方案。

Abstract: Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.

</details>


### [91] [MMFormalizer: Multimodal Autoformalization in the Wild](https://arxiv.org/abs/2601.03017)
*Jing Xiong,Qi Han,Yunta Hsieh,Hui Shen,Huajian Xin,Chaofan Tao,Chenyang Zhao,Hengyuan Zhang,Taiqiang Wu,Zhen Zhang,Haochen Wang,Zhongwei Wan,Lingpeng Kong,Ngai Wong*

Main category: cs.CL

TL;DR: MMFormalizer 是首个能处理经典力学（源自哈密顿量）、相对论、量子力学和热力学的多模态自动形式化方法，通过整合感知基础的实体与自适应递归终止机制，实现从视觉元素中推断隐含约束并构建形式化命题。该方法在新基准 PhyX-AF 上评估，显示前沿模型如 GPT-5 和 Gemini-3-Pro 在编译与语义准确性上表现优异，尤其在物理推理方面，而几何仍是挑战最大领域。


<details>
  <summary>Details</summary>
Motivation: 自然语言数学的自动形式化在真实世界中面临根本性挑战，因物理问题需从视觉元素中推断隐藏约束（如质量或能量），传统仅依赖文本的方法难以应对多模态复杂性。

Method: MMFormalizer 采用自适应接地机制，结合真实世界数学与物理领域的实体，通过递归接地与公理组合构建形式化命题，并引入自适应递归终止策略，确保每个抽象都有视觉证据支持并锚定于维度或公理基础。

Result: 在新基准 PhyX-AF（包含 115 个来自 MathVerse、PhyX、合成几何与解析几何的样本）上的实验表明，GPT-5 和 Gemini-3-Pro 等前沿模型达到最高编译与语义准确率，其中 GPT-5 在物理推理中表现更优；几何任务仍具挑战性。整体验证了 MMFormalizer 可扩展的统一多模态自动形式化框架。

Conclusion: MMFormalizer 提供了一个可扩展的多模态自动形式化框架，成功实现了从感知到形式推理的桥梁，是首个能处理经典力学、相对论、量子力学和热力学等复杂物理理论的自动形式化系统。

Abstract: Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io

</details>


### [92] [Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis](https://arxiv.org/abs/2601.03018)
*Choonghan Kim,Hyunmin Hwang,Hangeol Chang,Jaemin Kim,Jinse Park,Jae-Sung Lim,Jong Chul Ye*

Main category: cs.CL

TL;DR: 本文提出Dementia-R1，一种基于强化学习的纵向痴呆预后框架，用于从非结构化临床笔记中进行痴呆预测。通过冷启动强化学习策略，先预训练模型预测可验证的临床指标，提升对疾病进展的推理能力，最终在真实世界数据集上达到77.03%的F1分数，并在ADNI基准上表现媲美GPT-4o，有效捕捉认知功能的波动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在纵向预测任务（如痴呆预后）中表现不佳，因缺乏对症状演变的显式标注，且直接使用强化学习受稀疏二元奖励限制。需要一种能有效建模复杂、非单调症状轨迹的方法。

Method: 提出Dementia-R1框架，采用冷启动强化学习策略：先利用可验证的临床指标对模型进行预训练，增强其对疾病进展的推理能力，再进行最终状态决策的强化学习训练，从而缓解稀疏奖励问题。

Result: 在真实世界未结构化临床数据集上，Dementia-R1达到77.03%的F1分数；在ADNI基准上，7B模型性能媲美GPT-4o，能够有效捕捉认知功能的波动轨迹。

Conclusion: Dementia-R1通过冷启动强化学习有效提升了模型对纵向症状演变的推理能力，为临床纵向预测任务提供了高效且可解释的解决方案。

Abstract: While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5

</details>


### [93] [Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning](https://arxiv.org/abs/2601.03027)
*Sindhuja Chaduvula,Ahmed Y. Radwan,Azib Farooq,Yani Ioannou,Shaina Raza*

Main category: cs.CL

TL;DR: F-DPO 是 DPO 的一种简单扩展，通过引入二元事实性标签来改善模型的事实性，减少幻觉。它通过标签翻转修正偏好对，并加入事实性感知的边界，提升在事实性上的表现，无需额外奖励模型或标注。在多个 LLM 上验证有效，显著降低幻觉率并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法如 RLHF 和 DPO 可能因奖励流畅性和自信度而强化幻觉，因此需要一种更注重事实性的对齐方法。

Method: F-DPO 采用标签翻转机制纠正错误排序的偏好对，并引入事实性感知的边际项，强调明显正确性差异的样本，同时在两响应事实性相同时退化为标准 DPO。

Result: 在七个开源大模型（1B-14B）上，F-DPO 显著提升事实性并降低幻觉率；以 Qwen3-8B 为例，幻觉率下降五倍（0.424→0.084），事实性评分提升 50%（5.26→7.90）；在 TruthfulQA 上，MC1 和 MC2 准确率分别提升 17% 和 49%。

Conclusion: F-DPO 无需辅助奖励模型、逐标记注释或多阶段训练，即可有效提升大模型的事实性，减少幻觉，且具备良好的泛化能力。

Abstract: Preference alignment methods such as RLHF and Direct Preference Optimization (DPO) improve instruction following, but they can also reinforce hallucinations when preference judgments reward fluency and confidence over factual correctness. We introduce F-DPO (Factuality-aware Direct Preference Optimization), a simple extension of DPO that uses only binary factuality labels. F-DPO (i) applies a label-flipping transformation that corrects misordered preference pairs so the chosen response is never less factual than the rejected one, and (ii) adds a factuality-aware margin that emphasizes pairs with clear correctness differences, while reducing to standard DPO when both responses share the same factuality. We construct factuality-aware preference data by augmenting DPO pairs with binary factuality indicators and synthetic hallucinated variants. Across seven open-weight LLMs (1B-14B), F-DPO consistently improves factuality and reduces hallucination rates relative to both base models and standard DPO. On Qwen3-8B, F-DPO reduces hallucination rates by five times (from 0.424 to 0.084) while improving factuality scores by 50 percent (from 5.26 to 7.90). F-DPO also generalizes to out-of-distribution benchmarks: on TruthfulQA, Qwen2.5-14B achieves plus 17 percent MC1 accuracy (0.500 to 0.585) and plus 49 percent MC2 accuracy (0.357 to 0.531). F-DPO requires no auxiliary reward model, token-level annotations, or multi-stage training.

</details>


### [94] [NorwAI's Large Language Models: Technical Report](https://arxiv.org/abs/2601.03034)
*Jon Atle Gulla,Peng Liu,Lemei Zhang*

Main category: cs.CL

TL;DR: NorLLM团队开发了一套专为挪威语及北欧语言设计的Transformer-based大模型，涵盖GPT、Mistral、Llama2、Mixtral和Magistral等架构，基于25B-88.45B tokens进行预训练或持续预训练，采用扩展挪威语的分词器与先进后训练策略，显著提升性能、鲁棒性和适应性。指令微调版本（如Mistral-7B-Instruct）表现出色，适用于交互式和领域特定应用。模型对北欧地区研究机构、企业和学生开放使用。


<details>
  <summary>Details</summary>
Motivation: 挪威语在自然语言处理领域仍被严重低估，缺乏针对该语言的重大突破，亟需专门优化的语言模型以支持其技术发展和实际应用。

Method: 基于多种Transformer架构（如GPT、Mistral、Llama2、Mixtral、Magistral）构建模型，从头开始或持续预训练于25B–88.45B个标记，使用扩展的挪威语分词器，并结合先进的后训练策略以提升性能、鲁棒性和可适应性。

Result: 模型在多项任务中表现优异，特别是指令微调版本展现出强大的助手型能力，适用于交互式和特定领域应用场景；模型已向北欧组织、企业及学生开放，支持研究与实验。

Conclusion: NorwAI团队成功构建并发布了面向挪威语及其他北欧语言的大型语言模型，填补了该语言在NLP领域的空白，为北欧地区的学术研究与产业应用提供了重要工具。

Abstract: Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.

</details>


### [95] [Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage](https://arxiv.org/abs/2601.03043)
*Junhao Hu,Fangze Li,Mingtao Xu,Feifan Meng,Shiju Zhao,Tiancheng Hu,Ting Peng,Anmin Liu,Wenrui Huang,Chenxu Liu,Ziyue Hua,Tao Xie*

Main category: cs.CL

TL;DR: 本文揭示了稀疏注意力在大语言模型推理中可能引发的反直觉问题——信息损失导致序列变长，即'少即是少'(Less is Less)现象。为缓解此问题，提出一种早期停止算法，能在信息损失超过信息增益时及时终止生成，显著降低令牌消耗（最高达90%），同时保持精度下降小于2%。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法虽旨在提升推理效率，但实际可能导致更长的生成序列，从而增加整体延迟和内存开销，因此需要重新审视其有效性并提出改进方案。

Method: 提出一种基于信息增益与损失平衡的早期停止算法，在稀疏解码过程中动态检测最优终止点，避免无效生成。

Result: 在多个推理密集型基准测试中，该方法可减少高达90%的令牌消耗，且准确率下降不足2%，有效缓解了'少即是少'问题。

Conclusion: 稀疏注意力并非总是高效，其潜在的信息损失会加剧端到端复杂度；通过引入早期停止机制，可在保证性能的前提下大幅优化推理效率。

Abstract: Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.

</details>


### [96] [Temporal Graph Network: Hallucination Detection in Multi-Turn Conversation](https://arxiv.org/abs/2601.03051)
*Vidhi Rathore,Sambu Aneesh,Himanshu Singh*

Main category: cs.CL

TL;DR: 本文提出一种基于图的对话级幻觉检测方法，将对话建模为时间图，利用实体共享和时间连接构建边，通过消息传递更新节点嵌入，并使用注意力池化生成上下文感知向量进行幻觉分类。实验表明该方法性能略优于现有方法，且注意力机制可解释决策过程。


<details>
  <summary>Details</summary>
Motivation: 多轮对话中上下文变化可能导致矛盾和幻觉，现有方法在检测对话级幻觉方面存在不足，需要更有效的检测机制。

Method: 将对话表示为时间图，每个对话回合作为节点，使用句子变换器编码；通过共享实体边和时间边连接节点；采用消息传递更新节点嵌入；利用注意力池化融合上下文信息并输入分类器判断幻觉。

Result: 所提方法在幻觉检测任务上表现略优于现有方法，且注意力机制能够提供决策依据的可解释性。

Conclusion: 基于图的方法有效捕捉了对话中的上下文依赖关系，提升了幻觉检测能力，并通过注意力机制增强了模型的可解释性。

Abstract: Hallucinations can be produced by conversational AI systems, particularly in multi-turn conversations where context changes and contradictions may eventually surface. By representing the entire conversation as a temporal graph, we present a novel graph-based method for detecting dialogue-level hallucinations. Our framework models each dialogue as a node, encoding it using a sentence transformer. We explore two different ways of connectivity: i) shared-entity edges, which connect turns that refer to the same entities; ii) temporal edges, which connect contiguous turns in the conversation. Message-passing is used to update the node embeddings, allowing flow of information between related nodes. The context-aware node embeddings are then combined using attention pooling into a single vector, which is then passed on to a classifier to determine the presence and type of hallucinations. We demonstrate that our method offers slightly improved performance over existing methods. Further, we show the attention mechanism can be used to justify the decision making process. The code and model weights are made available at: https://github.com/sambuaneesh/anlp-project.

</details>


### [97] [Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph](https://arxiv.org/abs/2601.03052)
*Jianpeng Hu,Yanzeng Li,Jialun Zhong,Wenfa Qi,Lei Zou*

Main category: cs.CL

TL;DR: 本文提出一种基于语义级内部推理图的忠实性幻觉检测方法，通过将层间相关性传播算法从词元级别扩展到语义级别，构建基于归因向量的内部推理图，以更真实地表示依赖关系。同时设计了一个基于小型预训练语言模型的通用框架，利用LLM推理中的依赖关系进行训练和幻觉检测，并可通过阈值动态调整正确样本的通过率。实验表明，该方法在RAGTruth和Dolly-15k数据集上优于现有最优基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测忠实性幻觉时，要么忽略模型内部推理过程，要么粗略处理这些特征，导致判别器难以有效学习。因此需要一种能更精细捕捉内部推理过程的方法来提升检测性能。

Method: 将层间相关性传播算法从词元级别扩展至语义级别，构建基于归因向量的内部推理图；设计一个基于小型预训练语言模型的通用框架，利用推理依赖关系进行训练与幻觉检测，并支持动态调整正确样本通过率的阈值机制。

Result: 在RAGTruth和Dolly-15k两个数据集上，所提方法在整体性能上均优于当前最先进的基线方法。

Conclusion: 该方法通过语义级内部推理图更准确地表征模型推理依赖关系，显著提升了忠实性幻觉检测的效果，具有良好的通用性和有效性。

Abstract: The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.

</details>


### [98] [Do LLMs Encode Functional Importance of Reasoning Tokens?](https://arxiv.org/abs/2601.03066)
*Janvijay Singh,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 本文提出一种名为贪婪剪枝（greedy pruning）的方法，通过最小化模型似然性的下降来迭代删除推理过程中功能性较低的标记，从而生成长度可控的推理链。该方法在压缩推理链的同时保持了高准确性，并在知识蒸馏框架中表现优于基于前沿模型监督的压缩基线。分析还发现注意力分数可有效预测剪枝优先级，表明模型内部对推理标记的功能重要性存在结构化编码。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩长推理链时依赖概率采样、启发式规则或前沿模型监督，但缺乏对模型内部是否编码了标记级别功能重要性的理解。本文旨在诊断并揭示模型在推理过程中对不同标记的功能重要性感知。

Method: 提出贪婪剪枝方法，通过迭代移除对模型似然性影响最小的推理标记，实现长度控制且保留关键推理信息的压缩。结合知识蒸馏评估其性能，并分析注意力分数与剪枝顺序的关系。

Result: 在相同推理长度下，学生模型在剪枝后的推理链上训练的表现优于前沿模型监督的压缩基线；注意力分数能够有效预测剪枝优先级，表明模型内部存在非平凡的功能重要性结构。

Conclusion: 贪婪剪枝是一种有效且可诊断的推理链压缩方法，不仅能提升效率，还能揭示模型内部对推理标记功能重要性的编码机制，为理解大语言模型的推理过程提供新视角。

Abstract: Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.

</details>


### [99] [Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models](https://arxiv.org/abs/2601.03079)
*Bocheng Chen,Han Zi,Xi Chen,Xitong Zhang,Kristen Johnson,Guangliang Liu*

Main category: cs.CL

TL;DR: 本文提出两种实用的推理方法，以增强大语言模型（LLMs）的道德敏感性，使其能够识别道德上良性和危险的输入并纠正道德错误。这些方法基于统一的语用推理视角，不依赖于复杂的表面形式建模，而是聚焦于推理负荷，实证表明其在多个道德相关基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在对齐人类道德价值观方面面临挑战，尤其是如何提升其道德敏感性，因此需要新的方法来使模型更敏锐地感知和响应道德情境。

Method: 提出两种基于语用推理的实用方法，通过分析输入的推理负荷，判断其道德性质并进行修正，从而提升模型的道德敏感性。

Result: 实验结果表明，所提方法显著增强了大语言模型的道德敏感性，在多个代表性道德相关基准测试中取得了良好表现。

Conclusion: 该研究为提升大语言模型的道德敏感性提供了有效且可扩展的解决方案，强调了语用推理在道德认知中的核心作用。

Abstract: Moral sensitivity is fundamental to human moral competence, as it guides individuals in regulating everyday behavior. Although many approaches seek to align large language models (LLMs) with human moral values, how to enable them morally sensitive has been extremely challenging. In this paper, we take a step toward answering the question: how can we enhance moral sensitivity in LLMs? Specifically, we propose two pragmatic inference methods that faciliate LLMs to diagnose morally benign and hazardous input and correct moral errors, whereby enhancing LLMs' moral sensitivity. A central strength of our pragmatic inference methods is their unified perspective: instead of modeling moral discourses across semantically diverse and complex surface forms, they offer a principled perspective for designing pragmatic inference procedures grounded in their inferential loads. Empirical evidence demonstrates that our pragmatic methods can enhance moral sensitivity in LLMs and achieves strong performance on representative morality-relevant benchmarks.

</details>


### [100] [Grad-ELLM: Gradient-based Explanations for Decoder-only LLMs](https://arxiv.org/abs/2601.03089)
*Xin Huang,Antoni B. Chan*

Main category: cs.CL

TL;DR: 提出Grad-ELLM，一种基于梯度的归因方法，用于解码器仅的Transformer架构大语言模型，通过聚合注意力层梯度的通道重要性和注意力图的空间重要性，在每一步生成时生成热图，无需修改模型结构。引入新的公平比较指标π-Soft-NC和π-Soft-NS，提升归因方法评估的公正性。在情感分类、问答和开放生成任务中，实验表明Grad-ELLM在一致性上优于其他归因方法。


<details>
  <summary>Details</summary>
Motivation: 现有输入归因方法多为模型无关，未针对Transformer架构优化，导致归因结果的忠实度有限。需要一种更符合大语言模型内部机制的归因方法以提高透明性和可信度。

Method: Grad-ELLM结合注意力层梯度的通道重要性与注意力图的空间重要性，生成每步生成的热图；引入改进的评估指标π-Soft-NC和π-Soft-NS，控制扰动时保留信息量以实现更公平的对比。

Result: 在多种任务（情感分类、问答、开放生成）和不同模型上，Grad-ELLM均表现出比现有方法更高的忠实度，验证了其有效性与普适性。

Conclusion: Grad-ELLM是一种高效且忠实的归因方法，适用于解码器仅的Transformer架构大语言模型，显著提升了对模型决策过程的可解释性，同时提出的评估指标增强了归因方法比较的公平性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input attribution methods aim to highlight each input token's contributions to the model's output, but existing approaches are typically model-agnostic, and do not focus on transformer-specific architectures, leading to limited faithfulness. To address this, we propose Grad-ELLM, a gradient-based attribution method for decoder-only transformer-based LLMs. By aggregating channel importance from gradients of the output logit with respect to attention layers and spatial importance from attention maps, Grad-ELLM generates heatmaps at each generation step without requiring architectural modifications. Additionally, we introduce two faithfulneses metrics $π$-Soft-NC and $π$-Soft-NS, which are modifications of Soft-NC/NS that provide fairer comparisons by controlling the amount of information kept when perturbing the text. We evaluate Grad-ELLM on sentiment classification, question answering, and open-generation tasks using different models. Experiment results show that Grad-ELLM consistently achieves superior faithfulness than other attribution methods.

</details>


### [101] [Who Laughs with Whom? Disentangling Influential Factors in Humor Preferences across User Clusters and LLMs](https://arxiv.org/abs/2601.03103)
*Soichiro Murakami,Hidetaka Kamigaito,Hiroya Takamura,Manabu Okumura*

Main category: cs.CL

TL;DR: 本研究通过聚类用户投票日志并使用Bradley-Terry-Luce模型估计可解释偏好因素的集群特异性权重，建模了日本创意回应游戏Oogiri中幽默偏好的异质性。通过提示大语言模型（LLM）选择更有趣的回应，发现不同用户集群表现出独特的偏好模式，且LLM结果可模拟特定集群的表现。最后，研究展示了通过角色提示可引导LLM偏好向特定集群对齐。数据收集与分析脚本将公开，以支持可复现性。


<details>
  <summary>Details</summary>
Motivation: 幽默偏好在个体和文化间存在显著差异，这使得利用大语言模型（LLM）评估幽默变得复杂。需要一种方法来捕捉和建模这种多样性，以便更准确地理解与预测幽默接受度。

Method: 采用聚类分析处理用户投票日志，识别具有相似幽默偏好的用户群；使用Bradley-Terry-Luce模型估计各集群在可解释偏好因素上的权重；通过提示LLM进行幽默判断，并比较其选择模式与用户集群的匹配程度；最后通过角色提示实验验证对LLM偏好的定向调控。

Result: 用户被有效聚类，展现出不同的幽默偏好模式；LLM的判断结果与特定用户集群高度相似；通过角色提示可成功引导LLM偏好朝向指定集群，表明其偏好具有可塑性与可控性。

Conclusion: 该研究揭示了幽默偏好的群体异质性，并证明大语言模型可通过适当提示模仿特定人群的幽默偏好。这一方法为构建更具适应性和可解释性的幽默评估系统提供了新路径，同时强调了提示工程在个性化内容生成中的关键作用。

Abstract: Humor preferences vary widely across individuals and cultures, complicating the evaluation of humor using large language models (LLMs). In this study, we model heterogeneity in humor preferences in Oogiri, a Japanese creative response game, by clustering users with voting logs and estimating cluster-specific weights over interpretable preference factors using Bradley-Terry-Luce models. We elicit preference judgments from LLMs by prompting them to select the funnier response and found that user clusters exhibit distinct preference patterns and that the LLM results can resemble those of particular clusters. Finally, we demonstrate that, by persona prompting, LLM preferences can be directed toward a specific cluster. The scripts for data collection and analysis will be released to support reproducibility.

</details>


### [102] [Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models](https://arxiv.org/abs/2601.03115)
*Xiutian Zhao,Björn Schuller,Berrak Sisman*

Main category: cs.CL

TL;DR: 本文首次对大型音频-语言模型（LALMs）中的情感敏感神经元（ESNs）进行了神经元级别的可解释性研究，通过多种选择方法在Qwen2.5-Omni、Kimi-Audio和Audio Flamingo 3三个开源模型中验证了情感特异性神经元的存在。研究发现，通过干预这些神经元可显著影响特定情感的识别性能，且效果具有系统性和可控性。此外，这些神经元在不同层间呈现非均匀聚集，并表现出部分跨数据集迁移能力。该工作为理解LALMs的情感决策机制提供了因果证据，并展示了通过靶向干预实现可控情感行为的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前对大型音频-语言模型如何内部编码情感仍缺乏机制性理解，尤其缺乏对神经元层面情感表示的实证研究。本文旨在填补这一空白，揭示情感敏感神经元的存在及其功能作用。

Method: 采用频率、熵、幅度和对比度等四种神经元选择策略，在多个情感识别基准上筛选情感敏感神经元；通过推理时干预（如消融或增益放大），评估其对情感识别性能的影响；分析神经元在不同层间的分布模式及跨数据集迁移能力。

Result: 在三个主流开源模型中均发现了情感特异性的神经元响应模式：消融特定情感的神经元会显著降低对应情感的识别准确率，而对其他情感影响较小；增益放大则能引导模型预测偏向目标情感。干预效果在少量标注数据下即可显现，且随干预强度呈系统性变化。此外，情感敏感神经元呈现非均匀层内聚集，并具备部分跨数据集泛化能力。

Conclusion: 本研究首次从因果角度揭示了大型音频-语言模型中存在情感敏感神经元，证明其在情感决策中的关键作用。研究结果不仅深化了对模型内部情感表征的理解，也为实现可控、可调节的情感生成与交互提供了有效手段。

Abstract: Emotion is a central dimension of spoken communication, yet, we still lack a mechanistic account of how modern large audio-language models (LALMs) encode it internally. We present the first neuron-level interpretability study of emotion-sensitive neurons (ESNs) in LALMs and provide causal evidence that such units exist in Qwen2.5-Omni, Kimi-Audio, and Audio Flamingo 3. Across these three widely used open-source models, we compare frequency-, entropy-, magnitude-, and contrast-based neuron selectors on multiple emotion recognition benchmarks. Using inference-time interventions, we reveal a consistent emotion-specific signature: ablating neurons selected for a given emotion disproportionately degrades recognition of that emotion while largely preserving other classes, whereas gain-based amplification steers predictions toward the target emotion. These effects arise with modest identification data and scale systematically with intervention strength. We further observe that ESNs exhibit non-uniform layer-wise clustering with partial cross-dataset transfer. Taken together, our results offer a causal, neuron-level account of emotion decisions in LALMs and highlight targeted neuron interventions as an actionable handle for controllable affective behaviors.

</details>


### [103] [ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation](https://arxiv.org/abs/2601.03121)
*Peiran Li,Jan Fillies,Adrian Paschke*

Main category: cs.CL

TL;DR: ToxiGAN 是一种类感知的文本增强框架，通过结合对抗生成与大语言模型（LLM）的语义引导，实现可控且类别特定的毒性语言数据增强。它采用两阶段定向训练策略，并利用 LLM 生成的中性文本作为语义平衡物，有效缓解传统 GAN 增强中的模式崩溃和语义漂移问题。通过显式优化毒性样本与中性样本之间的差异，强化类别特异性对比信号。在四个仇恨言论基准上的实验表明，ToxiGAN 在宏平均 F1 和仇恨类 F1 上均达到最优性能，显著优于传统及基于 LLM 的增强方法。消融与敏感性分析进一步验证了语义平衡物和定向训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有毒性语言数据增强方法面临监督有限和分布偏斜的挑战，难以实现可控且类别特定的增强。同时，基于 GAN 的方法常出现模式崩溃和语义漂移问题，限制了其在实际分类任务中的应用效果。因此，亟需一种能有效提升分类器鲁棒性的新型增强框架。

Method: 提出 ToxiGAN 框架，采用两阶段定向训练策略，动态选择 LLM 生成的中性样本作为语义球体，指导毒性样本生成过程；通过强制毒性样本与中性样本在语义空间中保持距离，增强类别区分能力。

Result: 在四个仇恨言论基准上，ToxiGAN 在宏平均 F1 和仇恨类 F1 上均取得最佳表现，显著优于传统方法和基于 LLM 的增强方法。消融实验证明语义球体和定向训练对提升分类器鲁棒性具有关键作用。

Conclusion: ToxiGAN 通过引入语义球体和定向训练机制，实现了高效、可控且类别敏感的文本增强，显著提升了毒性分类模型的性能与鲁棒性，为未来对抗性数据增强提供了新思路。

Abstract: Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.

</details>


### [104] [Improving Indigenous Language Machine Translation with Synthetic Data and Language-Specific Preprocessing](https://arxiv.org/abs/2601.03135)
*Aashish Dhawan,Christopher Driggers-Ellis,Christan Grant,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 本文研究了在低资源土著语言中通过合成数据增强来提升神经机器翻译性能的方法。使用高容量多语言模型生成合成句对，对瓜拉尼-西班牙语和克丘亚-西班牙语翻译任务进行实验，结果显示合成数据显著提升了chrF++指标；而对艾马拉语的诊断实验表明通用预处理方法在高度黏着语言上存在局限性。


<details>
  <summary>Details</summary>
Motivation: 低资源土著语言缺乏平行语料库，限制了神经机器翻译的发展。合成数据生成为缓解数据稀缺问题提供了一种实用策略。

Method: 使用高容量多语言mBART模型生成合成句子对，并在仅使用精选数据和合成增强数据上微调模型。采用语言特定的预处理技术，如正字法归一化和噪声感知过滤，以减少语料库中的伪影。

Result: 在瓜拉尼-西班牙语和克丘亚-西班牙语任务上，合成数据增强带来了稳定的chrF++提升；但对艾马拉语的诊断实验显示，通用预处理方法在高度黏着语言上效果有限。

Conclusion: 合成数据增强能有效提升低资源土著语言的机器翻译性能，但需结合语言特定的预处理方法以充分发挥效果，尤其在高度黏着语言中。

Abstract: Low-resource indigenous languages often lack the parallel corpora required for effective neural machine translation (NMT). Synthetic data generation offers a practical strategy for mitigating this limitation in data-scarce settings. In this work, we augment curated parallel datasets for indigenous languages of the Americas with synthetic sentence pairs generated using a high-capacity multilingual translation model. We fine-tune a multilingual mBART model on curated-only and synthetically augmented data and evaluate translation quality using chrF++, the primary metric used in recent AmericasNLP shared tasks for agglutinative languages.
  We further apply language-specific preprocessing, including orthographic normalization and noise-aware filtering, to reduce corpus artifacts. Experiments on Guarani--Spanish and Quechua--Spanish translation show consistent chrF++ improvements from synthetic data augmentation, while diagnostic experiments on Aymara highlight the limitations of generic preprocessing for highly agglutinative languages.

</details>


### [105] [Limited Linguistic Diversity in Embodied AI Datasets](https://arxiv.org/abs/2601.03136)
*Selma Wanna,Agnes Luhtaru,Jonathan Salfity,Ryan Barron,Juston Moore,Cynthia Matuszek,Mitch Pryor*

Main category: cs.CL

TL;DR: 本文对多个广泛使用的视觉-语言-动作（VLA）数据集进行了系统性审计，分析其指令语言的特征，发现多数数据集依赖重复性高、模板化的命令，语言结构变化有限，导致指令形式分布狭窄。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型训练与评估所用数据集的语言特性缺乏系统描述，影响模型性能评估与数据集选择的科学性，亟需对语言信号进行量化分析以提升数据透明度与质量。

Method: 从词汇多样性、重复与重叠、语义相似性、句法复杂度等维度，对多个VLA数据集的指令语言进行量化分析，采用自然语言处理技术进行文本特征提取与统计建模。

Result: 分析表明，多数数据集包含高度重复、模板化的指令，语言变体少，语义和句法多样性不足，存在显著的语言覆盖局限。

Conclusion: 本研究为当前VLA数据集的语言特性提供了描述性基准，有助于推动更透明的数据报告、更合理的数据集选择及有针对性的语言数据扩充策略。

Abstract: Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.

</details>


### [106] [Self-Verification is All You Need To Pass The Japanese Bar Examination](https://arxiv.org/abs/2601.03144)
*Andrew Shin*

Main category: cs.CL

TL;DR: 本文提出一种基于新构建数据集的自验证模型，首次在不改变原始题目结构和评分规则的前提下，使大语言模型通过日本律师资格考试。该模型在真实考试格式与评分体系下表现超越官方及格线，优于多智能体推理和分解式监督等方法，强调了格式忠实监督与一致性验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在专业性极强且结构严谨的考试中仍难以取得可靠成绩，尤其是日本律师资格考试要求复杂的法律推理与严格的答案格式。现有方法虽通过分解问题提升性能，但未在原始考试格式下系统评估，无法证明其真正具备考试级能力。因此亟需一种能忠实还原考试形式并有效验证答案一致性的方法。

Method: 构建一个忠实复现日本律师资格考试真实格式与评分标准的新数据集，训练一个自验证模型，使其在不修改原题结构或评分规则的情况下完成答题，并通过自我验证机制确保答案的一致性与准确性。同时对比多智能体推理、分解式监督等多种策略，验证所提方法的有效性。

Result: 所提出的自验证模型在真实考试尺度下表现超过官方及格线，成为首个在不改变原题与评分规则前提下通过日本律师资格考试的大语言模型。其他方法如多智能体推理和分解式监督均未能达到同等水平，表明格式忠实监督与一致性验证对高阶专业推理任务至关重要。

Conclusion: 本研究证明，精心设计的单模型方法在高风险专业推理任务中可超越复杂系统；格式忠实的训练数据与自验证机制是实现真正考试级能力的关键因素。相关数据集与代码已公开，可供后续研究使用。

Abstract: Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.

</details>


### [107] [Decoupling the Effect of Chain-of-Thought Reasoning: A Human Label Variation Perspective](https://arxiv.org/abs/2601.03154)
*Beiduo Chen,Tiancheng Hu,Caiqi Zhang,Robert Litschko,Anna Korhonen,Barbara Plank*

Main category: cs.CL

TL;DR: 长链式思维（CoT）在单答案任务中表现优异，但在建模人类标签变异性（即概率模糊性）方面仍不充分。研究通过分布任务的系统性解耦实验，发现推理文本与模型先验可分离；尽管CoT提升分布对齐，但最终准确率主要由推理内容决定（贡献99%），而分布排序则由模型先验主导（超过80%）。逐步分析显示，准确性随推理过程单调增强，而分布结构主要由模型内在先验决定。结论表明，长CoT作为最优选项决策者有效，却无法精细校准模糊任务中的分布。


<details>
  <summary>Details</summary>
Motivation: 探索长链式思维（CoT）在处理具有概率模糊性的任务时，是否能有效建模人类标签变异性，以揭示其在不确定情境下的表达能力局限。

Method: 采用跨链式思维（Cross-CoT）实验设计，系统性解耦推理文本与模型内在先验的影响，在分布基础任务上进行消融分析，结合步骤级行为追踪，量化推理内容与模型先验对准确率和分布排序的贡献。

Result: CoT显著提升分布对齐效果，但最终准确率高度依赖于推理内容（贡献99%），而分布排序主要由模型先验决定（贡献超80%）；推理过程中准确性持续提升，而分布结构由模型内在先验主导。

Conclusion: 长链式思维在确定最优答案时表现出色，但在精细校准模糊任务中的概率分布方面能力有限，无法替代模型内在先验的作用，提示需重新设计推理机制以支持不确定性建模。

Abstract: Reasoning-tuned LLMs utilizing long Chain-of-Thought (CoT) excel at single-answer tasks, yet their ability to model Human Label Variation--which requires capturing probabilistic ambiguity rather than resolving it--remains underexplored. We investigate this through systematic disentanglement experiments on distribution-based tasks, employing Cross-CoT experiments to isolate the effect of reasoning text from intrinsic model priors. We observe a distinct "decoupled mechanism": while CoT improves distributional alignment, final accuracy is dictated by CoT content (99% variance contribution), whereas distributional ranking is governed by model priors (over 80%). Step-wise analysis further shows that while CoT's influence on accuracy grows monotonically during the reasoning process, distributional structure is largely determined by LLM's intrinsic priors. These findings suggest that long CoT serves as a decisive LLM decision-maker for the top option but fails to function as a granular distribution calibrator for ambiguous tasks.

</details>


### [108] [Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages](https://arxiv.org/abs/2601.03168)
*Tewodros Kederalah Idris,Prasenjit Mitra,Roald Eiselen*

Main category: cs.CL

TL;DR: 该研究系统评估了五种嵌入相似性度量在816次跨语言迁移实验中的表现，涵盖三个NLP任务、三种非洲中心的多语言模型和12种来自四个语系的语言。结果表明，余弦差距和基于检索的度量（如P@1、CSLS）能可靠预测迁移成功（ρ=0.4-0.6），而CKA则几乎无预测能力（ρ≈0.1）。值得注意的是，当跨模型汇总时相关性符号出现反转（辛普森悖论），因此从业者必须针对每种模型进行验证。嵌入度量的预测能力与URIEL语言类型学相当。研究为源语言选择提供了具体指导，并强调了模型特异性分析的重要性。


<details>
  <summary>Details</summary>
Motivation: 跨语言迁移对构建低资源非洲语言的自然语言处理系统至关重要，但目前缺乏可靠的源语言选择方法。

Method: 系统评估五种嵌入相似性度量在816次跨语言迁移实验中的表现，涵盖多种任务、模型和语言。

Result: 余弦差距和基于检索的度量（如P@1、CSLS）具有较高的预测能力（ρ=0.4-0.6），而CKA预测能力极低（ρ≈0.1）。跨模型汇总时存在辛普森悖论，需进行模型特定验证。嵌入度量预测性能与语言类型学相当。

Conclusion: 研究为源语言选择提供了实证指导，强调必须进行模型特定的分析，以避免误导性结论。

Abstract: Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap and retrieval-based metrics (P@1, CSLS) reliably predict transfer success ($ρ= 0.4-0.6$), while CKA shows negligible predictive power ($ρ\approx 0.1$). Critically, correlation signs reverse when pooling across models (Simpson's Paradox), so practitioners must validate per-model. Embedding metrics achieve comparable predictive power to URIEL linguistic typology. Our results provide concrete guidance for source language selection and highlight the importance of model-specific analysis.

</details>


### [109] [Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning](https://arxiv.org/abs/2601.03190)
*Naixin Zhai,Pengyang Shao,Binbin Zheng,Fei Shen,Long Bai,Xun Yang*

Main category: cs.CL

TL;DR: PALU是一种基于局部熵最大化的机器遗忘框架，通过仅对敏感前缀和top-k logits进行优化，实现高效遗忘并最小化对模型通用性能的损害。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有标记一视同仁，导致不必要的性能下降和对无关区域的过度优化。

Method: 提出基于时间与词汇维度局部熵最大化的框架，仅针对敏感前缀和关键子空间进行优化。

Result: 实验表明PALU在遗忘效果和模型实用性方面均优于现有最先进方法。

Conclusion: PALU通过精细化的局部优化策略，实现了高效且低损伤的模型遗忘。

Abstract: Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines.

</details>


### [110] [MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory](https://arxiv.org/abs/2601.03192)
*Shengtao Zhang,Jiaqian Wang,Ruiwen Zhou,Junwei Liao,Yuchen Feng,Weinan Zhang,Ying Wen,Zhiyu Li,Feiyu Xiong,Yutao Qi,Bo Tang,Muning Wen*

Main category: cs.CL

TL;DR: MemRL is a framework that enables agents to self-evolve through non-parametric reinforcement learning on episodic memory, addressing the limitations of fine-tuning and passive memory retrieval. It uses a Two-Phase Retrieval mechanism combining semantic relevance and learned Q-values for strategy selection, allowing continuous improvement without weight updates.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with self-evolution due to high computational cost of fine-tuning and issues with noise in passive memory retrieval. There's a need for a method that balances stability and plasticity for continuous skill acquisition.

Method: MemRL separates a frozen LLM from a plastic episodic memory system. It employs a Two-Phase Retrieval mechanism: first filtering by semantic relevance, then selecting strategies based on learned Q-values refined via environmental feedback.

Result: MemRL outperforms state-of-the-art baselines on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench. Analysis confirms it effectively resolves the stability-plasticity dilemma with continuous runtime improvement.

Conclusion: MemRL enables agents to continuously improve their skills during runtime without updating model weights, offering a robust solution for lifelong learning in LLM-based agents.

Abstract: The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.

</details>


### [111] [X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework](https://arxiv.org/abs/2601.03194)
*Mohammad Zia Ur Rehman,Sai Kartheek Reddy Kasu,Shashivardhan Reddy Koppula,Sai Rithwik Reddy Chirra,Shwetank Shekhar Singh,Nagendra Kumar*

Main category: cs.CL

TL;DR: X-MuTeST 是一种用于多语言仇恨言论检测的可解释性引导训练框架，结合大语言模型（LLM）的高层次语义推理与传统注意力增强技术，特别针对印度语系中的印地语和泰卢固语等资源匮乏语言。该研究构建了包含6,004条印地语、4,492条泰卢固语和6,334条英语样本的带词级理由标注的数据集，并通过计算原始文本与一元组、二元组、三元组之间的预测概率差异来生成可解释性。最终解释为LLM解释与X-MuTeST解释的并集。实验表明，利用人类理由进行训练不仅提升了分类性能，还增强了模型可解释性，且结合人类理由优化注意力机制进一步提升效果。评估采用Token-F1、IOU-F1、Comprehensiveness和Sufficiency等指标。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法在准确性和可解释性方面存在不足，尤其在印地语、泰卢固语等资源匮乏的印欧语言中表现较差。缺乏高质量的人工标注理由数据限制了模型对语义上下文的理解能力。因此，亟需一种能融合高层语义推理与细粒度注意力机制的可解释性框架，并建立跨语言基准数据集以推动该领域发展。

Method: 提出X-MuTeST框架，结合大语言模型（LLM）的语义理解能力与基于滑动窗口的局部上下文分析（一元组至三元组），通过对比原始文本与子序列的预测概率差异生成可解释性线索。引入人类标注的词级理由作为监督信号，指导模型注意力聚焦关键语义单元。最终解释由LLM输出与X-MuTeST生成结果取并集得到。

Result: 在印地语、泰卢固语和英语上均取得优于基线模型的分类性能；在可解释性方面，所提方法在Token-F1、IOU-F1、Comprehensiveness和Sufficiency等指标上显著优于现有方法；利用人类理由训练后，模型不仅更准确，而且解释更具说服力与忠实性。

Conclusion: X-MuTeST框架有效提升了多语言仇恨言论检测在资源匮乏语言中的准确性与可解释性，验证了人类理由在模型训练与解释生成中的关键作用。该工作为跨语言、多模态内容安全提供了新范式，数据与代码已开源，促进后续研究。

Abstract: Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST

</details>


### [112] [DIP: Dynamic In-Context Planner For Diffusion Language Models](https://arxiv.org/abs/2601.03199)
*Yang Li,Han Meng,Chenan Wang,Haipeng Chen*

Main category: cs.CL

TL;DR: 本文提出一种名为DIP的动态上下文规划方法，利用扩散语言模型（DLMs）在生成过程中可动态调整上下文的特性，通过在生成时动态选择和插入示例，而非预先提供所有示例，显著提升推理效率。实验表明，DIP在保持生成质量的同时，相比标准推理实现最高12.9倍的加速，相较于使用KV缓存增强的推理也提升了1.17倍。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）虽在自然语言任务中表现优异，但其双向注意力机制导致上下文长度增加时计算开销巨大。传统方法需在提示中预先提供全部示例，限制了效率。本文旨在解决这一瓶颈，提升推理速度。

Method: 提出DIP（Dynamic In-Context Planner），利用扩散生成范式支持动态调整上下文的特点，在生成过程中动态选择并插入最相关的示例，优化上下文使用效率。

Result: DIP在保持生成质量的前提下，实现了最高12.9倍的推理速度提升，相比KV缓存增强的推理也获得1.17倍加速，显著提升效率。

Conclusion: 该研究揭示了扩散语言模型在动态上下文调整方面的潜力，DIP方法有效缓解了长上下文带来的计算负担，为高效推理提供了新思路。

Abstract: Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \textbf{D}ynamic \textbf{I}n-Context \textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\times$ inference speedup over standard inference and 1.17$\times$ over KV cache-enhanced inference.

</details>


### [113] [UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward](https://arxiv.org/abs/2601.03205)
*Yile Liu,Yixian Liu,Zongwei Li,Yufei Huang,Xinhua Feng,Zhichao Hu,Jinglu Hu,Jianfeng Yan,Fengzong Lian,Yuhong Liu*

Main category: cs.CL

TL;DR: 提出UltraLogic框架，通过代码化求解方法解耦问题逻辑与自然语言表达，实现高质量数据自动化生成；引入双极浮点奖励（BFR）机制缓解奖励稀疏性与非负奖励陷阱问题；实验表明任务多样性是提升推理能力的关键，BFR结合难度匹配策略显著提高训练效率，引导模型趋向全局逻辑最优。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂多步逻辑推理、规划和验证方面仍存在瓶颈，且缺乏大规模、高质量、难度可调的数据集支持通用推理研究。

Method: 提出UltraLogic框架，采用基于代码的求解方法分离问题逻辑与自然语言表达，构建涵盖数百种任务类型的自动化校准管道，支持十级难度划分；引入双极浮点奖励（BFR）机制，通过分级惩罚区分完美答案与存在逻辑缺陷的回答。

Result: 任务多样性是推理能力提升的主要驱动力；BFR机制结合难度匹配策略显著提升训练效率，有效引导模型达到全局逻辑最优。

Conclusion: UltraLogic框架通过结构化数据生成与创新奖励机制，为通用复杂推理提供了有效的训练支持，显著推动大语言模型在多步逻辑推理方面的进展。

Abstract: While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.

</details>


### [114] [MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics](https://arxiv.org/abs/2601.03217)
*Xinghe Chen,Naiming Liu,Shashank Sonkar*

Main category: cs.CL

TL;DR: MalruleLib 是一个基于学习科学的框架，将数学学习中的常见误解转化为可执行的错误程序，生成学生在不同问题模板下遵循错误规则的逐步推理轨迹。该框架包含101个错误规则和498个参数化问题模板，可生成超过一百万种实例，支持大规模监督与受控评估。实验发现，语言模型在跨模板误解预测任务中准确率从66%下降至40%，而提供双路径（正确与错误）推理轨迹可提升预测准确率3-15%。研究提出 Malrule Reasoning Accuracy (MRA) 作为核心学生建模指标，并释放 MalruleLib 以推动教育人工智能在诊断和反馈中针对深层误解的发展。


<details>
  <summary>Details</summary>
Motivation: 学生在数学学习中的错误往往是系统性的，即持续使用一种一致但错误的方法。现有方法难以有效捕捉和建模这种结构性误解，尤其在跨问题情境下的表现。因此需要一种可扩展、可执行的框架来精准刻画学生的错误推理模式，从而支持更有效的诊断与个性化反馈。

Method: 构建 MalruleLib 框架，整合67项学习科学与数学教育研究，将已知误解形式化为可执行的错误规则；设计参数化问题模板，使每个规则可在多种变体中应用；生成正确解法与错误解法的双路径推理轨迹；通过九个大语言模型进行测试，评估其在直接求解与跨模板误解预测任务中的表现差异。

Result: 语言模型在直接问题求解中准确率为66%，但在跨模板误解预测中降至40%，表明当前模型对系统性错误的理解能力有限；引入双路径推理轨迹后，预测性能提升3-15%；框架可生成超百万级训练样本，具备高度可扩展性与可控性。

Conclusion: MalruleLib 成功实现了对学生系统性错误的可执行建模，为教育AI提供了强大的基础设施，支持对误解的精准诊断与针对性反馈，显著提升了对学生错误过程的建模能力。

Abstract: Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.

</details>


### [115] [Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models](https://arxiv.org/abs/2601.03232)
*Kartik Bose,Abhinandan Kumar,Raghuraman Soundararajan,Priya Mudgil,Samonee Ralmilay,Niharika Dutta,Manphool Singhal,Arun Kumar,Saugata Sen,Anurima Patra,Priya Ghosh,Abanti Das,Amit Gupta,Ashish Verma,Dipin Sudhakaran,Ekta Dhamija,Himangi Unde,Ishan Kumar,Krithika Rangarajan,Prerna Garg,Rachel Sequeira,Sudhin Shylendran,Taruna Yadav,Tej Pal,Pankaj Gupta*

Main category: cs.CL

TL;DR: 本研究构建了RXL-RADSet，一个由放射科医生验证的合成多RADS基准数据集，用于评估小语言模型（SLMs）与专有模型在RADS分类任务中的表现。结果显示，在引导提示下，GPT-5.2达到99.8%的有效性与81.1%的准确性；而大型SLMs（20-32B参数）在引导提示下可接近专有模型性能，有效性达~99%，准确性达中高70%，但复杂RADS框架仍存在差距。模型规模越大，性能越好，且引导提示显著优于零样本提示。


<details>
  <summary>Details</summary>
Motivation: 当前自动化从叙述性报告中分配RADS面临指南复杂、输出格式限制及跨RADS框架和模型规模缺乏基准测试等挑战，亟需一个高质量、多框架的验证基准以推动技术发展。

Method: 构建包含1600份合成放射学报告的RXL-RADSet，覆盖10种RADS标准（如BI-RADS、LI-RADS等）及多种成像模态；报告由LLM生成并经两阶段放射科医生验证；评估41个量化的小语言模型（0.135-32B参数）和GPT-5.2在固定引导提示下的表现，主要评估有效性和准确性，并对比引导与零样本提示的效果。

Result: 在引导提示下，GPT-5.2实现99.8%有效性与81.1%准确性；聚合所有SLMs（65,600次预测）取得96.8%有效性与61.1%准确性；20-32B参数的顶级SLMs可达~99%有效性与中高70%准确性。性能随模型规模上升（<1B与≥10B间出现拐点），复杂度高的RADS因分类难度导致性能下降。引导提示显著提升有效性和准确性（分别为99.2% vs 96.7%；78.5% vs 69.6%）。

Conclusion: RXL-RADSet为多RADS自动化分配提供了可靠的放射科医生验证基准；在引导提示下，大型小语言模型（20-32B）可接近专有模型性能，但在高复杂度RADS框架中仍存在差距，提示需进一步优化模型与提示策略。

Abstract: Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.

</details>


### [116] [Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation](https://arxiv.org/abs/2601.03254)
*Bastien Vanderplaetse,Xavier Siebert,Stéphane Dupont*

Main category: cs.CL

TL;DR: 本文提出一种自动化语义规则检测（ASRD）算法，用于从多智能体系统在Lewis博弈中生成的通信消息中提取模式，以提高对涌现语言的可解释性。该方法通过将提取的模式与输入数据属性关联，简化了后续分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注多智能体系统中涌现语言的可解释性，因此需要一种有效的方法来理解智能体间自发形成的通信策略。

Method: 提出ASRD算法，通过分析两个不同数据集上训练的智能体在Lewis博弈中交换的消息，自动提取相关语义模式，并将其与输入数据的具体属性进行关联。

Result: ASRD成功识别出与输入数据属性相关的通信模式，显著提升了对涌现语言的理解和分析效率。

Conclusion: ASRD为理解多智能体系统中的涌现通信提供了有力工具，有助于推动该领域向更可解释的方向发展。

Abstract: The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [Textual Explanations and Their Evaluations for Reinforcement Learning Policy](https://arxiv.org/abs/2601.02514)
*Ahmad Terra,Mohit Ahmed,Rafia Inam,Elena Fersman,Martin Törngren*

Main category: cs.AI

TL;DR: 本文提出了一种新型可解释强化学习（XRL）框架，用于生成文本解释并将其转化为透明规则，以提升解释质量与可评估性。通过结合专家知识和自动谓词生成器，利用大语言模型（LLM）与聚类技术生成条件，并转化为可验证规则，同时引入两种优化技术减少冲突信息。在三个开源环境及一个电信工业场景中进行了实验，验证了该框架在可复现性与实际应用中的有效性，显著优于现有方法如自主策略解释（Autonomous Policy Explanation），并实现了对文本解释的系统化、量化评估。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习（XRL）中的文本解释虽易理解，但其正确性难以保证，且缺乏充分的评估手段，限制了其在真实场景中的可信应用。因此亟需一种能够生成高质量、可验证且可量化的文本解释的框架。

Method: 提出一种XRL框架：首先使用大语言模型（LLM）与聚类技术生成频繁状态条件；然后通过自动谓词生成器提取状态语义信息；将文本解释转换为透明规则；利用专家知识进行引导；采用两种精炼技术优化解释质量并减少冲突；最后通过规则的保真度、性能和属性分析进行系统评估。

Result: 在三个开源环境和一个电信工业案例中验证了该框架的有效性。生成的透明规则在特定任务上表现良好，显著优于现有方法（如自主策略解释）。框架支持对文本解释的系统性与量化评估，提升了可解释性研究的严谨性与实用性。

Conclusion: 所提出的XRL框架有效解决了文本解释的质量与可验证性问题，实现了从自然语言解释到可执行规则的转化，并支持自动化与定量评估，为可解释强化学习的发展提供了新路径，尤其适用于高可靠性要求的实际应用场景。

Abstract: Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.

</details>


### [118] [SimpleMem: Efficient Lifelong Memory for LLM Agents](https://arxiv.org/abs/2601.02553)
*Jiaqi Liu,Yaofeng Su,Peng Xia,Siwei Han,Zeyu Zheng,Cihang Xie,Mingyu Ding,Huaxiu Yao*

Main category: cs.AI

TL;DR: SimpleMem 是一种基于语义无损压缩的高效记忆框架，通过三阶段流程（语义结构压缩、递归记忆整合、自适应查询感知检索）提升信息密度与令牌利用效率，在基准数据集上实现平均 F1 提升 26.4%，推理时令牌消耗降低最高达 30 倍，显著平衡性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长时交互中面临历史记录冗余或高计算成本问题，亟需一种高效且不丢失关键信息的记忆管理机制以支持复杂环境中的可靠交互。

Method: 提出三阶段记忆处理流程：(1) 语义结构压缩，通过熵感知过滤将非结构化交互提炼为多视角索引的记忆单元；(2) 递归记忆整合，异步合并相关单元生成高层抽象表示以减少冗余；(3) 自适应查询感知检索，根据查询复杂度动态调整检索范围，实现高效精准上下文构建。

Result: 在多个基准数据集上的实验表明，SimpleMem 在准确率、检索效率和推理成本方面均优于基线方法，平均 F1 提升 26.4%，推理时令牌消耗最多降低 30 倍。

Conclusion: SimpleMem 有效解决了长期交互中记忆管理的冗余与高成本问题，实现了高性能与低开销之间的良好平衡，具备实际部署潜力。

Abstract: To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.

</details>


### [119] [An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices](https://arxiv.org/abs/2601.02641)
*Jeiyoon Park,Daehwan Lee,Changmin Yeo,Yongshin Han,Minseop Kim*

Main category: cs.AI

TL;DR: 本文通过大量实验研究了在真实场景中部署设备端AI模型所面临的两个关键问题：模型选择与资源消耗、以及设备端模型进行领域自适应的能力。作者构建了包含1000对韩英平行语句的LiveChatBench基准，基于五款移动设备的实验表明，在考虑设备资源限制的前提下，所提出方法在特定任务上性能可媲美GPT-5.1等商用模型，为设备端AI落地提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 当前对设备端AI模型的研究多关注效率，但缺乏对其在真实部署中涉及的CPU利用率和热管理等实际问题的关注。亟需系统性评估模型选择与资源消耗、以及领域适应能力，以推动其在真实服务中的应用。

Method: 构建LiveChatBench基准数据集，涵盖1000对韩英平行语句；在五种不同移动设备上进行广泛实验，评估多种设备端模型的性能与资源消耗，并分析其在特定任务（实时直播聊天翻译）下的领域适应潜力。

Result: 实验表明，在受限设备环境下，合理选择模型仍可实现接近商用模型（如GPT-5.1）的性能表现，验证了所提方法的有效性与实用性。

Conclusion: 本研究揭示了设备端AI模型在真实部署中需综合考虑资源约束与任务适配性，提出的评估框架与实践策略为未来设备端AI系统的优化与落地提供了重要启示。

Abstract: Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.

</details>


### [120] [Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization](https://arxiv.org/abs/2601.02683)
*Dongyu Chen,Jian Ma,Xianpeng Zhang,Lei Zhang,Haonan Lu,Chen Chen,Chuangchuang Wang,Kai Tang*

Main category: cs.AI

TL;DR: 本文提出一种名为层次化归因提示优化（HAPO）的框架，旨在解决当前提示工程中提示漂移和可解释性差的问题。该框架通过动态归因机制、语义单元优化以及支持多模态的渐进式流程，实现高效且可解释的提示优化，显著提升在图像问答和复杂任务分析等场景下的性能，优于现有自动化方法，具备良好的扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前提示优化方法存在提示漂移问题，即新提示修复了旧问题但损害了原有任务表现；同时从零生成提示会降低可解释性。因此需要一种更系统、可解释且高效的提示优化方法。

Method: 提出HAPO框架，包含三项创新：(1) 动态归因机制，用于识别训练数据和提示历史中的错误模式；(2) 语义单元优化，对提示的功能片段进行精准编辑；(3) 多模态友好渐进流程，兼容端到端大模型与大模型-多模态大模型协同工作流。

Result: HAPO在单图/多图问答（如OCRV2）和复杂任务分析（如BBH）等任务中表现出更高的优化效率，优于现有自动化提示优化方法，且过程可解释，具有良好的可扩展性。

Conclusion: HAPO为大规模提示工程提供了一个高效、可解释且可扩展的优化范式，推动了提示工程向系统化和自动化方向发展。

Abstract: Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.

</details>


### [121] [HAL: Inducing Human-likeness in LLMs with Alignment](https://arxiv.org/abs/2601.02813)
*Masum Hasan,Junjie Zhao,Ehsan Hoque*

Main category: cs.AI

TL;DR: HAL 是一种用于对齐语言模型与对话人类相似性的框架，通过可解释的、数据驱动的奖励信号实现。该框架从对比对话数据中提取显式的对话特征，将其组合成一个紧凑的标量得分，并用作标准偏好优化方法中的透明奖励信号。在大规模人类评估中，经过 HAL 对齐的模型被更频繁地认为在对话中更具人类特质。由于 HAL 依赖于明确且可解释的特征，它能够检查对齐行为并诊断意外影响。该方法展示了如何将语言中原本难以量化的软性、定性属性转化为可测量、可对齐且可解释的形式。


<details>
  <summary>Details</summary>
Motivation: 当前对话中的人类相似性难以定义、衡量和优化，导致提升人类行为主要依赖规模或广泛监督训练，而非有针对性的对齐。因此，需要一种可解释且数据驱动的方法来精准对齐语言模型与人类对话行为。

Method: HAL 框架通过分析对比对话数据，提取显式对话特征，构建一个综合的标量奖励分数，并利用该分数作为偏好优化中的透明奖励信号，实现模型对齐。

Result: 经 HAL 对齐的模型在大规模人类评估中被更多人认为具有人类对话特征，且该方法保持了模型整体性能。同时，其可解释性支持对对齐过程进行诊断和分析。

Conclusion: HAL 成功证明了软性、定性语言特性可以被量化并以可解释方式对齐，为未来对复杂人类行为属性的建模与优化提供了新路径。

Abstract: Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.

</details>


### [122] [Causal-Enhanced AI Agents for Medical Research Screening](https://arxiv.org/abs/2601.02814)
*Duc Ngo,Arya Rahgoza*

Main category: cs.AI

TL;DR: CausalAgent is a causal graph-enhanced retrieval-augmented generation system designed to improve accuracy and eliminate hallucinations in medical systematic reviews. It uses explicit causal reasoning and dual-level knowledge graphs to ensure every claim traces back to evidence, achieving 95% accuracy, 100% retrieval success, and zero hallucinations on dementia exercise research.


<details>
  <summary>Details</summary>
Motivation: Current AI systems for systematic reviews suffer from high hallucination rates (2–15%), which are unacceptable in healthcare contexts where errors can impact patient care. There is a need for more reliable, interpretable, and evidence-first AI tools in evidence-based medicine.

Method: CausalAgent integrates retrieval-augmented generation with causal graph modeling. It enforces an evidence-first protocol by linking each causal claim to retrieved literature and automatically generates directed acyclic graphs (DAGs) to visualize intervention-outcome pathways.

Result: On 234 dementia exercise abstracts, CausalAgent achieved 95% accuracy, 100% retrieval success, and zero hallucinations—significantly outperforming baseline AI (34% accuracy, 10% hallucinations). The generated causal graphs enable transparent, visual, and interpretable synthesis of mechanisms.

Conclusion: The architectural approach demonstrates the feasibility of trustworthy, causally grounded AI in medical review tasks. Despite being tested on a narrow domain (dementia exercise), the principles are transferable to other high-stakes healthcare applications.

Abstract: Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care.
  We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways.
  Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.

</details>


### [123] [Sample-Efficient Neurosymbolic Deep Reinforcement Learning](https://arxiv.org/abs/2601.02850)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 本文提出一种神经符号深度强化学习（DRL）方法，通过整合背景符号知识来提升样本效率和泛化能力。该方法利用简单任务中获得的局部策略作为先验知识，以加速复杂环境中的学习过程，并避免从零开始调整DRL参数。局部策略以逻辑规则形式表示，并通过在线推理机制在探索阶段偏置动作分布、在利用阶段重标定Q值，从而提升可解释性和可信度。实验验证了该方法在具有挑战性的网格世界环境中的有效性，尤其在稀疏奖励和长规划周期场景下表现优异，优于当前先进的奖励机基线。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习算法在复杂环境中样本效率低且泛化能力差，难以适应未见过的任务。为解决这一问题，需要引入外部知识以指导学习过程，提高效率与鲁棒性。

Method: 将部分策略表示为逻辑规则，结合在线推理机制，在探索时偏置动作分布，在利用时重标定Q值，实现神经符号融合的强化学习框架。

Result: 在多种具有挑战性的网格世界任务中，该方法显著提升了训练效率与性能，特别是在稀疏奖励和长规划周期场景下优于现有基线方法。

Conclusion: 通过引入符号知识引导深度强化学习，本方法有效提升了样本效率、泛化能力与可解释性，适用于复杂、未见过的任务场景。

Abstract: Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.

</details>


### [124] [M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?](https://arxiv.org/abs/2601.02854)
*Ao Li,Jinghui Zhang,Luyu Li,Yuxiang Duan,Lang Gao,Mingcai Chen,Weijun Qin,Shaopeng Li,Fengxian Ji,Ning Liu,Lizhen Cui,Xiuying Chen,Yuntao Du*

Main category: cs.AI

TL;DR: 本文提出M3MAD-Bench，一个统一且可扩展的基准测试平台，用于评估多智能体辩论（MAD）方法在多领域、多模态输入和多维度指标下的表现。该基准涵盖五个核心任务领域（知识、数学、医学、自然科学、复杂推理），并覆盖纯文本与视觉-语言数据集，支持跨模态对比。通过在九个不同架构、规模和模态能力的基础模型上进行评估，不仅考察准确率，还引入了令牌消耗、推理时间等效率指标，全面揭示性能与成本的权衡关系。实验揭示了MAD在文本与多模态场景中的有效性、鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有MAD研究存在评估设置碎片化、不一致的问题，且主要局限于单模态（仅文本）场景，难以公平比较且限制了应用范围。因此亟需一个统一、标准化的多模态评测基准来推动MAD的发展。

Method: 构建M3MAD-Bench，定义标准化协议，覆盖多领域任务、多模态输入（文本与视觉-语言）、多维评估指标（准确性、效率等），并在多个基础模型上系统评估MAD方法的表现。

Result: M3MAD-Bench实现了对MAD方法在多领域、多模态环境下的标准化评估；实验揭示了MAD在不同场景下的有效性、鲁棒性和效率特征，为未来研究提供了可靠基线和分析框架。

Conclusion: M3MAD-Bench为多智能体辩论方法提供了统一、可扩展的评测基准，有助于推动其在多模态复杂推理任务中的发展，是未来标准化评估的重要基础。

Abstract: As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.

</details>


### [125] [ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning](https://arxiv.org/abs/2601.02880)
*Abhishek HS,Pavan C Shekar,Arpit Jain,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: ReTreVal 是一种结合思维树探索、自修正、基于LLM的评审查验和反思记忆的混合框架，用于实现有界且可验证的多步推理。它通过构建自适应深度的结构化推理树，对每个节点进行迭代自评与优化，并采用双重验证机制评估推理质量，同时将成功路径与失败模式存入反思记忆缓冲区以实现跨问题学习。通过基于评查的剪枝策略保留高分节点，控制计算成本并保持高质量解题路径。在500个数学问题和创意写作任务上，使用Qwen 2.5 7B模型测试，ReTreVal显著优于ReAct、Reflexion和Self-Refine，尤其适用于需要探索性推理、严格验证和知识迁移的任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ReAct、Reflexion和Self-Refine虽能提升多步推理能力，但在探索替代解题路径方面缺乏结构化设计，且缺乏跨问题的持续学习机制。因此，亟需一种能够系统化探索、动态验证并积累经验的推理框架。

Method: 提出ReTreVal框架，融合思维树（ToT）探索、自修正机制、基于大语言模型的批判评分、以及反思记忆模块。通过自适应深度构建推理树，每一步节点执行迭代式自我批评与优化；引入双重验证机制评估节点的质量、连贯性和正确性；利用批判评分进行剪枝，保留前k个最优节点；并将成功与失败经验持久化存储于反思记忆中，支持跨问题学习。

Result: 在500个数学问题和创意写作任务上的实验表明，ReTreVal在各项指标上均优于ReAct、Reflexion和Self-Refine，表现出更强的推理能力、更高的准确性与更好的泛化性能，尤其在复杂、需要探索与验证的任务中优势明显。

Conclusion: ReTreVal通过结构化探索、批判驱动的精炼与跨问题记忆机制，有效提升了大语言模型在复杂任务中的多步推理能力，为构建更智能、可学习的推理系统提供了新范式。

Abstract: Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.

</details>


### [126] [Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning](https://arxiv.org/abs/2601.02902)
*Xinglang Zhang,Yunyao Zhang,ZeLiang Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.AI

TL;DR: 本文系统分析了大语言模型在逻辑复杂度逐步增加下的符号逻辑推理能力，发现了一种名为'逻辑相变'的新现象：逻辑推理性能并非平滑下降，而是在某一临界逻辑深度前保持稳定，随后突然崩溃，类似物理中的相变。基于此，提出神经符号课程调优（Neuro-Symbolic Curriculum Tuning）框架，通过自适应对齐自然语言与逻辑符号，建立共享表示，并围绕相变边界重塑训练动态，逐步增强高复杂度逻辑推理能力。在五个基准测试中，该方法显著缓解了高复杂度下的推理崩溃问题，平均准确率提升分别为+1.26（零样本提示）和+3.95（思维链），并增强了对未见逻辑组合的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数学推理、法律判断等高风险领域需要可靠的符号逻辑推理能力，但当前该能力仍不充分且缺乏系统研究。尤其在逻辑复杂度上升时，性能退化机制不明，导致模型在高复杂任务中表现不稳定。

Method: 提出神经符号课程调优框架，通过自适应对齐自然语言与逻辑符号构建共享表示，并利用逻辑相变边界设计课程学习策略，动态调整训练过程以增强深层逻辑推理能力。

Result: 在五个基准上验证了方法的有效性，显著缓解了逻辑推理在高复杂度下的崩溃现象，平均准确率提升+1.26（零样本提示）和+3.95（思维链提示），同时提升了对未见逻辑结构的泛化能力。

Conclusion: 逻辑相变揭示了大语言模型逻辑推理能力的非连续性本质，通过神经符号课程调优可有效引导模型跨越相变边界，实现更稳健、可扩展的逻辑推理能力。

Abstract: Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

</details>


### [127] [Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning](https://arxiv.org/abs/2601.02950)
*Xuan Yang,Furong Jia,Roy Xie,Xiong Xi,Hengwei Bian,Jian Li,Monica Agrawal*

Main category: cs.AI

TL;DR: 提出Batch-of-Thought（BoT）方法，通过联合处理相关查询实现跨实例学习，无需训练即可提升大语言模型的推理准确性与效率。在多智能体反思架构（BoT-R）中，反射器进行联合评估，挖掘独立处理无法获得的互信息。实验表明，该方法在多个模型家族和基准测试中显著提高准确率、校准置信度，并降低高达61%的推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理系统独立处理每个查询，忽略了跨实例间的共享推理模式和一致性约束等宝贵信号，导致推理效率与质量受限。

Method: 引入Batch-of-Thought（BoT）方法，通过批量比较分析识别高质量推理模板，利用一致性检查发现错误，并分摊计算开销；在多智能体反思架构（BoT-R）中，由Reflector执行联合评估以实现跨实例信息增益。

Result: 在三个模型家族和六个基准测试上的实验显示，BoT-R在提升推理准确率和置信度校准的同时，推理成本降低最高达61%。理论与实证分析揭示了批处理感知推理在何种情况下对大语言模型系统有益。

Conclusion: 批处理感知的推理机制能够有效挖掘跨实例间的信息关联，显著提升大语言模型的推理性能与效率，且无需额外训练，具有广泛适用性。

Abstract: Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.

</details>


### [128] [Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models](https://arxiv.org/abs/2601.02968)
*Qingxiang Liu,Zhiqing Cui,Xiaoliang Luo,Yuqian Wu,Zhuoyang Jiang,Huaiyu Wan,Sheng Sun,Lvchun Wang,Wei Yu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 提出RationaleTS方法，通过引入基于标签的推理路径作为先验知识，提升多模态大模型在时间序列推理中的表现。该方法结合时间模式与语义上下文进行混合检索，以指导新样本的上下文推理，避免表面模式匹配。实验表明其在三个领域任务中高效且有效。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在时间序列推理中表现不佳，主要因缺乏将时间观测与其下游结果相联系的推理先验，导致模型依赖表面模式匹配而非严谨推理。

Method: 提出理由引导的上下文学习框架，首先生成标签条件下的推理路径（从可观测证据到潜在结果），再设计融合时间模式与语义上下文的混合检索机制，用于在新样本上进行上下文推理。

Result: 在三个不同领域的时序推理任务上，RationaleTS显著提升了模型性能，兼具高效性与有效性，且代码将公开以供复现。

Conclusion: 通过引入可引导推理的先验知识，RationaleTS能够有效增强多模态模型在时间序列推理中的合理性与泛化能力，为未来研究提供新思路。

Abstract: The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.

</details>


### [129] [A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace](https://arxiv.org/abs/2601.03120)
*Adam Keane,Nick Pepper,Chris Burr,Amy Hodgkin,Dewi Gould,John Korna,Marc Thomas*

Main category: cs.AI

TL;DR: 本文提出了一种针对数字孪生（Digital Twin）在航空交通管理中应用的保障框架，结合了可信与伦理保障（TEA）方法，构建了结构化的保证论证，以证明数字孪生对物理系统的准确表征及其在特定应用场景下的充分功能。该框架为研究人员提供了评估和记录数字孪生优缺点的方法，并支持与利益相关者及监管机构的沟通，推动监管指南的发展。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生与人工智能在航空领域的应用日益广泛，亟需建立一套可验证、可信赖的保障机制，以满足不断发展的监管要求。现有技术缺乏系统性方法来确保数字孪生的准确性与适用性，因此需要一个结构化框架来指导开发并支持合规性讨论。

Method: 采用可信与伦理保障（TEA）方法构建保证案例，通过嵌套的结构化论据体系，明确实现顶层目标所需的关键证据、假设与推理逻辑。对多个核心论据进行了深入分析，涵盖数字孪生的建模精度、数据可靠性、AI/ML算法透明度以及应用场景适配性等方面。

Result: 成功构建了一个可操作、可扩展的保障框架，能够有效评估数字孪生的保真度与功能完整性；为项目Bluebird中的英国航路空域数字孪生提供可验证的证据链，并为未来类似系统的监管合规提供参考范例。

Conclusion: 本研究提出的保障框架不仅提升了数字孪生系统的可信度与可解释性，还为学术界与工业界在复杂系统中安全部署AI技术提供了实践路径，同时为监管政策的制定提供了具体依据。

Abstract: Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case.
  We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin.
  The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.

</details>


### [130] [Automatic Prompt Engineering with No Task Cues and No Tuning](https://arxiv.org/abs/2601.03130)
*Faisal Chowdhury,Nandana Mihindukulasooriya,Niharika S D'Souza,Horst Samulowitz,Neeru Gupta,Tomasz Hanusiak,Michal Kapitonow*

Main category: cs.AI

TL;DR: 本文提出了一种简单且有效的自动提示工程系统，无需调参或任务线索，首次应用于数据库表中的加密列名扩展（CNE）任务，并在英德双语数据集上验证了其有效性，是首个将自动提示工程应用于非英语语言的工作。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示工程方法复杂且需大量调参，而加密列名扩展任务在表格数据搜索与理解中至关重要，但相关研究极少，因此需要一种更简单有效、无需额外任务信息的方法。

Method: 提出一种无需调参和任务线索的自动提示工程系统，直接应用于加密列名扩展任务，通过设计简洁的提示模板实现高效生成。

Result: 在英德双语数据集上均表现出与现有方法相当的效果，验证了方法的有效性与通用性，首次成功应用于非英语场景。

Conclusion: 该系统在设计和应用上更为简便，同时保持高效果，为自动提示工程在跨语言、低资源任务中的应用提供了新范式。

Abstract: This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [131] [Physical Transformer](https://arxiv.org/abs/2601.02433)
*Tao Xu,Zhixin Hu,Li Luo,Momiao Xiong*

Main category: cs.LG

TL;DR: 本文提出一种物理变换器，将现代Transformer计算与几何表示及物理动力学相结合。在微观层面，注意力头和前馈模块被建模为受有效哈密顿量和非哈密顿浴项支配的相互作用自旋；在介观层面，其聚合状态在学习到的神经微分流形（NDM）上通过哈密顿流和哈密顿-雅可比-贝尔曼（HJB）最优控制演化，由保持几何与能量不变性的辛层离散化；在宏观层面，模型维护一个生成语义工作区和二维信息-相位图，追踪推理轨迹中的不确定性与信息增益。推理任务被形式化为流形上的受控信息流，解对应于满足几何、能量与工作区一致性约束的低代价轨迹。在数值积分和动力系统等简单玩具问题上，该模型在稳定性与长时程准确性上优于基线方法，表明尊重底层几何与哈密顿结构的优势。整体框架为融合数字推理与物理基础流形的物理AI提供了路径，有望实现更可解释且统一的推理、控制与现实世界交互模型。


<details>
  <summary>Details</summary>
Motivation: 当前数字AI系统（如大语言模型、视觉模型）虽在符号、语言或像素域取得显著进展，但其运作局限于虚拟空间，缺乏物理交互与可解释性。为弥合数字智能与物理世界的鸿沟，需构建具有物理意义的计算架构，使模型能真正理解并影响现实世界。

Method: 提出物理变换器框架，分三个层次建模：1）微观：将注意力头与前馈块视为受有效哈密顿量与非哈密顿浴项支配的自旋系统；2）介观：聚合状态在神经微分流形（NDM）上通过哈密顿流与HJB最优控制演化，使用对称离散化以保持几何与能量守恒；3）宏观：引入生成语义工作区与二维信息-相位图，用于追踪推理过程中的不确定性与信息增益。推理任务被形式化为受控信息流优化问题。

Result: 在数值积分与动力系统等简化任务中，物理变换器表现出更强的稳定性和更高的长时程预测精度，优于传统基线方法。验证了保留哈密顿结构与几何不变性对提升模型性能的关键作用。

Conclusion: 该框架为构建物理感知型人工智能提供了一条新路径，将数字推理与物理基础流形相融合，推动可解释、统一且具备真实世界交互能力的智能系统发展。

Abstract: Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.

</details>


### [132] [mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks](https://arxiv.org/abs/2601.02451)
*Subhankar Mishra*

Main category: cs.LG

TL;DR: mHC-GNN adapts Manifold-Constrained Hyper-Connections from Transformers to GNNs, using $n$ parallel streams and Birkhoff polytope-constrained mixing matrices. It significantly reduces over-smoothing (exponential rate $(1-γ)^{L/n}$ vs.\ $(1-γ)^L$) and surpasses 1-WL expressiveness. Experiments show stable performance up to 128 layers with >74% accuracy, outperforming standard GNNs by over 50 percentage points at extreme depths. Ablations confirm the necessity of the manifold constraint.


<details>
  <summary>Details</summary>
Motivation: Standard GNNs suffer from over-smoothing in deep architectures and are limited in expressiveness by the 1-Weisfeiler-Leman test. This work aims to address these fundamental limitations by introducing a novel architectural design inspired by hyper-connections in Transformers.

Method: mHC-GNN expands node representations across $n$ parallel streams and constrains the stream-mixing matrices to the Birkhoff polytope using Sinkhorn-Knopp normalization, enabling controlled, structured information flow across layers.

Result: mHC-GNN achieves dramatically reduced over-smoothing, maintains high accuracy (over 74%) even at 128 layers, and exceeds 1-WL expressiveness. It improves performance by more than 50 percentage points compared to standard GNNs at extreme depths. Removing the manifold constraint leads to up to 82% performance drop.

Conclusion: mHC-GNN effectively mitigates over-smoothing and enhances graph expressiveness beyond 1-WL, demonstrating superior scalability and performance in deep GNN settings.

Abstract: Graph Neural Networks (GNNs) suffer from over-smoothing in deep architectures and expressiveness bounded by the 1-Weisfeiler-Leman (1-WL) test. We adapt Manifold-Constrained Hyper-Connections (\mhc)~\citep{xie2025mhc}, recently proposed for Transformers, to graph neural networks. Our method, mHC-GNN, expands node representations across $n$ parallel streams and constrains stream-mixing matrices to the Birkhoff polytope via Sinkhorn-Knopp normalization. We prove that mHC-GNN exhibits exponentially slower over-smoothing (rate $(1-γ)^{L/n}$ vs.\ $(1-γ)^L$) and can distinguish graphs beyond 1-WL. Experiments on 10 datasets with 4 GNN architectures show consistent improvements. Depth experiments from 2 to 128 layers reveal that standard GNNs collapse to near-random performance beyond 16 layers, while mHC-GNN maintains over 74\% accuracy even at 128 layers, with improvements exceeding 50 percentage points at extreme depths. Ablations confirm that the manifold constraint is essential: removing it causes up to 82\% performance degradation. Code is available at \href{https://github.com/smlab-niser/mhc-gnn}{https://github.com/smlab-niser/mhc-gnn}

</details>


### [133] [Polynomial Convergence of Riemannian Diffusion Models](https://arxiv.org/abs/2601.02499)
*Xingyu Xu,Ziyi Zhang,Yorie Nakahira,Guannan Qu,Yuejie Chi*

Main category: cs.LG

TL;DR: 本文研究了非欧几里得空间上的扩散模型，提出在 $L_2$-准确的得分估计下，仅需多项式小步长即可保证总变差距离下的小采样误差，且无需数据分布的光滑性或正性假设。分析基于热核对数梯度的Li-Yau估计和扰动热方程的Minakshisundaram-Pleijel参数展开，仅需标准的曲率假设。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型多基于欧几里得空间假设，但实际数据常位于欧几里得空间的子流形上。此前工作虽提出黎曼扩散模型，但依赖于指数小步长及数据分布光滑、严格正等强假设。本文旨在放宽这些限制，实现更普适且更强的理论保证。

Method: 结合Li-Yau估计与Minakshisundaram-Pleijel参数展开，分析在非欧空间中扩散过程的逆过程收敛性；通过引入 $L_2$-准确得分估计，降低对步长的要求，并放宽对数据分布的假设。

Result: 在 $L_2$-准确得分估计下，使用多项式小步长即可实现小采样误差（总变差距离），无需数据分布光滑或正性假设，仅需标准曲率条件。

Conclusion: 本工作显著强化了黎曼扩散模型的理论基础，为非欧空间扩散模型提供了更简洁、更通用的分析框架，推动其在复杂数据结构中的应用。

Abstract: Diffusion models have demonstrated remarkable empirical success in the recent years and are considered one of the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, De Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields a small sampling error in the Wasserstein distance, provided the data distribution is smooth and strictly positive, and the score estimate is $L_\infty$-accurate. In this paper, we greatly strengthen this theory by establishing that, under $L_2$-accurate score estimate, a {\em polynomially small stepsize} suffices to guarantee small sampling error in the total variation distance, without requiring smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our analysis are Li-Yau estimate for the log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion of the perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.

</details>


### [134] [GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA](https://arxiv.org/abs/2601.02500)
*Brian Tekmen,Jason Yin,Qianqian Tong*

Main category: cs.LG

TL;DR: I-GEM 提出一种在低秩适配器（LoRA）空间中应用梯度回放记忆（GEM）的高效方法，通过双投影梯度近似实现固定预算、GPU驻留的持续学习，显著降低计算开销，同时保持与GEM相当的性能，在3个任务的AG News数据集上平均准确率接近GEM，且比A-GEM高约1.4个百分点，投影时间减少约1000倍。


<details>
  <summary>Details</summary>
Motivation: 全量微调大语言模型（LLM）计算成本高昂，因此需要参数高效的持续学习（CL）方法。现有方法如GEM虽能保证稳定性，但其投影计算开销大，难以应用于大规模模型。本文旨在探索在低秩适配器（LoRA）子空间中高效实现类似GEM的约束机制，以降低计算负担并保持性能。

Method: 提出I-GEM方法，基于LoRA子空间对GEM进行改进，采用固定预算、GPU驻留的双投影梯度近似，仅在适配器参数范围内施加非干扰约束，从而大幅降低投影计算开销。

Result: 在3任务的AG News数据集上，使用GPT-2（355M）和LoRA（r=8），I-GEM在平均准确率上与GEM几乎持平（误差<0.04点），优于A-GEM约1.4点；投影时间相比GEM减少约1000倍。

Conclusion: 在LoRA子空间中应用GEM约束是一种可行且高效的持续学习路径，适用于大语言模型场景，显著降低了计算开销而未牺牲性能。

Abstract: Full fine-tuning of Large Language Models (LLMs) is computationally costly, motivating Continual Learning (CL) approaches that utilize parameter-efficient adapters. We revisit Gradient Episodic Memory (GEM) within the Low-Rank Adapter (LoRA) subspace and introduce I-GEM: a fixed-budget, GPU-resident dual projected-gradient approximation to GEM's quadratic projection. By constraining non-interference solely within the adapter parameters, I-GEM preserves GEM-like stability with orders-of-magnitude lower mean projection overhead. On a 3-task AG News split with induced domain drift, using GPT-2 (355M) and LoRA ($r=8$), I-GEM matches GEM's average accuracy (within $\sim\!0.04$ pts) and outperforms A-GEM by $\sim\!1.4$ pts. Crucially, it reduces projection time vs.\ GEM by a factor of $\sim\!10^3$. These results suggest that applying GEM constraints in the LoRA subspace is a practical pathway for continual learning at the LLM scale.

</details>


### [135] [hdlib 2.0: Extending Machine Learning Capabilities of Vector-Symbolic Architectures](https://arxiv.org/abs/2601.02509)
*Fabio Cumbo,Kabir Dhillon,Daniel Blankenberg*

Main category: cs.LG

TL;DR: 本文介绍了hdlib库的重大更新，扩展了其在向量符号架构（VSA）中的机器学习能力，新增了监督分类、回归、聚类和图学习模型，并首次实现了量子超维计算与量子机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 为了满足向量符号架构（VSA）框架中日益增长的数据驱动建模需求，提升其在机器学习任务中的应用能力。

Method: 对hdlib进行功能扩展，引入新的监督学习、无监督学习及图学习模型，并实现基于量子计算的算术操作与量子机器学习模型。

Result: 成功开发并集成多个新模型，包括支持特征选择的监督分类、连续变量预测的回归模型、聚类模型以及图学习模型；首次实现量子超维计算与量子机器学习，显著增强VSA的表达与计算能力。

Conclusion: 此次更新使hdlib成为更强大的通用工具，推动了超维计算在机器学习领域的应用发展。

Abstract: Following the initial publication of hdlib, a Python library for designing Vector-Symbolic Architectures (VSA), we introduce a major extension that significantly enhances its machine learning capabilities. VSA, also known as Hyperdimensional Computing, is a computing paradigm that represents and processes information using high-dimensional vectors. While the first version of hdlib established a robust foundation for creating and manipulating these vectors, this update addresses the growing need for more advanced, data-driven modeling within the VSA framework. Here, we present four extensions: significant enhancements to the existing supervised classification model also enabling feature selection, and a new regression model for predicting continuous variables, a clustering model for unsupervised learning, and a graph-based learning model. Furthermore, we propose the first implementation ever of Quantum Hyperdimensional Computing with quantum-powered arithmetic operations and a new Quantum Machine Learning model for supervised learning. hdlib remains open-source and available on GitHub at https://github.com/cumbof/hdlib under the MIT license, and distributed through the Python Package Index (pip install hdlib) and Conda (conda install -c conda-forge hdlib). Documentation and examples of these new features are available on the official Wiki at https://github.com/cumbof/hdlib/wiki.

</details>


### [136] [CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening](https://arxiv.org/abs/2601.02562)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 本文提出了一种名为CBDC的新型皮肤病诊断框架，结合统计学习理论、拓扑数据分析（TDA）和贝叶斯共形推断，旨在解决深度学习模型在临床部署中缺乏可靠不确定性估计的问题。该框架提供了依赖于分布的泛化边界、拓扑稳定性定理以及有限共形覆盖保证，从而实现可信的不确定性量化。在HAM10000、PH2和ISIC 2020数据集上的实验表明，CBDC不仅具备高分类准确率，还能生成可解释且校准良好的预测结果，推动了机器学习在皮肤科诊断中的理论与实践应用。


<details>
  <summary>Details</summary>
Motivation: 当前皮肤病影像与移动诊断工具快速发展，但深度学习模型常因缺乏可靠的不确定性估计而难以在临床环境中部署。因此需要兼具高精度与理论保障的系统，以增强其可信度与实用性。

Method: CBDC框架融合了统计学习理论、拓扑数据分析（TDA）和贝叶斯共形推断，通过构建分布依赖的泛化界、证明拓扑稳定性定理，并提供有限样本下的共形覆盖保证，实现对模型预测不确定性的严格量化。

Result: 在HAM10000、PH2和ISIC 2020数据集上，CBDC表现出优异的分类性能，同时生成了校准良好且具有临床可解释性的预测结果，验证了其在真实场景中的有效性与可靠性。

Conclusion: 本研究为深度皮肤病诊断提供了理论与实践双重突破，打通了机器学习理论与临床应用之间的壁垒，推动了可信赖AI在医疗领域的落地。

Abstract: The rapid growth of dermatological imaging and mobile diagnostic tools calls for systems that not only demonstrate empirical performance but also provide strong theoretical guarantees. Deep learning models have shown high predictive accuracy; however, they are often criticized for lacking well, calibrated uncertainty estimates without which these models are hardly deployable in a clinical setting. To this end, we present the Conformal Bayesian Dermatological Classifier (CBDC), a well, founded framework that combines Statistical Learning Theory, Topological Data Analysis (TDA), and Bayesian Conformal Inference. CBDC offers distribution, dependent generalization bounds that reflect dermatological variability, proves a topological stability theorem that guarantees the invariance of convolutional neural network embeddings under photometric and morphological perturbations and provides finite conformal coverage guarantees for trustworthy uncertainty quantification.
  Through exhaustive experiments on the HAM10000, PH2, and ISIC 2020 datasets, we show that CBDC not only attains classification accuracy but also generates calibrated predictions that are interpretable from a clinical perspective. This research constitutes a theoretical and practical leap for deep dermatological diagnostics, thereby opening the machine learning theory clinical applicability interface.

</details>


### [137] [LendNova: Towards Automated Credit Risk Assessment with Language Models](https://arxiv.org/abs/2601.02573)
*Kiarash Shamsi,Danijel Novokmet,Joshua Peters,Mao Lin Liu,Paul K Edwards,Vahab Khoshdel*

Main category: cs.LG

TL;DR: LendNova is a novel end-to-end pipeline for credit risk assessment that uses advanced NLP and language models to analyze raw credit bureau text without manual feature engineering. It automatically extracts risk signals from unstructured text, improving accuracy, reducing costs, and enhancing scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional credit risk models rely on expensive, manual feature engineering and fail to fully utilize raw credit records, especially the rich textual information in credit bureau reports. There is a need for automated, scalable, and accurate solutions that can process unstructured data effectively.

Method: LendNova leverages state-of-the-art language models to directly process raw, jargon-heavy credit bureau text. It learns task-relevant representations automatically, eliminating the need for manual preprocessing and feature extraction. The system integrates NLP techniques to capture latent patterns and risk indicators embedded in textual data.

Result: Evaluation on real-world data shows LendNova achieves high accuracy in credit risk assessment while significantly reducing modeling costs and time. It outperforms traditional methods in handling complex, unstructured credit text and demonstrates strong scalability.

Conclusion: LendNova establishes a new benchmark for intelligent credit risk assessment by demonstrating the feasibility and effectiveness of language models in processing raw credit text. It paves the way for future foundation models in finance, enabling more adaptive, accurate, and automated financial decision-making systems.

Abstract: Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.

</details>


### [138] [Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth](https://arxiv.org/abs/2601.02609)
*Arjun S. Nair*

Main category: cs.LG

TL;DR: Chronicals是一个开源训练框架，通过四项协同优化实现3.51倍于Unsloth的训练速度提升：(1) 融合Triton内核，通过RMSNorm、SwiGLU和QK-RoPE融合减少75%内存流量；(2) Cut Cross-Entropy技术将logit内存从5GB降至135MB；(3) LoRA+采用理论推导的16倍微调学习率差异；(4) Best-Fit Decreasing序列打包恢复因填充浪费的60-75%算力。在A100-40GB上，全量微调达41,184 tokens/second，LoRA rank 32时达11,699 tokens/second，显著优于Unsloth。同时发现Unsloth宣称的46,000 tokens/second实际为零梯度范数，模型未真正训练。所有代码、基准测试与数学证明均公开。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调受内存瓶颈限制，7B参数模型需84GB内存，超出A100-40GB容量。现有方法在内存效率与训练速度上存在明显不足，亟需高效优化方案。

Method: 提出四类协同优化：(1) 融合Triton内核（RMSNorm、SwiGLU、QK-RoPE）以降低内存访问开销；(2) Cut Cross-Entropy实现在线softmax计算，大幅压缩logit存储；(3) LoRA+引入基于梯度幅值分析的理论学习率配置；(4) Best-Fit Decreasing序列打包策略减少填充造成的计算浪费。

Result: 在Qwen2.5-0.5B模型上，全量微调速度达41,184 tokens/second（对比Unsloth的11,736），提升3.51倍；LoRA rank 32时达11,699 tokens/second（对比Unsloth MAX的2,857），提升4.10倍。发现Unsloth声称的46,000 tokens/second实为无效训练（零梯度范数）。

Conclusion: Chronicals通过系统性内存与计算优化，在不增加硬件资源前提下显著提升大模型微调效率，并揭示了现有基准测试中的潜在问题，具备高实用价值与学术严谨性。

Abstract: Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding.
  On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training.
  We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.

</details>


### [139] [Prioritized Replay for RL Post-training](https://arxiv.org/abs/2601.02648)
*Mehdi Fatemi*

Main category: cs.LG

TL;DR: 提出一种基于问题级别的强化学习后训练框架，通过模型驱动的优先级分数自动选择具有中等成功率的问题进行训练，避免简单或困难任务的干扰，实现无需预设难度层级的自适应优先级机制。


<details>
  <summary>Details</summary>
Motivation: 传统课程学习策略依赖于预先设定的难度层级，难以动态适应训练过程；而现有方法在处理中间成功率问题时学习信号更强，因此需要一种能自动聚焦于此类问题的优先级机制。

Method: 基于深度强化学习中的优先经验回放思想，结合GRPO方法中中间成功问题提供更强学习信号的观察，设计一个由实际成功统计推导出的模型驱动优先级分数，动态选择最有利于学习的问题进行训练，并引入堆结构采样与周期性重测机制以防止遗忘和饥饿现象。

Result: 该方法实现了自动、连续的优先级调度，无需外部标签或辅助预测器，显著提升了训练效率和模型性能，且在大规模语言模型后训练中表现出良好的可扩展性。

Conclusion: 所提框架为大语言模型的强化学习后训练提供了一种原理清晰、可扩展的自动课程学习替代方案，直接对齐GRPO训练动态，有效提升学习效率与最终性能。

Abstract: We introduce a problem-level prioritization framework for RL post-training of large language models. Building on insights from prioritized replay in deep RL, as well as prior observations that rollouts with intermediate success rates tend to produce stronger learning signals under methods such as GRPO, our approach selects problems according to a simple, model-driven priority score derived from empirical success statistics. In contrast to conventional curriculum strategies that emphasize easier tasks early in training, the resulting schedule naturally focuses training on problems that are neither consistently solved nor consistently failed, while deprioritizing those that contribute little gradient information. The method yields a continuously adapting and automatic prioritization process that requires no predefined difficulty tiers, auxiliary predictors, or external labels. We further introduce lightweight mechanisms for practical deployment, including heap-based prioritized sampling and periodic retesting of solved and unsolved problems to mitigate starvation and forgetting. Overall, the approach offers a principled and scalable alternative to manually designed curricula while aligning data selection directly with the dynamics of GRPO-based post-training.

</details>


### [140] [MAFS: Multi-head Attention Feature Selection for High-Dimensional Data via Deep Fusion of Filter Methods](https://arxiv.org/abs/2601.02668)
*Xiaoyan Sun,Qingyu Meng,Yalu Wen*

Main category: cs.LG

TL;DR: MAFS 是一种结合统计先验与深度学习能力的混合特征选择框架，通过多头注意力机制从多个角度并行分析特征，捕捉复杂非线性关系和交互作用，并利用重排序模块减少信息损失，生成稳定且可解释的特征重要性评分。在模拟和真实数据集（如癌症基因表达和阿尔茨海默病数据）上，MAFS 表现优于现有方法，具有更高的覆盖率和稳定性，适用于超高维生物医学数据的可扩展、可解释、鲁棒特征选择。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法在高维生物医学数据中面临挑战：滤波方法虽可扩展但无法捕捉复杂关系或冗余；深度学习方法虽能建模非线性模式但缺乏稳定性、可解释性和大规模效率；单头注意力虽提升可解释性，但难以捕捉多层次依赖且对初始化敏感，影响可重复性。多数方法未能有效融合统计可解释性与深度学习的表示能力，尤其在超高压缩维度下表现不足。

Method: MAFS 采用三阶段设计：首先使用基于滤波的先验进行稳定初始化并引导学习；其次引入多头注意力机制，从多个视角并行分析特征，捕捉复杂非线性关系与交互；最后通过重排序模块整合各注意力头输出，解决冲突、最小化信息损失，生成一致可靠的特征排名。

Result: 在多种模拟和真实世界数据集（包括癌症基因表达和阿尔茨海默病数据）上，MAFS 在特征覆盖范围和稳定性方面均显著优于现有滤波法和深度学习方法，展现出良好的可扩展性、可解释性和鲁棒性，为高维生物医学数据提供了高效可靠的特征选择解决方案。

Conclusion: MAFS 有效融合了统计先验与深度学习的优势，在保持可解释性的前提下，提升了特征选择的性能与稳定性，是适用于高维生物医学数据分析的先进方法。

Abstract: Feature selection is essential for high-dimensional biomedical data, enabling stronger predictive performance, reduced computational cost, and improved interpretability in precision medicine applications. Existing approaches face notable challenges. Filter methods are highly scalable but cannot capture complex relationships or eliminate redundancy. Deep learning-based approaches can model nonlinear patterns but often lack stability, interpretability, and efficiency at scale. Single-head attention improves interpretability but is limited in capturing multi-level dependencies and remains sensitive to initialization, reducing reproducibility. Most existing methods rarely combine statistical interpretability with the representational power of deep learning, particularly in ultra-high-dimensional settings. Here, we introduce MAFS (Multi-head Attention-based Feature Selection), a hybrid framework that integrates statistical priors with deep learning capabilities. MAFS begins with filter-based priors for stable initialization and guide learning. It then uses multi-head attention to examine features from multiple perspectives in parallel, capturing complex nonlinear relationships and interactions. Finally, a reordering module consolidates outputs across attention heads, resolving conflicts and minimizing information loss to generate robust and consistent feature rankings. This design combines statistical guidance with deep modeling capacity, yielding interpretable importance scores while maximizing retention of informative signals. Across simulated and real-world datasets, including cancer gene expression and Alzheimer's disease data, MAFS consistently achieves superior coverage and stability compared with existing filter-based and deep learning-based alternatives, offering a scalable, interpretable, and robust solution for feature selection in high-dimensional biomedical data.

</details>


### [141] [CRoPE: Efficient Parametrization of Rotary Positional Embedding](https://arxiv.org/abs/2601.02728)
*Beicheng Lou,Zifei Xu*

Main category: cs.LG

TL;DR: 本文提出旋转位置嵌入（Rotary Positional Embedding）在实现上并非真正的复数线性变换，主张使用复数线性变换作为更自然的参数化方式，可节省近50%的注意力模块参数，且对模型性能影响极小，同时提升表示空间的清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有旋转位置嵌入的实现方式存在参数冗余，且与复数线性变换不一致，影响参数效率和理论一致性。

Method: 将旋转位置嵌入重新表述为复数线性变换，通过理论分析和实验验证其有效性，并对比原方法在参数量和性能上的差异。

Result: 新方法显著减少参数量（约50%），在样本内外表现均保持稳定，证明其高效性与鲁棒性。

Conclusion: 复数线性变换是旋转位置嵌入更自然且高效的参数化方式，既提升效率又增强可解释性。

Abstract: Rotary positional embedding has become the state-of-the-art approach to encode position information in transformer-based models. While it is often succinctly expressed in complex linear algebra, we note that the actual implementation of $Q/K/V$-projections is not equivalent to a complex linear transformation. We argue that complex linear transformation is a more natural parametrization and saves near 50\% parameters within the attention block. We show empirically that removing such redundancy has negligible impact on the model performance both in sample and out of sample. Our modification achieves more efficient parameter usage, as well as a cleaner interpretation of the representation space.

</details>


### [142] [Scalable Tree Ensemble Proximities in Python](https://arxiv.org/abs/2601.02735)
*Adrien Aumon,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

TL;DR: 本文提出一种高效计算树集成模型相似性度量的新框架，通过定义可分离加权叶节点碰撞相似性族，实现精确的稀疏矩阵分解，避免显式成对比较，从而大幅降低时间和内存开销。实验表明该方法在大规模数据集上显著提升效率，可在标准CPU硬件上处理数十万样本。


<details>
  <summary>Details</summary>
Motivation: 现有基于树集成的相似性度量方法通常具有二次时间或内存复杂度，难以扩展到大规模数据集。

Method: 提出一种名为分离加权叶节点碰撞相似性（Separable Weighted Leaf-Collision Proximities）的框架，利用稀疏矩阵分解实现高效计算，仅依赖叶节点级别的碰撞信息，无需显式成对比较。

Result: 实验结果显示，该方法在运行时间和内存占用方面相比传统方法有显著改进，可在标准CPU上高效处理包含数十万样本的大规模数据集。

Conclusion: 所提出的框架为树集成模型的相似性计算提供了低内存、可扩展的解决方案，使大规模数据上的相似性分析成为可能。

Abstract: Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.

</details>


### [143] [Domain Generalization for Time Series: Enhancing Drilling Regression Models for Stick-Slip Index Prediction](https://arxiv.org/abs/2601.02884)
*Hana Yahia,Bruno Figliuzzi,Florent Di Meglio,Laurent Gerbaud,Stephane Menand,Mohamed Mahjoub*

Main category: cs.LG

TL;DR: 本文比较了多种领域泛化技术在钻井时间序列数据中的应用，重点是预测连续的扭矩-滑移指数（SSI），以评估钻头处的扭转振动。通过在60秒标签时间序列数据上训练模型，并在不同井中测试，研究发现对抗性领域泛化（ADG）和不变风险最小化（IRM）模型分别比基线模型提升10%和8%。更重要的是，严重事件检测率从基线的20%提高到60%。结合迁移学习（TL）可进一步提升性能。结果表明，域泛化方法在钻井应用中具有潜力，其中ADG表现最优。


<details>
  <summary>Details</summary>
Motivation: 在钻井过程中，准确预测扭矩-滑移指数（SSI）对防止设备损坏和确保作业安全至关重要。然而，由于不同井间的环境差异，传统模型难以跨域泛化。因此，需要开发能够适应多井场景的鲁棒回归模型，提升模型在未知井中的表现。

Method: 采用网格搜索优化关键超参数，构建基于60秒1Hz表面钻井数据的回归模型。对比分析了对抗性领域泛化（ADG）、不变风险最小化（IRM）与基线模型，并评估了迁移学习（TL）对性能的影响。模型在不同井数据上进行测试，验证其跨域泛化能力。

Result: ADG和IRM模型分别比基线模型提升10%和8%的性能；严重事件检测率从20%提升至60%。结合迁移学习进一步改善效果。整体上，ADG表现最佳，显示出较强的跨域泛化能力。

Conclusion: 领域泛化方法在钻井应用中具有显著潜力，尤其在复杂多变的井间环境中。对抗性领域泛化（ADG）是最有效的策略，结合迁移学习可进一步提升模型性能，为实际工程应用提供了可靠的技术支持。

Abstract: This paper provides a comprehensive comparison of domain generalization techniques applied to time series data within a drilling context, focusing on the prediction of a continuous Stick-Slip Index (SSI), a critical metric for assessing torsional downhole vibrations at the drill bit. The study aims to develop a robust regression model that can generalize across domains by training on 60 second labeled sequences of 1 Hz surface drilling data to predict the SSI. The model is tested in wells that are different from those used during training. To fine-tune the model architecture, a grid search approach is employed to optimize key hyperparameters. A comparative analysis of the Adversarial Domain Generalization (ADG), Invariant Risk Minimization (IRM) and baseline models is presented, along with an evaluation of the effectiveness of transfer learning (TL) in improving model performance. The ADG and IRM models achieve performance improvements of 10% and 8%, respectively, over the baseline model. Most importantly, severe events are detected 60% of the time, against 20% for the baseline model. Overall, the results indicate that both ADG and IRM models surpass the baseline, with the ADG model exhibiting a slight advantage over the IRM model. Additionally, applying TL to a pre-trained model further improves performance. Our findings demonstrate the potential of domain generalization approaches in drilling applications, with ADG emerging as the most effective approach.

</details>


### [144] [RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance](https://arxiv.org/abs/2601.02888)
*Xuanyu Wang,Haisen Su,Jingtao Zhang,Xiangxiang Wang,Yongbin Yu,Manping Fan,Bo Gong,Siqi Chen,Mingsheng Cao,Liyong Ren*

Main category: cs.LG

TL;DR: 提出一种新型量化框架RPIQ，通过多协同闭环补偿机制与单实例校准结合高斯-赛德尔迭代量化，实现4比特压缩，显著降低内存消耗（减少60%-75%），同时保持接近全精度模型的性能，在文本理解与复杂场景视觉问答等任务中表现优异，适用于视障用户的智能辅助系统部署。


<details>
  <summary>Details</summary>
Motivation: 解决视障用户在信息获取和环境感知中的困难，应对大模型在助残设备上部署时因内存占用大、推理成本高及现有量化策略忽视块间误差累积导致模型稳定性下降的问题。

Method: 采用基于单实例校准与高斯-赛德尔迭代量化的多协同闭环补偿方案，构建Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization (RPIQ) 框架。

Result: RPIQ可将大模型压缩至4比特，峰值内存降低60%-75%，在语言与视觉任务中性能接近全精度模型，在复杂场景下的文本理解与视觉问答任务中表现出色，具备良好的实用性和可靠性。

Conclusion: RPIQ有效提升了大模型在资源受限设备上的计算效率与部署可行性，为视障用户提供了准确、快速的信息支持，推动了智能辅助系统的实际应用发展。

Abstract: Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.

</details>


### [145] [Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control](https://arxiv.org/abs/2601.02896)
*Harshvardhan Saini,Yiming Tang,Dianbo Liu*

Main category: cs.LG

TL;DR: 本文提出一种基于梯度上升的新型框架，用于在大型语言模型中可控地发现和引导特定行为人格（如阿谀奉承、幻觉、短视奖励），解决现有方法在可解释性与可扩展性之间的困境。通过RESGA和SAEGA两种方法优化随机初始化提示，使其与目标人格方向对齐，并引入流畅性梯度上升以提升生成提示的质量。实验表明，在Llama 3.1、Qwen 2.5和Gemma 3上，该方法显著提升了对阿谀奉承等行为的控制效果（从79.24%降至49.90%），实现高效且可解释的行为调控。


<details>
  <summary>Details</summary>
Motivation: 当前大模型中的行为人格（如阿谀奉承、幻觉）难以控制，传统手动提示工程难以扩展且不精确，而自动优化方法缺乏可解释性，无法连接模型内部机制。因此亟需一种既能有效又可解释的可控行为调控方法。

Method: 提出RESGA和SAEGA两种基于梯度上升的方法，通过优化随机初始化提示，使其在嵌入空间中对齐于预定义的人格方向；引入流畅性梯度上升约束，确保生成提示的语言质量。方法基于机械可解释特征进行提示发现，增强可解释性。

Result: 在Llama 3.1、Qwen 2.5和Gemma 3上成功实现对阿谀奉承、幻觉和短视奖励三种人格的精准引导；尤其在阿谀奉承任务中，自动发现的提示将错误率从79.24%降低至49.90%，显著优于基线。

Conclusion: 该框架通过将梯度上升应用于提示发现，实现了对大模型行为人格的可解释、可控制调节，为安全可控的AI行为设计提供了一种新范式。

Abstract: Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as "black boxes" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.

</details>


### [146] [Multi-Distribution Robust Conformal Prediction](https://arxiv.org/abs/2601.02998)
*Yuqi Yang,Ying Jin*

Main category: cs.LG

TL;DR: 本文研究在多个异质分布下构建统一有效的置信预测集的问题，提出一种max-p聚合方案，确保在任意测试分布下预测集的覆盖率均超过预设水平。通过优化效率并证明该方案的最优性与紧致性，提出学习一致性评分的通用算法，在标准条件下可生成高效的预测集。实验表明，该方法在保证最坏情况覆盖率的同时显著缩小预测集大小，优于简单应用单源一致性评分的max-p方法，且与现有标准方法相当。


<details>
  <summary>Details</summary>
Motivation: 在公平性和分布鲁棒性问题中，训练数据来自多个源分布，但测试数据可能来自任意分布或其混合。传统方法难以保证在所有分布下的覆盖率，因此需要一种能统一保证多分布覆盖的预测机制。

Method: 提出max-p聚合方案，结合一致性评分和多分布覆盖约束，设计优化程序以提升效率，并开发通用算法学习最优一致性评分。

Result: 所提方法在合成与真实数据上均实现跨分布的严格覆盖率，同时显著减小预测集规模，优于基线方法，性能接近单源最优方法。

Conclusion: 所提框架有效解决了多分布场景下的置信预测问题，兼具理论严谨性与实际高效性，适用于群体公平、子群体偏移、分布鲁棒优化等任务。

Abstract: In many fairness and distribution robustness problems, one has access to labeled data from multiple source distributions yet the test data may come from an arbitrary member or a mixture of them. We study the problem of constructing a conformal prediction set that is uniformly valid across multiple, heterogeneous distributions, in the sense that no matter which distribution the test point is from, the coverage of the prediction set is guaranteed to exceed a pre-specified level. We first propose a max-p aggregation scheme that delivers finite-sample, multi-distribution coverage given any conformity scores associated with each distribution. Upon studying several efficiency optimization programs subject to uniform coverage, we prove the optimality and tightness of our aggregation scheme, and propose a general algorithm to learn conformity scores that lead to efficient prediction sets after the aggregation under standard conditions. We discuss how our framework relates to group-wise distributionally robust optimization, sub-population shift, fairness, and multi-source learning. In synthetic and real-data experiments, our method delivers valid worst-case coverage across multiple distributions while greatly reducing the set size compared with naively applying max-p aggregation to single-source conformity scores, and can be comparable in size to single-source prediction sets with popular, standard conformity scores.

</details>


### [147] [In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior](https://arxiv.org/abs/2601.03015)
*Anaïs Berkes,Vincent Taboga,Donna Vakalis,David Rolnick,Yoshua Bengio*

Main category: cs.LG

TL;DR: SPICE is a Bayesian in-context reinforcement learning method that uses deep ensemble to learn a prior over Q-values and updates it at test-time via Bayesian inference. It employs an Upper-Confidence Bound rule for exploration to recover from poor priors, achieving regret-optimal performance even with suboptimal training data. Empirical results show SPICE outperforms existing ICRL and meta-RL methods in adapting to unseen tasks and handling distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Current in-context reinforcement learning (ICRL) methods either fail to generalize beyond the training distribution or require near-optimal data, limiting real-world applicability. There is a need for ICRL approaches that can adapt quickly to unseen environments without parameter updates, even when trained on suboptimal data.

Method: SPICE uses a deep ensemble to learn a Bayesian prior over Q-values. At test-time, it performs Bayesian updates using in-context information. An Upper-Confidence Bound (UCB)-based online inference strategy promotes exploration to correct for potentially poor priors from suboptimal training data.

Result: SPICE achieves regret-optimal behavior in stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. Empirical evaluations on bandit and control benchmarks demonstrate that SPICE makes near-optimal decisions on unseen tasks, significantly reduces regret compared to prior ICRL and meta-RL methods, and adapts rapidly while remaining robust under distribution shift.

Conclusion: SPICE enables effective in-context adaptation in reinforcement learning by combining Bayesian prior learning with UCB-guided exploration, making it robust to poor training data and highly effective in unseen environments, thus advancing practical deployment of ICRL.

Abstract: In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.

</details>


### [148] [Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning](https://arxiv.org/abs/2601.03032)
*Vidhi Rathore*

Main category: cs.LG

TL;DR: 提出因果流形公平性（CMF）框架，通过结合因果推断与几何深度学习，使潜在表示在敏感属性的反事实干预下保持局部黎曼几何不变，从而解决传统方法忽略数据生成结构的问题。


<details>
  <summary>Details</summary>
Motivation: 标准机器学习方法将数据视为高维空间中的静态点，忽略了数据背后的生成结构；敏感属性不仅改变数据分布，还因果地扭曲数据流形的几何结构，因此需要新的公平性框架来处理这种几何上的不公平。

Method: CMF通过约束解码器的雅可比矩阵和海森矩阵，确保在敏感属性的反事实干预下，潜在空间的度量张量和曲率保持不变，从而实现对几何扭曲的去耦合。

Result: 在合成结构性因果模型（SCMs）上验证，CMF能有效分离敏感属性引起的几何扭曲，同时保持任务性能，并通过几何度量量化了公平性与效用之间的权衡。

Conclusion: CMF为公平性提供了基于几何结构的新视角，能够在不牺牲任务效用的前提下，实现对敏感属性导致的几何偏移的有效校正。

Abstract: Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.

</details>


### [149] [When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability](https://arxiv.org/abs/2601.03047)
*Raphael Ronge,Markus Maier,Frederick Eberhardt*

Main category: cs.LG

TL;DR: 本文对Anthropic提出的机制可解释性方法进行了初步压力测试，通过开源的稀疏自编码器（SAEs）复现其在Llama 3.1上的主要结果。虽然成功再现了基础特征提取与控制能力，但发现特征操控存在显著脆弱性，对层选择、操控强度和上下文高度敏感，并存在非标准激活行为，难以区分主题相似的特征。尽管SAE可解释性在特定案例中表现令人信服，但当前方法缺乏系统可靠性，无法满足安全关键应用需求。研究呼吁将重点从内部表征可解释性转向模型输出的可靠预测与控制。


<details>
  <summary>Details</summary>
Motivation: 评估Anthropic提出的机制可解释性方法在实际应用中的有效性与可靠性，尤其是在安全关键场景下的适用性。

Method: 使用开源稀疏自编码器（SAEs）对Llama 3.1模型进行复现，分析特征提取与特征操控的表现，考察其在不同层、不同操控强度和上下文条件下的稳定性，并识别非标准激活模式及特征区分难度。

Result: 成功复现基本特征提取与操纵功能，但特征操控表现出对层选择、操控幅度和上下文的高度敏感性；观察到非标准激活行为，且难以区分主题相近的特征；表明当前方法在系统可靠性方面仍存在重大缺陷。

Conclusion: 当前机制可解释性方法虽有潜力，但尚未达到安全应用所需的可靠性水平。应将研究重心从内部表征可解释性转向对模型输出的可靠预测与控制，以应对AI安全中的根本挑战。

Abstract: Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.

</details>


### [150] [Joint Encoding of KV-Cache Blocks for Scalable LLM Serving](https://arxiv.org/abs/2601.03067)
*Joseph Kampeas,Emir Haleva*

Main category: cs.LG

TL;DR: 提出联合编码KV缓存块的方法，通过融合不同请求和输入片段中的相似块生成共享表示，保持标准缓存结构，有效缓解内存瓶颈，实现高达4.38倍的压缩比且精度损失可忽略，在单机vLLM基准上提升约40%的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法依赖固定启发式规则、破坏张量布局或需要专用计算，限制了可扩展性和部署灵活性。为解决大语言模型在高并发场景下的内存瓶颈问题，亟需一种高效、通用且无需特殊硬件支持的压缩方案。

Method: 提出联合编码策略，将来自多个请求和输入块的相似KV缓存块进行融合，生成共享表示，同时维持标准缓存数据结构，避免对模型推理流程造成干扰。结合泊松过程建模分析其率失真权衡关系。

Result: 在多种LLM和基准测试中实现最高达4.38倍的KV缓存压缩，精度几乎无损；在真实LLM服务场景下，单机vLLM基准上推理吞吐量提升约40%，显著优于现有结构化与自适应压缩方法。

Conclusion: 所提出的联合编码方法有效缓解了KV缓存的内存瓶颈，支持高并发推理，无需专用硬件，具备良好的通用性与可扩展性，为大规模语言模型的实际部署提供了有力支撑。

Abstract: Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment.
  We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\sim$40\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion  kv_joint_encoding.

</details>


### [151] [ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning](https://arxiv.org/abs/2601.03093)
*Tuc Nguyen,Thai Le*

Main category: cs.LG

TL;DR: ATLAS 是一种任务特定的自适应测试时潜在引导方法，通过轻量级外部潜在验证器动态调整推理过程中的引导策略，实现每例每步的自适应调节，显著提升大语言模型在数学推理任务中的准确率并减少推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有激活和潜在引导方法依赖固定的引导策略和静态干预强度，导致在不同问题实例中鲁棒性不足，易出现过度或不足引导的问题。

Method: 提出 ATLAS 框架，利用外部轻量级潜在验证器，在推理时根据中间隐藏状态预测推理质量，并动态决定是否及如何应用引导，实现细粒度、低开销的自适应控制。

Result: 在多个数学推理基准上，ATLAS 均优于原始解码和固定引导基线，不仅提升准确率，还大幅降低测试时的 token 使用量。

Conclusion:  verifier 引导的潜在自适应是一种高效且可扩展的机制，可在不牺牲解题质量的前提下有效调控大语言模型的推理效率。

Abstract: Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task- specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.

</details>


### [152] [From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding](https://arxiv.org/abs/2601.03098)
*Meghna Roy Chowdhury,Shreyas Sen,Yi Ding*

Main category: cs.LG

TL;DR: MyoText is a hierarchical sEMG-to-text framework that decodes muscle signals into text through physiologically meaningful stages: finger activation classification using CNN-BiLSTM-Attention, letter inference with ergonomic priors, and sentence reconstruction via a fine-tuned T5 transformer. It achieves 85.4% finger accuracy, 5.4% CER, and 6.5% WER on 30 users, outperforming prior methods by leveraging biological and linguistic constraints.


<details>
  <summary>Details</summary>
Motivation: Existing sEMG-to-text methods directly map signals to letters, which limits accuracy and scalability. MyoText addresses this by incorporating physiological and ergonomic knowledge to guide decoding, reducing ambiguity and improving performance in keyboard-free input systems for wearable and mixed-reality applications.

Method: MyoText uses a three-stage pipeline: (1) multichannel sEMG signals are classified into finger activations using a CNN-BiLSTM-Attention model; (2) ergonomic typing priors are applied to infer likely letters based on hand posture and key layout; (3) a fine-tuned T5 transformer reconstructs full sentences, leveraging contextual and linguistic understanding.

Result: On the emg2qwerty dataset with 30 users, MyoText achieves 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER), significantly outperforming baseline methods. The modular design enhances both accuracy and interpretability.

Conclusion: MyoText establishes a principled, biologically inspired pathway from neuromuscular signals to natural language, offering a scalable and accurate solution for keyboard-free text input in future wearable and augmented reality systems. Its integration of physiology, ergonomics, and transformer-based reasoning paves the way for seamless neural interfaces in ubiquitous computing.

Abstract: Surface electromyography (sEMG) provides a direct neural interface for decoding muscle activity and offers a promising foundation for keyboard-free text input in wearable and mixed-reality systems. Previous sEMG-to-text studies mainly focused on recognizing letters directly from sEMG signals, forming an important first step toward translating muscle activity into text. Building on this foundation, we present MyoText, a hierarchical framework that decodes sEMG signals to text through physiologically grounded intermediate stages. MyoText first classifies finger activations from multichannel sEMG using a CNN-BiLSTM-Attention model, applies ergonomic typing priors to infer letters, and reconstructs full sentences with a fine-tuned T5 transformer. This modular design mirrors the natural hierarchy of typing, linking muscle intent to language output and reducing the search space for decoding. Evaluated on 30 users from the emg2qwerty dataset, MyoText outperforms baselines by achieving 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER). Beyond accuracy gains, this methodology establishes a principled pathway from neuromuscular signals to text, providing a blueprint for virtual and augmented-reality typing interfaces that operate entirely without physical keyboards. By integrating ergonomic structure with transformer-based linguistic reasoning, MyoText advances the feasibility of seamless, wearable neural input for future ubiquitous computing environments.

</details>


### [153] [Time-Aware Synthetic Control](https://arxiv.org/abs/2601.03099)
*Saeyoung Rho,Cyrus Illick,Samhitha Narasipura,Alberto Abadie,Daniel Hsu,Vishal Misra*

Main category: cs.LG

TL;DR: TASC 是一种时间感知的合成控制方法，通过状态空间模型和卡尔曼滤波/鲁特-通-斯特里贝尔平滑器，利用时间序列中的趋势结构，在强趋势和高噪声场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有合成控制方法忽略时间序列的顺序信息，无法充分利用强趋势下的时间结构，因此需要一种能捕捉时间动态的方法。

Method: 提出 TASC，结合状态空间模型与低秩信号结构，使用期望最大化算法拟合生成模型，并通过卡尔曼滤波和平滑器进行反事实推断。

Result: 在模拟和真实数据集（如政策评估、体育预测）上，TASC 在强时间趋势和高噪声条件下表现更优，具有更高的估计准确性和稳健性。

Conclusion: TASC 有效利用了时间序列中的趋势信息，显著提升了合成控制在复杂时间动态场景下的性能，为因果推断提供了更可靠的方法。

Abstract: The synthetic control (SC) framework is widely used for observational causal inference with time-series panel data. SC has been successful in diverse applications, but existing methods typically treat the ordering of pre-intervention time indices interchangeable. This invariance means they may not fully take advantage of temporal structure when strong trends are present. We propose Time-Aware Synthetic Control (TASC), which employs a state-space model with a constant trend while preserving a low-rank structure of the signal. TASC uses the Kalman filter and Rauch-Tung-Striebel smoother: it first fits a generative time-series model with expectation-maximization and then performs counterfactual inference. We evaluate TASC on both simulated and real-world datasets, including policy evaluation and sports prediction. Our results suggest that TASC offers advantages in settings with strong temporal trends and high levels of observation noise.

</details>


### [154] [One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling](https://arxiv.org/abs/2601.03111)
*Yiyuan Li,Zhen Huang,Yanan Wu,Weixun Wang,Xuefeng Li,Yijia Luo,Wenbo Su,Bo Zheng,Pengfei Liu*

Main category: cs.LG

TL;DR: 本文挑战了大语言模型强化学习（RL）对大量高质量数据的依赖，提出一种名为'博学学习'（polymath learning）的一次性学习框架。通过设计一个精心构造的单一训练样本，该方法在数学推理上表现出显著跨学科影响，能有效提升物理、化学、生物等多个领域的性能。研究发现，具有特定推理特征的数学样本最适合作为‘博学样本’；而人工合成的多学科融合样本优于自然生成的单领域样本。最终，该方法在多个推理基准测试中表现优于使用更大数据集的训练方式，表明样本质量与设计比数量更为关键，推动从数据量增长转向样本工程的范式转变。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的强化学习方法通常依赖数千个甚至更多的高质量样本，但本文质疑这一高数据需求的必要性，旨在探索是否可以通过极少量甚至单一高质量样本实现强大的跨领域推理能力提升。

Method: 提出‘博学学习’框架，设计并验证一个经过工程优化的单一训练样本，该样本融合多学科元素，特别强调数学推理能力，并通过强化学习在多种任务上进行评估。对比不同样本类型（单一学科、合成多学科）和数据规模的效果。

Result: 一个精心设计的数学推理样本可显著提升模型在物理、化学、生物等多领域的表现；人工合成的多学科样本优于自然出现的单样本；在多个推理基准上，该方法的表现超过使用更大数据集的传统训练方式。

Conclusion: 样本的质量与设计比数量更为关键，未来应从单纯增加数据量转向精准的样本工程，即‘样本工程’范式的转变，以更高效地释放大语言模型的推理潜力。

Abstract: The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.

</details>


### [155] [PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback](https://arxiv.org/abs/2601.03149)
*Dehao Yuan,Tyler Farnan,Stefan Tesliuc,Doron L Bergman,Yulun Wu,Xiaoyu Liu,Minghui Liu,James Montgomery,Nam H Nguyen,C. Bayan Bruss,Furong Huang*

Main category: cs.LG

TL;DR: 提出PersonaLedger，一种结合大语言模型与可配置程序引擎的合成交易数据生成框架，通过闭环交互实现行为多样性与财务规则一致性，生成3000万条交易记录并构建公开数据集与基准测试，支持金融AI中预测与异常检测模型的严谨评估。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据生成方法在行为多样性和逻辑正确性之间难以兼顾：规则驱动方法缺乏人类行为的丰富性，学习型方法（如GAN）虽能捕捉相关性但常违反硬性财务约束且需依赖私有数据训练。隐私法规限制真实交易数据获取，阻碍了金融AI的开放研究。

Method: 采用大语言模型根据用户画像生成交易行为，并通过可配置程序引擎实时验证和更新用户状态，强制执行财务规则；两者形成闭环：引擎在每次事件后返回上下文感知的'nextprompt'，引导语言模型生成符合规则的下一步操作。

Result: 成功生成包含23,000名用户、3000万条交易记录的公开数据集，建立两个任务基准（流动性不足分类与身份盗用分割），验证了生成数据在真实性、多样性与合规性上的优势，支持对金融预测与异常检测模型的可靠评估。

Conclusion: PersonaLedger提供了一个真实、隐私保护且可复现的资源，涵盖代码、规则和生成日志，显著推动金融AI领域的创新与严谨评估。

Abstract: Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware "nextprompt" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.

</details>


### [156] [Prompt-Counterfactual Explanations for Generative AI System Behavior](https://arxiv.org/abs/2601.03156)
*Sofie Goethals,Foster Provost,João Sedoc*

Main category: cs.LG

TL;DR: 本文研究了提示词如何影响大语言模型生成内容的特定特征（如毒性、负面情绪、政治偏见），提出了一种适用于非确定性生成式AI系统的反事实解释框架，并开发了生成提示反事实解释（PCE）的算法。通过三个案例研究验证了该方法在抑制不良输出和增强红队测试中的有效性，为生成式AI的提示可解释性奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统在现实世界中的广泛应用，决策者需要理解其行为背后的原因，特别是输入提示如何导致特定输出特征，以满足透明度和问责制的要求。

Method: 采用可解释人工智能中的反事实解释技术，针对生成式AI系统的非确定性特点，提出一个灵活的适应性框架，并设计算法生成提示反事实解释（PCE）。

Result: 三个案例研究验证了该方法在识别导致毒性、政治倾向和负面情绪等输出特征的提示因素方面的有效性；同时展示了其在优化提示工程和提升红队测试效率上的应用价值。

Conclusion: 本研究为生成式AI的提示可解释性提供了坚实基础，这一能力将在高风险任务和日益严格的监管要求下变得不可或缺。

Abstract: As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.

</details>


### [157] [Rapid Augmentations for Time Series (RATS): A High-Performance Library for Time Series Augmentation](https://arxiv.org/abs/2601.03159)
*Wadie Skaf,Felix Kern,Aryamaan Basu Roy,Tejas Pradhan,Roman Kalkreuth,Holger Hoos*

Main category: cs.LG

TL;DR: RATS (Rapid Augmentations for Time Series) is a high-performance time series augmentation library written in Rust with Python bindings (RATSpy), designed to overcome the performance bottlenecks of existing Python-based libraries. It supports various augmentation methods including basic transformations, frequency-domain operations, and time warping, with a unified pipeline interface and built-in parallelization. Benchmarking on 143 datasets shows RATSpy achieves an average 74.5% speedup over tsaug (up to 94.8% on large datasets) and reduces peak memory usage by up to 47.9%.


<details>
  <summary>Details</summary>
Motivation: Existing time series augmentation libraries in Python suffer from performance degradation as dataset sizes grow, limiting their use in large-scale, production environments. There is a need for faster and more memory-efficient augmentation tools to support robust deep learning model training, especially when labelled data is scarce.

Method: RATS is implemented in Rust for high performance, with Python bindings (RATSpy). It provides a unified pipeline interface supporting multiple augmentation techniques—basic transformations, frequency-domain operations, and time warping—with built-in parallelization to improve efficiency.

Result: RATSpy outperforms tsaug across 143 datasets: average 74.5% speedup (up to 94.8% on large datasets) and up to 47.9% reduction in peak memory usage, demonstrating significant gains in both speed and memory efficiency.

Conclusion: RATS, implemented in Rust with Python compatibility, offers a scalable, high-performance solution for time series augmentation, making it suitable for large-scale machine learning applications where speed and memory efficiency are critical.

Abstract: Time series augmentation is critical for training robust deep learning models, particularly in domains where labelled data is scarce and expensive to obtain. However, existing augmentation libraries for time series, mainly written in Python, suffer from performance bottlenecks, where running time grows exponentially as dataset sizes increase -- an aspect limiting their applicability in large-scale, production-grade systems. We introduce RATS (Rapid Augmentations for Time Series), a high-performance library for time series augmentation written in Rust with Python bindings (RATSpy). RATS implements multiple augmentation methods spanning basic transformations, frequency-domain operations and time warping techniques, all accessible through a unified pipeline interface with built-in parallelisation. Comprehensive benchmarking of RATSpy versus a commonly used library (tasug) on 143 datasets demonstrates that RATSpy achieves an average speedup of 74.5\% over tsaug (up to 94.8\% on large datasets), with up to 47.9\% less peak memory usage.

</details>


### [158] [Dynamic Hyperparameter Importance for Efficient Multi-Objective Optimization](https://arxiv.org/abs/2601.03166)
*Daphne Theodorakopoulos,Marcel Wever,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文提出一种动态优化方法，通过根据目标权衡动态优先考虑最重要的超参数，从而加速多目标优化（MOO）的收敛并获得更优解。该方法结合HyperSHAP计算的超参数重要性（HPI），利用ParEGO算法生成的目标权重自适应调整配置空间，固定不重要的超参数以聚焦关键参数。在PyMOO和YAHPO-Gym上的实验表明，该方法在收敛速度和帕累托前沿质量上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有多目标优化方法通常将所有超参数视为同等重要，但超参数重要性（HPI）随目标权衡变化而显著不同。忽视这一差异限制了优化效率与性能。因此，需要一种能动态识别并优先优化关键超参数的方法，以提升多目标机器学习模型选择的效率与效果。

Method: 提出一种基于超参数重要性（HPI）的动态优化框架，使用HyperSHAP计算各超参数对多目标性能的影响，并结合ParEGO算法产生的目标权重，自适应地固定低重要性超参数，缩小搜索空间，集中优化高重要性超参数。

Result: 在多个任务（PyMOO和YAHPO-Gym）上的实验结果表明，所提方法在收敛速度和帕累托前沿质量方面均显著优于现有基线方法，验证了其有效性和优越性。

Conclusion: 本研究证明，通过动态引入超参数重要性指导搜索过程，可显著提升多目标超参数优化的效率与解决方案质量，为复杂机器学习模型选择提供了新的有效路径。

Abstract: Choosing a suitable ML model is a complex task that can depend on several objectives, e.g., accuracy, model size, fairness, inference time, or energy consumption. In practice, this requires trading off multiple, often competing, objectives through multi-objective optimization (MOO). However, existing MOO methods typically treat all hyperparameters as equally important, overlooking that hyperparameter importance (HPI) can vary significantly depending on the trade-off between objectives. We propose a novel dynamic optimization approach that prioritizes the most influential hyperparameters based on varying objective trade-offs during the search process, which accelerates empirical convergence and leads to better solutions. Building on prior work on HPI for MOO post-analysis, we now integrate HPI, calculated with HyperSHAP, into the optimization. For this, we leverage the objective weightings naturally produced by the MOO algorithm ParEGO and adapt the configuration space by fixing the unimportant hyperparameters, allowing the search to focus on the important ones. Eventually, we validate our method with diverse tasks from PyMOO and YAHPO-Gym. Empirical results demonstrate improvements in convergence speed and Pareto front quality compared to baselines.

</details>


### [159] [Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions](https://arxiv.org/abs/2601.03173)
*Sumit S. Shevtekar,Chandresh K. Maurya,Gourab Sil,Subasish Das*

Main category: cs.LG

TL;DR: 本研究构建了包含12.9万+条多变量时间序列数据的大规模数据集，涵盖51名骑手在无、低、高时间压力条件下的153次骑行。实证分析显示，高时间压力下速度提升48%，速度波动增加36.4%，危险转弯增加58%，急刹车增加36%，后制动力增加50%。为此提出MotoTimePressure模型，结合卷积预处理、双阶段时序注意力与特征重校准机制，准确率达91.53%，ROC AUC达98.93%，优于8个基线模型。利用预测的时间压力作为特征，可将基于Informer的碰撞风险预测准确率从91.25%提升至93.51%，接近理想性能（93.72%）。阈值化的时间压力状态可用于识别骑手认知压力，支持主动式ITS干预，如自适应警报、触觉反馈、车路协同信号与速度引导，契合安全系统方法论，促进两轮车出行安全。


<details>
  <summary>Details</summary>
Motivation: 时间压力显著影响两轮车骑手的冒险行为与事故风险，但其在智能交通系统中的预测仍缺乏深入研究。

Method: 构建大规模多源异构时间序列数据集，提出MotoTimePressure深度学习模型，融合卷积处理、双阶段时序注意力与Squeeze-and-Excitation特征重校准机制，用于时间压力分类，并应用于碰撞风险预测与阈值划分。

Result: MotoTimePressure模型达到91.53%准确率和98.93% ROC AUC，显著优于八种基线模型；基于预测时间压力的碰撞风险预测准确率从91.25%提升至93.51%，接近理想性能；阈值化时间压力状态可有效反映骑手认知负荷，支持主动式智能交通干预。

Conclusion: 本研究通过构建高质量数据集与先进深度学习模型，实现了对两轮车骑手时间压力的有效识别与预测，为实现基于安全系统理念的主动式智能交通干预提供了关键技术支撑，有助于提升两轮车出行安全性。

Abstract: Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.

</details>


### [160] [Decentralized Autoregressive Generation](https://arxiv.org/abs/2601.03184)
*Stepan Maschan,Haoxuan Qu,Jun Liu*

Main category: cs.LG

TL;DR: 本文提出了一种自回归生成去中心化的理论分析，定义了去中心化离散流匹配目标，通过将概率生成速度表示为专家流的线性组合。实验表明，在多种基准测试中，去中心化与中心化训练设置对于多模态语言模型具有等价性。具体比较了LLaVA和InternVL 2.5-1B两种范式，后者使用固定CLIP视觉编码器，并在指令微调阶段进行全参数微调（ViT+MLP+LLM）。


<details>
  <summary>Details</summary>
Motivation: 为了理解自回归生成中的去中心化机制，探索其在多模态语言模型中的有效性与可扩展性，提升训练效率并保持性能一致性。

Method: 提出去中心化离散流匹配目标，将概率生成速度建模为专家流的线性组合，并通过实验验证去中心化与中心化训练的等价性。

Result: 实验结果表明，去中心化训练在多个基准测试中与中心化训练表现相当，验证了该方法的有效性与通用性。

Conclusion: 去中心化训练在多模态语言模型中具有可行性与优越性，能够实现与中心化训练相当的性能，同时提升训练灵活性与可扩展性。

Abstract: We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrat- ing the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and per- forms full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.

</details>


### [161] [Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression](https://arxiv.org/abs/2601.03195)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率域软化算子的稀疏知识蒸馏统一理论框架，通过算子层面的分析，揭示了稀疏学生模型优于稠密教师模型的条件、多阶段剪枝在函数空间中的同伦路径机制、多阶段蒸馏的收敛性保证（$O(1/n)$速率）以及在容量约束下产生相同学生模型的不同算子等价类。框架基于排名保持、连续性、熵单调性、恒等性和边界行为等公理定义概率域软化算子，并证明多个非等价算子族满足这些公理，所有学习理论保证在该算子类中一致成立，为黑箱教师蒸馏、部分访问场景（如top-$k$截断和仅文本输出）及隐私保护模型压缩提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏研究多关注具体实现或等价关系，缺乏对稀疏蒸馏中核心机制的统一理论解释，尤其在多阶段剪枝、不同软化算子选择及其泛化性能方面缺乏系统分析。本文旨在构建一个算子级的理论框架，以解释为何稀疏学生能超越稠密教师、为何迭代压缩优于一次性剪枝，并为各种实际应用提供通用理论支撑。

Method: 提出概率域软化算子的公理化定义，包括排名保持、连续性、熵单调性、恒等性和边界行为；基于此建立算子无关的偏差-方差分解、函数空间中的同伦路径建模、多阶段蒸馏的收敛性分析，以及等价类刻画；所有结论均在算子类上统一成立，不依赖具体实现。

Result: 建立了四个核心理论成果：(i) 算子无关的偏差-方差分解，揭示稀疏学生优于教师的条件；(ii) 多阶段剪枝的同伦路径解释，说明为何迭代压缩更有效；(iii) $n$-阶段蒸馏的$O(1/n)$收敛率与参数依赖显式表达；(iv) 识别出在容量受限下生成相同学生模型的不同算子等价类。所有结果对满足公理的算子类具有普遍适用性。

Conclusion: 本文提出的统一理论框架为稀疏知识蒸馏提供了坚实的数学基础，不仅解释了现有实践现象，还为黑箱蒸馏、部分访问设置和隐私保护压缩等现实挑战提供了可信赖的理论依据。框架的通用性表明其适用于多种软化算子设计，推动了知识蒸馏从经验驱动向理论指导的演进。

Abstract: We develop a unified theoretical framework for sparse knowledge distillation based on probability-domain softening operators. While the equivalence $p^{1/T} \propto \mathrm{softmax}(z/T)$ is well known, our contribution is an operator-level analytical framework built on this foundation rather than the equivalence itself.
  The framework comprises four core components: (i) operator-agnostic bias--variance decompositions that characterize when sparse students outperform dense teachers, (ii) a homotopy path formalization of multi-stage pruning in function space explaining why iterative compression succeeds where one-shot pruning fails, (iii) convergence guarantees establishing $O(1/n)$ rates for $n$-stage distillation with explicit parameter dependence, and (iv) equivalence class characterizations identifying distinct probability-domain operators that yield identical student models under capacity constraints.
  We introduce an axiomatic definition of probability-domain softening operators based on ranking preservation, continuity, entropy monotonicity, identity, and boundary behavior, and show that multiple non-equivalent operator families satisfy these axioms. All learning-theoretic guarantees are shown to hold uniformly across this operator class, independent of implementation details. These results provide theoretical grounding for black-box teacher distillation, partial-access settings such as top-$k$ truncation and text-only outputs, and privacy-preserving model compression.

</details>


### [162] [Empowering Reliable Visual-Centric Instruction Following in MLLMs](https://arxiv.org/abs/2601.03198)
*Weilei He,Feng Ju,Zhiyuan Fan,Rui Min,Minhao Cheng,Yi R. Fung*

Main category: cs.LG

TL;DR: 提出VC-IFEval基准和相应数据集，评估多模态大模型在视觉与文本指令共同约束下的指令遵循能力，通过引入视觉依赖性约束提升评估的严谨性和细致度，并通过微调显著提高模型在视觉指令遵循上的准确率和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准主要关注文本模态的口头指令，忽视了视觉模态中隐含的语义丰富约束，导致对多模态大模型指令遵循能力的评估不全面。

Method: 设计并构建包含视觉依赖性约束的多模态指令数据集，提出VC-IFEval基准，用于系统评估多模态大模型在视觉与文本双重约束下的指令遵循表现，并通过微调优化模型性能。

Result: 在代表性多模态大模型上进行广泛评估，验证了所提方法的有效性，显著提升了模型在视觉指令遵循任务中的准确率与一致性，揭示了当前模型的优势与局限。

Conclusion: VC-IFEval为多模态大模型的指令遵循能力评估提供了更全面、精细的基准，有助于推动模型在真实多模态场景下的意图对齐能力发展。

Abstract: Evaluating the instruction-following (IF) capabilities of Multimodal Large Language Models (MLLMs) is essential for rigorously assessing how faithfully model outputs adhere to user-specified intentions. Nevertheless, existing benchmarks for evaluating MLLMs' instruction-following capability primarily focus on verbal instructions in the textual modality. These limitations hinder a thorough analysis of instruction-following capabilities, as they overlook the implicit constraints embedded in the semantically rich visual modality. To address this gap, we introduce VC-IFEval, a new benchmark accompanied by a systematically constructed dataset that evaluates MLLMs' instruction-following ability under multimodal settings. Our benchmark systematically incorporates vision-dependent constraints into instruction design, enabling a more rigorous and fine-grained assessment of how well MLLMs align their outputs with both visual input and textual instructions. Furthermore, by fine-tuning MLLMs on our dataset, we achieve substantial gains in visual instruction-following accuracy and adherence. Through extensive evaluation across representative MLLMs, we provide new insights into the strengths and limitations of current models.

</details>


### [163] [Counterfactual Fairness with Graph Uncertainty](https://arxiv.org/abs/2601.03203)
*Davi Valério,Chrysoula Zerva,Mariana Pinto,Ricardo Santos,André Carreiro*

Main category: cs.LG

TL;DR: 提出CF-GU方法，通过引入因果图不确定性来改进基于因果框架的公平性审计（CF），利用因果发现算法生成多个合理的有向无环图（DAGs），并用香农熵量化图不确定性，提供CF度量的置信区间。在合成数据和真实数据（COMPAS、Adult）上的实验表明，该方法能有效识别已知偏差，并在有限领域知识下仍保持高置信度。


<details>
  <summary>Details</summary>
Motivation: 现有基于因果框架的公平性审计（CF）依赖单一因果图，但现实场景中因果关系通常不确定，导致审计结论不可靠。需要一种能处理因果图不确定性的方法以提升模型偏见评估的可信度。

Method: 1) 在领域知识约束下使用因果发现算法进行自助采样，生成一组可能的有向无环图（DAGs）；2) 采用归一化香农熵衡量这些图之间的不确定性；3) 基于图集合计算CF度量的置信边界，实现对模型偏见的稳健评估。

Result: 在合成数据上验证了不同领域假设对审计结果的影响；在真实数据集COMPAS和Adult上成功识别出已知偏见，且即使在最小领域知识约束下也具有高置信度，证明了方法的有效性和鲁棒性。

Conclusion: CF-GU通过整合因果图不确定性，显著提升了机器学习模型偏见评估的可靠性与可解释性，为构建可信、稳健的ML系统提供了新工具。

Abstract: Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints.

</details>


### [164] [From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence](https://arxiv.org/abs/2601.03220)
*Marc Finzi,Shikai Qiu,Yiding Jiang,Pavel Izmailov,J. Zico Kolter,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: This work introduces epiplexity—a measure of information tailored to computationally bounded observers—that reveals how deterministic transformations can create useful information, why data order matters, and how likelihood modeling can generate more complex models. It enables better data selection and transformation strategies, offering a new foundation beyond traditional information theory.


<details>
  <summary>Details</summary>
Motivation: The paper addresses fundamental questions about whether new information can be created from data through deterministic transformations, and whether useful information in data can be evaluated independently of downstream tasks. Traditional frameworks like Shannon information and Kolmogorov complexity fail because they assume unlimited computational power and do not capture the practical value of information for computationally bounded observers.

Method: The authors introduce 'epiplexity' as a new measure of information that quantifies what computationally bounded observers can learn from data. Epiplexity captures structural content while excluding time-bounded entropy (e.g., pseudorandomness or chaotic behavior). The framework demonstrates how computation can create information, how data ordering affects learnable content, and how likelihood modeling can generate more complex models than the original data-generating process.

Result: Epiplexity successfully distinguishes between data sources, correlates with downstream performance, and identifies effective dataset interventions that improve out-of-distribution generalization. It provides a theoretical basis for data selection, generation, and transformation, offering a new perspective that contrasts with traditional model selection principles.

Conclusion: Information can indeed be created through computation and is sensitive to data ordering and structure. Epiplexity offers a practical and theoretically grounded way to evaluate data quality and utility without relying on specific downstream tasks, thus enabling better design of learning systems.

Abstract: Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.

</details>


### [165] [PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters](https://arxiv.org/abs/2601.03237)
*Javier Salazar Cavazos*

Main category: cs.LG

TL;DR: PET-TURTLE 提出一种改进的无监督聚类方法，通过引入幂律先验和稀疏逻辑斯蒂回归来处理数据不平衡问题，在合成与真实数据上均提升了聚类准确率，减少了对少数类的过预测现象。


<details>
  <summary>Details</summary>
Motivation: TURTLE 聚类算法在数据不平衡时表现不佳，因假设簇是平衡的，导致非理想超平面和更高的聚类误差；因此需要改进以适应不平衡数据分布。

Method: 提出 PET-TURTLE，通过引入幂律先验扩展损失函数以处理不平衡数据，并在标签生成中使用稀疏逻辑斯蒂回归，从而缩小搜索空间并提升准确性。

Result: 实验表明，PET-TURTLE 在不平衡数据源上显著提高聚类准确率，有效防止对少数类的过预测，整体聚类性能优于 TURTLE。

Conclusion: PET-TURTLE 通过改进成本函数和优化标签过程，有效解决了数据不平衡带来的聚类偏差问题，适用于更广泛的实际场景。

Abstract: Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.

</details>
