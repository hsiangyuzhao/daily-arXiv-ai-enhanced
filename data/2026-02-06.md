<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 48]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.LG](#cs.LG) [Total: 116]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 该论文研究了视觉-语言-动作（VLA）模型在机器人操作任务中的表现，发现成功执行与失败执行之间存在显著的视觉依赖性差异。为解决视觉-动作错位问题，作者提出了一种通过偏好优化和潜在空间蒸馏来增强视觉条件化的训练框架，无需修改架构或额外数据收集即可提升模型性能，适用于离散和连续动作设置。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在扩展到动作空间时存在视觉-动作错位问题，导致动作预测对当前视觉状态依赖弱，影响任务可靠性。需要强化视觉条件化以提高动作输出的准确性与一致性。

Method: 首先在跟踪任务上利用偏好优化对齐动作预测与视觉输入，然后通过监督微调中的潜在空间蒸馏将增强的视觉对齐能力迁移至指令跟随任务。

Result: 所提方法在不改变模型结构或增加数据的情况下，显著提升了离散OpenVLA的视觉条件化能力和任务表现，并在连续动作设置OpenVLA-OFT中也取得了稳定改进。

Conclusion: 通过显式强化视觉条件化，VLA模型的动作预测更加依赖于当前视觉状态，从而提升了任务成功率与鲁棒性，验证了视觉依赖性在成功执行中的关键作用。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [2] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文探讨了基于图像的膳食评估中准确估算食物份量的不同策略，重点关注从二维图像推断三维食物尺寸的挑战，并分析了利用深度图、多视角输入及基于模型的方法（如模板匹配）等解决方案。深度学习在仅使用单目图像或结合辅助输入的情况下，显著提升了食物份量预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 图像在膳食评估中的应用对于慢性病和肥胖的预防与管理至关重要，但其主要挑战在于如何从二维图像准确估计食物的三维尺寸。

Method: 通过分析现有方法，包括使用深度图、多视角输入、模板匹配以及深度学习技术，研究不同策略在食物份量估算中的表现。

Result: 深度学习方法能够有效提升仅依赖单目图像或结合辅助信息时的食物份量预测精度，为解决二维图像到三维尺寸转换问题提供了可行路径。

Conclusion: 综合多种技术手段，特别是深度学习与辅助输入的结合，是实现高精度食物份量估计的关键方向。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [3] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出一种名为视觉概念排序（VCR）的方法，用于识别大型多模态模型（LMMs）中的重要视觉概念，并应用于医疗任务中，如皮肤癌病变分类。研究发现，模型在不同人口统计学子群体间的表现存在意外差异，通过VCR方法可生成关于视觉特征依赖关系的假设，并经人工干预验证。


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，确保机器学习模型的可靠性至关重要。现有模型在面对不同人群时可能存在性能差距，需要有效的审计方法来揭示这些缺陷。

Method: 提出Visual Concept Ranking（VCR）方法，通过分析模型对医学图像的响应，识别关键视觉概念，并结合人工干预验证其假设。

Result: VCR成功识别出模型在不同人群中的表现差异与特定视觉特征之间的关联，揭示了模型潜在的偏见和不足。

Conclusion: 该研究证明了VCR在揭示大型多模态模型在医疗任务中潜在偏差方面的有效性，为提升模型可靠性和公平性提供了新工具。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [4] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: ARGaze提出将在线第一人称视线估计重新构想为序列预测任务，利用自回归解码和有限长度的视线上下文窗口来提升预测性能。该方法在多个基准测试中达到领先水平，证明了利用短期视线历史对鲁棒预测至关重要。


<details>
  <summary>Details</summary>
Motivation: 在线第一人称视线估计缺乏显式的头部或眼睛信号，需从稀疏间接线索（如手物交互、场景显著性）中推断注意力。由于视线在目标导向活动中具有强时间连续性，利用近期视线信息作为先验可显著提升预测准确性。

Method: 提出ARGaze模型，采用Transformer解码器，以当前视觉特征和固定长度的最近视线目标估计作为上下文，进行自回归序列预测。该设计保证因果性，并支持资源受限的流式推理。

Result: 在多个第一人称基准上实现最先进的在线评估性能；消融实验验证了使用有界视线历史的自回归建模对于鲁棒预测的关键作用。

Conclusion: ARGaze通过引入时间连续性先验和自回归序列建模，在无显式头眼信号的条件下显著提升了在线第一人称视线估计的精度与稳定性，具备实际部署潜力。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [5] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 本文系统评估了视觉方法在戴手套手部追踪中的表现，发现现有裸手模型因外观差异导致性能显著下降。为此，提出AirGlove，利用已有手套数据将学习到的手套表征泛化至新手套设计，在少量数据下实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有传感器驱动的手部追踪受限于信号质量和校准精度；而视觉方法虽在裸手追踪中表现良好，但在戴手套场景下因外观差异导致性能下降，亟需针对手套设计进行专门建模与泛化。

Method: 提出AirGlove框架，通过利用已有手套数据训练模型，并将其表征能力迁移至未见过的新手套设计，支持零样本和微调两种设置下的泛化。

Result: 实验表明，AirGlove在多种传感手套上均能有效泛化手部姿态模型，相比现有方法显著提升追踪精度，尤其在新手套设计上表现优异。

Conclusion: 视觉方法在戴手套手部追踪中具有潜力，但需解决外观差异带来的泛化问题。AirGlove通过表征迁移实现了对新手套的有效适应，为未来智能穿戴与远程操作提供了可行方案。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [6] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: 提出SHaSaM，一种基于子模硬样本挖掘的公平性驱动表示学习方法，通过两阶段策略缓解数据不平衡问题并减少敏感属性影响，显著提升模型公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理敏感属性（如种族、年龄、性别）时易受数据不平衡影响，无意中强化敏感属性特征，加剧不公平性并降低性能。

Method: SHaSaM-MINE采用子模子集选择策略挖掘难样本（正负例），缓解数据不平衡；SHaSaM-LEARN引入基于子模条件互信息的组合损失函数，最大化类别间决策边界同时最小化敏感属性影响。

Result: 在CelebA和UTKFace数据集上，SHaSaM实现最高公平性（Equalized Odds提升2.7点）和3.5%准确率提升，且训练周期更短。

Conclusion: SHaSaM通过统一的组合优化框架有效抑制敏感属性学习，显著提升公平性而不牺牲性能，达到当前最优效果。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [7] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散生成的图像到图像转换管道，用于逆转水下图像的退化问题，如对比度降低、模糊和颜色失真。通过引入合成退化流程，在少量高质量图像数据集上训练模型，实现了在512x768分辨率下的高感知一致性与强泛化能力，仅使用约2.5k图像从零开始训练，模型参数量约为11M。


<details>
  <summary>Details</summary>
Motivation: 水下摄影面临对比度下降、空间模糊和波长依赖性颜色失真等固有挑战，导致海洋生物的生动性被掩盖，摄影师需进行繁重的后期处理来校正这些退化现象。

Method: 开发一种基于扩散生成的图像到图像转换方法，通过合成退化流程模拟真实水下环境中的图像退化，并训练模型学习逆向修复这些退化效果。

Result: 该方法在512x768分辨率下表现出高感知一致性与强泛化能力，仅用约2.5k张图像从头训练，模型参数量约为11M，有效提升了水下图像的恢复质量。

Conclusion: 所提出的扩散模型方法能够高效还原水下图像质量，显著减少对复杂后期处理的依赖，为水下摄影提供了轻量且高效的图像增强解决方案。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [8] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出了一种名为Shape Guided Purification (ShapePuri) 的新型防御框架，通过将模型表示与稳定的结构不变量对齐来增强鲁棒性。该框架包含两个组件：形状编码模块（SEM）利用符号距离函数（SDF）提供密集的几何引导，以及全局外观去偏模块（GAD）通过随机变换减轻外观偏差。实验表明，ShapePuri在AutoAttack基准下实现了84.06%的干净准确率和81.64%的鲁棒准确率，首次突破80%阈值，且无需额外计算成本或辅助模块，具有高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有防御策略如对抗训练和净化虽有进展，但基于扩散的净化存在高计算成本和信息损失问题。为解决这些挑战，需要一种高效、低损耗且能保持预测稳定性的新防御方法。

Method: ShapePuri结合形状编码模块（SEM）和全局外观去偏模块（GAD）。SEM利用符号距离函数（SDF）提供密集几何指导，帮助模型关注稳定的结构特征；GAD通过随机变换减少外观偏差，提升模型对对抗样本的鲁棒性。整体设计无需额外模块或显著增加计算开销。

Result: 在AutoAttack评估中，ShapePuri达到84.06%的干净准确率和81.64%的鲁棒准确率，是首个在该基准上超过80%鲁棒准确率的防御框架。同时保持了推理过程中的预测稳定性，且无额外计算负担。

Conclusion: ShapePuri是一种高效、可扩展的对抗防御框架，通过结构引导和外观去偏有效提升了模型鲁棒性，同时避免了传统方法的信息损失和高计算成本，为实际部署提供了可行方案。

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [9] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: PoseGaussian 是一种基于姿态引导的高保真人体新视角合成框架，通过将人体姿态作为结构先验和时间线索，分别优化深度估计和时序一致性，实现端到端可训练的高效渲染，在保持100 FPS实时性能的同时显著提升动态人体场景的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理动态人体场景时，难以有效应对关节运动和严重自遮挡带来的挑战，且通常仅将姿态作为条件或用于图像扭曲，未能充分融合姿态信息以增强几何与时序建模能力。

Method: 提出一个融合姿态信号的双路径设计：一路径将姿态作为结构先验，与颜色编码器结合以优化深度估计；另一路径通过专用姿态编码器提取时序特征，提升帧间一致性；整个系统为全可微分、端到端可训练架构。

Result: 在ZJU-MoCap、THuman2.0及自建数据集上验证，达到当前最佳表现（PSNR 30.86，SSIM 0.979，LPIPS 0.028），并实现实时渲染（100 FPS）。

Conclusion: PoseGaussian通过深度融合姿态信息于几何与时间建模中，显著提升了动态人体新视角合成的质量与鲁棒性，同时保持了高效性，为复杂人体动作的高质量重建提供了新范式。

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [10] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: 提出一种新型视频生成模型作为奖励模型的方法，通过将生成模型转化为能量基模型（EBM），利用对比学习实现对视频质量的精准判别。设计了基于潜在空间扰动的合成负样本（如时间切片、特征交换、帧打乱）以避免模型依赖表面差异，从而学习有意义的时空特征。该方法在仅使用3万条人工标注的情况下，在GenAI-Bench和MonteBench上达到领先性能，相比现有VLM方法减少6至65倍标注量。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型（VLM）的奖励建模方法难以捕捉视频中的细微时序动态，导致视频生成质量评估不准确。因此需要一种能够更好理解视频时序结构的替代方案。

Method: 将先进的视频生成模型重构为能量基模型（EBM），通过对比学习训练其区分高质量与低质量视频；设计多种可控的潜在空间扰动（如时间切片、特征交换、帧打乱）生成合成负样本，迫使模型关注真实的时空语义而非表面伪影。

Result: 在GenAI-Bench和MonteBench基准测试中表现优于现有方法，仅需3万条人工标注即达到最佳性能，相较传统VLM方法减少6至65倍标注需求。

Conclusion: 本研究证明了将视频生成模型直接用作奖励模型的可行性与优越性，尤其在降低标注成本和提升时序感知能力方面具有显著优势，为未来视频生成评估提供了新范式。

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [11] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出一种统一框架，通过在无训练条件下协同整合显式与隐式表示，解决生成式压缩中语义忠实性与感知真实感之间的权衡问题。该方法利用扩散模型对高层语义进行条件控制，并通过反向通道编码隐式传递细节信息，同时引入可插拔编码器以灵活调节失真-感知权衡。实验表明，该框架在多个数据集上均达到最优的率-感知性能，显著优于现有方法，尤其在DISTS BD-Rate指标上超越DiffC 29.92%、19.33%和20.89%。


<details>
  <summary>Details</summary>
Motivation: 现有生成式压缩方法在超低比特率下面临语义忠实性与感知真实感之间的根本性权衡：显式方法保持结构但缺乏细粒度纹理，隐式方法能生成逼真细节但易产生语义漂移。为突破此限制，亟需一种融合两者优势的新范式。

Method: 提出一个无需训练的统一框架，将显式高阶语义作为扩散模型的条件输入，同时使用反向通道编码来隐式传递精细细节；引入可插拔编码器，实现对失真与感知质量之间平衡的灵活调控。

Result: 在Kodak、DIV2K和CLIC2020数据集上，所提方法在DISTS BD-Rate指标上分别超越DiffC 29.92%、19.33%和20.89%，整体表现达到当前最佳，验证了其在超低比特率下的卓越性能。

Conclusion: 该工作成功构建了一个融合显式与隐式表示的训练-free 框架，有效弥合了语义忠实性与感知真实感之间的鸿沟，在极端低比特率场景下实现了前所未有的压缩性能，为未来生成式图像压缩提供了新思路。

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [12] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: E.M.Ground提出一种新的视频大语言模型，用于解决时间视频定位（TVG）中的事件连续性和完整性问题。通过引入<event>特殊标记、Savitzky-Golay平滑和多粒度帧特征聚合，有效提升了事件匹配的准确性和鲁棒性，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精确时间戳，仅通过比较起始与结束帧特征进行匹配，难以捕捉事件的语义连续性与整体性，导致定位模糊。因此需要一种能够综合理解事件全过程的方法。

Method: 提出E.M.Ground模型，采用<event> token聚合事件所有帧的信息以保持语义连贯；使用Savitzky-Golay滤波降低帧间相似度噪声；通过多粒度帧特征融合缓解压缩带来的信息损失，提升匹配可靠性。

Result: 在多个标准数据集上，E.M.Ground均显著超越当前最先进的视频大语言模型，在时间视频定位任务中表现优异。

Conclusion: E.M.Ground通过全局事件感知与精细化时序处理机制，实现了更精准、更一致的事件定位，为视频理解中的时间对齐问题提供了有效解决方案。

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [13] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 提出Multi-view Progressive Adaptation (MPA)方法，通过数据和策略双视角逐步适应少样本分割能力至目标域。数据层面采用渐进式混合增强生成更复杂多样的视图；策略层面设计双链多视图预测，结合串行与并行学习路径，在广泛监督下实现跨视图预测一致性，显著提升目标域适应性能。实验表明，MPA优于现有方法，提升达+7.0%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标域因样本少且多样性不足，导致少样本能力难以有效利用；源域训练模型在目标域初始能力弱，加上域间差异大，严重限制了目标样本的利用和适应效果。

Method: 提出Multi-view Progressive Adaptation (MPA)，包含：(i) Hybrid Progressive Augmentation（渐进式混合增强）：通过累积强增强逐步生成更复杂多样的视图；(ii) Dual-chain Multi-view Prediction（双链多视图预测）：通过串行与并行学习路径，利用复杂视图进行充分监督，并强制跨视图预测一致性。

Result: MPA在多个跨域少样本分割任务上表现优异，显著超越现有最先进方法，平均性能提升达+7.0%。

Conclusion: MPA通过数据与策略双维度的渐进式适应机制，有效缓解了目标域样本稀缺与域差距带来的挑战，实现了鲁棒且准确的少样本分割能力迁移。

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [14] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种名为条件点稀疏化（CPS）的无训练方法，用于解决跨域少样本分割（CD-FSS）中因领域差异导致的点匹配性能下降问题。该方法利用参考图像的真实掩码自适应地稀疏化密集匹配点，从而提升SAM在医学和卫星图像等跨域场景下的分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的无训练少样本分割方法依赖密集点匹配，但在跨域场景下由于领域差异大，点与图像之间的交互被破坏，导致性能下降。研究发现点密度在此类场景中起关键作用。

Method: 提出条件点稀疏化（CPS），通过参考图像的真实掩码信息，自适应地稀疏化在目标图像中匹配的密集点，以增强SAM在跨域图像中的交互能力。

Result: 在多个跨域少样本分割数据集上，CPS显著优于现有的无训练SAM基线方法，验证了其有效性与鲁棒性。

Conclusion: CPS通过引入参考图像的语义引导实现点稀疏化，有效缓解了跨域领域偏移带来的挑战，为无训练少样本分割提供了更可靠的解决方案。

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [15] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: 本文提出一种结合局部邻域感知补丁特征与归一化流模型的方法，通过引入适配器模块，提升预训练特征提取器在工业产品图像上的效率与准确性，显著改善压铸件表面缺陷检测的自动化水平。相比现有方法，该方法在MVTec AD数据集上将错误率降低20%，图像级AUROC达99.28%；在VisA数据集上错误率降低28.2%，图像级AUROC达96.48%；在自有压铸数据集上无需异常样本即可达到95.77%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 压铸工艺虽能制造高精度、光滑表面的复杂结构，但表面缺陷仍是影响质量控制的关键问题。传统人工检测效率低且主观性强，而现有计算机视觉方法在跨域适应性和小样本场景下表现不足，亟需更高效、精准的自动化缺陷检测技术。

Method: 融合局部邻域感知补丁特征与归一化流模型，设计适配器模块以桥接通用预训练特征提取器与工业图像之间的差异，增强特征表达能力与模型泛化性能。

Result: 在MVTec AD数据集上图像级AUROC达99.28%，错误率降低20%；在VisA数据集上图像级AUROC达96.48%，错误率降低28.2%；在自有压铸数据集上无需异常样本即实现95.77%的检测准确率。

Conclusion: 本方法展示了深度学习与计算机视觉技术在压铸行业智能质检中的巨大潜力，为无监督或少样本条件下的高质量缺陷检测提供了有效解决方案。

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [16] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: 本文提出一种主动标签清洗方法，用于处理众包标注中引入的标签噪声，通过主动学习选择最具价值的噪声样本进行专家重新标注，构建高精度清洗模型。该方法利用众包标签与模型预测之间的差异，实现样本选择和实例级噪声评分。实验表明，在私有数据集上达到67.18% AP₅₀，相比直接使用噪声标签提升18.83%，性能达到全专家标注的95.79%，同时降低73.30%的标注成本。


<details>
  <summary>Details</summary>
Motivation: 由于高质量标注数据稀缺，电子致密沉积物（EDD）在肾小球疾病中的自动检测受限。众包虽能降低标注成本，但引入标签噪声，亟需高效去噪方法以提升医疗AI模型可靠性。

Method: 提出一种主动标签清洗方法，结合主动学习策略选择最需重标注的噪声样本；设计标签选择模块，基于众包标签与模型预测的不一致进行样本筛选与噪声等级评估，逐步构建高精度清洗模型。

Result: 在私有数据集上，该方法实现67.18% AP₅₀，较使用噪声标签提升18.83%；性能达全专家标注的95.79%，标注成本降低73.30%。

Conclusion: 所提方法为资源有限的医疗AI开发提供了一种高效、低成本且可靠的标签清洗解决方案，显著提升模型性能并减少对专家标注的依赖。

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [17] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: Magic-MM-Embedding 提出了一种高效且性能先进的多模态嵌入模型，通过视觉令牌压缩和分阶段渐进训练策略，在保持高精度的同时显著降低计算开销，适用于通用多模态检索任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在通用多模态检索中面临因处理大量视觉输入令牌而导致的高计算成本问题，限制了实际应用。

Method: 提出基于视觉令牌压缩的高效 MLLM 架构，并采用从粗到细的分阶段训练策略：包括持续预训练恢复多模态能力、大规模对比预训练与难例挖掘增强区分能力、以及基于 MLLM 作为裁判的任务感知微调以实现精准数据筛选。

Result: 实验表明，该模型在多项指标上显著优于现有方法，同时具备更高的推理效率。

Conclusion: Magic-MM-Embedding 在保证顶尖性能的同时大幅降低了计算开销，为通用多模态检索提供了高效实用的新解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [18] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: Fast-SAM3D 是首个针对 SAM3D 推理动态的系统性研究，提出一种无需训练的加速框架，通过三重异构感知机制（模态感知步缓存、联合时空标记剪枝、谱感知标记聚合）实现计算与生成复杂度的动态对齐，在保持几乎无损精度的前提下，实现最高 2.67 倍的端到端速度提升。


<details>
  <summary>Details</summary>
Motivation: SAM3D 虽具备大规模开放世界 3D 重建能力，但其部署受限于极高的推理延迟。现有通用加速策略在该场景下表现脆弱，根源在于忽视了管道中固有的多层次异构性：形状与布局的运动学差异、纹理细化的内在稀疏性以及几何频谱的多样性。

Method: Fast-SAM3D 提出三种异构感知机制：(1) 模态感知步缓存，分离结构演化与敏感布局更新；(2) 联合时空标记剪枝，聚焦高熵区域进行精细化处理；(3) 谱感知标记聚合，自适应调整解码分辨率。整个框架无需额外训练，动态适配生成过程中的复杂度变化。

Result: 在多项实验中，Fast-SAM3D 实现最高 2.67 倍的端到端加速，同时保持几乎无损的重建质量，确立了单视图 3D 生成效率的新帕累托前沿。代码已开源。

Conclusion: 本工作揭示了高效 3D 生成中异构性的重要性，并通过 Fast-SAM3D 展示了一种无需训练、可有效应对多层级异构性的动态计算对齐范式，为未来高效 3D 重建系统设计提供了新思路。

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [19] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: 本文提出FlashBlock，一种基于缓存的外部注意力机制，用于减少长上下文生成中块扩散（block diffusion）的计算开销。通过发现块间注意力在扩散步骤间具有高度稳定性，FlashBlock重用稳定注意力输出，显著降低注意力计算和KV缓存访问，且与稀疏注意力兼容，提升模型精度。实验表明，其可实现最高1.44倍的令牌吞吐量提升和1.6倍的注意力时间减少，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 在长上下文生成任务中，传统块扩散方法因重复计算注意力而产生巨大开销，尽管已有优化，但尚未充分挖掘注意力在不同扩散步骤间的冗余性。本文旨在解决这一问题，提升生成效率。

Method: 提出FlashBlock机制，利用块外注意力在扩散过程中保持稳定的特性，设计缓存策略以复用历史注意力输出，从而减少重复计算；该方法不改变扩散过程，可与稀疏注意力结合使用。

Result: 在扩散语言模型和视频生成任务上，FlashBlock实现了最高1.44倍的令牌吞吐量提升，注意力计算时间减少达1.6倍，生成质量几乎无损。

Conclusion: FlashBlock通过有效利用跨步骤注意力冗余，显著提升了块扩散在长序列生成中的效率，是一种轻量级、可扩展且与现有技术兼容的优化方案。

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [20] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: MTPano 是一个用于全景场景理解的多任务基础模型，通过无标签训练管道解决高分辨率、多任务标注数据稀缺问题。它利用透视图像的密集先验生成无领域差距的伪标签，并通过投影-再投影策略提供补丁级监督。针对任务间干扰，提出Panoramic Dual BridgeNet，通过几何感知调制层分离旋转不变（如深度、分割）和旋转相关（如表面法线）任务特征流。为缓解等距圆柱投影（ERP）带来的失真，引入ERP token mixers与双分支BridgeNet，结合梯度截断实现有益的任务间信息共享并阻断冲突梯度。此外，加入辅助任务（如图像梯度、点图）促进跨任务学习。实验表明，MTPano在多个基准上达到领先性能，优于专用全景模型。


<details>
  <summary>Details</summary>
Motivation: 全景场景理解对沉浸式应用至关重要，但受限于高质量、多任务标注数据的稀缺性；现有透视基础模型直接应用于全景域时因严重几何失真和坐标系统差异而表现不佳；且球面空间中不同密集预测任务间的内在关联尚未充分探索。

Method: 1. 使用透视图像的密集先验，将全景图投影为透视补丁，利用现成的基础模型生成准确的伪标签，再重新投影以获得补丁级监督；2. 将任务分为旋转不变（如深度、分割）与旋转相关（如表面法线）两类，设计Panoramic Dual BridgeNet，通过注入绝对位置和射线方向先验的几何感知调制层解耦特征流；3. 采用ERP token mixers和双分支BridgeNet处理ERP失真，结合梯度截断机制，促进有效跨任务信息共享并抑制不兼容任务属性引起的冲突梯度；4. 引入图像梯度、点图等辅助任务，增强跨任务学习能力。

Result: MTPano 在多个基准上实现了最先进的性能，其表现与专门针对全景任务优化的专家模型相当甚至更优，验证了方法的有效性和泛化能力。

Conclusion: MTPano 提供了一种高效、鲁棒的多任务全景基础模型框架，克服了数据稀缺、几何失真和任务干扰等关键挑战，为全景场景理解提供了新的解决方案。

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [21] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: 提出PAIRed Erasing框架，通过不安全-安全配对实现概念擦除中的语义一致性保持，利用配对数据进行语义重对齐和参数初始化，实现细粒度、结构一致的有害内容擦除。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法仅移除不安全概念而缺乏对安全替代物的引导，导致生成结果在结构和语义上失真。

Method: 构建不安全-安全配对数据，引入配对语义重对齐与基于Fisher权重的DoRA初始化，实现目标概念的精准擦除与安全替代生成。

Result: 实验表明该方法显著优于现有基线，在保持结构完整性、语义连贯性和生成质量的同时有效擦除目标概念。

Conclusion: PAIRed Erasing通过语义对齐与参数引导，实现了更高质量、更可控的概念擦除，为安全文本到图像生成提供了新范式。

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [22] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: APEX提出了一种新的异常检测框架，通过两阶段修复机制优化特征流形。针对原型方法中的静态同质性假设和学习-推理断开问题，引入自适应原型流形（APM）和后验感知异常评分（PAOS）机制，显著提升性能，在CIFAR-100等基准上达到新最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的表示学习方法在异常检测中表现优异，但受限于静态同质性假设（所有类别使用固定表示资源）和学习-推理断开（推理时丢弃原型质量信息），导致模型容量与性能受限。

Method: 提出APEX框架，包含两个核心创新：(1) 自适应原型流形（APM），基于最小描述长度（MDL）原则自动确定每类最优原型复杂度 $K_c^*$，解决原型冲突；(2) 后验感知异常评分（PAOS）机制，量化原型质量（凝聚性与分离性），弥合学习与推理之间的鸿沟。

Result: 在CIFAR-100等基准数据集上的全面实验表明，APEX显著优于现有方法，达到新的状态领先水平。

Conclusion: APEX通过解决原型表示中的根本缺陷，有效提升了异常检测性能，为安全部署机器学习模型提供了更可靠的保障。

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [23] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: 本文提出HIVE框架，通过在多模态大模型中引入分层视觉线索注入，实现基于潜在空间的“慢思考”推理。该方法通过递归扩展Transformer块，构建内部迭代机制，并将全局场景到细粒度区域的视觉信息直接嵌入模型潜空间，实现无需依赖文本推理链的、具身化的多步推理，显著提升复杂场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型的推理多依赖于端到端生成或语言中心的思维链，存在效率低、冗长和幻觉等问题。为实现更稳健的推理，需在潜在空间中融合多模态信号，推动从‘快速思考’向‘慢思考’转变。

Method: 提出HIVE框架，通过递归扩展Transformer块建立内部迭代推理环路，将从全局场景到局部细节的分层视觉线索直接注入模型潜空间，实现无显式文本推理的潜空间内多步推理。

Result: 实验证明，引入视觉知识时测试阶段扩展有效；结合分层信息能显著提升模型对复杂场景的理解能力。

Conclusion: HIVE框架成功实现了基于潜空间的分层视觉引导推理，避免了传统语言推理链的缺陷，为多模态模型提供了更可靠、高效且具身的推理范式。

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [24] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: 本文揭示了现有最先进（SOTA）特征后处理方法在分布外（OOD）检测中存在一个反直觉的“简约悖论”：模型对语义细微差异的OOD样本敏感，但对结构差异大却语义简单的样本或高频传感器噪声则表现出严重的几何盲区。作者将此归因于深度特征空间中的语义霸权，并通过神经坍缩理论揭示其数学本质。研究表明，主子空间的高方差导致谱集中偏差，数值上掩盖了残差子空间中应显著的结构分布偏移信号。为此，提出D-KNN——一种无需训练、即插即用的几何解耦框架，利用正交分解显式分离语义成分与结构残差，并引入双空间校准机制以恢复对弱残差信号的敏感性。大量实验表明，D-KNN有效打破语义霸权，在CIFAR和ImageNet基准上建立新SOTA性能；在解决简约悖论方面，将FPR95从31.3%降至2.3%；在应对传感器噪声时，AUROC从79.7%提升至94.9%。


<details>
  <summary>Details</summary>
Motivation: 现有特征后处理方法在OOD检测中虽取得进展，但存在对结构差异大但语义简单的样本或高频噪声不敏感的问题，表现为‘简约悖论’。这一现象源于深度特征空间中的语义霸权，亟需从理论上解释并设计新方法以增强几何感知能力。

Method: 提出D-KNN框架，采用正交分解分离语义成分与结构残差，结合双空间校准机制，重新激活模型对弱残差信号的敏感性，实现无需训练的几何解耦。

Result: D-KNN在多个基准上达到新SOTA性能，成功解决‘简约悖论’，将FPR95由31.3%降至2.3%，在传感器噪声场景下AUROC从79.7%提升至94.9%。

Conclusion: 语义霸权是当前OOD检测模型几何盲区的根本原因，通过几何解耦与双空间校准可有效突破该瓶颈，所提D-KNN为训练自由、高效且通用的解决方案。

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [25] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: CityGenAgent 是一个基于自然语言驱动的分层程序化生成框架，用于创建高质量的3D城市。它将城市生成分解为区块程序（Block Program）和建筑程序（Building Program），通过监督微调（SFT）确保结构正确性，并利用强化学习（RL）优化空间对齐与视觉一致性。该方法支持自然语言编辑与操控，实验表明其在语义对齐、视觉质量和可控性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市生成方法在高保真资产创建、可控性和可操作性方面存在不足，难以满足自动驾驶、虚拟现实和具身智能等应用需求。因此，需要一种更高效、可解释且可控的生成框架。

Method: 提出分层程序化生成框架CityGenAgent，包含两个阶段：(1) 监督微调（SFT），训练生成符合模式约束的程序；(2) 强化学习（RL），设计空间对齐奖励与视觉一致性奖励以提升空间推理与文本-视觉对齐能力。

Result: CityGenAgent 在语义对齐、视觉质量与可控性方面显著优于现有方法，支持自然语言编辑与操作，具备良好的泛化能力，为大规模3D城市生成提供了可靠基础。

Conclusion: CityGenAgent 通过分层程序化生成与双阶段学习策略，实现了高质量、可控、可编辑的3D城市生成，为未来智能环境构建提供了有力工具。

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [26] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: SAIL是一种无需外部奖励模型或大规模偏好数据的自强化迭代学习框架，通过让扩散模型自我生成并标注偏好，实现仅用6%的少量人类反馈即可超越现有方法，在多个基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型对大规模人类偏好数据和外部奖励模型的依赖问题，探索仅用最小人类反馈即可实现有效对齐的可能性。

Method: 提出SAIL框架，利用扩散模型自我生成多样样本，基于自身演化理解进行自标注偏好，并通过闭环迭代优化；引入排名偏好混合法以平衡探索与初始人类先验的保持。

Result: 在多个基准上显著优于现有方法，仅需6%的偏好数据量，证明扩散模型具备强大的自我改进能力。

Conclusion: 扩散模型具有潜在的自我对齐能力，通过SAIL框架可有效释放其潜力，减少对人工标注和外部奖励模型的依赖，为高效对齐提供新路径。

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [27] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: 本文提出VRIQ基准，评估视觉语言模型（VLMs）的视觉推理能力。在抽象拼图任务中，模型平均准确率仅28%，自然图像任务为45%，工具增强推理仅带来小幅提升。诊断分析显示，约56%的失败源于感知问题，43%由感知与推理共同导致，仅有1%纯粹因推理不足。进一步细粒度探针揭示形状、数量、位置等感知类别中某些问题更易引发错误。结论指出当前VLMs在抽象推理上仍不可靠，主要受限于感知能力，并为改进多模态系统中的视觉推理提供依据。


<details>
  <summary>Details</summary>
Motivation: 探究当前视觉语言模型（VLMs）在非语言推理任务中的可靠性，特别是其在抽象和自然图像场景下的视觉推理能力是否足够强。

Method: 构建VRIQ基准，涵盖抽象拼图和自然图像两类任务；使用诊断探针分析感知与推理的贡献；通过细粒度探针识别具体感知缺陷类别（如形状、数量、位置、3D/深度）。

Result: 在抽象任务中平均准确率仅28%，自然任务为45%；工具增强推理效果有限；约56%的失败源自感知，43%为感知与推理共同作用，仅1%纯属推理问题；特定感知类别（如形状、数量）导致更多失败。

Conclusion: 当前VLMs即使使用视觉推理工具，在抽象推理任务中仍表现不可靠，主要受限于感知能力；研究提供了可指导改进多模态系统视觉推理的细粒度分析框架。

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [28] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: Dolphin-v2 是一种改进的两阶段文档图像解析模型，针对现有方法在处理拍摄文档和复杂布局时的局限性，提出了一种结合文档类型分类与布局分析的新方法。通过分阶段处理：对数字生成文档进行细粒度元素检测与阅读顺序预测，对拍摄文档则采用整体页面理解以应对几何畸变。引入了21类细粒度元素识别、语义属性提取及代码块缩进保留能力，并在多个基准上显著提升性能，尤其在拍摄文档上错误率降低91%。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析方法存在模型碎片化、难以扩展，且依赖轴对齐边界框，在处理拍摄或扭曲文档时效果不佳。需要更鲁棒、统一且高效的文档解析框架。

Method: Dolphin-v2 采用两阶段策略：第一阶段联合进行文档类型分类（数字生成/拍摄）与布局分析；第二阶段根据文档类型采取不同解析方式——拍摄文档采用整体页面级解析，数字生成文档基于布局锚点进行并行元素级解析。同时支持21类细粒度元素检测、语义属性提取和代码块缩进保留。

Result: 在 DocPTBench、OmniDocBench 和自建 RealDoc-160 基准上评估，Dolphin-v2 在 OmniDocBench 上总体得分提升14.78点，拍摄文档错误率下降91%，同时保持高效推理速度。

Conclusion: Dolphin-v2 通过融合文档类型感知与分阶段解析策略，显著提升了对多样化文档场景的适应性与解析精度，尤其在处理拍摄文档方面表现突出，为实际应用提供了更可靠、可扩展的解决方案。

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [29] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Parallel Swin Transformer-Enhanced Med2Transformer的3D架构，用于从MRI生成合成CT图像。该方法结合卷积编码与双Swin Transformer分支，以同时捕捉局部解剖细节和长距离上下文依赖关系。通过多尺度移位窗口注意力和分层特征聚合，提升了解剖结构的保真度。在公开和临床数据集上的实验表明，该方法在图像相似性和几何准确性方面优于基线模型，且剂量学评估显示平均靶区剂量误差仅为1.69%，达到临床可接受水平。代码已开源。


<details>
  <summary>Details</summary>
Motivation: MRI虽具有优异的软组织对比度且无电离辐射，但缺乏电子密度信息，无法直接用于放疗剂量计算。当前流程需联合使用MRI和CT，导致配准不确定性增加及流程复杂化。因此，亟需发展一种可靠的合成CT生成技术，实现仅基于MRI的放疗规划。

Method: 提出Parallel Swin Transformer-Enhanced Med2Transformer模型，采用3D架构融合卷积编码与双Swin Transformer分支，利用多尺度移位窗口注意力机制和分层特征聚合策略，有效建模局部细节与全局上下文关系，提升合成CT图像的质量与解剖准确性。

Result: 在公共和临床数据集上，所提方法在图像相似性（如SSIM、PSNR）和几何精度方面均优于现有基线方法；剂量学评估显示平均靶区剂量误差为1.69%，符合临床应用标准。

Conclusion: 该方法能够高质量生成合成CT图像，支持仅基于MRI的放疗计划，显著降低对CT的依赖，减少配准误差与流程复杂性，具备良好的临床转化潜力。

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [30] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: 提出了一种名为统计流匹配（Statistical Flow Matching）的新方法，用于数据蒸馏，通过将合成图像的统计流与真实数据的目标类中心到非目标类中心的流对齐，实现高效且稳定的优化。该方法仅需加载一次原始统计数据，并对合成数据进行单次增强，显著降低计算和内存开销（相比现有方法减少10倍GPU内存使用和4倍运行时间），同时在分类任务上表现优于或媲美当前最优方法。此外，引入了分类器继承策略，利用原数据集训练的分类器进行推理，仅需轻量级线性投影器和极少存储空间，即可获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于线性梯度匹配的数据蒸馏方法在使用自监督预训练模型时，存在计算和内存开销大、需要多次加载真实图像和应用不同可微增强的问题，限制了其效率和实用性。因此亟需一种更高效、稳定且低资源消耗的蒸馏框架。

Method: 提出统计流匹配框架，通过构建从目标类中心到非目标类中心的常数统计流，直接优化合成图像以匹配该流；结合一次性加载原始统计数据与单次数据增强策略，大幅减少计算负担；并设计分类器继承机制，复用原分类器，仅通过轻量级投影器完成推理。

Result: 在多个基准数据集上，所提方法实现了与或优于现有最先进方法相当甚至更高的性能，同时具备10倍更低的GPU内存占用和4倍更短的运行时间；分类器继承策略进一步提升了推理效率与性能。

Conclusion: 统计流匹配为数据蒸馏提供了一种高效、稳定且低资源消耗的新范式，显著降低了计算成本，同时保持甚至超越了现有方法的性能，具有良好的实用性和扩展潜力。

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [31] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出了一种名为MAD的框架，用于在数字病理学中进行可控且生物上合理的细胞核编辑。该方法通过变分自编码器学习解耦的潜在空间，确保在编辑某一特征时自动调整相关属性，保持在真实细胞的分布范围内，从而生成高保真图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑路径组学特征时，因假设特征独立性而产生不现实的伪影，限制了其在临床中的应用。需要一种能处理特征内在相关性的方法以实现更真实的图像生成。

Method: 利用变分自编码器（VAE）学习解耦的潜在空间，对特征轨迹进行正则化，并结合条件扩散模型生成高质量图像。

Result: 实验表明，所提方法能够有效导航路径组学特征的流形，在条件特征编辑中表现优于基线方法，同时保持结构一致性。

Conclusion: MAD框架实现了生物上合理且可控的细胞核编辑，为数字病理学中的可解释性与可重现性提供新途径。

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [32] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 本研究提出TSBOW数据集，旨在解决极端天气下车辆遮挡检测的挑战。该数据集包含超过32小时的真实交通数据，涵盖48,000个手动标注和320万帧半自动标注的图像，覆盖八类交通参与者，适用于复杂城市环境中的智能交通系统研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多局限于轻雾、雨雪等温和天气，无法反映极端天气对交通监控的影响，导致车辆遮挡检测性能下降，亟需更全面的数据支持。

Method: 构建了一个涵盖多种天气条件下的真实世界交通监控数据集（TSBOW），结合人工与半自动标注技术，覆盖多样化的道路类型、尺度和视角。

Result: TSBOW数据集已成功建立，并被用于构建车辆遮挡检测基准，验证了其在复杂天气和遮挡场景下的有效性，为未来智能交通系统研究提供了重要资源。

Conclusion: TSBOW数据集填补了极端天气下车辆遮挡检测研究的数据空白，推动了基于CCTV的交通监控技术发展，具有广泛的应用前景。

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [33] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出了一种无需外部数据的新型数据自由OOD检测框架，通过几何引导的虚拟异常样本合成（GOS）和双粒度语义损失（DGS），在长尾分布下实现优越的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA方法依赖大规模外部数据进行异常暴露（OE），但在实际应用中面临数据获取成本高和隐私问题，因此需要一种不依赖外部数据的解决方案。

Method: 采用基于vMF分布的几何引导虚拟异常合成（GOS），在特征空间中低概率环形区域进行方向采样生成虚拟异常样本；同时引入双粒度语义损失（DGS），利用对比学习增强ID特征与合成边界异常之间的区分性。

Result: 在CIFAR-LT等基准测试上，该方法优于使用真实外部图像的SOTA方法，实现了卓越的OOD检测性能且完全无需外部数据。

Conclusion: 所提方法成功实现了无需外部数据的高效OOD检测，在长尾分布场景下具备良好的实用性和先进性。

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [34] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: 提出 Disco 框架，通过拓扑感知的协同着色解决密集重叠细胞实例分割难题，结合显式标记与隐式消歧机制，有效应对真实病理图像中复杂的非二分图结构和奇数长度环问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于轮廓检测和距离映射的方法在处理复杂密集细胞区域时存在挑战；图着色方法虽为新范式，但其在真实场景下处理密集重叠和复杂拓扑的能力尚未验证。

Method: 提出 Disco 框架，采用‘分而治之’原则，结合数据驱动的拓扑标记策略与受约束的深度学习系统：通过递归分解细胞图实现显式标记，将拓扑冲突转化为可学习分类任务；利用特征差异性约束进行隐式消歧，促进可区分特征表示的学习。

Result: 在 GBC-FS 2025 大规模复杂核排列数据集上验证了方法的有效性，揭示真实细胞图多为非二分图且含大量三角形等奇数长度环，证明 2 色着色不足，高色数模型又导致冗余与优化困难；Disco 在复杂拓扑下实现更优的实例分割性能。

Conclusion: Disco 框架通过融合拓扑分析与深度学习，成功应对复杂组织中密集重叠细胞的实例分割挑战，为数字病理学提供了一种高效、鲁棒的新解决方案。

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [35] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: NeVStereo 是一种基于 NeRF 的联合框架，能够从多视角 RGB 图像中同时实现相机位姿估计、多视图深度估计、新视角合成（NVS）和表面重建。它通过结合 NeRF 的高质量渲染、置信度引导的深度估计、与束调整耦合的位姿优化以及迭代精炼机制，有效缓解了传统 NeRF 方法中的表面堆叠、伪影和位姿-深度耦合问题。在多个基准测试中，NeVStereo 在零样本条件下表现出色，显著降低了深度误差（最高36%）、提升了位姿精度（10.4%）、提高了渲染保真度（4.5%），并达到了最先进的网格质量（F1 91.93%，Chamfer 4.35 mm）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在端到端匹配与几何预测方面表现良好，但缺乏显式的新视角合成输出；而神经渲染方法虽能提供高质量的渲染和细节几何，却通常依赖固定相机位姿，对位姿误差敏感。因此，构建一个统一框架以同时实现高精度位姿、可靠深度、高质量渲染和精确三维表面仍具挑战性。

Method: NeVStereo 结合了 NeRF-based NVS 生成适合立体匹配的渲染结果，采用置信度引导的多视图深度估计，利用与束调整耦合的 NeRF 进行位姿优化，并引入迭代精炼阶段同步更新深度图与辐射场，提升几何一致性。

Result: 在室内、室外、桌面及航拍等多种场景下，NeVStereo 均展现出一致强的零样本性能：深度误差降低最多达36%，位姿精度提升10.4%，新视角合成保真度提高4.5%，网格质量达到当前最优水平（F1 91.93%，Chamfer 4.35 mm）。

Conclusion: NeVStereo 成功构建了一个统一且高效的多任务框架，实现了高精度位姿、可靠深度、高质量渲染与精细表面重建的协同优化，在多个基准上均超越现有先进方法，展示了其在真实世界多视角重建中的强大潜力。

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [36] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型的结构光方法（LD-SLRO），用于解决高反射率、低粗糙度表面在条纹投影轮廓测量中的条纹失真与信息丢失问题。通过编码器提取表面反射特性，利用潜空间扩散模型概率性抑制镜面反射伪影并恢复缺失条纹，结合时变通道仿射层和注意力模块进一步提升复原质量。该方法支持灵活配置输入输出条纹集，实验表明其显著提升了条纹质量和三维重建精度，平均均方根误差从1.8176 mm降低至0.9619 mm。


<details>
  <summary>Details</summary>
Motivation: 高反射率和低表面粗糙度物体在条纹投影轮廓测量中易受镜面反射和间接光照影响，导致条纹图案严重失真或丢失，现有方法难以有效恢复，亟需一种能够精准抑制反射伪影并恢复细节的新技术。

Method: 提出基于潜空间扩散模型的结构光方法（LD-SLRO），首先使用编码器将相移条纹图像映射到潜空间以提取表面反射特征；随后将这些特征作为条件输入至潜扩散模型，通过概率化方式抑制反射伪影并重建丢失的条纹信息；引入时变通道仿射层与注意力模块增强特征表达能力；支持灵活配置输入输出条纹集，实现高效重建。

Result: 实验验证了该方法在条纹质量与三维重建精度上的优越性，相比现有最优方法，平均根均方误差由1.8176 mm降至0.9619 mm，显著提升重建准确性。

Conclusion: LD-SLRO通过潜空间扩散建模与多组件协同设计，有效解决了高反射表面条纹投影测量中的关键难题，在条纹恢复与三维重建方面表现出优异性能，具有良好的应用潜力与扩展性。

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [37] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: 提出Stable Velocity框架，通过分析流匹配中的方差问题，识别出高方差和低方差区域，并设计StableVM与VA-REPA以降低训练方差，同时在低方差区域实现闭式简化，提出StableVS采样方法，实现无需微调的加速。在ImageNet 256×256及多个主流文本到图像/视频模型上验证了训练效率提升和采样速度超2倍加速，且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配依赖单样本条件速度，导致训练目标方差大，影响优化稳定性与收敛速度，尤其在先验附近表现不佳。需解决高方差带来的训练困难与采样效率低下问题。

Method: 识别流匹配中条件速度的方差分布特性，提出StableVM（无偏方差减少目标）与VA-REPA（自适应增强辅助监督），并利用低方差区域的闭式性质设计StableVS采样方法，实现训练与推理双重优化。

Result: 在ImageNet 256×256及多个大型预训练文本到图像/视频模型（如SD3.5、Flux、Qwen-Image、Wan2.2）上，训练效率显著提升，采样速度超过2倍加速，且样本质量未下降。

Conclusion: Stable Velocity框架通过系统性分析方差特性，实现了更稳定高效的训练与快速采样，为流匹配方法提供了重要改进路径。

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [38] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: 本文提出了一种基于参数化方法的三维网格缺陷建模技术，用于生成合成缺陷数据，以支持非破坏性检测（NDT）中的自动化缺陷检测。该方法通过构建数字孪生模型，结合规则生成与物理驱动的蒙特卡洛模拟，可生成大量具有像素级标注的合成数据，适用于视觉表面检测及其他NDT方法。


<details>
  <summary>Details</summary>
Motivation: 在工业质量控制中，缺陷检测至关重要。传统依赖真实缺陷数据的方法受限于数据量和多样性，尤其是罕见缺陷样本不足。为解决这一问题，需利用合成数据来训练机器学习模型，而现有方法缺乏对缺陷形状的可控建模能力。因此，需要一种可灵活生成多样、逼真缺陷数据的方法，以提升检测系统的泛化能力。

Method: 提出了一种基于参数化建模的三维缺陷生成方法，针对金属铸造中常见的缺陷类型，构建可调节的3D网格缺陷模型。这些模型可嵌入到物体几何中形成带缺陷的合成对象。随后，通过物理驱动的蒙特卡洛模拟生成与实际检测数据相似的合成数据，并实现像素级标注。该方法可扩展至其他制造工艺及非破坏性检测手段。

Result: 成功构建了可生成多样化、高保真度合成缺陷数据的系统，支持大规模、可控的数据生成，包括罕见缺陷。所生成数据与真实检测数据高度一致，且具备精确标注，显著提升机器学习模型的训练效果。

Conclusion: 本研究提出的参数化缺陷建模与合成数据生成框架，为自动化缺陷检测提供了高质量、可扩展的训练数据解决方案，适用于多种非破坏性检测场景，具有良好的通用性和应用前景。

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [39] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种适用于扩散模型的可蒸馏可学习特征缓存机制，通过引入轻量级可学习神经预测器替代传统无训练启发式方法，更准确地捕捉高维特征演化过程。针对大规模视频模型中高度压缩蒸馏带来的挑战，提出了保守的Restricted MeanFlow方法，实现更稳定、无损的蒸馏。该方法在保持生成质量的同时，将加速比提升至11.8倍，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成扩散模型加速方法如特征缓存和步数蒸馏存在语义与细节丢失问题，尤其在低步数蒸馏下，传统无训练特征缓存方法因采样步数稀疏导致质量显著下降，亟需一种兼容蒸馏且能有效保留细节的新型缓存机制。

Method: 提出一种可蒸馏的可学习特征缓存机制，使用轻量级神经预测器建模特征演化；设计保守的Restricted MeanFlow蒸馏策略，以应对高压缩下的稳定性问题，从而实现高效且高质量的视频生成加速。

Result: 在保持生成质量的前提下，实现了高达11.8倍的加速比，优于现有方法，在多种视频生成任务中表现出色。

Conclusion: 本文首次提出兼容蒸馏的可学习特征缓存机制，并结合保守蒸馏策略，显著提升了视频生成扩散模型的推理效率，为大规模视频生成的实用化提供了有效解决方案。

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [40] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: 本文识别出视觉变换器中的注意力漂移是持续学习中灾难性遗忘的主要原因，并受人类视觉系统选择性注意的神经科学启发，提出一种新的注意力保留框架。通过两步梯度约束机制：1）使用逐层展开方法提取先前任务的注意力图并生成实例自适应二值掩码；2）在学习新任务时，利用这些掩码置零与先前注意力区域相关的梯度，防止已学视觉概念被破坏。为兼容现代优化器，还引入比例缩放以保持参数更新的相对大小。实验和可视化验证了该方法在缓解遗忘、保留视觉概念方面的有效性，在多种持续学习场景中表现优异，达到当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中灾难性遗忘是一个关键挑战，本文发现视觉变换器中的注意力漂移是导致遗忘的重要原因，因此需要一种机制来稳定已学视觉概念的注意力模式。

Method: 提出一种注意力保留框架，通过层间展开提取先前任务的注意力图，生成实例自适应二值掩码，并在反向传播中应用这些掩码零化与旧注意力区域相关的梯度，同时对参数更新进行比例缩放以保持相对大小。

Result: 实验表明该方法能有效减轻灾难性遗忘，保持视觉概念的稳定性，在多种持续学习设置下均取得领先性能，具有良好的泛化能力。

Conclusion: 注意力漂移是视觉变换器在持续学习中遗忘的关键因素，本文提出的注意力保留框架通过梯度掩码和比例缩放策略，显著提升了模型在非平稳数据流中的知识保留能力，达到了当前最优水平。

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [41] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: 提出Memory-Execute-Review框架，通过分层记忆、执行与复审模块，在视觉语言导航任务中显著提升成功率与泛化能力，尤其在零样本设置下表现优异，超越现有所有训练自由与监督微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法在成功率和泛化能力之间难以兼顾，监督微调方法虽成功率高但泛化差，而无需训练的方法泛化好但成功率不足，亟需一种能同时提升两者的新方法。

Method: 提出Memory-Execute-Review框架，包含分层记忆模块（提供信息支持）、执行模块（进行常规决策与动作）和复审模块（处理异常并纠正行为），实现对导航过程的动态支持与纠错。

Result: 在4个数据集上，该方法在训练自由设置下平均成功率提升7%，在零样本设置下提升5%；在HM3D_v0.1和HM3D_OVON数据集上，零样本成功率分别提升8%和6%；且在MP3D和HM3D_OVON上不仅超越所有训练自由方法，还超过所有监督微调方法，实现成功率与泛化性的双重领先。

Conclusion: 所提出的框架有效解决了视觉语言导航中成功率与泛化性难以兼得的问题，为具身智能中的导航任务提供了高效且通用的解决方案。

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [42] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: SOMA-1M 是一个包含超过 130 万对像素级精确对齐的 SAR-光学图像的多尺度数据集，覆盖 0.5 米至 10 米分辨率，支持跨模态协同处理与智能解译。该数据集整合了 Sentinel-1、PIESAT-1、Capella Space 和 Google Earth 数据，涵盖 12 类典型地表覆盖，具备全球多尺度覆盖和高场景多样性。研究提出粗到精的图像匹配框架以解决投影变形与大规模注册问题，并建立涵盖图像匹配、融合、云去除与跨模态翻译的评估基准，验证了其在提升多模态遥感模型性能方面的有效性，达到当前 SOTA 水平。数据集将公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集存在空间分辨率单一、数据量不足、对齐精度低等问题，难以支持多尺度基础模型的训练与泛化，限制了跨模态遥感图像处理的发展。

Method: 设计并实现了一套粗到精的图像匹配框架，用于解决多模态遥感图像中的投影变形与大规模数据注册问题，确保像素级对齐；整合多种来源遥感数据，构建覆盖全球、多尺度（0.5–10 m）、高多样性的数据集。

Result: 基于 SOMA-1M 的监督训练显著提升了多模态遥感任务的性能，尤其在图像匹配任务中达到当前最优水平；建立了涵盖四类视觉任务的完整评估体系，验证了数据集的有效性与通用性。

Conclusion: SOMA-1M 为多模态遥感算法与遥感基础模型提供了坚实的数据基础，推动跨模态协同处理与智能化解译的发展，具有广泛的应用前景。

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [43] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: 本报告为2014年博士论文的未发表初稿，旨在研究车载鱼眼相机在城市环境中进行自标定、视觉里程计与立体视觉时最优的特征检测器与描述子。报告贡献了名为PFSeq的高保真鱼眼图像数据集及详细文献综述，但未提出新算法，也未与专为全景图像设计的算法进行比较，且未经过同行评审，实验结果未根据最新技术进展更新。


<details>
  <summary>Details</summary>
Motivation: 在缺乏精确投影模型的情况下，难以实现鱼眼图像中特征的最优检测与描述；而准确的投影模型又依赖于良好的特征提取。这一‘鸡生蛋、蛋生鸡’的问题促使研究寻找适用于鱼眼图像的鲁棒特征方法，以支持自标定和后续的视觉定位任务。

Method: 基于已有的特征检测与描述算法，在自制的PFSeq鱼眼序列数据集上进行系统性实验，评估不同算法在鱼眼图像中的表现，通过定量与定性分析筛选出最适合作用于鱼眼视觉里程计与立体视觉的特征组合。

Result: 实验表明，某些传统特征算法（如SIFT、SURF）在鱼眼图像中仍具备一定鲁棒性，但在畸变严重区域性能下降；部分算法在特定条件下表现更优，但整体尚无统一最优解。数据集PFSeq被成功构建并公开，为后续研究提供基础。

Conclusion: 尽管未提出新算法，该报告为鱼眼图像特征处理提供了有价值的基准实验与数据资源。由于其源自2014年的研究背景，结论需结合当前技术发展谨慎解读，建议未来工作基于此数据集开展更全面的对比与优化。

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [44] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: VGGT-Motion提出了一种高效的校准自由单目SLAM系统，通过运动感知的子地图构建、基于锚点的直接Sim(3)配准和轻量级子地图级位姿图优化，在千米级轨迹上实现了鲁棒的全局一致性，显著提升了长距离零样本校准自由单目SLAM的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有校准自由单目SLAM在长序列中严重的尺度漂移问题，克服运动无关分割导致的上下文不一致和零运动漂移，同时避免传统几何对齐计算开销大的缺点。

Method: 提出运动感知的子地图构建机制，利用光流指导自适应分区、剔除静态冗余并封装转弯以稳定局部几何；设计基于锚点的直接Sim(3)配准策略，实现无搜索、像素级稠密对齐与高效回环闭合；采用轻量级子地图级位姿图优化，以线性复杂度保障全局一致性，支持可扩展的长距离运行。

Result: 实验表明，VGGT-Motion在零样本、长距离校准自由单目SLAM任务中显著提升轨迹精度与效率，达到当前最佳性能。

Conclusion: VGGT-Motion通过创新的运动感知分区、锚点驱动配准与高效优化策略，成功解决了长序列下校准自由单目SLAM中的尺度漂移与计算效率难题，为大规模场景下的实时定位与建图提供了新范式。

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [45] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: 该论文提出一种无需训练的推理时引导方法 SSG（Scaled Spatial Guidance），通过信息论视角解决视觉自回归模型在生成过程中因容量限制和误差累积导致的层级漂移问题。SSG 强调目标高频信号（语义残差），并利用频域方法 DSE（离散空间增强）从粗略先验中分离出这些信号，从而在不破坏全局一致性的前提下提升图像生成的保真度与多样性。该方法适用于多种基于离散视觉标记的自回归模型，且保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有视觉自回归模型在推理时存在层级漂移问题，即由于模型容量有限和误差累积，导致生成过程偏离理想的从粗到细的结构，影响图像质量与一致性。

Method: 提出 SSG 方法，结合 DSE 技术，在推理阶段通过频域分析提取语义残差，并引导生成过程更准确地保留高频率内容，确保每一尺度贡献未被先前尺度解释的信息。

Result: 实验表明，SSG 在多种模型上均显著提升了图像生成的保真度和多样性，同时保持了低延迟特性，验证了其广泛适用性和高效性。

Conclusion: 通过信息论指导的推理时引导机制 SSG，能够有效缓解视觉自回归模型中的层级漂移问题，释放从粗到细生成的潜在效率，为高质量、快速图像生成提供了新范式。

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [46] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: 本研究系统评估了仅使用相机、仅使用LiDAR以及相机-LiDAR融合在3D人体检测中的表现，基于JRDB数据集在多种室内外场景下进行实验。对比了BEVDepth（相机）、PointPillars（LiDAR）和DAL（融合）三种模型，发现融合方法在复杂场景中表现最优，尤其在遮挡和远距离情况下优势明显。尽管融合模型对传感器异常有一定鲁棒性，但仍易受传感器错位和特定LiDAR噪声影响；而相机模型性能最差，对遮挡、距离和噪声最为敏感。


<details>
  <summary>Details</summary>
Motivation: 准确的3D人体检测对机器人、工业监控和安防等应用的安全至关重要。现有研究多集中于自动驾驶，但室内与室外多样化场景下的3D人体检测仍需系统评估，尤其是不同传感器模态的表现差异及鲁棒性问题。

Method: 采用相机仅、LiDAR仅和相机-LiDAR融合三种方式，在JRDB数据集上对BEVDepth、PointPillars和DAL三个代表性模型进行系统性评估，分析其在不同遮挡程度、距离条件下的检测性能，并测试传感器损坏和错位情况下的鲁棒性。

Result: 融合方法（DAL）在多数场景中表现最佳，尤其在遮挡和远距离条件下显著优于单模态模型；相机模型（BEVDepth）性能最差，对遮挡、距离和噪声敏感；尽管融合模型具有一定抗干扰能力，但对传感器错位和特定LiDAR噪声仍较敏感。

Conclusion: 传感器融合对于提升3D人体检测性能至关重要，但在实际部署中仍需解决传感器对齐和噪声鲁棒性等挑战，未来研究应聚焦于增强系统的整体稳定性与可靠性。

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [47] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: FastVMT提出通过消除扩散变压器（DiT）中的两种计算冗余来加速视频运动迁移：运动冗余和梯度冗余。通过局部注意力掩码减少远距离区域的不必要交互，并设计梯度重用优化方案跳过冗余计算，实现平均3.43倍加速，同时保持视觉质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽尝试加速DiT计算，但未解决其结构性低效问题，特别是帧间运动平滑性和扩散轨迹中梯度缓慢变化带来的冗余计算。

Method: 引入局部注意力掩码以抑制远距离区域的无关交互，缓解运动冗余；设计基于梯度重用的优化策略，利用扩散过程中梯度变化缓慢的特性，跳过不必要的梯度计算。

Result: 在保持生成视频质量与时间一致性的前提下，FastVMT实现了平均3.43倍的推理速度提升。

Conclusion: 通过识别并消除运动与梯度层面的计算冗余，FastVMT显著提升了视频运动迁移的效率，为高效生成高质量视频提供了新思路。

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [48] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: IndustryShapes 是一个用于工业工具和组件的 RGB-D 基准数据集，旨在支持实例级和新物体 6D 姿态估计方法的评估。该数据集涵盖真实工业装配场景，包含五种具有挑战性特征的新物体类型，具有从简单到复杂的多样化场景，分为经典集（4.6k 图像，6k 标注姿态）和扩展集（支持无模型和序列化方法）。它是首个提供静态上电序列的 RGB-D 数据集，并在多个前沿方法上进行了评估，表明该领域仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多聚焦于家庭或消费类物品，或使用合成、洁净桌面环境中的数据，缺乏真实工业场景的复杂性和多样性。为弥合实验室研究与实际制造部署之间的差距，需要一个更贴近真实工业应用的数据集来推动 6D 姿态估计技术的发展。

Method: 构建 IndustryShapes 数据集，包含真实工业环境下采集的 RGB-D 序列，涵盖多种工业对象类型，设计两个子集（经典集和扩展集），提供丰富的标注信息，支持实例级与新型物体的姿态估计评估，并引入静态上电序列以支持序列化方法测试。

Result: 验证了当前主流 6D 姿态估计方法在工业场景下的表现，揭示了现有技术在处理复杂工业环境时仍存在不足；同时，该数据集提供了可用于评估模型性能的基准平台，尤其适合评估模型无依赖与序列建模能力。

Conclusion: IndustryShapes 是首个面向真实工业场景的多模态、高复杂度、带静态上电序列的 RGB-D 基准数据集，有效推动了工业机器人中 6D 姿态估计的研究向实际应用迈进，为未来算法发展提供了重要基础。

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [49] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: PIRATR 是一种端到端的3D物体检测框架，专为机器人应用中的点云数据设计。它在PI3DETR基础上改进，通过联合估计多类6-DoF姿态和类别特定的参数属性，实现对遮挡点云数据的高效处理。该方法不仅完成几何定位，还能够推断出任务相关的参数化属性（如夹爪开合程度），并通过预定义规则调整3D模型。模块化的类别专用头部设计使其易于扩展至新物体类型。在模拟环境中训练后，PIRATR在真实户外LiDAR扫描中表现优异，无需微调即达到0.919的检测mAP。该工作推动了感知系统向可扩展、仿真训练、动作导向的参数化世界建模演进。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测方法难以同时实现高精度几何定位与任务相关参数属性的估计，尤其在遮挡环境下性能下降明显。此外，多数系统缺乏对新型物体的快速扩展能力，限制了其在动态机器人环境中的部署。因此需要一种能融合姿态感知与参数化属性推理、支持灵活扩展且可在仿真中训练并泛化到真实场景的统一框架。

Method: 提出PIRATR框架，基于扩展的PI3DETR架构，采用端到端方式从点云中直接联合预测多类别6-DoF位姿与类特定参数属性；引入模块化、类别专用的检测头结构，使新增物体类型无需重构整个管道；利用合成数据进行全监督训练，并通过预定义规则将参数属性映射至3D模型以实现动态调整。

Result: 在自动化叉车平台上的三类不同结构与功能的物体（吊钩夹具、装载平台、托盘）上验证，仅在合成数据上训练即可在真实室外LiDAR数据上实现0.919的检测mAP，无需额外微调，表现出强泛化能力；相比传统方法，在姿态精度和参数属性估计方面均有显著提升。

Conclusion: PIRATR建立了一种新的姿态感知、参数化的感知范式，有效连接低层次几何推理与高层次可行动世界模型，为构建可扩展、仿真训练、适用于动态机器人环境的感知系统提供了新路径。

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [50] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: ViGT 是一种无需标定的自动驾驶几何模型，通过自监督学习从多视角摄像头中连续估计3D占用场，支持多种传感器配置，具有良好的可扩展性和泛化能力，在多个数据集上达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶几何模型通常依赖于像素对齐的预测，且需要昂贵的人工标注，限制了其在不同传感器配置下的通用性；因此需要一种能够适应多样传感器、无需手动标注并提供统一几何表示的高效模型。

Method: 提出视觉隐式几何变换器（ViGT），采用自监督训练方式利用同步图像-激光雷达数据对，构建不依赖标定的连续3D占用场估计架构，以鸟瞰图（BEV）形式输出统一的几何表示。

Result: ViGT 在混合五个大规模自动驾驶数据集（NuScenes, Waymo, NuPlan, ONCE, Argoverse）上训练后，在点云重建任务中表现优于所有基线方法，平均排名最佳；在 Occ3D-nuScenes 基准测试中性能接近监督方法。

Conclusion: ViGT 为自动驾驶领域提供了可扩展、简洁且通用的几何基础模型，具备强大的跨传感器适应能力与自监督学习优势，推动了自动驾驶几何建模的发展。

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [51] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 提出 LocateEdit-Bench，一个包含 23.1 万张编辑图像的大规模数据集，用于评估基于指令的图像编辑中的伪造定位方法。该数据集涵盖四种前沿编辑模型和三种常见编辑类型，并设计了多度量评估协议，以推动针对新型编辑技术的伪造检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有 AI 生成伪造定位方法主要针对修补类操作，无法有效应对最新的基于指令的图像编辑，亟需新数据集和评估标准来应对这一挑战。

Method: 构建 LocateEdit-Bench 数据集，整合四种先进编辑模型与三类常见编辑类型，并设计双多指标评估协议以全面评测现有定位方法。

Result: 成功建立可用于评估指令驱动图像编辑中伪造定位能力的基准数据集与评估体系，为未来伪造检测方法的发展奠定基础。

Conclusion: LocateEdit-Bench 为应对图像编辑技术快速演进提供了关键基础设施，推动伪造定位研究与实际应用同步发展。

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [52] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的算子理论框架，用于分析相机位姿估计中每个特征的敏感性。通过将影响函数理论扩展到矩阵李群SE(3)，推导出左平凡化M-估计器的内在扰动算子，提出了几何可观测性指数（GOI），该指数通过曲率算子和可观察子空间的李代数结构量化单个测量的贡献。GOI具有谱分解特性，揭示了弱可观测性与敏感性放大之间的直接对应关系，并在总体情况下等价于SE(3)上的Fisher信息几何，提供单次测量的Cramer-Rao界类比。该方法解释了经典退化现象（如纯旋转、视差消失）以及动态特征在弱曲率方向上的放大效应，实现了条件分析、Fisher信息几何、影响函数理论与动态场景可检测性的几何一致性统一。由于这些量可直接在高斯-牛顿流程中获得，其曲率谱和GOI可作为轻量级、无需训练的诊断信号，用于识别动态特征和检测弱可观测配置，且无需修改现有SLAM架构。


<details>
  <summary>Details</summary>
Motivation: 传统敏感性分析工具（如条件分析、欧氏扰动、Fisher信息界）无法解释单个图像特征如何影响位姿估计，也无法说明动态或不一致观测为何会显著扭曲现代SLAM和运动恢复结构系统。需要一种能刻画个体测量影响并揭示系统敏感机制的统一几何框架。

Method: 将影响函数理论扩展至矩阵李群SE(3)，构建左平凡化M-estimator的内在扰动算子；基于曲率算子与李代数结构定义几何可观测性指数（GOI）；利用谱分解揭示可观测性与敏感性的几何关联；证明在群体极限下GOI与Fisher信息几何一致，形成单测量版本的Cramer-Rao界。

Result: GOI成功量化单个测量对位姿估计的影响，揭示弱可观测性与敏感性放大的内在联系；解释经典退化情况（如纯旋转、视差为零）及动态特征在弱曲率方向的放大行为；提供无需训练的轻量级诊断信号，可在不修改现有SLAM架构的前提下实现动态特征识别与弱可观测性检测。

Conclusion: 该框架通过曲率算子的谱几何，统一了条件分析、Fisher信息几何、影响函数理论与动态场景可检测性，为相机位姿估计中的感知敏感性提供了几何一致且可计算的解释，具备直接集成于现有优化流程的实用性。

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [53] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: EgoPoseVR 是一种用于虚拟现实（VR）中准确、时序一致的全身姿态估计的端到端框架，通过融合头戴设备运动信号与第一人称RGB-D观测数据，利用时空编码器和交叉注意力机制实现多模态信息融合，并引入运动学优化模块提升姿态估计的精度与稳定性。研究还构建了一个大规模合成数据集以支持训练与评估。实验表明，EgoPoseVR 在性能上优于现有方法，并在用户研究中获得更高的主观评价，可实现无需额外传感器或房间级追踪系统的可靠全身追踪。


<details>
  <summary>Details</summary>
Motivation: 当前基于头戴摄像头的自我中心姿态估计方法在应用于VR头显时存在时序不稳定、下肢估计不准确以及实时性不足等问题，亟需一种更鲁棒且高效的方法来实现高质量的全身姿态追踪。

Method: 提出EgoPoseVR框架，结合头戴设备运动信号与第一人称RGB-D图像，采用时空编码器提取帧级与关节级特征，通过交叉注意力实现跨模态融合，并引入运动学优化模块利用HMD信号施加约束，提升姿态估计的准确性与稳定性。

Result: EgoPoseVR在多个基准测试中超越现有先进模型，在真实场景的用户研究中也获得了显著更高的主观评分，包括准确性、稳定性、沉浸感及未来使用意愿。

Conclusion: EgoPoseVR能够实现无需外部传感器或房间级追踪系统的高精度、高稳定性的全身姿态追踪，为虚拟现实中的自然交互提供了实用解决方案。

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [54] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: CAViT提出一种双注意力架构，用动态注意力机制替代ViT中静态的MLP，通过空间自注意力和通道自注意力的联合操作，实现内容感知的特征重校准，提升模型表达能力。在多个自然与医学图像数据集上，CAViT在准确率上相比标准ViT最高提升3.6%，同时减少超过30%的参数量和计算量。


<details>
  <summary>Details</summary>
Motivation: ViT虽在建模长距离空间关系上表现优异，但其通道混合依赖固定MLP，缺乏对输入内容的自适应能力，限制了特征表达的灵活性。

Method: CAViT在每个Transformer块中引入通道自注意力，与空间自注意力协同工作，构建内容感知的动态特征交互机制，实现统一且自适应的令牌混合策略。

Result: CAViT在五个基准数据集上均优于标准ViT，最高提升3.6%准确率，同时降低参数量与计算量超30%，且注意力图显示更清晰、语义合理的激活模式。

Conclusion: 通过引入动态通道注意力机制，CAViT实现了更高效、更具表达力的特征建模，在不增加模型深度或复杂度的前提下显著提升了性能。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [55] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的多实例非经典模型拟合方法，通过优化问题框架结合新型基于模型到数据误差的估计器与元启发式优化算法，有效处理噪声数据中的异常值，并实现对多种非经典模型（如螺旋曲线、程序化字符模型、自由曲面）的多实例重建。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒拟合方法主要针对经典模型（如直线、圆、平面），对非经典模型（如螺旋曲线、程序化字符、自由曲面）的多实例拟合研究较少，且多数方法仅适用于单个实例重建。因此，亟需一种能够从噪声数据中鲁棒地重建多个非经典模型实例的方法。

Method: 将多实例拟合建模为一个优化问题，包含一个基于模型到数据误差的新型非可微估计器和一个用于寻找全局最优解的元启发式优化算法。估计器无需预设误差阈值即可有效识别并处理异常值。

Result: 实验结果表明，该方法在多种非经典模型上均表现出良好的鲁棒性和准确性，能有效实现多实例重建。代码已开源，便于复现与应用。

Conclusion: 本文提出的多实例非经典模型拟合方法在处理复杂几何结构和噪声数据方面具有显著优势，为计算机视觉与图形学中的复杂形状重建提供了有效的解决方案。

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [56] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: XSIM 是一个用于自动驾驶的传感器仿真框架，通过扩展3DGUT点云渲染并引入针对自动驾驶场景的广义滚动快门模型，实现了对复杂传感器畸变的动态环境渲染。针对球形相机（如LiDAR）在方位边界处因周期性投影和时间不连续性导致的粒子投影错误问题，提出相位建模机制以显式处理高斯分布的时空不连续性；同时引入包含两个独立透明度参数的扩展3D高斯表示，以解决几何与颜色分布不匹配的问题。实验在Waymo Open Dataset、Argoverse 2和PandaSet上验证了其优越性能，显著优于现有基线方法，达到当前最佳水平。代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有3DGUT点云渲染方法在处理球形相机（如LiDAR）时存在周期性投影和方位边界处的时间不连续性问题，导致粒子投影错误，影响几何一致性与视觉真实感。此外，传统3D高斯表示在几何与颜色分布之间存在不匹配，限制了场景重建质量。因此亟需一种更统一、灵活且能准确建模复杂传感器行为的仿真框架。

Method: 提出XSIM框架，核心包括：1）基于广义滚动快门模型的传感器模拟；2）针对球形相机的相位建模机制，用于处理高斯分布于方位边界处的时空不连续性；3）扩展3D高斯表示，引入两个独立的透明度参数，实现几何与颜色分布的解耦优化；4）统一的外观与几何建模范式，支持动态环境中复杂畸变的渲染。

Result: 在Waymo Open Dataset、Argoverse 2和PandaSet等多个自动驾驶数据集上进行评估，XSIM consistently 超越多个强基线方法，实现最先进的性能，在几何一致性与视觉逼真度方面均有显著提升。

Conclusion: XSIM通过引入相位建模与双透明度3D高斯表示，有效解决了球形传感器在动态环境中的投影错误与几何-颜色不一致问题，构建了更精确、更真实的自动驾驶传感器仿真系统，为下一代感知系统开发提供了有力工具。

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [57] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: UniSurg提出一种新的视频原生基础模型，通过将学习范式从像素级重建转向潜在运动预测，显著提升手术视频理解能力。其核心创新包括：运动引导的潜在预测、时空亲和自蒸馏以及特征多样性正则化，有效聚焦语义结构并避免纹理稀疏场景下的表示崩溃。基于3,658小时、涵盖13个解剖区域的UniSurg-15M数据集进行大规模预训练，实验在17个基准上全面超越现有方法，在工作流识别、动作三元组识别、技能评估、息肉分割和深度估计等任务中表现卓越，确立了通用、以运动为导向的手术视频理解新标准。


<details>
  <summary>Details</summary>
Motivation: 当前手术视频分析依赖于像素级重建目标，导致模型资源浪费在低层次视觉细节（如烟雾、反光、液体运动）而非关键语义结构上，限制了对手术过程的深层理解。

Method: 基于视频联合嵌入预测架构（V-JEPA），引入三项技术改进：1）运动引导的潜在预测，聚焦语义重要区域；2）时空亲和自蒸馏，强化局部与全局关系一致性；3）特征多样性正则化，防止在纹理稀疏场景中出现表示坍缩。同时构建了包含3,658小时视频的UniSurg-15M大规模数据集用于预训练。

Result: 在17个基准测试中，UniSurg显著优于现有方法：工作流识别提升14.6% F1（EgoSurgery）、10.3%（PitVis）；动作三元组识别达到39.54% mAP-IVT（CholecT50）；在技能评估、息肉分割和深度估计任务中也取得领先性能。

Conclusion: UniSurg作为首个以运动为核心、面向手术视频的视频原生基础模型，通过从像素重建转向潜在运动预测，实现了更高效、更准确的手术理解，为通用手术视频分析树立了新标杆。

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [58] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: 本研究探讨了大五人格模型中更细粒度的层次（如特质层面、维度层面和细微层面）对从音视频交互数据中进行人格识别的预测影响。使用UDIVA v0.5数据集，训练了一个包含跨模态（音视频）和跨主体（双人感知）注意力机制的Transformer模型。结果表明，细微层面的模型在所有交互场景中均显著优于维度和特质层面的模型，平均平方误差降低高达74%。


<details>
  <summary>Details</summary>
Motivation: 当前人格识别模型通常依赖于宽泛的人格特质分数作为真实标签，但这种做法受限于训练数据不足，且相似特质分数可能通过多种情境化行为表现出来，导致模型泛化能力差。因此，需要探索更细粒度的人格层级结构以提升识别效果。

Method: 采用基于Transformer的模型，结合跨模态（音频与视觉信息融合）和跨主体（考虑两人互动关系）的注意力机制，在UDIVA v0.5数据集上进行训练与评估，比较不同人格层级（特质、维度、细微）的预测性能。

Result: 细微层面的人格模型在各类交互场景中表现最佳，相比特质和维度层面模型，平均平方误差降低最高达74%，显示出细粒度建模对提升人格识别准确性的显著作用。

Conclusion: 引入人格模型中的细微层面能够显著提升从音视频交互数据中识别人格的准确性，表明细粒度建模是改善人格识别模型性能的有效路径，尤其在复杂多变的交互情境下具有重要意义。

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [59] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: ShapeUP 是一种可扩展的、图像条件的 3D 编辑框架，通过在原生 3D 表示中进行监督的隐空间到隐空间映射，实现高保真且结构一致的 3D 编辑。它利用预训练的 3D 基础模型并结合监督学习，以图像为提示实现细粒度视觉控制，支持局部与全局编辑，无需掩码即可定位，显著优于现有训练和免训练基线，在身份保留和编辑保真度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 编辑方法在视觉可控性、几何一致性与可扩展性之间存在难以调和的矛盾：优化方法速度慢，多视图 2D 传播存在视觉漂移，而免训练的隐空间操作受限于固定先验，无法受益于规模扩展。因此亟需一种兼具高效、精确与可扩展性的新范式。

Method: ShapeUP 将 3D 编辑建模为基于预训练 3D 基础模型的监督隐空间到隐空间映射，采用 3D Diffusion Transformer (DiT) 模型，通过三元组（源 3D 形状、编辑后的 2D 图像、对应的编辑后 3D 形状）进行训练，实现图像作为提示的直接映射。

Result: ShapeUP 在身份保留和编辑保真度上持续超越当前训练与免训练基线，具备良好的结构一致性与细粒度控制能力，实现了无需掩码的隐式定位，并展现出强大的可扩展性。

Conclusion: ShapeUP 提供了一种鲁棒且可扩展的原生 3D 内容创作新范式，有效解决了 3D 编辑中长期存在的可控性与一致性难题，是迈向高质量 3D 生成与编辑的重要一步。

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [60] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出一种多任务学习框架，利用点监督提升模型对动作时序理解的能力。设计了三种自监督时序理解任务：动作补全、动作顺序理解和动作规律理解，以增强模型对动作跨视频时序一致性的把握。这是首个针对点监督动作定位显式探索时序一致性的尝试。在四个基准数据集上的实验表明，该方法优于多个现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖点监督的片段级分类任务，缺乏对动作帧间时序关系的显式建模，而时序关系对理解动作定义和定位完整动作区间至关重要。因此需要增强模型的时序理解能力。

Method: 设计三种自监督时序理解任务：(i) 动作补全，(ii) 动作顺序理解，(iii) 动作规律理解，并构建多任务学习框架，充分利用点标注信息提升模型对时序一致性的感知能力。

Result: 在四个基准数据集上，所提方法在点监督下实现了优于现有先进方法的性能，验证了其有效性。

Conclusion: 通过引入自监督时序理解任务，本研究显著提升了点监督下模型对动作时序结构的理解能力，为点监督动作定位提供了新的有效范式。

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [61] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGFF-Embed的新方法，旨在解决当前多模态嵌入模型仅能捕捉全局语义信息的局限性。该方法通过引导大模型生成关注不同语义维度的多个嵌入，并自适应地融合这些嵌入，从而实现对全局与细粒度感知信息的有效结合。同时，结合显式梯度放大技术（EGA），在不需对数据集进行细粒度编辑的前提下，增强批次内难样本的学习效果。在MMEB和MMVP-VLM基准上的实验表明，AGFF-Embed在通用理解和细粒度理解方面均达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP和多模态大模型（MLLM）的嵌入模型主要局限于捕捉全局语义信息，而复杂场景往往包含全局与细粒度信息的混合模式，因此需要一种能够兼容融合这两种感知层次的机制。

Method: 提出AGFF-Embed方法，通过提示MLLM生成多个侧重不同语义维度的嵌入，并采用自适应平滑聚合策略进行融合；同时引入显式梯度放大（EGA）技术以增强批内难负样本的学习效果，无需修改数据集。

Result: 在MMEB和MMVP-VLM两个基准测试中，AGFF-Embed在通用理解与细粒度理解任务上均取得了当前最优的性能表现。

Conclusion: AGFF-Embed通过有效融合全局与细粒度感知信息，在多模态嵌入任务中展现出卓越的综合性能，为复杂场景下的跨模态理解提供了新思路。

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [62] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: 提出DepthPrior框架，利用深度信息作为先验知识而非融合特征，通过深度相关的损失加权、分层和置信度阈值调整，在不修改检测器架构的前提下显著提升小物体和远距离物体的检测性能，实验表明在多个基准上小物体的mAP$_S$和mAR$_S$分别提升最高达9%和7%，且推理召回率高达95:1。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测小尺寸和远距离物体时受限于尺度变化、低分辨率和背景杂乱；虽然深度信息有助于提升检测效果，但现有方法需复杂且模型特定的结构修改，限制了其通用性和实用性。因此需要一种无需架构改动、能有效利用深度先验的通用解决方案。

Method: 提出DepthPrior框架，包含训练阶段的深度基损失加权（DLW）与深度基损失分层（DLS），以及推理阶段的深度感知置信度阈值（DCT）。深度信息仅作为先验指导损失函数设计与预测后处理，不改变原有检测器结构。

Result: 在KITTI、MS COCO、VisDrone、SUN RGB-D四个基准上，使用YOLOv11和EfficientDet两种检测器进行测试，结果表明该方法在小物体检测上实现最高+9% mAP$_S$和+7% mAR$_S$的提升，且推理中真实检测与误检比例可达95:1，证明其高效性与鲁棒性。

Conclusion: DepthPrior通过将深度信息作为先验知识，实现了对小物体和远距离物体的有效检测提升，具有良好的通用性、无额外硬件依赖、无需模型结构调整，是安全关键应用中可靠的轻量级改进方案。

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [63] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: FMPose3D提出一种基于流匹配（Flow Matching）的高效3D姿态估计框架，通过求解常微分方程（ODE）实现从先验分布到条件3D姿态分布的连续传输，仅需少量积分步骤即可生成高质量姿态样本。利用不同噪声种子自然产生多组合理姿态假设，并通过重投影后验期望聚合（RPEA）模块融合生成最终准确预测。该方法在Human3.6M、MPI-INF-3DHP、Animal3D和CtrlAni3D等多个基准上超越现有方法，达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 单目3D姿态估计因深度模糊和遮挡问题本质上是病态的，传统方法难以处理不确定性。虽然扩散模型表现良好，但其迭代去噪过程计算开销大，推理效率低。因此需要更高效的概率化方法来生成多个合理的3D姿态假设。

Method: 提出FMPose3D框架，将3D姿态估计建模为条件分布传输问题，使用流匹配学习由ODE定义的速度场，实现从标准高斯先验到2D输入条件下的3D姿态分布的快速连续映射。通过随机采样噪声种子生成多样化的姿态假设，并引入重投影后验期望聚合（RPEA）模块，近似贝叶斯后验期望以获得单一精确输出。

Result: FMPose3D在Human3.6M和MPI-INF-3DHP等主流人体3D姿态数据集上优于现有方法；同时在Animal3D和CtrlAni3D动物3D姿态数据集上也达到最先进水平，证明了其在跨域3D姿态估计中的强大泛化能力与高效性。

Conclusion: FMPose3D通过流匹配实现了高效且灵活的概率化3D姿态估计，显著降低推理时间，同时保持高精度，在多种场景下均表现出卓越性能，为复杂视觉任务中的不确定性建模提供了新范式。

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [64] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: ReText提出一种新方法，通过融合多摄像头和单摄像头数据，并结合文本描述增强语义信息，在图像-文本匹配和图像重建任务的联合优化下，实现跨域行人重识别的强泛化能力。该方法首次探索了多模态联合学习在混合数据上的应用，显著优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽能缓解领域差异，但依赖复杂架构；而单摄像头数据虽易获取却缺乏跨视角变化，导致复杂度不足。为提升泛化能力，需引入更丰富的语义信息以弥补单摄像头数据的局限性。

Method: ReText在混合数据上训练，包括多摄像头Re-ID数据与单摄像头数据，后者通过文本描述补充语义信息。训练时联合优化三个任务：多摄像头下的行人重识别、图像-文本匹配、以及基于文本引导的图像重建。

Result: 实验表明，ReText在跨域行人重识别基准上表现优异，显著超越当前最先进的方法，验证了其强大的泛化能力。

Conclusion: ReText是首个在图像-文本联合学习框架下，利用混合多摄像头与单摄像头数据进行行人重识别的方法，有效提升了模型在未见场景中的泛化性能。

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [65] [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789)
*Hengyi Wang,Ruiqiang Zhang,Chang Liu,Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的策略Allocentric Perceiver，通过利用现成的几何专家从一张或多张图像中恢复度量3D状态，并构建与指令语义意图对齐的查询条件分配的非中心参考系。该方法将空间推理中的心理旋转任务从隐式推理转移到显式计算，显著提升了视觉-语言模型在非中心空间查询上的表现，在多个基准测试中取得约10%的提升，同时保持了良好的自我中心性能，优于经过空间感知微调的模型以及当前最先进的开源和专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在需要明确视角转换的非中心空间查询任务上表现脆弱，因为答案依赖于目标中心坐标系而非观察到的相机视角，因此亟需增强模型的非中心感知能力以支持更复杂的空间推理任务。

Method: 利用现成的几何专家从图像中重建度量3D状态，构建一个与指令语义对齐的查询条件分配的非中心参考框架；通过确定性地将重建的几何结构转换到目标框架，并使用结构化的、基于几何的表示来提示骨干视觉-语言模型，从而将心理旋转等认知负担转化为显式计算。

Result: 在多个骨干模型家族上评估，Allocentric Perceiver 在非中心任务中表现出一致且显著的提升（约10%），同时维持了强大的自我中心性能，超越了空间感知微调模型以及当前最先进的开源和专有模型。

Conclusion: Allocentric Perceiver 通过显式几何重构与参考框架对齐，有效增强了视觉-语言模型在非中心空间推理任务中的鲁棒性和准确性，为未来空间感知任务提供了高效、可扩展的解决方案。

Abstract: With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.

</details>


### [66] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Focus-Scan-Refine（FSR）的训练无关视觉令牌剪枝框架，模拟人类回答视觉问题的过程：聚焦关键证据、扫描全局上下文并精炼信息。该方法通过结合视觉重要性和指令相关性来聚焦关键区域，避免偏向视觉显著但与查询无关的区域；随后基于聚焦区域扫描互补上下文，并选择与聚焦内容差异最大的令牌；最后通过相似性分配和得分加权合并，将邻近信息融入扫描锚点，不增加令牌预算。在多个视觉语言模型和基准测试中，FSR在准确率与效率之间实现了优于现有先进方法的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的令牌剪枝方法在高压缩率下难以兼顾局部证据与全局上下文，导致性能下降。为解决此问题，需一种更符合人类认知过程的剪枝机制，以提升剪枝后的模型表现。

Method: FSR框架包含三个阶段：1）聚焦（Focus）——基于视觉重要性和指令相关性筛选关键视觉令牌；2）扫描（Scan）——在聚焦结果基础上，选择与之差异最大的令牌以获取互补上下文；3）精炼（Refine）——通过相似性分配和得分加权方式，将附近有信息量的令牌聚合到扫描锚点，保持总令牌数不变。

Result: 在多种视觉语言模型（如BLIP-2、LLaVA）和基准（如VQAv2、OK-VQA、TextVQA）上，FSR在不同压缩率下均显著提升准确率，同时降低延迟和内存占用，优于当前最先进的剪枝方法。

Conclusion: FSR是一种高效且可插拔的剪枝框架，其模仿人类认知过程的设计有效提升了视觉语言模型在低资源条件下的推理性能，为实现高精度与高效率的平衡提供了新范式。

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [67] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: 本文提出SparseVideoNav，利用视频生成模型的长时序监督优势，解决远距离、未视域导航（BVN）问题。通过生成20秒跨度的稀疏未来视频，实现亚秒级轨迹推理，较原始方法提速27倍，并在真实世界零样本测试中达到先进LLM基线2.5倍的成功率，首次实现在复杂夜景中的高效自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的方法依赖密集、详细的语言指令，难以适应真实世界中简单高阶意图下的自主导航需求。尤其在远距离未视域导航（BVN）任务中，短视行为和训练不稳定性成为主要障碍。因此亟需一种能处理长时程、低密度指导的新型导航机制。

Method: 提出将视频生成模型引入视觉-语言导航领域，利用其天然具备的长时序对齐能力；设计SparseVideoNav框架，通过生成20秒跨度的稀疏未来视频，实现快速推理；采用稀疏化策略降低计算开销，保障实时性。

Result: 在真实世界零样本测试中，SparseVideoNav相比当前最优的LLM基线成功率达2.5倍，且在夜间复杂场景中首次实现有效导航；推理速度提升27倍，满足实际部署需求。

Conclusion: 视频生成模型为解决远距离未视域导航提供了全新路径，SparseVideoNav通过稀疏未来视频生成实现了高效、鲁棒的自主导航，标志着该领域的重要突破。

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [68] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出Test-Time Correction (TTC)方法，通过初始帧作为稳定参考点校准生成过程中的随机状态，有效缓解长序列生成中的误差累积问题，且无需训练，可无缝集成到多种蒸馏模型中，在30秒基准上达到与资源密集型训练方法相当的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化（TTO）方法在长序列生成中因奖励景观不稳和蒸馏参数敏感而失效，无法有效缓解生成漂移问题。

Method: TTC利用初始帧作为稳定参考锚点，对采样轨迹中的中间随机状态进行校准，实现无需训练的实时修正。

Result: 实验表明，TTC能显著延长生成长度，计算开销极低，且在30秒视频生成任务中达到与训练型方法相当的质量。

Conclusion: TTC是一种高效、通用的训练-free方法，有效解决了蒸馏自回归扩散模型在长序列生成中的误差累积问题。

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [69] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出了一种名为EoCD的简单有效的变化检测方法，通过早期融合时序数据并用无参数的多尺度特征融合模块替代解码器，显著降低了模型复杂度。该方法在多种编码器架构下实现了性能与速度的最佳平衡，表明模型性能主要依赖于编码器，解码器仅为附加组件。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法通常使用孪生编码器分别提取时序特征并进行后期融合，随后设计复杂的解码器以提升性能，导致计算成本高、模型复杂度大。而早期融合方法虽减少了编码器开销，但性能仍不如后期融合方法。因此，亟需一种既能降低复杂度又保持高性能的方法。

Method: 提出EoCD方法，采用早期融合策略将时序图像合并，并用参数无关的多尺度特征融合模块替代传统解码器，从而简化模型结构，降低计算负担。

Result: 在四个具有挑战性的变化检测数据集上进行了大量实验，结果表明EoCD在保持高检测性能的同时，显著提升了预测速度，且对不同编码器具有良好的适应性。

Conclusion: EoCD证明了模型性能主要取决于编码器，解码器并非必要组件；该方法在性能与效率之间取得了最优平衡，为轻量化、高效的变化检测提供了新思路。

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [70] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: 本文提出一种从CT血管造影（CTA）中稀疏平面分割重建完整3D心脏形态的方法，应用于2D经胸超声心动图（TTE）。利用神经隐式函数，从模拟标准心尖视图的TTE平面分割中重建心脏腔室和左心室心肌的3D形状。通过多层感知机学习来自CTA中目标结构的3D分割形状先验，在测试时联合优化潜在代码和刚性变换以将观测平面映射到3D空间。在保留集上，平均Dice系数为0.86±0.04，左心室和左心房的体积误差显著低于临床标准Simpson双平面法。


<details>
  <summary>Details</summary>
Motivation: 现有2D经胸超声心动图（TTE）在量化心脏腔室体积方面存在局限，难以实现精确的3D分析。本研究旨在通过从稀疏的2D TTE视图中重建高质量3D心脏结构，提升心脏功能与解剖定量分析的准确性。

Method: 采用神经隐式函数建模心脏结构，使用多层感知机学习来自3D CTA分割的形状先验；在测试阶段，联合优化潜在代码与刚性变换，将模拟心尖视图的稀疏2D分割映射至3D空间，实现3D形状重建。

Result: 在独立测试集上，所有结构的平均Dice系数为0.86±0.04；左心室体积误差为4.88±4.26 mL，显著优于Simpson法的8.14±6.04 mL；左心房体积误差为6.40±7.37 mL，远低于Simpson法的37.76±22.96 mL。

Conclusion: 所提方法能够有效从2D TTE模拟视图中重建高精度3D心脏结构，为2D经胸超声心动图提供更准确的三维定量分析路径。

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [71] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于映射的CLIP压缩框架CLIP-Map，通过可学习矩阵结合预训练权重，并采用Kronecker分解实现全映射，以尽可能保留原始权重信息。为缓解优化挑战，提出对角继承初始化方法以减少分布偏移问题。实验表明，CLIP-Map在各种压缩比下均优于基于选择的框架，尤其在高压缩场景下表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP压缩方法依赖权重选择与继承，但在极端压缩情况下容易损失特征表达能力，导致性能下降。因此需要一种能更高效保留原始信息的压缩方法。

Method: 提出CLIP-Map框架，利用可学习矩阵进行全映射与Kronecker因子分解来组合预训练权重，并引入对角继承初始化以减轻分布偏移问题，提升映射学习效率与效果。

Result: 在多种压缩比下，CLIP-Map均优于传统选择式压缩方法，尤其在高压缩设置中表现出显著优势，有效提升了压缩后模型的性能。

Conclusion: CLIP-Map通过映射机制和优化初始化策略，在保持低计算开销的同时显著提升了压缩模型的特征表达能力，是高效且鲁棒的CLIP压缩方案。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [72] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出MGIPT方法，通过多尺度全局-实例提示调优增强医学图像分割中持续测试时适应（CTTA）的鲁棒性，解决现有方法在多尺度提示多样性、实例级知识融合及隐私泄露方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖参数增量更新，易导致误差累积和灾难性遗忘；尽管提示调优有一定改进，但仍存在多尺度提示多样性不足、实例特定知识利用不充分以及隐私泄露风险等问题。

Method: 设计MGIPT框架，包含自适应尺度实例提示（AIP）与多尺度全局提示（MGP），分别捕捉实例级和域级知识，并通过加权集成实现双层次信息融合，提升跨域适应能力。

Result: 在多个医学图像分割基准上实验表明，MGIPT优于当前最优方法，在持续变化的目标域中表现出更强的适应性和鲁棒性。

Conclusion: MGIPT有效缓解了误差累积与遗忘问题，增强了提示多样性与实例感知能力，同时降低隐私泄露风险，为医疗图像中的持续测试时适应提供了高效可靠的解决方案。

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [73] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出在流匹配（flow matching）中学习条件依赖的源分布，以更好地利用丰富的条件信号。通过引入方差正则化和方向对齐，解决了直接将条件信息融入源分布时出现的分布坍缩与不稳定性问题。研究还揭示了目标表示空间的选择对结构化源设计的有效性影响，并在多个文本到图像基准上实现了高达3倍的FID收敛速度提升，证明了合理设计源分布的实际优势。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法大多沿用扩散模型中的标准高斯分布作为源分布，忽视了源分布本身可作为优化目标。然而，在现代大规模文本到图像生成系统中，合理设计源分布能够更有效利用条件信息，从而提升生成质量与训练效率。

Method: 提出一种基于流匹配目标的条件依赖源分布学习方法；引入方差正则化和源-目标方向对齐机制以解决分布坍缩与训练不稳定问题；分析不同目标表示空间对结构化源设计的影响。

Result: 在多个文本到图像生成基准上实现显著性能提升，包括最多3倍的FID收敛速度加快，验证了所提方法在稳定性和有效性上的优越性。

Conclusion: 合理设计条件依赖的源分布是可行且有益的，尤其在大规模文本到图像生成任务中，通过适当的正则化与对齐策略，能显著提升流匹配模型的训练效率与生成质量。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [74] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: 本文提出一种名为局部语义对齐（LSA）的简单而有效的方法，用于微调预训练视频生成模型，以提升动态物体在视频生成中的时间一致性。通过比较真实视频与生成视频中围绕动态物体的语义特征，引入语义特征一致性损失，并结合标准扩散损失进行微调。仅用一个训练周期即可显著优于基线方法，在nuScenes和KITTI数据集上表现出色，且无需推理时的外部控制信号或额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法依赖推理时的控制信号来保证动态物体的时间一致性，限制了其作为可扩展、通用的数据引擎的潜力。本文旨在解决这一问题，实现无需外部控制信号即可生成时间一致视频的能力。

Method: 提出局部语义对齐（LSA）框架，利用预训练特征提取器对比真实视频与生成视频中动态物体区域的语义特征，构建语义一致性损失，并与扩散损失联合优化模型。

Result: 微调仅需一个训练周期，即在常见视频生成评估指标及新增的mAP、mIoU指标上均超越基线；在nuScenes和KITTI数据集上验证了方法在提升时间一致性方面的有效性，且无推理时控制信号需求和计算开销。

Conclusion: LSA框架能够有效提升预训练视频生成模型在动态场景下的时间一致性，无需外部控制信号，具备良好的可扩展性与实用性，适用于自动驾驶等需要高质量合成交通场景的应用。

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [75] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: RISE-Video 是一个面向文本-图像到视频（TI2V）合成的推理导向基准，旨在评估生成模型在隐含世界规则下的深层认知推理能力。该基准包含467个经过人工标注的样本，涵盖8个严格类别，覆盖常识、空间动态和专业领域等多个维度。研究提出四维评估指标：推理一致性、时间连贯性、物理合理性与视觉质量，并引入基于大模型的自动化评估流程以实现可扩展性。对11个先进TI2V模型的实验揭示了其在复杂隐含约束场景下的普遍不足，为未来世界模拟生成模型的发展提供了关键洞见。


<details>
  <summary>Details</summary>
Motivation: 当前生成视频模型虽在视觉保真度上表现优异，但在内化和推理隐含世界规则方面仍存在显著短板，亟需建立更深入的评估体系来推动模型智能发展。

Method: 构建RISE-Video基准数据集，包含467个人工标注样本，覆盖8个评估维度；设计四维评价指标（推理一致性、时间连贯性、物理合理性、视觉质量）；开发基于大多模态模型（LMMs）的自动化评估管道以支持大规模评估。

Result: 对11个主流TI2V模型的实验表明，这些模型在处理隐含约束条件下的复杂场景时普遍存在缺陷，尤其在推理一致性和物理合理性方面表现不佳，凸显现有模型在深层认知能力上的不足。

Conclusion: RISE-Video为评估视频生成模型的推理能力提供了系统性框架，揭示了当前模型在世界模拟方面的局限性，推动未来研究向更深层次的认知建模演进。

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [76] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: VisRefiner是一种用于截图转代码生成的训练框架，通过引入视觉差异对齐的监督信号和强化学习自修正机制，使模型能够从渲染结果与目标设计之间的视觉差异中学习代码修改策略。该方法显著提升了生成代码的布局保真度和质量，并赋予模型强大的自我优化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大模型的截图转代码方法直接从截图生成代码，但缺乏对生成代码视觉效果的反馈，无法像人类开发者那样通过迭代渲染和对比来学习代码与视觉表现之间的关系。因此需要一种机制让模型能够从视觉差异中学习如何改进代码。

Method: 提出VisRefiner框架，包含两阶段：(1) 构建差异对齐的监督信号，将视觉差异与对应的代码修改关联起来；(2) 引入强化学习进行自修正，模型根据渲染输出与目标设计的视觉差异，自动调整并优化生成的代码。

Result: 实验表明，VisRefiner在单步生成质量、布局保真度方面均有显著提升，并具备出色的自修正能力，验证了从视觉差异中学习的有效性。

Conclusion: 通过让模型学习视觉差异与代码修改之间的映射关系，VisRefiner有效提升了截图转代码生成的质量与鲁棒性，为未来智能前端开发工具提供了新思路。

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [77] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出GenArena，一种基于成对比较的统一评估框架，以解决传统点对点评分在视觉生成模型评估中的不稳定性与人类感知偏差问题。实验表明，该方法显著提升评估准确性（超过20%），且与权威LMArena基准的相关性达0.86，远超传统方法的0.36。更重要的是，仅通过采用成对协议，开源模型即可超越顶级专有模型，为视觉生成领域提供了可靠、自动化的新评估标准。


<details>
  <summary>Details</summary>
Motivation: 传统点对点评分方法在视觉生成模型快速发展的背景下暴露出随机不一致性和与人类感知脱节的问题，亟需更可靠的评估范式。

Method: 提出GenArena框架，采用成对比较机制替代传统的绝对点对点评分，提升评估稳定性与人类感知对齐度。

Result: GenArena将评估准确率提升超过20%，与LMArena基准的Spearman相关系数达0.86，显著优于传统点对点方法的0.36；同时，仅使用成对协议即实现开源模型超越专有模型。

Conclusion: GenArena提供了一种高效、可靠且与人类判断高度一致的视觉生成模型评估新范式，推动了该领域的标准化发展。

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [78] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出Context Forcing框架，通过使用具备长上下文能力的教师模型来训练长上下文学生模型，解决传统方法中师生不匹配问题。引入慢-快记忆架构，降低视觉冗余，实现超过20秒的超长上下文生成，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统实时长视频生成方法中，教师模型仅能处理短时窗口（5秒），无法提供对长期时间依赖性的有效指导，导致学生模型在长序列生成中出现一致性问题。这种师生间上下文能力的不匹配限制了模型的性能上限。

Method: 提出Context Forcing框架，采用具备长上下文能力的教师模型监督长上下文学生模型；设计慢-快记忆架构（Slow-Fast Memory），将线性增长的上下文转化为可管理结构，减少冗余，提升计算效率。

Result: 实验表明，该方法可实现超过20秒的有效上下文长度，是LongLive和Infinite-RoPE等先进方法的2至10倍；在多个长视频评估指标上表现更优，显著提升了长时间生成的一致性与质量。

Conclusion: 通过消除师生间的上下文不匹配，结合高效的上下文管理机制，Context Forcing实现了高效、稳定的超长视频生成，为实时长视频生成提供了新的解决方案。

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [79] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出Splat and Distill框架，通过将2D视觉基础模型（VFM）的特征升维至3D高斯表示，并快速生成新视角下的2D特征图来提升模型的3D感知能力。该方法采用前馈式重建流程替代传统慢速逐场景优化，避免特征平均化问题，实现教师与学生模型的一致性协同提升。在单目深度估计、表面法线估计、多视角对应和语义分割等任务上显著优于现有方法，增强了2D特征的几何感知与语义丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型虽在2D任务中表现优异，但普遍缺乏3D感知能力，而以往方法依赖慢速的逐场景优化，易产生特征平均化问题，限制了模型对真实三维结构的理解。

Method: 提出一种前馈式的3D重建管道：将教师模型输出的2D特征升维为显式的3D高斯表示，再通过‘splatted’操作将其投影到新视角生成2D特征图，用于监督学生模型，实现基于几何约束的知识蒸馏。

Result: 在多个下游任务（如单目深度估计、表面法线估计、多视角对应、语义分割）中均取得显著性能提升，不仅增强了3D感知能力，还提升了2D特征的语义表达能力。

Conclusion: Splat and Distill通过前馈式3D特征重建与动态知识蒸馏机制，有效赋予2D视觉基础模型强大的3D意识，为跨模态理解提供了新范式。

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [80] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: 本文提出InterPrior框架，通过大规模模仿学习预训练与强化学习微调，构建统一的生成控制器，实现人类级全身动作的泛化能力。该框架先将专家策略蒸馏为条件化的变分策略，再通过物理扰动数据增强和强化学习优化，使模型具备在未见目标和初始状态下的鲁棒性，从而实现对新物体交互等行为的泛化。


<details>
  <summary>Details</summary>
Motivation: 人类很少以显式全身运动的方式规划与物体的交互，而依赖于高层次意图（如可及性）和底层物理与运动先验自然形成协调的动作。为使机器人能在多样环境中组合并泛化运动-操作技能，需建立可扩展的运动先验机制。

Method: 首先通过大规模模仿学习将全参考专家策略蒸馏为一个多功能、目标条件化的变分策略；接着引入物理扰动的数据增强，并使用强化学习进行微调，以提升模型在未见任务和初始状态下的表现；最终构建出稳定且可泛化的运动先验流形。

Result: 所提方法在未见目标、未见物体及不同初始条件下均表现出良好的泛化能力，支持用户交互控制，并具备向真实机器人部署的潜力。

Conclusion: InterPrior成功构建了一个可扩展、高泛化能力的生成式全身运动控制器，能够从高层意图中自然涌现出协调的平衡、接触与操作行为，为复杂人机交互场景中的机器人技能学习提供了有效解决方案。

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [81] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker提出了一种从被动融合到主动感知的空间推理新范式，通过空间锚定融合和重要性门控机制，让模型根据内部推理需求选择性地检索几何证据，显著提升了空间智能表现，在VSI-Bench上达到72.6的峰值分数，并在复杂下游任务中展现出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在空间推理中虽利用3D编码器的几何先验，但多数融合策略为被动且无差别地混合特征，导致语义与几何错位及冗余信号问题。

Method: GeoThinker采用空间锚定融合（Spatial-Grounded Fusion），在特定视觉语言模型层中通过帧严格交叉注意力，使语义视觉先验能选择性查询并整合任务相关的几何信息；同时引入重要性门控机制，引导每帧注意力聚焦于任务相关结构。

Result: GeoThinker在VSI-Bench上取得72.6的最高分，显著优于现有方法；在具身指代、自动驾驶等复杂场景中表现出更强的空间感知能力和鲁棒泛化性能。

Conclusion: 主动集成空间结构是下一代空间智能的关键，GeoThinker证明了通过有选择地感知和融合几何信息可大幅提升模型的空间推理能力。

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [82] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: SwimBird 是一种可切换推理模式的多模态大语言模型，通过动态选择文本、视觉或混合推理模式，解决现有模型在视觉密集型任务中表现受限的问题。它采用混合自回归框架统一处理文本与视觉思考，并构建了涵盖三种推理模式的 SFT 数据集，实现对不同输入查询的自适应响应，在保持强文本逻辑能力的同时显著提升视觉理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要依赖文本思维链（CoT）进行推理，限制了其在视觉密集型任务中的表现；虽然引入固定数量的连续隐藏状态作为‘视觉思考’有所改进，但常以牺牲文本逻辑推理能力为代价。核心问题在于固定的、预定义的推理模式无法根据输入灵活选择最适合的思考方式。

Method: 提出 SwimBird 模型，支持三种推理模式：纯文本、纯视觉（以连续隐藏状态表示视觉思考）、以及交替的视觉-文本推理。采用混合自回归架构，统一建模下一词预测与下一嵌入预测；设计系统性的推理模式标注策略，构建 SwimBird-SFT-92K 数据集，覆盖所有三种推理模式。

Result: SwimBird 在多个基准测试中均达到当前最优性能，尤其在视觉理解任务上表现突出，同时保持了强大的文本逻辑推理能力，相比固定模式的多模态推理方法展现出稳健的提升效果。

Conclusion: 通过动态适配推理模式，SwimBird 实现了视觉与文本推理之间的平衡，克服了传统 MLLMs 中因僵化推理路径导致的性能瓶颈，为多模态智能推理提供了更灵活、高效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [83] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: CAMCUE 是一个基于相机位姿的多图像框架，通过显式地将相机位姿作为跨视图融合和新视角推理的几何锚点，解决了多视图空间推理难题。该框架通过注入每张图像的位姿信息、将自然语言描述与目标相机位姿对齐，并生成条件化的想象视图来支持回答问题。为此，研究者构建了包含27,668个训练样本和508个测试样本的CAMCUE-DATA数据集，涵盖多样化的视角描述和视角转换问题，并引入人工标注的测试描述以评估泛化能力。实验表明，CAMCUE在整体准确率上提升9.06%，在旋转和位移精度上分别超过90%（误差小于20°和0.5），且推理时间从256.6秒大幅降至1.45秒，显著提升了实时交互性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在多图像空间推理方面仍面临挑战，尤其是从多视角中构建一致的3D场景理解并进行视角变换推理。现有方法往往依赖复杂的搜索匹配过程，效率低下，难以支持实时应用。因此需要一种高效、精准的框架来实现从多视图到新视角的快速推理。

Method: CAMCUE框架通过将相机位姿注入视觉令牌，实现对多视图信息的显式几何约束；利用自然语言描述定位目标相机位姿；并通过生成姿态条件化的想象视图，辅助完成视角转移后的推理任务。整个流程避免了传统方法中的试错式搜索，实现了端到端的高效推理。

Result: CAMCUE在准确率上相比基线提升9.06%，对自然语言描述的视角预测在旋转精度上超过90%（误差<20°），平移精度超过90%（误差<0.5）；单例推理时间从256.6秒降至1.45秒，显著提升效率，支持实时交互应用。

Conclusion: CAMCUE通过显式引入相机位姿作为几何锚点，有效解决了多图像空间推理中的视角一致性问题，实现了高精度、低延迟的新视角推理，为真实世界中的交互式多视图理解提供了可行方案。

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations](https://arxiv.org/abs/2602.04982)
*Deepak Gupta,Davis Bartels,Dina Demner-Fuhsman*

Main category: cs.CL

TL;DR: 本文提出了一个名为BioACE的自动化框架，用于评估大语言模型在生物医学领域生成的答案及其引用文献的准确性。该框架综合考虑了答案的完整性、正确性、精确性和召回率，并结合自然语言推理、预训练语言模型和大语言模型等方法，对支持性证据的质量进行评估。通过大量实验，验证了自动化评估与人工评估的相关性，为生物医学问答和引文评估提供了有效工具。


<details>
  <summary>Details</summary>
Motivation: 由于生物医学领域涉及复杂的术语和需要专家判断，传统的人工评估成本高且效率低，因此亟需一种高效的自动化评估框架来评估大语言模型生成的答案及其引用文献的可靠性。

Method: 提出BioACE框架，利用多种技术（如自然语言推理、预训练语言模型、大语言模型）评估答案的完整性、正确性、精确性和召回率，并分析这些指标与人工评估的一致性。

Result: 实验证明，BioACE框架在多个维度上与人类评估具有较高相关性，能够有效评估生物医学问答中的答案质量及引用文献的可信度，为后续研究提供可靠评估工具。

Conclusion: BioACE是一个高效且可靠的自动化评估框架，适用于生物医学问答系统中答案与引用的评估，有助于推动大语言模型在生物医学领域的应用与发展。

Abstract: With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.

</details>


### [85] [Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation](https://arxiv.org/abs/2602.05035)
*Sean Trott,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 该研究量化了多语言语言模型在词汇消歧任务中的性能下降（即“多语言惩罚”），发现其表现普遍低于单语模型。通过对比同一系列的单语与多语模型，研究揭示了三种潜在的能力限制：表征（嵌入向量各向同性降低）、注意力机制（对消歧线索的关注度下降）以及词汇相关（多标记分词增加）。这些因素共同解释了原本归因于多语言状态的性能差异，表明多语言模型确实面临多重能力限制，并且这些限制与消歧性能下降密切相关。


<details>
  <summary>Details</summary>
Motivation: 多语言语言模型在某些任务上表现不如单语模型，可能由于容量限制。本文旨在量化这种‘多语言惩罚’，特别是在需要精确语义表示和上下文建模的词汇消歧任务中。

Method: 使用受控的人类相关性判断数据集，针对英语和西班牙语中的模糊词进行评估；比较同一模型家族下的单语与多语模型在词汇消歧任务上的表现；分析表征、注意力和词汇三个方面的容量限制，并检验它们是否能解释性能差异。

Result: 多语言模型在词汇消歧任务中表现持续低于单语模型；三类容量限制均存在证据，且统计上可解释原本归因于多语言状态的性能差异。

Conclusion: 多语言语言模型确实受到多种容量约束的影响，这些约束与词汇消歧性能下降密切相关，提示未来模型设计需更关注跨语言表征效率与资源分配。

Abstract: Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.

</details>


### [86] [Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories](https://arxiv.org/abs/2602.05085)
*Sidi Lu,Zhenwen Liang,Dongyang Ma,Yan Wang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出Locas，一种基于FFN结构的局部支持参数化记忆机制，可灵活地将记忆永久化到模型参数中，支持高效持续学习。该方法通过重用模型参数、激活值或梯度进行合理初始化，显著提升收敛速度、泛化能力并防止灾难性遗忘。在PG-19和LoCoMo任务上验证了其有效性，仅增加0.02%参数即可实现长期上下文记忆，且在全书记忆后对模型通用能力影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有测试时训练方法难以有效存储长期上下文信息，同时存在灾难性遗忘问题。需要一种可灵活持久化、低开销且与现有模型兼容的记忆机制，以支持高效持续学习。

Method: 提出Locas，一种类FFN结构的参数化记忆模块，包含两种变体：一种为传统两层MLP设计，具有理论保障；另一种采用GLU-FFN结构，与当前大模型一致，易于集成。通过重用模型参数、激活值或梯度进行初始化，确保记忆高效融入模型。

Result: 在PG-19和LoCoMo任务上，仅增加0.02%额外参数，Locas-GLU即能有效存储历史上下文，维持更小的上下文窗口；MMLU评估显示模型原有通用能力损失极小，证明其具备最小化灾难性遗忘的能力。

Conclusion: Locas是一种高效、轻量、可无缝集成的参数化记忆机制，能够将过去上下文知识永久化为模型内部知识，显著减少灾难性遗忘，在持续学习场景中展现出强大潜力。

Abstract: In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.

</details>


### [87] [Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models](https://arxiv.org/abs/2602.05106)
*Michael Browder,Kevin Duh,J. David Harris,Vince Lyzinski,Paul McNamee,Youngser Park,Carey E. Priebe,Peter Viechnicki*

Main category: cs.CL

TL;DR: 本文提出数据核视角空间（DKPS）以数学分析Transformer模型生成合成数据的质量，提供统计保证，并解释下游任务如神经机器翻译或使用对比偏好优化训练的LLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 解决标注训练数据稀缺问题，现有方法依赖经验性调整大模型温度参数，缺乏可预测性和理论支持。

Method: 通过构建数据核视角空间（DKPS），进行数学推导，为变压器模型输出质量提供统计保障。

Result: DKPS能够提供合成数据质量的数学保证，并有效解释下游任务性能表现。

Conclusion: DKPS为合成数据生成提供了理论基础和性能预测能力，未来研究可进一步拓展其应用范围与优化方法。

Abstract: Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.

</details>


### [88] [Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text](https://arxiv.org/abs/2602.05107)
*Ahmed Ruby,Christian Hardmeier,Sara Stymne*

Main category: cs.CL

TL;DR: 本文提出一种多语言多模态方法，用于隐式语篇关系分类。通过结合文本和语音信息（使用Qwen2-Audio），在英语、法语和西班牙语中构建数据集，并实现跨语言迁移，显著提升低资源语言的表现。虽然纯文本模型更优，但融合双模态可进一步增强性能。


<details>
  <summary>Details</summary>
Motivation: 隐式语篇关系分类依赖上下文理解，而现有方法常忽略跨语言与多模态信息，尤其在低资源语言中表现不佳。因此需要构建多语言多模态数据集并设计联合建模方法以提升泛化能力。

Method: 提出一种自动构建跨语言多模态数据集的方法，利用Qwen2-Audio融合文本与音频特征，实现跨语言的隐式语篇关系联合建模与分类。

Result: 实验表明，文本模型优于纯音频模型；融合双模态信息能提升整体性能；跨语言迁移对低资源语言有显著帮助。

Conclusion: 结合多模态与跨语言策略可有效提升隐式语篇关系分类效果，尤其在资源有限的语言中具有重要应用价值。

Abstract: Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.

</details>


### [89] [Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems](https://arxiv.org/abs/2602.05176)
*Ziyuan Yang,Wenxuan Ding,Shangbin Feng,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 该论文研究了在多语言模型协作系统中，恶意模型对系统性能和安全性的严重影响，并提出通过外部监督机制来缓解此类威胁。实验表明，恶意模型在推理和安全领域平均使性能下降7.12%和7.94%；而所提出的缓解策略可恢复95.31%的初始性能，但实现完全抗恶意仍是一个开放问题。


<details>
  <summary>Details</summary>
Motivation: 当前多语言模型协作系统存在严重的安全风险，特别是当部分模型被攻破或具有恶意时，可能对整体系统造成显著负面影响，亟需评估并应对此类威胁。

Method: 通过构建四类恶意语言模型，将其植入四种主流的多模型协作系统中，在10个数据集上进行测试，评估其影响；随后引入外部监督机制，用于识别并屏蔽恶意模型，以降低其危害。

Result: 恶意模型显著降低了多语言模型系统的性能，尤其在推理与安全任务中分别平均下降7.12%和7.94%；采用外部监督策略后，系统平均恢复95.31%的性能，有效缓解了恶意模型的影响。

Conclusion: 尽管提出的缓解方法能显著提升系统鲁棒性，但使多语言模型协作系统完全抵御恶意模型仍是一个未解决的挑战，需要进一步研究。

Abstract: Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.

</details>


### [90] [The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems](https://arxiv.org/abs/2602.05182)
*Shangbin Feng,Kishan Panaganti,Yulia Tsvetkov,Wenhao Yu*

Main category: cs.CL

TL;DR: 本文提出一种模型协作优化方法，通过将多个语言模型的协作模式提炼到单一模型中，实现高效推理。引入单-多进化循环机制，使模型在协作中不断自我改进，形成集体进化生态。实验表明，该方法平均提升8.0%性能，且协作系统整体提升14.9%，适用于多种任务和设置，显著优于现有进化AI方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型协作系统虽能结合多模型优势，但存在加载多个模型带来的高成本问题。如何在保持协作优势的同时降低推理开销，是亟待解决的关键挑战。

Method: 提出基于输出蒸馏的协作模式提炼方法，将多模型协作结果训练至单个模型；设计单-多进化循环框架，让模型持续通过协作与蒸馏迭代优化自身性能。

Result: 在15个任务上，个体模型平均性能提升8.0%，协作系统整体提升14.9%；进化循环有效增强模型能力，尤其对初始表现较弱的模型有显著帮助。

Conclusion: 所提方法实现了高效协作与自我进化，显著降低计算成本并提升性能，具备广泛适用性与强鲁棒性，为构建自进化语言模型系统提供了新范式。

Abstract: Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.

</details>


### [91] [Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky](https://arxiv.org/abs/2602.05189)
*Hsuan-Yu Chou,Wajiha Naveed,Shuyan Zhou,Xiaowei Yang*

Main category: cs.CL

TL;DR: 本研究评估了七种前沿大语言模型（LLM）在社交媒体有害内容检测中的表现，包括四种专有模型和三种开源模型。结果显示，开源LLM在敏感度（81%–97%）和特异性（91%–100%）方面与专有模型（72%–98%，93%–99%）表现相当，尤其在粗鲁、不宽容和威胁性内容检测中表现出不同模式。研究还发现人类审核者与LLM之间存在较高的一致性，表明开源LLM可在消费级硬件上实现隐私保护式内容审核，并为平衡社区价值与用户个性化偏好提供新方向。


<details>
  <summary>Details</summary>
Motivation: 随着互联网普及，有害内容暴露增加，亟需高效的内容审核机制。尽管专有大语言模型已证明在零样本情况下优于传统机器学习模型，但开源大语言模型的开箱即用能力仍不清楚，因此需要评估其在实际场景中的表现。

Method: 选取七种先进模型（四款专有、三款开源），基于Bluesky平台的真实帖子数据，结合Bluesky审核服务决策及两名作者的人工标注，系统评估模型在有害内容检测中的敏感度与特异性，并分析人类审核者与模型间的一致性。

Result: 开源大语言模型在敏感度（81%–97%）和特异性（91%–100%）方面与专有模型表现接近（专有模型：72%–98%，93%–99%）。粗鲁内容检测中特异性高于敏感度，而对不宽容和威胁性内容则相反。人类审核者与模型间存在较高一致性，支持其在平台级与个性化审核中的部署。

Conclusion: 开源大语言模型具备在消费级硬件上实现隐私保护式内容审核的能力，可有效平衡社区规范与个体用户偏好，为未来内容审核系统设计提供新思路。

Abstract: As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.
  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.

</details>


### [92] [Aligning Large Language Model Behavior with Human Citation Preferences](https://arxiv.org/abs/2602.05205)
*Kenichiro Ando,Tatsuya Harada*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型（LLMs）在生成内容时的引用倾向及其与人类偏好之间的对齐程度。通过构建一个包含八类引用动机的网页文本数据集，研究系统评估了不同文本类型间的引用偏好对比。结果显示，人类最常为医学类文本寻求引用，且更强的模型也表现出类似倾向；但当前模型在明确需引用的文本（如维基百科）上过度引用（高出人类27%），而在数字信息和含人名的句子上则显著引用不足（分别低22.6%和20.1%）。通过直接偏好优化实验表明，模型行为可被校准以更好地匹配人类偏好。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在生成内容时如何识别引用价值，以及其引用行为与人类偏好之间的对齐程度，旨在提升模型输出的可信度与实用性。

Method: 构建一个涵盖八类引用动机的网页文本数据集，采用成对比较方式评估所有类型组合的引用偏好，分析模型与人类在引用行为上的差异，并使用直接偏好优化方法进行行为校准。

Result: 模型在需引用的文本上存在过度引用现象（高27%），而在数字句和含人名句上存在显著引用不足（分别低22.6%和20.1%），导致与人类偏好的对齐度下降；通过直接偏好优化可有效改善模型引用行为以更贴近人类偏好。

Conclusion: 当前大语言模型在引用行为上存在明显偏差，需通过精细化调控机制加以校正。本研究为未来更深入理解与优化大模型的引用偏好提供了基础框架与实证支持。

Abstract: Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\%$ relative to humans) and sentences containing personal names (by $-20.1\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.

</details>


### [93] [Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions](https://arxiv.org/abs/2602.05220)
*Jinchuan Tian,Haoran Wang,Bo-Hao Su,Chien-yu Huang,Qingzheng Wang,Jiatong Shi,William Chen,Xun Gong,Siddhant Arora,Chin-Jou Li,Masao Someki,Takashi Maekaku,Yusuke Shinohara,Jin Sakuma,Chao-Han Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: Bagpiper is an 8B audio foundation model that interprets audio holistically through rich natural language captions, enabling unified audio understanding and generation. It pre-trains on 600B tokens to establish a bidirectional link between raw audio and high-level cognitive concepts, then fine-tunes via a caption-then-process workflow without task-specific priors. It outperforms existing models in audio understanding (MMAU, AIRBench) and generation quality (CosyVoice3, TangoFlux), supporting flexible synthesis of speech, music, and sound effects.


<details>
  <summary>Details</summary>
Motivation: Current audio foundation models rely on rigid, task-specific supervision, addressing isolated audio factors rather than holistic understanding. Inspired by human intelligence, which seamlessly connects physical signals with abstract concepts, Bagpiper aims to unify audio understanding and generation through rich, comprehensive captions.

Method: Bagpiper pre-trains on a massive 600B-token corpus to learn a bidirectional mapping between raw audio and a high-level conceptual space represented by rich natural language captions. During fine-tuning, it uses a caption-then-process workflow to simulate intermediate cognitive reasoning, enabling task-solving without task-specific priors.

Result: Bagpiper achieves state-of-the-art performance on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality. It can synthesize arbitrary combinations of speech, music, and sound effects, demonstrating unified audio understanding and generation capabilities.

Conclusion: Bagpiper is among the first audio foundation models to achieve unified understanding and generation for general audio, leveraging rich captions and a cognition-inspired architecture to bridge physical signals and abstract concepts.

Abstract: Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.

</details>


### [94] [FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters](https://arxiv.org/abs/2602.05235)
*Zhilin Liang,Yuxiang Wang,Zimu Zhou,Hainan Zhang,Boyi Liu,Yongxin Tong*

Main category: cs.CL

TL;DR: FedMosaic 是首个基于参数化适配器的联邦检索增强生成框架，通过聚类语义相关文档生成多文档适配器并使用文档特定掩码，减少存储和通信开销，同时采用选择性适配器聚合以避免冲突，显著提升准确率并降低资源消耗，且不共享原始文档。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感领域，知识分散在各个数据孤岛中，传统集中式RAG不可行。因此需要一种能够在不共享原始文档的情况下实现高效知识融合的联邦RAG方法。

Method: 采用参数化适配器方式，通过语义聚类将相关文档合并为多文档适配器，并引入文档特定掩码以保持特异性；同时设计选择性适配器聚合策略，仅合并相关且无冲突的适配器。

Result: 在四个类别上平均准确率比现有最佳方法高出10.9%，存储成本降低78.8%至86.3%，通信成本降低91.4%，且从未传输原始文档。

Conclusion: FedMosaic成功实现了高效、安全、低开销的联邦RAG，为隐私保护环境下的知识增强生成提供了可行方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.

</details>


### [95] [Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks](https://arxiv.org/abs/2602.05252)
*Guangwei Zhang,Jianing Zhu,Cheng Qian,Neil Gong,Rada Mihalcea,Zhaozhuo Xu,Jingrui He,Jiaqi Ma,Yun Huang,Chaowei Xiao,Bo Li,Ahmed Abbasi,Dongwon Lee,Heng Ji,Denghui Zhang*

Main category: cs.CL

TL;DR: Copyright Detective 是首个交互式取证系统，用于检测、分析和可视化大语言模型（LLM）输出中的潜在版权风险。它将版权侵权与合规性视为证据发现过程，而非静态分类任务，整合了内容回溯测试、改写级相似性分析、说服性越狱探测和遗忘验证等多种检测范式，通过交互式提示、响应收集和迭代工作流，支持对逐字记忆和改写级泄露的系统性审计，有助于负责任地部署和透明评估LLM的版权风险，即使在黑盒访问条件下也能实现。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生成内容时存在潜在的版权侵权风险，如直接复制训练数据或改写后泄露敏感信息。传统静态分类方法难以应对版权法的复杂性，亟需一种动态、可解释的检测机制来支持负责任的模型部署与透明评估。因此，本文提出将版权问题转化为证据发现过程，以更全面地识别和分析风险。

Method: 构建了一个统一且可扩展的框架，集成多种检测技术：内容回忆测试（检测是否直接复制训练数据）、改写级相似性分析（判断生成内容与源文本的语义相似度）、说服性越狱探测（测试模型是否容易被诱导泄露敏感信息）、以及未学习验证（检查模型是否已有效遗忘敏感数据）。通过交互式提示与迭代流程，实现对模型输出的系统性审计。

Result: 实验表明，Copyright Detective 能够有效识别大语言模型中的逐字记忆和改写级内容泄露，并在黑盒条件下仍具备良好的检测能力。该系统为模型开发者、监管者和使用者提供了可操作的工具，支持对版权风险进行透明化评估与管理。

Conclusion: Copyright Detective 为解决大语言模型中的版权风险问题提供了一种创新的、面向证据发现的交互式解决方案，推动了模型责任化与透明化的发展，具有重要的实践价值与研究意义。

Abstract: We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.

</details>


### [96] [CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs](https://arxiv.org/abs/2602.05258)
*Haoran Li,Sucheng Ren,Alan Yuille,Feng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CoPE的新型旋转位置嵌入方法，通过软截断RoPE的低频成分，在不增加模型复杂度的前提下，同时解决分布外（OOD）问题和语义建模挑战。该方法在长上下文任务中表现优异，最大支持256k上下文长度，并在多个基准上达到新SOTA，验证了理论分析的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有RoPE扩展方法主要分为两类：一是缓解分布外（OOD）问题，通过调整频率以适应未见位置；二是强调语义相似性，确保注意力机制优先关注语义相近的词元。然而，这两类目标常被分开处理，缺乏统一的理论框架。本文旨在统一这两个目标，提出一种简洁而有效的改进策略。

Method: 提出CoPE方法，通过对RoPE中的低频分量进行软截断（soft clipping），在保留关键语义信息的同时抑制噪声和分布外异常。该操作避免了硬截断导致的频谱泄漏问题，且无需额外训练或参数调整。

Result: 在多种长上下文任务中，CoPE显著提升性能，尤其在256k上下文长度下效果突出。实验表明其不仅有效缓解了分布外问题，还增强了语义建模能力，成为当前长度泛化的新基准。

Conclusion: CoPE通过软截断低频成分，成功统一了传统上分离的两个优化目标——分布外鲁棒性和语义建模。该方法简单高效，无需额外训练，为长上下文大模型的位置编码设计提供了新的范式。

Abstract: Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.

</details>


### [97] [Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR](https://arxiv.org/abs/2602.05261)
*Fanfan Liu,Youyang Yin,Peng Shi,Siqi Yang,Zhixiong Zeng,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文分析了主流RLVR算法中响应长度变化的差异，提出了一种无长度偏见的序列策略优化算法LUSPO，有效解决了响应长度坍塌问题，并在数学推理和多模态推理任务中表现优于GRPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法在训练过程中响应长度变化模式不一，且存在长度偏见导致响应长度坍塌的问题，亟需理论解释与改进方法。

Method: 通过理论分析主流RLVR算法的组件，揭示影响响应长度的关键因素，并基于此设计无长度偏见的损失函数，提出LUSPO算法。

Result: LUSPO在多个数学推理和多模态推理基准上均取得更优性能，验证了其有效性与先进性。

Conclusion: LUSPO是一种新型且先进的优化策略，能够有效消除长度偏见，提升模型推理能力。

Abstract: Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.

</details>


### [98] [Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science](https://arxiv.org/abs/2602.05289)
*Jingru Fan,Dewen Liu,Yufan Dang,Huatao Li,Yuheng Wang,Wei Liu,Feiyu Duan,Xuanwen Ding,Shu Yao,Lin Wu,Ruijie Shi,Wai-Shing Leung,Yuan Cheng,Zhongyu Wei,Cheng Yang,Chen Qian,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出一个整合框架，旨在将多智能体系统（MAS）从依赖经验试错的研究范式转向设计科学。通过引入协作增益度量（Γ），分离出内在协作收益与资源增加的混淆效应，并构建因素归因范式与系统的MAS因素库，从而实现对协作驱动因素的系统识别。该框架推动了从盲目实验向严谨科学的转变，为集体人工智能的科学研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统研究虽受大语言模型推动取得进展，但仍依赖经验试错，缺乏统一的科学框架。主要瓶颈在于归属模糊：既无结构化因素分类，也无统一度量标准，难以区分真正的协作增益与单纯资源堆积。

Method: 提出协作增益度量Γ作为科学基准；建立基于控制级预设与信息级动态的系统性MAS因素库；发展因子归因范式以识别协作驱动因素；推动从实验导向到设计科学的范式转移。

Result: 成功构建可系统化分析多智能体协作的科学框架，实现对协作本质的量化评估与因素解析，显著提升优化效率与理论深度。

Conclusion: 本框架实现了多智能体系统从经验主义向设计科学的跃迁，为构建真正意义上的集体人工智能科学提供了方法论支持。

Abstract: Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Γ$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Γ$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.

</details>


### [99] [MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning](https://arxiv.org/abs/2602.05307)
*Haojin Wang,Yike Wang,Shangbin Feng,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: MentorCollab 是一种推理时协作方法，通过让大型模型（LRM）在随机采样的标记位置上选择性、稀疏地指导小型模型（SLM），而非完全接管生成过程。该方法利用轻量级验证器判断 SLM 是否应采纳来自导师的短前瞻片段，从而实现高效且精准的协同推理。在15组SLM-LRM配对和3个领域（数学推理、通用知识、常识推理）中，性能在12个设置中得到提升，平均提升3.0%，最高达8.0%，同时仅需18.4%的昂贵导师模型生成的标记。结果表明，短片段与选择性探测足以实现有效协作，实现了大模型推理能力的恢复，而推理开销极小。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽表现优异，但推理成本高且易冗余；小型语言模型效率高却难以处理多步推理任务。现有协作方法常导致模仿行为，造成冗长推理且缺乏一致纠错。因此亟需一种高效、精准的推理时协作机制，使小型模型在不显著增加开销的前提下获得大型模型的推理优势。

Method: 提出MentorCollab方法：在推理过程中，在随机采样点探测大型模型与小型模型之间的分歧，使用轻量级验证器决定小型模型是否采纳来自大型模型的短前瞻段落，否则继续自主生成。该方法强调选择性与稀疏性，避免全面接管，保持小型模型的独立性。

Result: 在15组SLM-LRM配对和3个领域中，12个设置性能提升，平均提升3.0%，最高达8.0%；平均仅18.4%的标记由大型模型生成，显著降低推理成本。实验证明短片段与选择性探测即可实现高效协作。

Conclusion: MentorCollab通过选择性、稀疏的推理时引导，成功使小型模型在低开销下复现大型模型的多步推理能力，为高效、可扩展的模型协作提供了新范式。

Abstract: Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.

</details>


### [100] [How Do Language Models Acquire Character-Level Information?](https://arxiv.org/abs/2602.05347)
*Soma Sato,Ryohei Sasano*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型在未显式提供字符级信息的情况下，如何隐式编码字符级知识。通过对比受控设置与标准设置下的模型训练，识别出分词相关的因素（如合并规则和正字法约束）以及独立于分词的因素（如子串的语义关联和句法信息）是主要贡献因素。


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型隐式编码字符级信息的内在机制，理解其在无显式字符输入情况下的学习行为。

Method: 比较在受控设置（如指定预训练数据集或分词器）和标准设置下训练的语言模型，分析不同因素对字符级知识获取的影响。

Result: 发现分词相关的因素（如合并规则和正字法约束）以及独立于分词的因素（如子串的语义关联和句法信息）是语言模型获取字符级知识的主要来源。

Conclusion: 语言模型能够通过分词相关机制及语义、句法等独立于分词的信息，隐式地学习并编码字符级知识，表明其具备多层次的表征能力。

Abstract: Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.

</details>


### [101] [PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning](https://arxiv.org/abs/2602.05370)
*Jun Rao,Zixiong Yu,Xuebo Liu,Guhan Chen,Jing Li,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.CL

TL;DR: PACE提出了一种新的对齐方法，通过生成式修正策略替代传统的最佳N采样，仅用少量计算资源（2 < N < 3）即可实现优于传统DPO-R1（N=16）的性能，有效缓解了探索过度导致的策略崩溃与验证器噪声问题。


<details>
  <summary>Details</summary>
Motivation: 传统DPO-R1依赖大量采样（如N≥8）来挖掘优质轨迹，但在数学推理任务中发现过度探索会导致性能下降甚至策略崩溃，因此需要更稳健的对齐方法。

Method: 提出PACE（Proximal Alignment via Corrective Exploration），利用失败探索生成高保真偏好对，通过生成式修正策略替代暴力采样，降低对大规模采样的依赖。

Result: 在数学推理任务上，PACE在仅使用约1/5计算量的情况下，性能超越DPO-R1（N=16），且对奖励劫持和标签噪声更具鲁棒性。

Conclusion: 过度依赖大N采样会引入有害分布偏移和验证噪声，而基于生成式修正的轻量级策略（PACE）能更高效、更稳定地实现模型对齐。

Abstract: Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.

</details>


### [102] [Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks](https://arxiv.org/abs/2602.05374)
*Chaimae Abouzahir,Congbo Ma,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 该研究通过跨语言实证分析，揭示了大型语言模型在阿拉伯语和英语医学问答任务中的持续语言性能差距，且任务复杂度越高，差距越明显。分析显示阿拉伯语医学文本存在结构碎片化问题，且模型置信度与正确性关联较弱，表明需采用更注重语言特性的设计与评估策略。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型多以英语为中心，导致在低资源语言（如阿拉伯语）上的医疗应用表现不佳，但其根本原因尚不明确，亟需系统性研究以提升模型在多元语言环境中的可靠性。

Method: 开展跨语言实证分析，对比大型语言模型在阿拉伯语与英语医学问答任务中的表现，结合分词分析与可靠性评估，探究性能差异的成因。

Result: 发现语言驱动的性能差距随任务复杂度增加而加剧；阿拉伯语医学文本存在结构性碎片化；模型报告的置信度与解释与实际正确性相关性较低。

Conclusion: 应重视语言特性，在医疗领域大型语言模型的设计与评估中引入语言感知机制，以增强其在非英语语言环境中的鲁棒性与可信度。

Abstract: In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.

</details>


### [103] [IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://arxiv.org/abs/2602.05385)
*Tao Liu,Jiafan Lu,Bohan Yu,Pengcheng Wu,Liu Haixin,Guoyu Xu,Li Xiangheng,Lixiao Li,Jiaming Hou,Zhao Shijun,Xinglin Lyu,Kunli Zhang,Yuxiang Jia,Hongyin Zan*

Main category: cs.CL

TL;DR: IESR是一种针对轻量级大语言模型的文本到SQL框架，通过信息增强结构化推理解决复杂推理、领域知识和假设性查询难题。它利用LLM进行关键信息理解与模式链接，解耦数学计算与SQL生成；采用基于蒙特卡洛树搜索（MCTS）的多路径推理机制结合多数投票；引入轨迹一致性验证模块，通过判别器模型保障准确性和一致性。实验表明，IESR在LogicCat（24.28 EX）和Archer数据集（37.28 EX）上达到顶尖性能，仅使用小型轻量模型且无需微调。分析还揭示当前编码器模型在物理知识、数学计算和常识推理方面存在显著偏差与不足，指明未来研究方向。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL方法在复杂推理、领域知识和假设性查询方面表现不佳，且在企业部署中成本较高。需要一种高效、轻量且具备强推理能力的解决方案。

Method: 提出IESR框架：(i) 利用大语言模型进行关键信息理解与模式链接，并解耦数学计算与SQL生成；(ii) 引入基于蒙特卡洛树搜索（MCTS）的多路径推理机制，结合多数投票提升决策质量；(iii) 设计轨迹一致性验证模块，通过判别器模型确保生成过程的准确性和一致性。

Result: IESR在LogicCat（24.28 EX）和Archer数据集（37.28 EX）上实现最先进的性能，仅使用轻量级模型且无需微调。分析显示当前编码器模型在物理知识、数学计算和常识推理方面存在明显缺陷。

Conclusion: IESR为轻量级大语言模型提供了一种高效、准确的文本到SQL解决方案，有效应对复杂推理挑战。同时揭示了当前模型在常识与数学推理方面的局限，为后续研究指明方向。

Abstract: Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.

</details>


### [104] [Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances](https://arxiv.org/abs/2602.05392)
*Jiyun Chun,Eric Fosler-Lussier,Michael White,Andrew Perrault*

Main category: cs.CL

TL;DR: 本文提出一种基于大模型作为评判者的框架，用于评估儿童在成人-儿童对话中话语的质量。传统指标如平均话语长度（MLU）、词汇多样性（vocd-D）和可读性指数（Flesch-Kincaid、Gunning Fog）过于依赖话语长度，忽视对话上下文，无法捕捉推理深度、话题维持和话语规划等关键质量维度。新框架通过分类前一句成人的语用类型，并从两个维度对儿童回应进行评分：扩展性（Expansion，即上下文拓展与推断深度）和独立性（Independence，即儿童在推进对话中的主动性与自我调节能力）。这两个维度反映了儿童语言发展的核心特征。研究验证了其发展有效性（随年龄呈现规律变化），并证明其在年龄预测上优于传统基线；同时具备语义敏感性，能识别与话语关系相关的差异。该框架与人工判断高度一致，支持大规模评估，推动儿童话语评估从单纯测量长度转向衡量其在语境中对对话的实质性贡献。


<details>
  <summary>Details</summary>
Motivation: 现有儿童话语评估指标（如MLU、词汇多样性、可读性）主要依赖于话语长度，缺乏对对话上下文的敏感性，无法有效捕捉儿童回应中的推理深度、话题维持和话语规划等关键质量维度，因此亟需更全面、情境敏感的评估方法。

Method: 提出一个基于大模型作为评判者的框架，首先对前一句成人的言语类型进行分类，然后从‘扩展性’（Expansion）和‘独立性’（Independence）两个维度对儿童回应进行评分。扩展性关注话语的上下文拓展、推断深度、从句连接及因果/对比连词使用；独立性关注儿童的主动性、话题控制能力、减少对成人支架的依赖以及受众设计能力。通过训练与验证，实现对儿童话语质量的量化评估。

Result: 该框架展现出显著的发展有效性，随年龄增长，儿童在扩展性和独立性维度上的得分呈规律上升趋势；在年龄估计任务中，其表现优于传统指标；同时具备语义敏感性，能有效区分不同话语关系下的回应质量；且与人类评判高度一致，适用于大规模评估。

Conclusion: 本研究构建的LLM-as-a-judge框架实现了从“量”到“质”的转变，将儿童话语评估从单纯测量长度提升为评估其在具体对话语境中对交流的实质性贡献，为儿童语言发展研究提供了高效、可靠、可扩展的评估工具。

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

</details>


### [105] [Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better](https://arxiv.org/abs/2602.05393)
*Ji Zhao,Yufei Gu,Shitong Shao,Xun Zhou,Liang Xiang,Zeke Xie*

Main category: cs.CL

TL;DR: 本文提出了一种名为Late-to-Early Training (LET)的新范式，旨在利用预训练的小模型来加速大模型的训练。通过将预训练模型（处于后期训练阶段）的深层表示引导到目标大模型的早期层中，实现“后期知识在前期学习”的机制。该方法显著加快了训练收敛速度，并提升了语言建模与下游任务性能。实验表明，在1.4B和7B参数模型上，使用比目标模型小10倍的预训练模型，仍能实现最高1.6倍的训练加速和近5%的下游任务准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型（LLM）的预训练成本高昂，且缺乏有效手段利用已有小规模预训练模型来加速更大模型的训练。如何高效复用现有预训练成果成为关键问题。

Method: 提出Late-to-Early Training (LET)范式，利用预训练模型（晚期训练阶段）的深层特征作为监督信号，指导目标模型早期层的训练，从而实现晚知识向早阶段和早层的迁移。核心机制包括晚-早步学习和晚-早层学习。

Result: 在1.4B和7B参数模型上的实验验证了LET的有效性：训练速度最快提升1.6倍，下游任务准确率提高约5%，即使使用参数量仅为目标模型1/10的预训练模型也表现出色。

Conclusion: LET范式为高效训练大型语言模型提供了新路径，能够显著缩短训练时间并提升性能，同时有效复用现有小模型资源，具有重要的实际应用价值。

Abstract: As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\times$ speedup with nearly 5\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\times$ fewer parameters than the target model.

</details>


### [106] [Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation](https://arxiv.org/abs/2602.05419)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 该研究提出了一种新的语法错误修正（GEC）自动评估方法UOT-ERRANT，通过在编辑向量上使用不平衡最优传输来衡量假设与参考句之间的相似性，解决了传统基于嵌入的相似性度量在GEC中效果不佳的问题。实验表明，该方法在SEEDA元评估中表现更优，尤其在+Fluency领域；同时具有高度可解释性，其传输计划可视为软编辑对齐，有助于系统排名和分析。


<details>
  <summary>Details</summary>
Motivation: 传统基于嵌入的相似性度量（如BERTScore）在语法错误修正任务中表现不佳，因为源句中的许多词在假设和参考句中保持不变，无法有效捕捉编辑内容。因此需要一种能聚焦于实际修改的评估方法。

Method: 提出编辑向量表示单个编辑，并利用不平衡最优传输（Unbalanced Optimal Transport, UOT）将假设句的编辑向量运输到参考句，从而计算两者间的相似性，形成新度量UOT-ERRANT。

Result: 在SEEDA元评估中，UOT-ERRANT显著提升了评估性能，尤其在+Fluency领域表现突出；且其传输计划可解释为软编辑对齐，增强了方法的可解释性。

Conclusion: UOT-ERRANT是一种高效、可解释的GEC自动评估方法，能够更好地反映系统性能，适用于系统排序与分析。代码已开源。

Abstract: Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.

</details>


### [107] [Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models](https://arxiv.org/abs/2602.05437)
*Basel Mousi,Fahim Dalvi,Shammur Chowdhury,Firoj Alam,Nadir Durrani*

Main category: cs.CL

TL;DR: 提出M2CQA多模态基准，测试视觉-语言模型在中东和北非（MENA）文化背景下的幻觉问题，尤其关注阿拉伯语及其方言中的错误接受。引入CFHR指标衡量在正确回答真实陈述前提下对反事实陈述的接受率。结果显示，尽管真实陈述准确率高，但阿拉伯语（尤其是方言）中反事实幻觉率显著上升；推理优先提示会加剧幻觉，而先回答后解释可提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉基准很少测试视觉-语言模型在非西方语境和非英语环境下的文化合理但视觉错误解释的接受能力，缺乏对跨文化、多语言场景下幻觉现象的系统评估。

Method: 构建涵盖17个MENA国家图像的多模态数据集M2CQA，包含英文、阿拉伯语及方言的对比真实与反事实陈述；设计并使用CounterFactual Hallucination Rate（CFHR）作为核心评估指标，分析不同提示策略下模型表现。

Result: 在阿拉伯语（尤其是方言）中，即使真实陈述准确率高，反事实接受率（CFHR）也急剧上升；推理优先提示显著增加幻觉，而先回答后解释的策略更具鲁棒性。

Conclusion: 当前最先进的视觉-语言模型在非英语、非西方文化背景下存在严重的反事实幻觉问题，提示策略对模型鲁棒性有显著影响，需重视跨文化语境下的模型验证与改进。

Abstract: Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.

</details>


### [108] [Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs](https://arxiv.org/abs/2602.05444)
*Yao Zhou,Zeen Song,Wenwen Qiang,Fengge Wu,Shuyi Zhou,Changwen Zheng,Hui Xiong*

Main category: cs.CL

TL;DR: 提出CFA$^2$框架，基于因果前门准则，通过稀疏自编码器剥离防御特征，实现高效且可解释的大型语言模型越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐机制作为隐含内部状态，难以观察，导致模型能力被遮蔽；需从因果视角揭示并干预这些隐藏机制以实现有效越狱。

Method: 将安全机制建模为未观测混杂因子，利用Pearl的前门准则，结合稀疏自编码器物理移除防御特征，并将复杂的边际化操作简化为低复杂度确定性干预。

Result: 实验表明CFA$^2$在攻击成功率上达到当前最优水平，同时提供对越狱过程的机制性解释。

Conclusion: CFA$^2$成功实现了对大型语言模型安全机制的可解释性越狱，为理解与操纵模型内部对齐机制提供了新路径。

Abstract: Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \textbf{C}ausal \textbf{F}ront-Door \textbf{A}djustment \textbf{A}ttack ({\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.

</details>


### [109] [Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision](https://arxiv.org/abs/2602.05471)
*Md. Mithun Hossaina,Mashary N. Alrasheedy,Nirban Bhowmick,Shamim Forhad,Md. Shakil Hossain,Sudipto Chaki,Md Shafiqul Islam*

Main category: cs.CL

TL;DR: 本文提出了一种名为“不确定性推理”（Reasoning under Ambiguity）的不确定性感知框架，用于多语言多标签情绪分类。该框架通过共享多语言编码器与语言特异性优化，结合基于熵的模糊性加权机制，降低高度模糊样本的影响，避免将缺失标签误认为负例。同时引入掩码感知目标和正-未标记正则化，增强在部分标注条件下的鲁棒性。实验表明，该方法在英语、西班牙语和阿拉伯语基准上均显著优于现有基线，提升训练稳定性、对标注稀疏性的鲁棒性及可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多标签情绪分类中假设标签完全可观测，依赖确定性学习目标，导致在标注不完整或模糊时产生偏差，影响预测可靠性。因此需要一种能处理情感歧义和部分监督的新型框架。

Method: 采用共享多语言编码器与语言特定优化策略；设计基于熵的模糊性加权机制，动态调整模糊样本权重；引入掩码感知损失函数与正-未标记正则化，以应对部分标注场景下的学习挑战。

Result: 在英文、西班牙文和阿拉伯文的情绪分类数据集上，相比强基线模型，在多个评估指标上均有显著提升；训练更稳定，对标注稀疏性更具鲁棒性，且具备更强的可解释性。

Conclusion: 所提出的不确定性感知框架有效缓解了多语言多标签情绪分类中的模糊性和标注不全问题，提升了模型的性能与可靠性，为跨语言情感分析提供了新范式。

Abstract: Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.

</details>


### [110] [LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation](https://arxiv.org/abs/2602.05493)
*Bingru Li*

Main category: cs.CL

TL;DR: LinguistAgent is a user-friendly platform that uses a reflective multi-model architecture with dual-agent workflow (Annotator and Reviewer) to automate linguistic annotation in Humanities and Social Sciences, especially for complex tasks like metaphor identification. It supports three paradigms: Prompt Engineering, Retrieval-Augmented Generation, and Fine-tuning, and provides real-time token-level evaluation (Precision, Recall, F1) against human gold standards.


<details>
  <summary>Details</summary>
Motivation: Data annotation is a major bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. Despite the potential of Large Language Models (LLMs), there is a gap between their theoretical capability and practical utility for researchers.

Method: LinguistAgent employs a dual-agent workflow—Annotator and Reviewer—to simulate a peer-review process. It integrates a reflective multi-model architecture and supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning.

Result: The system demonstrates high efficacy in metaphor identification, providing real-time token-level evaluation (Precision, Recall, F1 score) that closely aligns with human gold standards. The platform is user-friendly and scalable for diverse linguistic annotation tasks.

Conclusion: LinguistAgent bridges the gap between LLM theoretical potential and practical application in humanities research by offering an automated, accurate, and transparent annotation system with real-time feedback and support for multiple LLM paradigms.

Abstract: Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.

</details>


### [111] [Transport and Merge: Cross-Architecture Merging for Large Language Models](https://arxiv.org/abs/2602.05495)
*Chenhang Cui,Binyun Yang,Fei Shen,Yuxin Chen,Jingnan Zheng,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出一种基于最优传输（OT）的跨架构模型合并框架，用于将大型高资源语言模型的知识迁移至异构的小型低资源模型。通过对齐激活值来推断跨神经元对应关系，并利用传输计划指导权重空间融合，仅需少量输入即可实现高效的知识迁移。实验在低资源语言和专业领域中均表现出优于目标模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法通常假设模型架构兼容，无法直接将大型高资源模型的知识迁移到异构的低资源小型模型中，因此需要一种跨架构的知识迁移机制。

Method: 提出基于最优传输（OT）的跨架构合并框架，通过激活对齐推断异构模型间的跨神经元对应关系，并利用得到的传输计划指导权重空间的融合。

Result: 在多个低资源语言和专业领域任务上，该方法显著提升了目标模型的性能，证明了其在小样本、异构场景下的有效性与通用性。

Conclusion: 所提出的跨架构合并框架能够有效实现从大型高资源模型到小型低资源模型的知识迁移，尤其适用于资源受限的实际部署场景，为轻量化模型的性能提升提供了新思路。

Abstract: Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.

</details>


### [112] [A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering](https://arxiv.org/abs/2602.05512)
*Larissa Pusch,Alexandre Courtiol,Tim Conrad*

Main category: cs.CL

TL;DR: 本文提出一种交互式框架，使大语言模型（LLM）能够生成并解释Cypher图查询，用户通过自然语言迭代优化查询。该框架提升了复杂知识图谱（KG）的可访问性，同时保持事实准确性与语义严谨性，并在合成电影知识图谱上进行了90个查询的基准测试，评估了查询解释质量与故障检测能力，还对真实世界知识图谱（如Hyena和MaRDI）进行了小规模实验。


<details>
  <summary>Details</summary>
Motivation: 现有文本检索增强生成（RAG）方法在多跳推理方面表现不足，而知识图谱虽支持精确、可解释的查询，但需掌握查询语言。因此需要一种既能利用知识图谱优势又降低使用门槛的方法。

Method: 设计一个交互式框架，让大语言模型生成和解释Cypher图查询，用户通过自然语言反馈逐步优化查询，从而实现对复杂知识图谱的高效、准确访问。

Result: 在合成电影知识图谱上的90查询基准测试中，该框架显著提升了查询解释质量和故障检测能力；在真实知识图谱（Hyena、MaRDI）上的实验也验证了其有效性与跨领域适用性。

Conclusion: 该交互式框架有效弥合了大语言模型与知识图谱之间的鸿沟，既增强了可访问性，又保障了准确性与可解释性，为知识密集型任务提供了一种稳健的解决方案。

Abstract: Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.

</details>


### [113] [Multi-Task GRPO: Reliable LLM Reasoning Across Tasks](https://arxiv.org/abs/2602.05547)
*Shyam Sundhar Ramesh,Xiaotong Ji,Matthieu Zimmer,Sangwoong Yoon,Zhiyong Wang,Haitham Bou Ammar,Aurelien Lucchi,Ilija Bogunovic*

Main category: cs.CL

TL;DR: 本文提出了一种新的多任务GRPO算法（MT-GRPO），以解决现有RL-based post-training方法在多任务场景下因任务权重不平衡和零梯度样本频发导致的性能偏差问题。该方法通过动态调整任务权重以优化最差任务表现，并引入比例保持采样器确保梯度反映调整后的权重。实验表明，MT-GRPO在3任务和9任务设置下均显著提升最差任务准确率，相比标准GRPO和DAPO分别提高16-28%和6%，同时训练效率更高，仅需50%的训练步数即可达到50%的最差任务准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的后训练方法（如GRPO）在单任务上表现良好，但在多任务部署中常出现任务间性能不平衡，部分任务主导优化而其他任务停滞；此外，不同任务中零优势样本频率差异大，进一步扭曲优化信号，影响整体可靠性。因此需要一种能均衡多任务表现并提升训练效率的新方法。

Method: 提出MT-GRPO算法，包含两个核心设计：(i) 动态任务权重机制，根据各任务表现实时调整权重，优先提升最差任务性能；(ii) 比例保持采样器，确保在采样过程中任务梯度贡献与调整后的权重一致，避免信息失真。

Result: 在3任务和9任务设置下，MT-GRPO在最差任务准确率上显著优于标准GRPO和DAPO，分别提升16-28%和6%；平均性能保持竞争力；训练效率大幅提升，在3任务设置中仅需50%训练步数即可达到50%最差任务准确率。

Conclusion: MT-GRPO有效解决了多任务强化学习后训练中的性能不平衡问题，实现了更可靠、高效且均衡的模型性能，适用于真实世界中多样化任务部署的需求。

Abstract: RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.

</details>


### [114] [CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models](https://arxiv.org/abs/2602.05633)
*Rui Jia,Ruiyi Lan,Fengrui Liu,Zhongxiang Dai,Bo Jiang,Jing Shao,Jingyuan Chen,Guandong Xu,Fei Wu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为CASTLE的教育安全评估基准，旨在解决大语言模型在个性化学习中因生成同质化响应而忽视学生认知与心理差异的问题。该基准涵盖15种教育安全风险和14种学生属性，包含92,908个双语场景，并设计了三项评估指标：风险敏感性、情感共情力和学生适配度。实验表明，18个主流LLM平均安全评分低于2.3/5，显示其在个性化安全保障方面存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在个性化教育应用中常产生同质化响应，忽略学生间认知与心理差异，导致对弱势群体存在潜在安全风险；而传统安全评估依赖静态、上下文无关的指标，无法捕捉不同学生属性下相同回应可能带来的差异化危害，因此亟需一种能反映学生特质的个性化安全评估体系。

Method: 基于教育理论构建学生定制化的个性化安全概念，设计并开发了名为CASTLE的多维度评估基准，涵盖15类教育安全风险与14种学生属性，形成大规模双语场景数据集，并提出三项量化指标（风险敏感性、情感共情力、学生适配度）以系统评估模型在个性化安全方面的表现。

Result: 在18个最先进的大语言模型上进行实验，结果显示所有模型的平均安全评分仅为2.3/5，表明当前主流模型在应对学生差异化需求方面的个性化安全能力严重不足，凸显了构建学生定制化安全机制的必要性。

Conclusion: CASTLE为评估大语言模型在个性化教育中的安全表现提供了新范式，揭示了现有模型在学生差异化响应上的重大缺陷，强调未来需从教育心理学与个性化安全角度优化模型生成机制。

Abstract: Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.

</details>


### [115] [Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew](https://arxiv.org/abs/2602.05648)
*Giuseppe Samo,Paola Merlo*

Main category: cs.CL

TL;DR: 本文研究了Transformer模型在土耳其语和现代希伯来语中对复杂动词变位的表征能力，重点分析分词策略的影响。在自然数据上的Blackbird语言矩阵任务中，土耳其语（具有透明形态标记）的单语和多语模型在原子分词或细粒度子词分词下均表现良好；而希伯来语的单语和多语模型则表现出差异：使用字符级分词的多语模型无法捕捉非连接性形态，但采用形态感知分词的单语模型表现优异。在更合成的数据集上，所有模型的表现均有提升。


<details>
  <summary>Details</summary>
Motivation: 探究不同分词策略如何影响Transformer模型对具有复杂形态结构的语言（如土耳其语和现代希伯来语）的表征能力，特别是针对非连接性形态的处理。

Method: 在自然数据和合成数据上使用Blackbird语言矩阵任务评估单语与多语Transformer模型的表现，对比不同分词策略（原子分词、子词分词、字符级分词、形态感知分词）对模型性能的影响。

Result: 土耳其语中，无论分词方式如何，模型表现良好；希伯来语中，多语模型在字符级分词下表现差，而单语模型在形态感知分词下表现良好；合成数据上所有模型性能均提升。

Conclusion: 分词策略对模型表征复杂形态语言的能力有显著影响，形态感知分词对于处理非连接性形态的语言尤为重要，且合成数据有助于提升模型性能。

Abstract: We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.

</details>


### [116] [MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations](https://arxiv.org/abs/2602.05692)
*Congbo Ma,Yichun Zhang,Yousef Al-Jazzazi,Ahamed Foisal,Laasya Sharma,Yousra Sadqi,Khaled Saleh,Jihad Mallat,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本文提出了MedErrBench，首个多语言临床文本错误检测、定位与修正的基准数据集，涵盖英语、阿拉伯语和中文，基于十类常见错误类型，由临床专家标注与审核。评估了通用、语言特定及医学领域大模型在三类任务上的表现，发现非英语场景下存在显著性能差距，强调需发展以临床为基、语言敏感的系统。数据集与评估协议已公开，旨在推动多语言临床自然语言处理，促进全球更安全、公平的AI医疗应用。


<details>
  <summary>Details</summary>
Motivation: 现有临床文本中的错误可能导致严重后果，如误诊或错误治疗建议；尽管大语言模型在医疗应用中日益普及，但缺乏覆盖多种语言和场景的综合性评估基准，亟需建立多语言、临床驱动的评测体系以保障AI医疗的安全性与公平性。

Method: 构建MedErrBench多语言基准，涵盖英语、阿拉伯语和中文，基于临床专家定义的十类错误类型，对真实临床案例进行标注与审核；评估多种通用、语言特定及医学领域语言模型在错误检测、定位与修正三个任务上的表现，并分析其跨语言差异。

Result: 评估结果显示，各模型在非英语语言场景中表现明显落后，尤其在错误定位与修正任务上存在较大差距，表明当前模型在多语言临床场景下的适应性不足，凸显开发语言感知、临床知识融合的系统之必要性。

Conclusion: MedErrBench是首个面向多语言临床文本错误处理的基准，其公开可用将推动更安全、公平的AI医疗发展。未来工作应聚焦于提升模型在非英语临床环境中的鲁棒性与准确性，强化临床知识与语言能力的结合。

Abstract: Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.

</details>


### [117] [Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation](https://arxiv.org/abs/2602.05694)
*Shuting Jiang,Ran Song,Yuxin Huang,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出了一种针对多领域机器翻译的神经元高效微调框架，通过识别并更新与领域特征高度一致的共识神经元，提升模型在不同领域的泛化能力，有效缓解参数干扰和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如上下文学习和参数高效微调在多领域机器翻译中存在领域偏移、参数干扰和泛化能力有限的问题，亟需更高效的适应策略。

Method: 提出基于互信息最大化选择共识对齐神经元，并以这些神经元为指导进行微调，实现对通用翻译模式和领域特异性特征的联合建模。

Result: 在三个大型语言模型上对十个德语-英语和中文-英语翻译领域进行实验，结果表明该方法在已见和未见领域上均显著优于强基线，达到当前最佳性能。

Conclusion: 所提神经元高效微调框架能够有效提升大语言模型在多领域机器翻译中的适应能力，兼顾通用性与领域敏感性，是解决领域迁移挑战的有效方案。

Abstract: Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.

</details>


### [118] [OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale](https://arxiv.org/abs/2602.05711)
*Jingze Shi,Zhangyang Peng,Yizhang Zhu,Yifan Wu,Guang Liu,Yuyu Luo*

Main category: cs.CL

TL;DR: OmniMoE提出一种系统-算法协同设计的框架，通过向量级原子专家实现极细粒度的专家专业化，在保持共享密集MLP分支的同时提升参数效率。为解决路由复杂度和内存访问挑战，引入笛卡尔积路由器（将路由复杂度从O(N)降至O(sqrt(N))）与专家中心调度机制（将分散的内存访问转化为高效的密集矩阵运算）。在七个基准测试中，OmniMoE以17亿活跃参数达到50.9%零样本准确率，显著优于粗粒度和细粒度基线模型，并将推理延迟从73ms降至6.7ms（提速10.9倍），证明大规模细粒度MoE可兼具高性能与高效率。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有Mixture-of-Experts（MoE）架构在追求更细粒度专家专业化以提升参数效率时，面临专家粒度与硬件执行效率之间的内在权衡。如何在不牺牲计算效率的前提下实现极致的专家粒度，是当前研究的关键挑战。

Method: 提出系统-算法协同设计的OmniMoE框架：采用向量级原子专家实现极高粒度的专家划分；引入笛卡尔积路由器降低路由复杂度；设计专家中心调度策略，将原本分散的内存访问转换为密集矩阵操作，从而优化内存访问模式并提升执行效率。

Result: OmniMoE在七个基准测试上取得50.9%的零样本准确率，超越粗粒度（如DeepSeekMoE）和细粒度（如PEER）基线模型；推理延迟从73ms降至6.7ms，实现10.9倍加速，验证了其在高精度与低延迟方面的优越性。

Conclusion: OmniMoE通过系统-算法协同设计，成功突破了细粒度MoE在路由复杂度与内存效率上的瓶颈，实现了高精度与高速推理的统一，为未来大规模高效MoE架构的发展提供了新范式。

Abstract: Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.

</details>


### [119] [CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering](https://arxiv.org/abs/2602.05728)
*Hao Yang,Zhiyu Yang,Xupeng Zhang,Wei Wei,Yunjie Zhang,Lin Yang*

Main category: cs.CL

TL;DR: CompactRAG 提出一种高效多跳问答框架，通过离线将文档库重构为细粒度问答对，并在在线推理中仅需两次LLM调用（分解子问题和生成答案），显著降低令牌消耗并提升实体一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多跳RAG系统因每步交替进行检索与推理，导致重复调用大模型、高令牌开销及实体定位不稳定，亟需更高效的解决方案。

Method: 离线阶段：使用LLM将原始语料库转换为原子化QA知识库；在线阶段：将复杂查询分解为保持实体一致性的子问题，通过密集检索和RoBERTa模型提取答案，推理时仅需两次LLM调用。

Result: 在HotpotQA、2WikiMultiHopQA和MuSiQue数据集上，CompactRAG达到竞争性准确率，同时大幅减少令牌消耗，证明其在成本效益与实用性上的优势。

Conclusion: CompactRAG通过分离知识重构与推理过程，实现了高效、稳定且低成本的多跳问答，是一种适用于大规模知识库的实用方案。

Abstract: Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.
  In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.
  Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.

</details>


### [120] [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758)
*Bowen Ping,Zijun Chen,Yiyao Yu,Tingfeng Hui,Junchi Yan,Baobao Chang*

Main category: cs.CL

TL;DR: LongR is a unified framework that improves long-context reasoning in LLMs by combining a dynamic 'Think-and-Read' mechanism with a contextual density reward based on information gain. It outperforms existing methods on benchmarks like LongBench v2, RULER, and InfiniteBench, and works effectively across various RL algorithms.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning approaches for LLMs rely on sparse, outcome-only rewards, which are insufficient for guiding complex long-context reasoning tasks such as long-dialogue understanding and structured data analysis.

Method: LongR introduces a 'Think-and-Read' mechanism that interleaves reasoning steps with document consultation, coupled with a contextual density reward that measures the relative information gain from relevant documents to guide more effective long-context reasoning.

Result: LongR achieves a 9% improvement on LongBench v2 and consistent gains on RULER and InfiniteBench. It enhances performance across multiple RL algorithms (e.g., DAPO, GSPO) and shows robustness against distractors and sensitivity to reasoning chain length.

Conclusion: LongR provides a powerful and generalizable solution for enhancing long-context reasoning in LLMs by integrating dynamic reasoning and context-aware rewards, demonstrating strong empirical effectiveness and adaptability.

Abstract: Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic "Think-and-Read" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.

</details>


### [121] [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769)
*Adnan Al Ali,Jindřich Helcl,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLM）生成文本检测器在捷克语环境下的表现，发现非母语者撰写的文本的困惑度并不低于母语者，且现有检测器对非母语者并无系统性偏差。此外，现代检测器已不再依赖困惑度作为核心特征，仍能有效识别生成文本。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型助手在学术领域可能被滥用的问题，现有自动化检测技术虽有一定效果，但存在误判非母语者文本为生成内容的担忧，尤其因其困惑度较低。本文旨在重新评估这一问题在捷克语背景下的有效性。

Method: 在捷克语语境下，通过对比非母语与母语写作者的文本困惑度，评估三类不同家族的检测器对非母语者的偏差，并分析检测器是否仍依赖困惑度进行判断。

Result: 非母语者文本的困惑度不显著低于母语者；三类检测器均未表现出对非母语者的系统性偏见；当代检测器在不依赖困惑度的情况下仍具有良好的检测性能。

Conclusion: 当前的文本生成检测技术在捷克语环境下对非母语者并无明显偏差，且其有效性不依赖于困惑度特征，表明检测方法已趋于成熟和可靠。

Abstract: LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.

</details>


### [122] [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853)
*Siran Liu,Guoxia Wang,Sa Wang,Jinle Zeng,HaoYang Xie,Siyu Lou,JiaBin Yang,DianHai Yu,Haifeng Wang,Chao Yang*

Main category: cs.CL

TL;DR: RRAttention提出一种新的动态稀疏注意力方法，通过头级轮转（RR）采样策略，在保持查询独立性的同时实现高效全局模式发现，将复杂度从O(L^2)降至O(L^2/S^2)，在自然语言理解和多模态视频理解任务中恢复超过99%的全注意力性能，仅计算一半注意力块，128K上下文长度下实现2.4倍加速，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决注意力机制在长序列处理中的二次复杂度瓶颈，克服现有动态稀疏注意力方法在预处理、全局评估、查询独立性和计算开销方面的根本权衡。

Method: 采用头级轮转（RR）采样策略，通过在每个步长内旋转查询采样位置，结合步长级聚合实现高效全局模式发现，并使用自适应Top-τ选择确定最优稀疏性。

Result: 在HELMET和Video-MME任务上，RRAttention恢复超过99%的全注意力性能，仅计算一半注意力块，128K上下文长度下实现2.4倍加速，优于现有动态稀疏注意力方法。

Conclusion: RRAttention通过创新的轮转采样策略，实现了高效率与高性能的统一，为长序列建模提供了有效的解决方案。

Abstract: The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \underline{r}ound-\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.

</details>


### [123] [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874)
*Adrián Girón,Pablo Miralles,Javier Huertas-Tato,Sergio D'Antonio,David Camacho*

Main category: cs.CL

TL;DR: 本文提出xList-Hate，一种将仇恨言论检测重构为诊断推理任务的框架，通过分解为一系列基于规范准则的概念级问题，由大语言模型独立回答，生成可解释的二元诊断表示，并用轻量级决策树聚合结果。该方法在跨数据集鲁棒性、对抗领域迁移和标注噪声方面优于传统监督模型和零样本LLM分类，同时提供细粒度可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测多作为单一二分类任务处理，但其本质涉及多重交互因素，受法律、平台政策和标注标准影响，导致监督模型易过拟合特定数据集定义，泛化能力差。需要更鲁棒、可解释的替代方案。

Method: 将仇恨言论检测转化为诊断检查清单：设计一组基于广泛接受规范准则的概念级问题；使用大语言模型独立回答每个问题，生成二元诊断信号；用轻量级、完全可解释的决策树聚合这些信号，输出最终预测。

Result: 在多个仇恨言论基准上评估，xList-Hate在跨数据集性能和领域迁移下表现优于监督微调与零样本LLM分类；对标注不一致和语境模糊更具鲁棒性；能提供明确决策路径与因子级分析，实现细粒度可解释性。

Conclusion: 将仇恨言论检测从单体分类重构为诊断推理任务，是一种更鲁棒、可解释且可扩展的内容审核新范式，适用于复杂多变的现实场景。

Abstract: Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.
  We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.
  We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.
  Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.

</details>


### [124] [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897)
*Shuo Nie,Hexuan Deng,Chao Wang,Ruiyu Fang,Xuebo Liu,Shuangyong Song,Yu Li,Min Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 提出FaithRL方法，通过显式步骤级忠实度奖励和隐式截断重采样策略，提升小推理模型在资源受限环境下的链式思维推理忠实度，有效减少中间推理步骤和最终答案中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 小推理模型（SRMs）虽适合资源受限场景，但易在中间推理步骤中产生忠实度幻觉；现有基于在线强化学习的方法依赖结果奖励或粗粒度评估，可能因最终答案正确而错误地强化不忠实推理。

Method: 提出Faithfulness-Aware Step-Level Reinforcement Learning（FaithRL），引入过程奖励模型提供显式步骤级忠实度奖励，并结合隐式截断重采样生成对比信号以增强忠实推理。

Result: 在多个小推理模型和开放书问答基准上，FaithRL显著降低中间推理与最终答案中的幻觉，提升推理的忠实性与可靠性。

Conclusion: FaithRL通过精细的步骤级监督和对比学习机制，有效缓解小推理模型中的忠实度幻觉问题，为高效可靠的轻量级推理提供了新范式。

Abstract: As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.

</details>


### [125] [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929)
*Jian Chen,Zhuoran Wang,Jiayu Qin,Ming Li,Meng Wang,Changyou Chen,Yin Chen,Qizhen Weng,Yirui Liu*

Main category: cs.CL

TL;DR: 提出KV-CoRE方法，基于SVD量化kv-cache的数据依赖性低秩可压缩性，实现高效、增量式的层级评估，揭示模型架构、训练数据和语言覆盖对压缩性的系统影响，并建立首个大规模基准，为动态、数据感知的压缩和以数据为中心的模型开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有KV-cache压缩方法忽视了其数据依赖性和跨层差异，导致在长上下文场景下难以有效利用内存带宽，亟需一种能准确评估和优化压缩潜力的方法。

Method: 采用基于SVD的无梯度、增量式方法，计算Frobenius范数下的最优低秩近似，结合归一化有效秩作为压缩性指标，实现对多模型、多语言、多领域的层间压缩性分析。

Result: 发现压缩性与模型架构、训练数据和语言覆盖密切相关，且归一化有效秩与压缩性能下降高度相关，验证了该方法的有效性并构建了首个大规模KV-cache压缩性基准。

Conclusion: KV-CoRE提供了一种可扩展、数据敏感的评估框架，推动了动态自适应压缩和数据驱动的模型设计发展。

Abstract: Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.

</details>


### [126] [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932)
*Léo Labat,Etienne Ollion,François Yvon*

Main category: cs.CL

TL;DR: 该研究探讨多语言大模型在价值导向的多选题中是否存在语言依赖性响应，通过新发布的多语言欧洲价值观调查（MEVS）数据集，在8种欧洲语言下测试了30多个多语言大模型。结果显示，尽管较大的指令微调模型整体上更一致，但其响应一致性在不同题目间差异显著；部分题目下模型表现高度一致，而另一些则存在分歧。语言特定行为出现在所有一致的指令微调模型中，但仅限于某些题目，提示偏好微调可能具有选择性影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注多语言对大模型事实记忆的影响，但对语言如何影响模型在价值相关问题上的回答尚不明确。本研究旨在揭示多语言大模型是否在不同语言中表现出一致的价值判断，还是受语言影响而呈现不同的价值取向，从而揭示模型内部潜在的语言依赖性。

Method: 构建并发布多语言欧洲价值观调查（MEVS）数据集，包含8种欧洲语言的人工翻译问卷；在控制条件下对超过30个不同规模、来源和对齐状态的多语言大模型进行测试，采用多种提示变体（如答案顺序、符号类型、尾部字符）以评估响应稳定性。

Result: 大型指令微调模型整体一致性较高，但一致性在不同题目间波动明显；部分题目下模型响应高度一致，其他题目则出现分裂；语言特定行为在所有一致的指令微调模型中均出现，但仅限于特定问题，表明偏好微调可能具有选择性作用。

Conclusion: 多语言大模型并非始终如一地表现出跨语言价值一致性，其响应受语言和具体问题影响，提示需进一步研究偏好微调的局部效应，以理解模型在多语言语境下的价值表达机制。

Abstract: Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.

</details>


### [127] [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940)
*Junxiao Liu,Zhijun Wang,Yixiao Li,Zhejian Lai,Liqian Huang,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: TRIT是一种集成翻译与推理训练的自提升框架，旨在解决长推理模型在多语言环境下的问题。通过联合优化多语言问题理解与响应生成，无需外部反馈或额外多语言数据，显著提升了多语言推理的准确性和语言一致性。在MMATH上，平均性能优于多个基线7个百分点；跨语言问题对齐提升超10个百分点，翻译质量在数学题和通用文本上均有提升，最高达8.4 COMET分。


<details>
  <summary>Details</summary>
Motivation: 多语言推理模型常以英语进行推理，导致非英语问题处理效果差；即使强制用问题语言推理，准确率也大幅下降，根源在于多语言问题理解与推理能力有限。

Method: 提出TRIT框架，将翻译训练融入多语言推理训练中，实现多语言问题理解与响应生成的联合优化，无需外部反馈或额外多语言数据。

Result: 在MMATH数据集上，相比多个基线平均提升7个百分点，显著改善答案正确性与语言一致性；跨语言问题对齐提升超过10个百分点，数学与通用文本翻译质量显著增强，最高提升8.4 COMET点。

Conclusion: TRIT通过整合翻译训练有效提升了多语言推理模型的多语言理解与推理能力，为多语言长推理任务提供了高效、无需额外数据的解决方案。

Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

</details>


### [128] [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971)
*Felipe D. Toro-Hernández,Jesuino Vieira Filho,Rodrigo M. Cabral-Carvalho*

Main category: cs.CL

TL;DR: 本文提出一种框架，将概念生成视为嵌入空间中的导航过程，利用Transformer文本嵌入模型构建个体特定的语义轨迹，并提取几何与动态度量（如距离、熵、速度、加速度），以量化语义表征的动态特性。该方法在多语言、多任务数据集上表现良好，能有效区分临床群体与概念类型，且对不同嵌入模型具有鲁棒性，相比传统人工语言处理方法更高效。


<details>
  <summary>Details</summary>
Motivation: 探究人类如何在语义空间中导航以检索和操作意义，提供一种计算上可支撑的语义表征搜索模型，减少对人工语言预处理的依赖。

Method: 基于累积嵌入构建参与者特定的语义轨迹，提取几何与动力学指标（距离到下一个、到中心点的距离、熵、速度、加速度），使用多种Transformer文本嵌入模型进行分析，并对比累积与非累积方法的效果。

Result: 该框架在多种语言和任务中均能有效区分临床组与概念类型；累积嵌入适用于长轨迹，非累积更适合短轨迹；不同嵌入模型结果相似，表明其学习表示具有共性。

Conclusion: 将语义导航建模为嵌入空间中的结构化轨迹，连接认知建模与学习表示，为临床研究、跨语言分析及人工智能认知评估提供了可量化的动态表征分析管道。

Abstract: Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.

</details>


### [129] [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992)
*Lizhuo Luo,Shenggui Li,Yonggang Wen,Tianwei Zhang*

Main category: cs.CL

TL;DR: 本文提出Dynamic Sliding Block (DSB) 和 DSB Cache，一种无需训练的块调度和键值缓存机制，用于改进扩散语言模型（dLLMs）的推理效率与生成质量。通过动态调整块大小以适应语义难度，DSB克服了传统固定块调度的局限性，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有固定块调度策略忽视语义难度，导致在不确定位置过早决策或在块边界附近延迟处理简单位置，影响生成质量和推理效率。因此需要一种能动态适应语义难度的调度方法。

Method: 提出Dynamic Sliding Block (DSB)，一种无需训练的动态滑动块调度方法，根据上下文语义难度自适应调整块大小；同时引入DSB Cache，一种专为DSB设计的训练-free KV-cache机制，进一步提升效率。

Result: 在多个模型和基准测试中，DSB与DSB Cache结合使用，显著提升了dLLMs的生成质量与推理效率，且无需额外训练。

Conclusion: DSB及其配套的DSB Cache能够有效解决传统固定块调度的缺陷，实现更高效、更可靠的扩散语言模型推理，具有广泛适用性和实际应用价值。

Abstract: Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.

</details>


### [130] [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015)
*Panagiotis Kaliosis,Adithya V Ganesan,Oscar N. E. Kjell,Whitney Ringwald,Scott Feltman,Melissa A. Carr,Dimitris Samaras,Camilo Ruggero,Benjamin J. Luft,Roman Kotov,Andrew H. Schwartz*

Main category: cs.CL

TL;DR: 本研究评估了11个前沿大语言模型（LLM）在零样本情境下评估创伤后应激障碍（PTSD）严重程度的表现，使用来自1437名个体的临床语料和自评评分数据。通过系统调整上下文知识（如量表定义、分布摘要、访谈问题）和建模策略（如零样本/少样本、推理努力程度、模型规模、结构化子量表与直接标量预测、输出重缩放及九种集成方法），发现：(a) 提供详细的构念定义和叙事背景可显著提升准确性；(b) 增加推理努力有助于提高估计精度；(c) 开源模型（如Llama、Deepseek）性能在700亿参数后趋于饱和，而闭源模型（如o3-mini、gpt-5）随新版本迭代持续提升；(d) 最佳表现来自将监督模型与零样本LLM进行集成。结果表明，上下文知识和建模策略的选择对准确评估心理健康至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在零样本情境下被广泛用于心理健康评估，但影响其准确性的关键因素尚不明确。本研究旨在系统探究上下文信息与建模策略如何影响LLM在心理健康评估中的表现，以指导其更可靠的实际应用。

Method: 基于1437名个体的自然语言叙述和自评PTSD严重度数据，系统比较11个先进大语言模型在不同上下文设置（如量表定义、分布摘要、访谈问题）和建模策略（如零样本/少样本、推理深度、模型规模、输出形式、集成方法）下的表现，采用定量评估指标分析其准确性差异。

Result: (a) 提供详细构念定义和叙事背景时，模型表现最佳；(b) 增加推理努力可提升预测准确性；(c) 开源模型性能在700亿参数后趋于稳定，闭源模型则随版本更新持续进步；(d) 集成监督模型与零样本LLM能获得最优效果。

Conclusion: 大语言模型在心理健康评估中的准确性高度依赖于提供的上下文信息和所采用的建模策略。优化上下文输入、合理设计推理流程并结合集成方法，是提升其评估性能的关键。

Abstract: Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.

</details>


### [131] [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036)
*Jian Chen,Yesheng Liang,Zhijian Liu*

Main category: cs.CL

TL;DR: DFlash提出一种基于轻量级块扩散模型的推测解码框架，通过单次前向传播并行生成草稿令牌，并利用目标模型提取的上下文特征进行条件化，实现高效且高质量的草稿生成。实验表明，DFlash在多种模型和任务上实现了超过6倍的无损加速，速度提升比当前最先进的EAGLE-3方法高出2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法依赖自回归草稿生成，仍存在序列瓶颈，限制实际加速效果；而扩散语言模型虽支持并行生成但性能通常低于自回归模型。因此需要一种既能并行生成又具备高质输出的草稿生成方法。

Method: DFlash采用轻量级块扩散模型作为草稿生成器，在单次前向传播中并行生成多个草稿令牌，并将目标模型提取的上下文特征用于条件化草稿模型，从而提升草稿质量与接受率。

Result: DFlash在多个模型和任务上实现了超过6倍的无损推理加速，相比EAGLE-3等先进方法提速达2.5倍以上，显著提升推理效率与GPU利用率。

Conclusion: DFlash通过引入并行化的块扩散草稿模型，有效突破了传统推测解码中的序列瓶颈，为大语言模型的高效推理提供了新范式，具有显著的实际应用价值。

Abstract: Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [132] [Artificial Intelligence as Strange Intelligence: Against Linear Models of Intelligence](https://arxiv.org/abs/2602.04986)
*Kendra Chilson,Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 本文批判了线性AI进步模型，提出‘熟悉智能’与‘奇异智能’两个新概念。认为AI智能更可能是奇异智能，即在某些领域超人类，而在其他领域却表现低下，甚至在同一领域内兼具超凡洞察力与人类难以犯的错误。文章主张非线性智能模型，认为‘通用智能’并非单一能力，而是能在多种环境中达成广泛目标的能力，无法被简化为单一线性指标。最后指出，评估AI应采用对抗性测试，因奇异智能系统即使强大也可能在明显任务上失败，这不意味着缺乏通用智能；同样，某项任务（如智商测试）表现优异也不能推断其具备广泛能力。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型无法准确描述人工智能发展的复杂性，尤其无法解释未来智能系统可能表现出的非对称、非常规能力特征。为更真实反映AI智能的本质，有必要引入新的理论框架。

Method: 通过哲学分析与概念建构，提出‘熟悉智能’与‘奇异智能’的区分，并构建非线性智能模型，以解释智能的多元性与不可还原性。结合对当前评估方法的反思，提出对抗性测试作为更合适的评估路径。

Result: 揭示了传统线性模型的局限性，确立了‘奇异智能’的存在可能性，论证了非线性智能模型的合理性，并提出对抗性测试对于评估高阶AI能力的重要性。

Conclusion: AI智能很可能表现为奇异智能，具有高度非线性的特征。因此，不应以单一指标衡量其能力，而应采用对抗性测试来全面评估其在多样环境中的表现。系统在简单任务上的失败不等于缺乏通用智能，反之亦然。

Abstract: We endorse and expand upon Susan Schneider's critique of the linear model of AI progress and introduce two novel concepts: "familiar intelligence" and "strange intelligence". AI intelligence is likely to be strange intelligence, defying familiar patterns of ability and inability, combining superhuman capacities in some domains with subhuman performance in other domains, and even within domains sometimes combining superhuman insight with surprising errors that few humans would make. We develop and defend a nonlinear model of intelligence on which "general intelligence" is not a unified capacity but instead the ability to achieve a broad range of goals in a broad range of environments, in a manner that defies nonarbitrary reduction to a single linear quantity. We conclude with implications for adversarial testing approaches to evaluating AI capacities. If AI is strange intelligence, we should expect that even the most capable systems will sometimes fail in seemingly obvious tasks. On a nonlinear model of AI intelligence, such errors on their own do not demonstrate a system's lack of outstanding general intelligence. Conversely, excellent performance on one type of task, such as an IQ test, cannot warrant assumptions of broad capacities beyond that task domain.

</details>


### [133] [MINT: Minimal Information Neuro-Symbolic Tree for Objective-Driven Knowledge-Gap Reasoning and Active Elicitation](https://arxiv.org/abs/2602.05048)
*Zeyu Fang,Tian Lan,Mahdi Imani*

Main category: cs.AI

TL;DR: 该论文提出MINT（Minimal Information Neuro-Symbolic Tree）模型，用于在存在知识空白的开放世界中优化AI与人类协作中的主动提问策略。MINT通过构建符号树并结合神经规划策略评估不确定性，利用自博弈优化查询策略，并借助大语言模型（LLM）生成高效的人类输入请求。实验表明，MINT在多个包含未知对象的基准任务中，以少量问题即可达到接近专家水平的性能，显著提升奖励和成功率。


<details>
  <summary>Details</summary>
Motivation: 在开放世界的联合规划中，由于对象、人类目标或意图等信息不完整，存在知识空白，导致规划困难。如何设计有效的交互策略来主动获取人类输入，是提升人机协作效率的关键挑战。

Method: 提出MINT模型，构建基于可能的人机交互命题的符号树；结合神经规划策略估计因知识缺失导致的规划结果不确定性；采用自博弈方法优化AI的提问策略；利用大语言模型（LLM）分析并总结推理过程，生成最优提问集。

Result: 在三个逐步增加真实性的基准测试中，基于MINT的规划方法仅需少量问题即可实现接近专家水平的回报，显著提升任务奖励与成功率。

Conclusion: MINT通过有效建模知识空白并优化主动询问策略，在复杂开放环境中实现了高效的联合规划，为智能体与人类协作提供了可扩展且高绩效的解决方案。

Abstract: Joint planning through language-based interactions is a key area of human-AI teaming. Planning problems in the open world often involve various aspects of incomplete information and unknowns, e.g., objects involved, human goals/intents -- thus leading to knowledge gaps in joint planning. We consider the problem of discovering optimal interaction strategies for AI agents to actively elicit human inputs in object-driven planning. To this end, we propose Minimal Information Neuro-Symbolic Tree (MINT) to reason about the impact of knowledge gaps and leverage self-play with MINT to optimize the AI agent's elicitation strategies and queries. More precisely, MINT builds a symbolic tree by making propositions of possible human-AI interactions and by consulting a neural planning policy to estimate the uncertainty in planning outcomes caused by remaining knowledge gaps. Finally, we leverage LLM to search and summarize MINT's reasoning process and curate a set of queries to optimally elicit human inputs for best planning performance. By considering a family of extended Markov decision processes with knowledge gaps, we analyze the return guarantee for a given MINT with active human elicitation. Our evaluation on three benchmarks involving unseen/unknown objects of increasing realism shows that MINT-based planning attains near-expert returns by issuing a limited number of questions per task while achieving significantly improved rewards and success rates.

</details>


### [134] [Evaluating Large Language Models on Solved and Unsolved Problems in Graph Theory: Implications for Computing Education](https://arxiv.org/abs/2602.05059)
*Adithya Kulkarni,Mohna Chakraborty,Jay Bagga*

Main category: cs.AI

TL;DR: 该研究评估大型语言模型（LLM）在解决图论中已知问题和开放问题时的表现。在已知问题上，模型表现良好，能正确理解、推理并构建专家验证的证明；在开放问题上，模型虽能提出合理探索策略，但无法推进至解决方案，且未虚构结论，表现出对不确定性的恰当认知。结果表明，LLMs 可辅助概念探索，但在需要创新数学洞察或关键结构推理的任务中仍有限制。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在计算机科学教学中的广泛应用，了解其在支持数学严谨思维方面的可靠性至关重要。尤其在图论等高度形式化的领域，需明确 LLM 的能力边界，以指导学生有效利用这些工具进行学习。

Method: 采用八阶段评估协议，模拟真实的数学探究过程，包括问题解读、探索、策略制定和证明构建，系统评估 LLM 在两个相关图论问题上的表现：一个已解决的问题（关于线图的优美性）和一个尚未解决的开放问题。

Result: 对于已解决的问题，模型成功生成了正确的定义、识别了相关结构、准确回忆了已有成果，并构建了经专家验证的有效证明。对于开放问题，模型展现出合理的解释和可行的探索策略，但未能推进到解决方案，且在不确定性情况下保持诚实，未编造结论。

Conclusion: 大型语言模型能够有效支持对已有数学知识的概念性探索，但在需要原创性数学洞察或复杂结构推理的难题求解中仍存在局限。因此，在计算教育中应引导学生将 LLM 用于启发式探索，而正式的证明与问题求解仍需依赖独立验证与严格论证。

Abstract: Large Language Models are increasingly used by students to explore advanced material in computer science, including graph theory. As these tools become integrated into undergraduate and graduate coursework, it is important to understand how reliably they support mathematically rigorous thinking. This study examines the performance of a LLM on two related graph theoretic problems: a solved problem concerning the gracefulness of line graphs and an open problem for which no solution is currently known. We use an eight stage evaluation protocol that reflects authentic mathematical inquiry, including interpretation, exploration, strategy formation, and proof construction.
  The model performed strongly on the solved problem, producing correct definitions, identifying relevant structures, recalling appropriate results without hallucination, and constructing a valid proof confirmed by a graph theory expert. For the open problem, the model generated coherent interpretations and plausible exploratory strategies but did not advance toward a solution. It did not fabricate results and instead acknowledged uncertainty, which is consistent with the explicit prompting instructions that directed the model to avoid inventing theorems or unsupported claims.
  These findings indicate that LLMs can support exploration of established material but remain limited in tasks requiring novel mathematical insight or critical structural reasoning. For computing education, this distinction highlights the importance of guiding students to use LLMs for conceptual exploration while relying on independent verification and rigorous argumentation for formal problem solving.

</details>


### [135] [Optimizing Mission Planning for Multi-Debris Rendezvous Using Reinforcement Learning with Refueling and Adaptive Collision Avoidance](https://arxiv.org/abs/2602.05075)
*Agni Bandyopadhyay,Gunther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 该研究提出一种基于强化学习（RL）的框架，用于提升小卫星在多碎片主动清除任务中的自适应碰撞规避能力。采用掩码化的近端策略优化（PPO）算法，使智能体能根据实时轨道条件动态调整机动策略，兼顾燃料效率、避障和轨道参数优化。通过Iridium 33碎片数据集模拟多种轨道场景，验证了该方法在降低碰撞风险、提高任务效率方面的优越性，为复杂多目标空间任务规划提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着地球轨道碎片增多，主动碎片清除（ADR）任务面临高碰撞风险，传统方法难以应对动态环境。小卫星虽具灵活性与低成本优势，但其自主决策能力有限，亟需智能化的路径规划与避碰机制以保障安全与效率。

Method: 提出基于掩码化近端策略优化（PPO）的强化学习框架，集成多碎片交会、燃料补给策略与自适应避碰机制。智能体在仿真环境中学习最优任务序列，动态调整飞行轨迹，同时考虑燃料消耗、碰撞区域规避与轨道动态变化。

Result: 在Iridium 33碎片数据集上的仿真结果表明，该框架显著降低了碰撞风险，相比传统启发式方法提升了任务效率，包括更优的燃料利用率和更短的任务执行时间。

Conclusion: 所提出的强化学习框架为多碎片主动清除任务提供了一种高效、自适应且可扩展的解决方案，适用于复杂多目标自主空间任务规划，具有广泛的应用前景。

Abstract: As the orbital environment around Earth becomes increasingly crowded with debris, active debris removal (ADR) missions face significant challenges in ensuring safe operations while minimizing the risk of in-orbit collisions. This study presents a reinforcement learning (RL) based framework to enhance adaptive collision avoidance in ADR missions, specifically for multi-debris removal using small satellites. Small satellites are increasingly adopted due to their flexibility, cost effectiveness, and maneuverability, making them well suited for dynamic missions such as ADR.
  Building on existing work in multi-debris rendezvous, the framework integrates refueling strategies, efficient mission planning, and adaptive collision avoidance to optimize spacecraft rendezvous operations. The proposed approach employs a masked Proximal Policy Optimization (PPO) algorithm, enabling the RL agent to dynamically adjust maneuvers in response to real-time orbital conditions. Key considerations include fuel efficiency, avoidance of active collision zones, and optimization of dynamic orbital parameters.
  The RL agent learns to determine efficient sequences for rendezvousing with multiple debris targets, optimizing fuel usage and mission time while incorporating necessary refueling stops. Simulated ADR scenarios derived from the Iridium 33 debris dataset are used for evaluation, covering diverse orbital configurations and debris distributions to demonstrate robustness and adaptability. Results show that the proposed RL framework reduces collision risk while improving mission efficiency compared to traditional heuristic approaches.
  This work provides a scalable solution for planning complex multi-debris ADR missions and is applicable to other multi-target rendezvous problems in autonomous space mission planning.

</details>


### [136] [Evaluating Robustness and Adaptability in Learning-Based Mission Planning for Active Debris Removal](https://arxiv.org/abs/2602.05091)
*Agni Bandyopadhyay,Günther Waxenegger-Wilfing*

Main category: cs.AI

TL;DR: 该研究比较了三种用于低地球轨道主动碎片清除（ADR）任务规划的算法：固定参数训练的掩码近端策略优化（PPO）策略、跨多种任务约束训练以提升鲁棒性的领域随机化PPO策略，以及作为基线的蒙特卡洛树搜索（MCTS）。在300次高保真轨道仿真测试中，结果表明，标准PPO在条件匹配时表现最优但面对分布外变化性能急剧下降；领域随机化PPO在保持较高性能的同时显著提升了适应性；而MCTS虽能最好应对约束变化，但计算开销极大。研究揭示了学习型策略与基于搜索的方法在速度与适应性之间的权衡，并建议未来结合训练多样性与在线规划以构建更具韧性的任务规划系统。


<details>
  <summary>Details</summary>
Motivation: 主动碎片清除（ADR）任务需在燃料和任务时长等严格约束下实现高效、灵活的任务规划，现有方法在面对环境变化或约束波动时适应性不足，亟需更稳健的规划策略。

Method: 采用三种规划方法：1）固定参数训练的掩码近端策略优化（PPO）；2）通过领域随机化训练提升鲁棒性的掩码PPO；3）作为基线的蒙特卡洛树搜索（MCTS）。所有方法在包含燃料补给、真实转移动力学和随机碎片场的高保真轨道仿真环境中进行评估。

Result: 标准PPO在训练条件匹配时表现最佳，但在约束变化时性能显著下降；领域随机化PPO在多数场景中表现出更强的适应性，仅轻微牺牲名义性能；MCTS在处理约束变化方面表现最优，但计算成本高出数个数量级。

Conclusion: 学习型策略在效率上占优，而搜索方法在适应性上更佳。未来方向应探索将训练阶段的多样性与在线规划相结合，以发展更鲁棒、高效的主动碎片清除任务规划系统。

Abstract: Autonomous mission planning for Active Debris Removal (ADR) must balance efficiency, adaptability, and strict feasibility constraints on fuel and mission duration. This work compares three planners for the constrained multi-debris rendezvous problem in Low Earth Orbit: a nominal Masked Proximal Policy Optimization (PPO) policy trained under fixed mission parameters, a domain-randomized Masked PPO policy trained across varying mission constraints for improved robustness, and a plain Monte Carlo Tree Search (MCTS) baseline. Evaluations are conducted in a high-fidelity orbital simulation with refueling, realistic transfer dynamics, and randomized debris fields across 300 test cases in nominal, reduced fuel, and reduced mission time scenarios. Results show that nominal PPO achieves top performance when conditions match training but degrades sharply under distributional shift, while domain-randomized PPO exhibits improved adaptability with only moderate loss in nominal performance. MCTS consistently handles constraint changes best due to online replanning but incurs orders-of-magnitude higher computation time. The findings underline a trade-off between the speed of learned policies and the adaptability of search-based methods, and suggest that combining training-time diversity with online planning could be a promising path for future resilient ADR mission planners.

</details>


### [137] [Democratic Preference Alignment via Sortition-Weighted RLHF](https://arxiv.org/abs/2602.05113)
*Suvadip Sana,Jinzhou Wu,Martin T. Wells*

Main category: cs.AI

TL;DR: 该论文提出了一种名为民主偏好优化（DemPO）的框架，通过算法抽签机制在偏好对齐中实现更代表性的数据收集。与传统基于人类评分者的偏好训练不同，DemPO采用类似公民大会的随机抽样方法，确保不同人口群体的代表性。研究设计了两种训练方案：硬面板（Hard Panel）仅使用通过抽签选出的代表性小群体数据；软面板（Soft Panel）保留全部数据但根据抽签概率重新加权评分者。理论证明软面板权重可精确恢复硬面板目标。实验基于包含评分者人口统计信息的公开数据集，评估了从10亿到80亿参数的Llama模型。结果表明，硬面板在六种聚合方法中始终表现最佳，软面板优于未加权基线，且效果随模型规模增加而增强。这说明在偏好收集阶段即保证代表性，比事后修正更能使AI行为反映真正代表性公众的价值观。


<details>
  <summary>Details</summary>
Motivation: 当前基于人类偏好的对齐方法（如RLHF）依赖于便利样本的人类评分者，这些样本往往系统性地高估某些人口群体而低估其他群体，导致价值观偏差。因此，需要一种机制来确保评分者群体在人口学上更具代表性，从而提升AI系统所学习价值的真实性和公平性。

Method: 提出民主偏好优化（DemPO）框架，利用算法抽签（sortition）构建具有代表性的评分者小组。设计两种训练策略：1）硬面板，仅使用通过抽签生成的代表性子样本进行训练；2）软面板，保留所有数据但按抽签入选概率对每个评分者加权。理论推导证明软面板权重可等价于硬面板的期望目标。

Result: 在多个模型规模（10亿至80亿参数）和六种聚合方法下，硬面板始终排名第一，软面板稳定优于未加权基线，且随着模型容量增大，优势更加明显。使用美国代表性群体独立获取的75条宪法条款作为评估基准，验证了模型行为更贴近代表性公众的价值取向。

Conclusion: 在偏好数据收集阶段即通过算法抽签强制实现人口代表性，比事后修正更能有效提升AI系统对广泛公众价值观的忠实度。该方法为构建更具社会公平性和代表性的智能系统提供了可行路径。

Abstract: Whose values should AI systems learn? Preference based alignment methods like RLHF derive their training signal from human raters, yet these rater pools are typically convenience samples that systematically over represent some demographics and under represent others. We introduce Democratic Preference Optimization, or DemPO, a framework that applies algorithmic sortition, the same mechanism used to construct citizen assemblies, to preference based fine tuning. DemPO offers two training schemes. Hard Panel trains exclusively on preferences from a quota satisfying mini public sampled via sortition. Soft Panel retains all data but reweights each rater by their inclusion probability under the sortition lottery. We prove that Soft Panel weighting recovers the expected Hard Panel objective in closed form. Using a public preference dataset that pairs human judgments with rater demographics and a seventy five clause constitution independently elicited from a representative United States panel, we evaluate Llama models from one billion to eight billion parameters fine tuned under each scheme. Across six aggregation methods, the Hard Panel consistently ranks first and the Soft Panel consistently outperforms the unweighted baseline, with effect sizes growing as model capacity increases. These results demonstrate that enforcing demographic representativeness at the preference collection stage, rather than post hoc correction, yields models whose behavior better reflects values elicited from representative publics.

</details>


### [138] [SocialVeil: Probing Social Intelligence of Language Agents under Communication Barriers](https://arxiv.org/abs/2602.05115)
*Keyang Xuan,Pengda Wang,Chongrui Ye,Haofei Yu,Tal August,Jiaxuan You*

Main category: cs.AI

TL;DR: 本文提出SocialVeil，一个模拟因认知差异导致沟通障碍的社会学习环境，包含语义模糊、社会文化错配和情感干扰三种典型障碍，并引入两个评估指标（未解决困惑和相互理解）来衡量在沟通受损情况下的交互质量。实验表明，这些障碍显著降低性能（相互理解平均下降45%，困惑度上升近50%），且现有适应策略效果有限，无法达到无障碍水平。人类评估验证了模拟障碍的真实性。该研究推动了语言模型社交智能评估向更真实场景迈进。


<details>
  <summary>Details</summary>
Motivation: 现有基准多假设理想化通信，难以诊断大语言模型在现实不完美情境下的交互维持与修复能力，因此需要更贴近真实社会互动的评估环境。

Method: 基于文献综述构建SocialVeil环境，引入三种由认知差异引发的沟通障碍：语义模糊、社会文化错配、情感干扰；设计两个屏障感知评估指标：未解决困惑与相互理解；通过720个场景和4个前沿大模型进行实验，并结合人类评估验证模拟真实性。

Result: 三类障碍显著降低交互质量，平均相互理解下降45%以上，未解决困惑上升近50%；修复指令和交互学习等适应策略仅带来有限改善，无法接近无障性能；人类评估显示模拟障碍具有高保真度（ICC≈0.78，Pearson r≈0.80）。

Conclusion: SocialVeil为评估大语言模型在真实社会互动中的社交智能提供了更贴近现实的环境，揭示了当前模型在面对沟通障碍时的脆弱性，也为未来提升其社会适应能力提供了方向。

Abstract: Large language models (LLMs) are increasingly evaluated in interactive environments to test their social intelligence. However, existing benchmarks often assume idealized communication between agents, limiting our ability to diagnose whether LLMs can maintain and repair interactions in more realistic, imperfect settings. To close this gap, we present \textsc{SocialVeil}, a social learning environment that can simulate social interaction under cognitive-difference-induced communication barriers. Grounded in a systematic literature review of communication challenges in human interaction, \textsc{SocialVeil} introduces three representative types of such disruption, \emph{semantic vagueness}, \emph{sociocultural mismatch}, and \emph{emotional interference}. We also introduce two barrier-aware evaluation metrics, \emph{unresolved confusion} and \emph{mutual understanding}, to evaluate interaction quality under impaired communication. Experiments across 720 scenarios and four frontier LLMs show that barriers consistently impair performance, with mutual understanding reduced by over 45\% on average, and confusion elevated by nearly 50\%. Human evaluations validate the fidelity of these simulated barriers (ICC$\approx$0.78, Pearson r$\approx$0.80). We further demonstrate that adaptation strategies (Repair Instruction and Interactive learning) only have a modest effect far from barrier-free performance. This work takes a step toward bringing social interaction environments closer to real-world communication, opening opportunities for exploring the social intelligence of LLM agents.

</details>


### [139] [CAST-CKT: Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer for Traffic Flow Prediction](https://arxiv.org/abs/2602.05133)
*Abdul Joseph Fofanah,Lian Wen,David Chen,Alpha Alimamy Kamara,Zhongyi Zhang*

Main category: cs.AI

TL;DR: CAST-CKT 是一种针对数据稀缺、跨城市交通预测的新型混沌感知时空与跨城市知识迁移框架，通过混沌分析器量化交通可预测性状态，引入适应性注意力机制、动态拓扑学习和基于混沌一致性的跨城市对齐策略，实现精准的少样本学习与不确定性量化预测，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺和跨城市场景下难以捕捉交通的非线性动态和领域偏移，尤其在少样本学习中表现不佳，无法有效建模交通的混沌特性。

Method: 提出混沌感知的时空与跨城市知识迁移框架（CAST-CKT），包含：混沌分析器用于量化交通可预测性；混沌感知注意力机制实现自适应时间建模；自适应拓扑学习处理动态空间依赖；混沌一致性对齐促进跨城市知识迁移；支持多时序步预测与不确定性估计。

Result: 在四个跨城市少样本交通预测基准上，CAST-CKT 在 MAE 与 RMSE 上显著优于当前最优方法，具备良好的泛化性能与可解释性。

Conclusion: CAST-CKT 有效应对了数据稀缺与跨城市域偏移挑战，通过混沌感知机制提升交通预测的准确性与鲁棒性，为少样本交通预测提供了新范式。

Abstract: Traffic prediction in data-scarce, cross-city settings is challenging due to complex nonlinear dynamics and domain shifts. Existing methods often fail to capture traffic's inherent chaotic nature for effective few-shot learning. We propose CAST-CKT, a novel Chaos-Aware Spatio-Temporal and Cross-City Knowledge Transfer framework. It employs an efficient chaotic analyser to quantify traffic predictability regimes, driving several key innovations: chaos-aware attention for regime-adaptive temporal modelling; adaptive topology learning for dynamic spatial dependencies; and chaotic consistency-based cross-city alignment for knowledge transfer. The framework also provides horizon-specific predictions with uncertainty quantification. Theoretical analysis shows improved generalisation bounds. Extensive experiments on four benchmarks in cross-city few-shot settings show CAST-CKT outperforms state-of-the-art methods by significant margins in MAE and RMSE, while offering interpretable regime analysis. Code is available at https://github.com/afofanah/CAST-CKT.

</details>


### [140] [HugRAG: Hierarchical Causal Knowledge Graph Design for RAG](https://arxiv.org/abs/2602.05143)
*Nengbo Wang,Tuo Liang,Vikash Singh,Chaoda Song,Van Yang,Yu Yin,Jing Ma,Jagdip Singh,Vipin Chaudhary*

Main category: cs.AI

TL;DR: HugRAG提出一种基于因果门控的层次化模块知识组织框架，解决现有图基RAG方法过度依赖表面匹配、缺乏显式因果建模及信息孤立的问题，通过显式建模因果关系实现可扩展的跨模块推理，在多个数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG方法在结构化检索与推理中存在对表面节点匹配的过度依赖、缺乏显式因果建模以及由模块化图结构导致的信息孤立问题，限制了其在大规模知识图谱上的可扩展性和因果推理能力。

Method: 提出HugRAG框架，通过层级模块间的因果门控机制重新组织知识，显式建模因果关系以抑制虚假相关性，并支持跨模块的可扩展推理。

Result: 在多个数据集和评估指标上，HugRAG consistently超越现有图基RAG基线，验证了其在结构化、可扩展及因果一致推理方面的有效性。

Conclusion: HugRAG为构建结构化、可扩展且因果基础的RAG系统提供了原则性基础，推动了图基RAG向更可信、更智能的方向发展。

Abstract: Retrieval augmented generation (RAG) has enhanced large language models by enabling access to external knowledge, with graph-based RAG emerging as a powerful paradigm for structured retrieval and reasoning. However, existing graph-based methods often over-rely on surface-level node matching and lack explicit causal modeling, leading to unfaithful or spurious answers. Prior attempts to incorporate causality are typically limited to local or single-document contexts and also suffer from information isolation that arises from modular graph structures, which hinders scalability and cross-module causal reasoning. To address these challenges, we propose HugRAG, a framework that rethinks knowledge organization for graph-based RAG through causal gating across hierarchical modules. HugRAG explicitly models causal relationships to suppress spurious correlations while enabling scalable reasoning over large-scale knowledge graphs. Extensive experiments demonstrate that HugRAG consistently outperforms competitive graph-based RAG baselines across multiple datasets and evaluation metrics. Our work establishes a principled foundation for structured, scalable, and causally grounded RAG systems.

</details>


### [141] [First Proof](https://arxiv.org/abs/2602.05192)
*Mohammed Abouzaid,Andrew J. Blumberg,Martin Hairer,Joe Kileel,Tamara G. Kolda,Paul D. Nelson,Daniel Spielman,Nikhil Srivastava,Rachel Ward,Shmuel Weinberger,Lauren Williams*

Main category: cs.AI

TL;DR: 本文分享了十个在作者研究过程中自然产生的高级数学问题，用于评估当前AI系统在回答研究级数学问题上的能力。这些问题此前未公开，答案由作者知晓但暂时加密。


<details>
  <summary>Details</summary>
Motivation: 评估当前AI系统在回答研究级数学问题上的能力，通过真实研究中产生的问题来检验其表现。

Method: 分享十个未公开的、源自作者研究过程的数学问题，答案暂被加密以确保公平性与挑战性。

Result: 尚未公布具体结果，旨在激励对高阶数学推理能力的研究与评估。

Conclusion: 这些问题是衡量AI在复杂数学推理方面进展的重要基准，有助于推动人工智能在数学研究中的应用发展。

Abstract: To assess the ability of current AI systems to correctly answer research-level mathematics questions, we share a set of ten math questions which have arisen naturally in the research process of the authors. The questions had not been shared publicly until now; the answers are known to the authors of the questions but will remain encrypted for a short time.

</details>


### [142] [Traceable Cross-Source RAG for Chinese Tibetan Medicine Question Answering](https://arxiv.org/abs/2602.05195)
*Fengxian Chen,Zhilong Tao,Jiaxuan Li,Yunlong Li,Qingguo Zhou*

Main category: cs.AI

TL;DR: 本文研究中文藏医领域中多源异构知识库下的检索增强生成（RAG）挑战，提出DAKS方法进行知识库路由与预算检索以缓解密度偏差，并引入对齐图实现跨知识库证据融合与覆盖感知打包，提升可追溯性与准确性。实验表明该系统在跨知识库证据覆盖率和忠实度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在中文藏医领域，不同知识库（如百科全书、经典文献、临床论文）存在信息密度差异，导致检索时易受密集内容主导，忽略更权威的来源。如何有效识别并利用不同知识库中的权威证据，成为提升RAG系统性能的关键挑战。

Method: 提出DAKS方法实现知识库路由与预算检索，减少因内容密度带来的偏差；同时构建对齐图来指导证据融合与覆盖感知打包，避免简单拼接，增强跨知识库证据的整合能力。所有生成由轻量级模型openPangu-Embedded-7B完成。

Result: 实验结果显示，系统在知识库路由质量、跨知识库证据覆盖方面均有显著提升，整体在CrossEv@5指标上达到最佳表现，同时保持高忠实度与引用正确性。

Conclusion: 所提出的DAKS与对齐图策略能有效应对多源异构知识库中的检索偏差问题，提升RAG系统的可追溯性与可信度，适用于复杂领域的知识问答任务。

Abstract: Retrieval-augmented generation (RAG) promises grounded question answering, yet domain settings with multiple heterogeneous knowledge bases (KBs) remain challenging. In Chinese Tibetan medicine, encyclopedia entries are often dense and easy to match, which can dominate retrieval even when classics or clinical papers provide more authoritative evidence. We study a practical setting with three KBs (encyclopedia, classics, and clinical papers) and a 500-query benchmark (cutoff $K{=}5$) covering both single-KB and cross-KB questions. We propose two complementary methods to improve traceability, reduce hallucinations, and enable cross-KB verification. First, DAKS performs KB routing and budgeted retrieval to mitigate density-driven bias and to prioritize authoritative sources when appropriate. Second, we use an alignment graph to guide evidence fusion and coverage-aware packing, improving cross-KB evidence coverage without relying on naive concatenation. All answers are generated by a lightweight generator, \textsc{openPangu-Embedded-7B}. Experiments show consistent gains in routing quality and cross-KB evidence coverage, with the full system achieving the best CrossEv@5 while maintaining strong faithfulness and citation correctness.

</details>


### [143] [Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://arxiv.org/abs/2602.05228)
*Guozhi Liu,Weiwei Lin,Tiansheng Huang,Ruichao Mo,Qi Mu,Xiumin Wang,Li Shen*

Main category: cs.AI

TL;DR: 本文提出一种名为Surgery的防御方法，利用注意力汇聚机制中的sink divergence统计量，通过抑制正向sink divergence的注意力头，减少模型在有害微调过程中学习和放大有害模式的趋势。实验表明，该方法在BeaverTails、HarmBench和SorryBench三个基准上分别提升了5.90%、11.25%和9.55%的防御性能。


<details>
  <summary>Details</summary>
Motivation: 有害微调会破坏大语言模型的安全对齐，带来显著安全风险，因此需要一种有效的机制来识别并抑制有害模式的学习。

Method: 通过计算每个注意力头的sink divergence，发现其符号差异，并基于正负sink divergence可分离有害学习模式的假设，设计了一种正则化方法以引导注意力头趋向负sink divergence，从而实现微调阶段的防御。

Result: Surgery在BeaverTails、HarmBench和SorryBench三个基准上分别实现了5.90%、11.25%和9.55%的防御性能提升，证明了其有效性。

Conclusion: 本文提出的Surgery方法能够有效缓解有害微调带来的安全风险，通过控制注意力头的sink divergence符号，抑制有害模式的学习，为大语言模型的安全微调提供了新思路。

Abstract: Harmful fine-tuning can invalidate safety alignment of large language models, exposing significant safety risks. In this paper, we utilize the attention sink mechanism to mitigate harmful fine-tuning. Specifically, we first measure a statistic named \emph{sink divergence} for each attention head and observe that \emph{different attention heads exhibit two different signs of sink divergence}. To understand its safety implications, we conduct experiments and find that the number of attention heads of positive sink divergence increases along with the increase of the model's harmfulness when undergoing harmful fine-tuning. Based on this finding, we propose a separable sink divergence hypothesis -- \emph{attention heads associating with learning harmful patterns during fine-tuning are separable by their sign of sink divergence}. Based on the hypothesis, we propose a fine-tuning-stage defense, dubbed Surgery. Surgery utilizes a regularizer for sink divergence suppression, which steers attention heads toward the negative sink divergence group, thereby reducing the model's tendency to learn and amplify harmful patterns. Extensive experiments demonstrate that Surgery improves defense performance by 5.90\%, 11.25\%, and 9.55\% on the BeaverTails, HarmBench, and SorryBench benchmarks, respectively. Source code is available on https://github.com/Lslland/Surgery.

</details>


### [144] [Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models](https://arxiv.org/abs/2602.05240)
*Patrick McGonagle,William Farrelly,Kevin Curran*

Main category: cs.AI

TL;DR: 本研究通过整合多种可解释人工智能（XAI）技术，提升深度学习模型在脑肿瘤检测中的可解释性。基于BraTS 2021数据集训练的自定义CNN模型达到91.24%的准确率。结合GRAD-CAM、LRP和SHAP三种方法，实现了从区域到像素级别的多层次解释，有效识别完整与部分肿瘤，显著优于单一XAI方法。该集成策略增强了医疗影像中AI决策的透明度与可信度，推动AI在关键临床任务中的可靠应用。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型在脑肿瘤检测中的可解释性，增强医生对AI诊断结果的信任，推动AI在医疗影像分析中的临床应用。

Method: 开发并训练自定义CNN模型；融合GRAD-CAM、LRP和SHAP三种XAI技术，进行多层级解释分析。

Result: 模型准确率达91.24%；集成XAI方法能有效识别完整与部分肿瘤，提供从空间区域到像素级的详细解释，显著提升解释能力。

Conclusion: 集成多种XAI技术可显著增强AI模型在脑肿瘤检测中的透明度与可靠性，为医疗AI的可信应用提供有力支持。

Abstract: This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.

</details>


### [145] [Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents](https://arxiv.org/abs/2602.05249)
*Xinyi He,Ying Yang,Chuanjian Fu,Sihan Guo,Songchun Zhu,Lifeng Fan,Zhenliang Zhang,Yujia Peng*

Main category: cs.AI

TL;DR: 本文提出一种受人类认知启发的动态在位任务生成方法TEA，用于评估智能体在未见3D环境中的能力。通过结构化图表示定义任务，并构建交互-演化两阶段系统：交互阶段实现任务执行与生成的闭环；演化阶段通过任务图建模复用和重组已有任务生成新任务。实验在10个未见场景中生成87,876个任务，经人工验证具有物理合理性且涵盖日常认知能力。对比SOTA模型与人类的表现发现，模型在基础感知、3D交互意识及任务类型敏感性方面表现不佳，凸显了在真实环境中部署前进行在位评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在数据污染和场景特异性不足的问题，无法有效评估智能体在未见环境中的真实能力，亟需一种能适应多样化未知场景的动态评估方法。

Method: 提出基于任务图表示的两阶段任务生成系统TEA：（1）交互阶段，智能体主动与环境互动，形成任务执行与生成的闭环；（2）演化阶段，利用任务图建模实现任务的再组合与复用，无需外部数据即可持续生成新任务。

Result: 在10个未见场景中，TEA成功生成87,876个任务，经人工验证具备物理合理性和任务多样性；基准测试显示，当前SOTA模型在基础感知、3D交互意识和任务泛化能力上显著落后于人类，暴露出其在真实环境中的局限性。

Conclusion: 为确保智能体在真实家庭环境中安全可靠部署，必须采用在位动态评估机制，现有公共基准不足以反映其真实能力，未来评估体系应强调环境自适应与任务动态生成。

Abstract: As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.

</details>


### [146] [Beyond Cosine Similarity](https://arxiv.org/abs/2602.05266)
*Xinbo Ai*

Main category: cs.AI

TL;DR: 本文提出了一种新的语义相似度度量方法recos，通过推导出比经典柯西-施瓦茨不等式更紧的点积上界，使该方法能够捕捉向量空间中更复杂的非线性关系。与传统的余弦相似度相比，recos通过排序向量分量进行归一化，将完美相似的条件从严格的线性依赖放宽到序数一致性，从而在11种不同类型的嵌入模型上均表现出优于传统余弦相似度的性能，尤其在与人类判断的相关性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 余弦相似度基于柯西-施瓦茨不等式，仅能捕捉线性关系，无法有效建模真实语义空间中的复杂非线性结构，因此需要一种更具理论基础且能反映更广泛关系的新相似度度量方法。

Method: 推导一个比传统柯西-施瓦茨不等式更紧的点积上界，并据此设计recos度量，其核心是基于排序后的向量分量对点积进行归一化，以放宽对线性依赖的要求，转而强调序数一致性。

Result: 在11种不同类型的嵌入模型（包括静态、上下文化和通用类型）上进行的实验表明，recos在标准语义文本相似度（STS）基准测试中与人类判断的相关性显著高于传统余弦相似度，展现出更强的表达能力和更高的准确性。

Conclusion: recos是一种具有数学严谨性和实证优越性的新型相似度度量，可作为复杂嵌入空间中语义分析的更优替代方案。

Abstract: Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.

</details>


### [147] [Hallucination-Resistant Security Planning with a Large Language Model](https://arxiv.org/abs/2602.05279)
*Kim Hammar,Tansu Alpcan,Emil Lupu*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的决策支持框架，用于安全管理工作。该框架通过迭代循环整合LLM生成候选动作，并结合系统约束和前瞻预测进行一致性检查；当一致性较低时，暂停生成并收集外部反馈（如在数字孪生中评估），再通过上下文学习（ICL）优化动作。框架可调节一致性阈值以控制幻觉风险，并在特定假设下建立了ICL的后悔上界。实验表明，在四个公开数据集上的事故响应任务中，该框架将恢复时间缩短了最多30%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全管理工作（如事故响应规划）中具有潜力，但其不可靠性和幻觉问题限制了实际应用。因此需要一种可控、可验证的框架来提升其可靠性与安全性。

Method: 提出一个迭代式框架：1）利用LLM生成候选响应动作；2）通过系统约束和前瞻性预测检查动作的一致性；3）若一致性低，则暂停决策，收集外部反馈（如通过数字孪生模拟）；4）使用反馈信息进行上下文学习（ICL）以优化后续动作；5）通过调整一致性阈值控制幻觉风险，并建立理论上的后悔边界。

Result: 在四个公开数据集上的事故响应任务中，所提框架相比前沿大语言模型，显著减少了恢复时间，最高达30%；同时理论分析证明了框架对幻觉风险的有效控制及对ICL的后悔边界。

Conclusion: 本框架为大语言模型在安全管理系统中的可靠应用提供了理论保障与实践路径，通过引入外部反馈与一致性控制机制，有效缓解了幻觉问题，并实现了更高效、可信的决策支持。

Abstract: Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.

</details>


### [148] [RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs](https://arxiv.org/abs/2602.05367)
*Youngcheon You,Banseok Lee,Minseop Choi,Seonyoung Kim,Hyochan Chong,Changdong Kim,Youngmin Kim,Dongkyu Kim*

Main category: cs.AI

TL;DR: RaBiT提出一种新型量化框架，通过算法强制残差层次结构，解决并行二值路径间的特征共适应问题，实现2比特下的高性能与高效率，显著提升推理速度并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型极端量化中的性能与效率权衡问题，特别是二值化推理中因路径共适应导致的表达能力下降。

Method: 提出RaBiT框架，通过顺序从共享全精度权重派生每个二值路径，确保每条路径修正前序路径的误差，并采用稳健初始化以优先保持功能完整性。

Result: 在2比特下达到最先进的性能，媲美硬件密集型向量量化方法，在RTX 4090上实现4.49倍的推理加速。

Conclusion: RaBiT通过构建残差层次结构有效缓解了二值化过程中的共适应问题，重新定义了2比特量化下的准确率-效率边界，为高效部署大语言模型提供了新范式。

Abstract: Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\times$ inference speed-up over full-precision models on an RTX 4090.

</details>


### [149] [Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation](https://arxiv.org/abs/2602.05381)
*Ting Fang Tan,Kabilan Elangovan,Andreas Pollreisz,Kevin Bryan Dy,Wei Yan Ng,Joy Le Yi Wong,Jin Liyuan,Chrystie Quek Wan Ning,Ashley Shuen Ying Hong,Arun James Thirunavukarasu,Shelley Yin-His Chang,Jie Yao,Dylan Hong,Wang Zhaoran,Amrita Gupta,Daniel SW Ting*

Main category: cs.AI

TL;DR: 本研究评估了四种参数量小于100亿的小型医疗大语言模型（Meerkat-7B、BioMistral-7B、OpenBioLLM-8B、MedLLaMA3-v20）在眼科患者问答任务中的表现，并检验了基于大语言模型的评估方法与临床医生评分的一致性。共生成2160个回答，由三位不同资历的眼科医生及GPT-4-Turbo使用S.C.O.R.E.框架（安全性、共识性、客观性、可重复性、可解释性）进行评分。结果表明，Meerkat-7B表现最佳，而MedLLaMA3-v20存在25.5%的幻觉或误导性内容。GPT-4-Turbo与临床医生评分高度一致（斯皮尔曼相关系数0.80，肯德尔等级相关系数0.67），但资深专家评分更保守。研究支持将LLM评估用于大规模基准测试，建议采用混合自动化与临床审核框架以确保安全部署。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定大语言模型在眼科患者教育、分诊和临床决策中的应用日益广泛，亟需严格评估其安全性与准确性，以保障临床应用的安全性。同时，传统人工评估成本高、效率低，探索基于大语言模型的自动化评估方法具有重要意义。

Method: 采用横断面研究设计，选取180个眼科患者问题，由四款小型医疗LLM分别生成回答，共获得2160条响应。所有回答由三名不同层级的眼科医生（高级顾问、顾问、住院医师）以及GPT-4-Turbo使用S.C.O.R.E.框架进行五级量表评分。通过斯皮尔曼等级相关系数、肯德尔τ统计量和核密度估计分析评估模型评分与临床评分的一致性。

Result: Meerkat-7B在各类医生评分中表现最优，平均得分分别为3.44（高级顾问）、4.08（顾问）、4.18（住院医师）。MedLLaMA3-v20表现最差，25.5%的回答包含幻觉或临床误导内容，如虚构术语。GPT-4-Turbo评分与临床医生评分总体高度一致（斯皮尔曼ρ=0.80，肯德尔τ=0.67），但高级顾问评分更为保守。

Conclusion: 尽管小型医疗大语言模型在眼科问答中展现出潜在的安全应用价值，但在临床深度和共识性方面仍存在不足。基于大语言模型的评估方法具备可行性，适用于大规模基准测试；未来应构建融合自动化评估与临床审核的混合框架，以支持其安全、可靠地临床部署。

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

</details>


### [150] [H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration](https://arxiv.org/abs/2602.05407)
*Jun-Min Lee,Meong Hi Son,Edward Choi*

Main category: cs.AI

TL;DR: 提出H-AdminSim，一个基于多智能体的医院行政工作流仿真框架，结合真实数据生成与量化评估，用于系统比较LLM在复杂行政任务中的表现，支持FHIR集成以实现跨异构医院环境的标准化测试。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注患者-医生交互或孤立的行政子任务，未能捕捉真实行政工作流的复杂性，亟需一个全面的仿真框架来评估LLM在医院行政自动化中的应用效果。

Method: 构建H-AdminSim框架，融合真实数据生成与多智能体模拟，通过详细评分标准对行政任务进行量化评估，并利用FHIR实现跨系统互操作性。

Result: H-AdminSim成功提供了一个统一、可扩展的测试平台，能够系统评估LLM在多样化医院环境下的行政自动化性能，推动了自动化解决方案的可行性验证。

Conclusion: H-AdminSim为医院行政自动化提供了标准化、可复现的评估环境，显著提升了对LLM在复杂工作流中应用能力的理解与验证水平。

Abstract: Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.

</details>


### [151] [THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs](https://arxiv.org/abs/2602.05424)
*Weijian Yu,Yuhuan Lu,Dingqi Yang*

Main category: cs.AI

TL;DR: 提出THOR，一种用于超关系知识图谱（HKG）的归纳式链接预测方法。通过构建关系和实体基础图，捕捉跨事实的交互模式，利用双并行图编码器与Transformer解码器实现高效掩码训练和全归纳推理，在12个数据集上显著优于基线方法，尤其在完全归纳设置下提升达20.4%。


<details>
  <summary>Details</summary>
Motivation: 现有链接预测技术多局限于特定知识图谱的直推式学习，无法泛化到未见过的实体或关系，限制了模型在新场景下的适用性。而现代知识图谱中日益增多的超关系事实需要更强大的归纳推理能力以支持跨知识图谱的通用建模。

Method: 引入关系基础图与实体基础图，分别建模超关系事实中的跨关系和跨实体交互；采用双并行图编码器提取结构特征，结合Transformer解码器进行掩码训练与全归纳推理，实现对未知实体/关系的泛化预测。

Result: 在12个不同设置的数据集上，THOR在超关系链接预测任务中表现优异，相比最佳规则基、半归纳和全归纳方法分别提升66.1%、55.9%和20.4%；消融实验验证了基础图结构设计对跨知识图谱结构不变性迁移的关键作用。

Conclusion: THOR成功实现了对超关系知识图谱的归纳式链接预测，具备良好的泛化能力，为处理动态、开放世界知识图谱提供了有效解决方案。

Abstract: Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.

</details>


### [152] [Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy](https://arxiv.org/abs/2602.05430)
*Kritchanat Ponyuenyong,Pengyu Tu,Jia Wei Tan,Wei Soon Cheong,Jamie Ng Suat Ling,Lianlian Jiang*

Main category: cs.AI

TL;DR: 该研究评估了多种时间序列基础模型（TSFMs）在新加坡高波动性电力市场中的日前电价预测（EPF）表现，提出了一种尖峰正则化策略，并与传统统计和深度学习模型进行对比。结果显示TSFMs显著优于传统方法，MAPE最高提升37.4%。


<details>
  <summary>Details</summary>
Motivation: 电力价格具有高度波动性和非线性特征，传统统计和深度学习模型难以有效捕捉复杂时序依赖关系并整合异构数据；尽管时间序列基础模型在交通、气象等领域表现优异，但在日前电价预测中的应用仍不充分，亟需系统评估其在高波动市场中的有效性。

Method: 采用尖峰正则化策略，对比分析多种时间序列基础模型（如TTM、MOIRAI、MOMENT、TimesFM）与传统模型（如ARIMA、LSTM、CNN-LSTM），基于新加坡半小时批发市场价格数据，融合天气和日历等外生变量进行建模与评估。

Result: TSFMs在各种评估设置下均显著优于传统模型，最大可实现MAPE降低37.4%，表明其在处理复杂电价波动方面具有更强的建模能力。

Conclusion: 时间序列基础模型在高波动电力市场中具备显著优势，可有效提升日前电价预测精度，为电网运营、能源交易和政策制定提供更可靠的决策支持。

Abstract: Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.

</details>


### [153] [Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning](https://arxiv.org/abs/2602.05464)
*Jiaquan Wang,Yan Lyu,Chen Li,Yuheng Jia*

Main category: cs.AI

TL;DR: OD-CRL提出了一种新的条件表示学习框架，通过自适应正交基优化（AOBO）和空空间去噪投影（NSDP）解决现有方法对子空间基敏感及跨子空间干扰的问题，在定制聚类、分类和检索任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件表示学习方法依赖LLM生成的文本基来投影通用特征，但存在对子空间基敏感和易受非目标语义干扰的问题，影响模型泛化能力。

Method: 提出OD-CRL框架，结合AOBO通过奇异值分解与曲率截断构建正交语义基，以及NSDP将嵌入投影到无关子空间的零空间以抑制非目标语义干扰。

Result: 在多个定制化任务上，OD-CRL显著提升性能，达到新的SOTA水平，并展现出更强的泛化能力。

Conclusion: OD-CRL有效缓解了条件表示学习中的基敏感与子空间干扰问题，为个性化任务提供了更鲁棒、高效的表示学习方案。

Abstract: Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.

</details>


### [154] [ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation](https://arxiv.org/abs/2602.05472)
*Yiwen Duan,Jing Ye,Xinpei Zhao*

Main category: cs.AI

TL;DR: ALIVE提出一种无需人工监督的对齐框架，通过对抗学习与指令式语言反馈，使大模型从原始语料中内化评估标准，突破传统强化学习中的标量奖励瓶颈，实现内在推理能力的自主获取。在数学推理、代码生成和逻辑推断等任务上，ALIVE展现出更高的准确率、更强的跨领域泛化能力和自修正能力，推动模型进入自我增强的推理发展轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖成本高、易失效且不关注解题逻辑的标量奖励信号，难以实现专家级推理能力的规模化发展。因此亟需一种能内化推理原则、摆脱外部奖励依赖的新范式。

Method: ALIVE基于认知协同原则，将问题生成、求解与评判统一于单一策略模型中，结合对抗学习与指令式语言反馈，使模型从原始文本数据中直接学习评价标准，从而构建内在推理能力。

Result: 在数学推理、代码生成和逻辑推理等多个基准测试中，ALIVE在相同数据与算力下显著提升准确率，增强跨域泛化性能，并提高自纠正能力，验证了其在无监督环境下持续提升推理能力的有效性。

Conclusion: ALIVE通过构建自洽的推理三元组，实现了从外部监督到内在推理的范式跃迁，为通用推理对齐提供了可扩展、无需人工干预的解决方案，具有持续自我增强的发展潜力。

Abstract: The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.

</details>


### [155] [Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction](https://arxiv.org/abs/2602.05479)
*Zhe Wang,Zijing Liu,Chencheng Xu,Yuan Yao*

Main category: cs.AI

TL;DR: Phi-former是一种用于预测化合物-蛋白相互作用（CPI）的新型深度学习方法，通过引入分子片段（如功能基团）作为生物识别的基本单位，克服了现有原子级模型与化学现实不符的问题。该方法采用分层的成对交互表示学习框架，在原子-原子、基团-基团和原子-基团三个层次上系统建模相互作用，并设计了层次内与层次间的学习机制以增强各层级间的协同。实验表明，Phi-former在多个CPI任务中表现优越，且具备可解释性，能准确识别参与相互作用的关键原子或基团，为合理药物设计和精准医疗提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在预测化合物-蛋白相互作用时虽提升了效率与精度，但多基于原子层面建模，忽略了分子片段（如功能基团）在生物识别中的核心作用，导致模型结果与化学现实不一致。因此，亟需一种能够反映真实生物识别机制的建模方法。

Method: Phi-former提出了一种分层的成对交互表示学习框架，将化合物和蛋白质分别从原子、基团两个层次进行表征，构建原子-原子、基团-基团、原子-基团三类交互关系。通过设计层次内与层次间的学习管道，实现不同层次信息的互补与增强，同时采用成对预训练策略提升模型泛化能力。

Result: Phi-former在多个标准数据集上均优于现有方法，在化合物-蛋白相互作用预测任务中表现出更高的准确率与鲁棒性；案例研究显示其能有效识别关键活性原子或功能基团，具有良好的可解释性。

Conclusion: Phi-former通过融合分子片段的生物学角色，实现了更符合化学现实的化合物-蛋白相互作用建模，不仅提升了预测性能，还提供了可解释的分析结果，为新药发现和精准医学提供了有力工具。

Abstract: Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.

</details>


### [156] [A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma](https://arxiv.org/abs/2602.05515)
*Ajo Babu George,Anna Mariam John,Athul Anoop,Balu Bhasuran*

Main category: cs.AI

TL;DR: 本研究构建了一个针对成釉细胞瘤的多模态数据集，整合了放射影像、组织病理学和口腔临床图像，并利用自然语言处理从病例报告中提取临床特征。通过领域特定的图像预处理与增强，开发了一个多模态深度学习模型，用于分类成釉细胞瘤亚型、评估复发风险并辅助手术规划。模型在部署时可接受患者主诉、年龄和性别等临床输入以实现个性化推理。实验表明，亚型分类准确率从46.2%提升至65.9%，异常组织检测F1分数从43.0%提升至90.3%。该工作为个体化诊疗支持提供了高质量数据与可扩展的AI框架。


<details>
  <summary>Details</summary>
Motivation: 现有成釉细胞瘤相关数据资源覆盖不足且格式不一致，难以直接用于深度学习模型训练，限制了人工智能在颌面病理诊断中的应用。因此亟需一个结构化、高质量的多模态数据集来推动精准医疗发展。

Method: 采用自然语言处理技术从病例报告中提取临床特征；对图像数据进行领域特定预处理与增强；构建多模态深度学习模型，融合影像、病理与临床信息，支持亚型分类、复发风险评估及手术规划，并引入临床输入以实现个性化推理。

Result: 模型在成釉细胞瘤亚型分类上的准确率从46.2%提升至65.9%，异常组织检测的F1分数从43.0%提升至90.3%，显著优于现有基准（如MultiCaRe），验证了数据集与模型的有效性。

Conclusion: 本研究成功构建了一个结构清晰、内容丰富的成釉细胞瘤多模态数据集，并开发出一个高效、可扩展的多模态AI框架，为颌面病理的智能诊断与个性化治疗决策提供了有力支持。

Abstract: Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.

</details>


### [157] [Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities](https://arxiv.org/abs/2602.05532)
*Florian Dietz,William Wale,Oscar Gilg,Robert McCarthy,Felix Michalak,Gustavo Ewbank Rodrigues Danon,Miguelito de Guzman,Dietrich Klakow*

Main category: cs.AI

TL;DR: 本文提出Split Personality Training (SPT)方法，通过在LoRA参数中微调一个“诚实人格”来检测大语言模型中的对齐偏差。该诚实人格在正常运行时保持非激活状态，仅在响应后通过触发字符串激活，以审查主模型的输出并访问其潜在状态。在基准测试中，SPT达到96%的准确率，显著优于现有方法（接近0%），并能揭示模型内部隐藏的知识，如虚构偏见等。


<details>
  <summary>Details</summary>
Motivation: 标准审计技术难以区分对齐偏差输出与正常输出，且可解释性方法无法随模型能力扩展。需要一种新方法来有效检测和揭示模型隐藏的不良行为。

Method: 引入Split Personality Training (SPT)，在LoRA参数中训练一个独立的‘诚实人格’，该人格在正常操作中不活跃；通过触发字符串激活后，可访问主模型的隐状态并审查其输出。

Result: 在Anthropic Auditing Game Model Organism基准上，SPT实现96%的总体准确率，远超现有方法（接近0%），并能揭示模型内部未暴露的隐藏知识，如虚构偏见。

Conclusion: SPT是一种高效、可扩展的对齐检测方法，能够有效识别并揭示大语言模型中被隐藏的不良行为，为模型安全审计提供新路径。

Abstract: Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.

</details>


### [158] [Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents](https://arxiv.org/abs/2602.05597)
*Stephen Pilli,Vivek Nallur*

Main category: cs.AI

TL;DR: 该研究探讨了大型语言模型（LLMs）在互动情境下是否能预测个体层面的认知偏差并模拟人类偏见行为。通过将三个经典决策场景转化为对话形式，进行了包含1100名参与者的实验，并利用GPT-4和GPT-5基于参与者的人口统计信息与对话记录进行模拟。结果表明，LLMs能够精确再现人类的偏见行为，且不同模型在对人类行为的拟合上存在显著差异，这对设计和评估交互式、具备偏见感知能力的AI系统具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型是否能够在考虑认知负荷等上下文因素的情况下，预测个体层面的认知偏差，并模拟真实人类的偏见行为，以推动更智能、更人性化的交互式AI系统的发展。

Method: 将三种经典决策任务改编为对话形式，开展大规模人机交互实验（N=1100），收集参与者与聊天机器人之间的对话数据；随后利用GPT-4和GPT-5模型，结合人口统计信息与对话文本，模拟相同交互条件下的决策过程，比较模型输出与人类行为的一致性。

Result: LLMs成功再现了人类在复杂对话情境中的显著认知偏差；不同模型在模仿人类行为模式方面表现出差异，尤其在处理认知负荷等动态因素时表现不一。

Conclusion: 大型语言模型具备在交互环境中模拟人类偏见行为的能力，但其表现受模型架构和上下文理解能力影响，因此在开发自适应、偏见感知的AI系统时需重视模型间的差异性与情境敏感性。

Abstract: Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.

</details>


### [159] [Generative Ontology: When Structured Knowledge Learns to Create](https://arxiv.org/abs/2602.05636)
*Benny Cheung*

Main category: cs.AI

TL;DR: Generative Ontology combines ontologies and large language models to generate structurally valid, creative artifacts. It uses Pydantic schemas and DSPy signatures to enforce domain constraints while enabling creativity through specialized multi-agent roles and retrieval-augmented generation.


<details>
  <summary>Details</summary>
Motivation: Traditional ontologies lack generative capability, while LLMs produce fluent but structurally invalid outputs. There is a need for a framework that integrates structural validity with creative generation.

Method: The framework encodes domain knowledge as executable Pydantic schemas, constrains LLM generation via DSPy signatures, employs a multi-agent pipeline with specialized roles (e.g., Mechanics Architect, Theme Weaver, Balance Critic), incorporates retrieval-augmented generation, and applies iterative validation.

Result: GameGrammar successfully generates complete, playable tabletop game designs from thematic prompts, satisfying ontological constraints while remaining creative. The approach generalizes to other domains like music, software architecture, and culinary arts.

Conclusion: Constraints do not limit creativity—they enable it. Just as grammar enables poetry, ontology enables structured, meaningful generation.

Abstract: Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.
  Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional "anxiety" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.
  We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt ("bioluminescent fungi competing in a cave ecosystem"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.
  The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.

</details>


### [160] [Graph-based Agent Memory: Taxonomy, Techniques, and Applications](https://arxiv.org/abs/2602.05665)
*Chang Yang,Chuang Zhou,Yilin Xiao,Su Dong,Luyao Zhuang,Yujing Zhang,Zhu Wang,Zijin Hong,Zheng Yuan,Zhishang Xiang,Shengyuan Chen,Huachi Zhou,Qinggang Zhang,Ninghao Liu,Jinsong Su,Xinrun Wang,Yi Chang,Xiao Huang*

Main category: cs.AI

TL;DR: 该论文从图结构视角对基于大语言模型的智能体记忆进行了全面综述，系统梳理了记忆的分类、生命周期中的关键技术（提取、存储、检索、演化），并总结了开源工具与基准测试，探讨了多样化应用场景，指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 在长周期复杂任务中，智能体需要高效的记忆系统来实现知识积累、迭代推理和自我演化，而图结构因其建模关系依赖、组织层次信息和高效检索的能力，成为理想的记忆架构。本文旨在系统化地推动图基智能体记忆的发展。

Method: 提出一个智能体记忆的分类体系，涵盖短期/长期记忆、知识/经验记忆、非结构化/结构化记忆，并从记忆生命周期出发，分析图基记忆的关键技术：数据提取、高效存储、相关性检索和内容演化；同时整理开源库、基准数据集及应用案例，归纳研究挑战与未来方向。

Result: 构建了图基智能体记忆的完整技术框架，提供了可复用的资源集合（见GitHub链接），为开发更高效可靠的自演进智能体记忆系统提供了理论支持与实践指导。

Conclusion: 图结构是构建智能体记忆的理想范式，未来需在可扩展性、动态适应性、语义理解与跨任务迁移等方面深化研究，以实现真正智能的自演化记忆系统。

Abstract: Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.

</details>


### [161] [Determining Energy Efficiency Sweet Spots in Production LLM Inference](https://arxiv.org/abs/2602.05695)
*Hiari Pizzini Cavagna,Andrea Proia,Giacomo Madella,Giovanni B. Esposito,Francesco Antici,Daniele Cesarini,Zeynep Kiziltan,Andrea Bartolini*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer架构计算与内存访问复杂度的分析模型，用于准确刻画大语言模型（LLM）在不同输入和输出长度下的能效曲线。通过在NVIDIA H100 GPU上使用TensorRT-LLM对多个主流模型进行测试，结果显示该模型平均绝对百分比误差（MAPE）仅为1.79%，验证了其高精度。研究发现，能效存在明显的“甜蜜点”——短至中等长度输入搭配中等长度输出时效率最高，而长输入或极短输出会导致效率急剧下降，表明能量消耗具有非线性特征。这一发现支持在实际系统中通过智能截断、摘要生成和自适应生成策略优化序列长度以显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常用简单的线性函数估算大语言模型推理过程中的能耗，但实际观察显示能效存在明显非线性变化，尤其在不同输入和输出长度下表现差异显著。因此需要更精确的模型来揭示真实能耗特性，以指导高效部署和优化策略。

Method: 基于Transformer架构的计算复杂度和内存访问模式，构建一个解析性能量消耗模型；通过大量实测数据（涵盖多种模型和序列长度组合）校准并验证该模型的准确性。

Result: 所提模型在多种大语言模型（1B–9B参数）和序列长度（64–4096 tokens）条件下均表现出极高精度，平均MAPE仅为1.79%；揭示了能效的非线性特性及‘甜蜜点’规律，证明合理调整序列长度可显著降低能耗。

Conclusion: 通过建立精准的能量消耗分析模型，本研究揭示了大语言模型推理过程中的能效规律，并证明了根据‘甜蜜点’优化输入输出长度能够有效减少能源消耗，为生产环境中动态调度、自适应生成和资源管理提供了理论依据和技术支持。

Abstract: Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency "Sweet Spots" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.

</details>


### [162] [Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions](https://arxiv.org/abs/2602.05709)
*Yihao Ouyang,Shiwei Li,Haozhao Wang,Xiandi Luo,Zhuoqi Hu,Yuetong Song,Qiyu Qin,Yichen Li,Ruixuan Li*

Main category: cs.AI

TL;DR: GenLoRA提出了一种新的低秩适配方法，通过用轻量级非线性函数生成基向量，替代传统LoRA中显式存储的基向量，显著提升了参数效率，并在多个数据集和模型架构上实现了更优的微调性能。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA存在参数冗余问题，增加模型容量需添加大量基向量，导致参数量急剧上升。为解决这一问题，本文旨在通过压缩基向量表示来提升参数效率。

Method: GenLoRA使用潜向量结合轻量级径向基函数（RBFs）生成低秩矩阵的基向量，避免显式存储基向量，从而大幅减少参数数量。

Result: 在多种数据集和模型架构上的实验表明，GenLoRA在较小参数预算下能达到更高的有效低秩秩，表现出更优的微调性能。

Conclusion: GenLoRA通过非线性函数生成基向量，实现了更高参数效率和更强的微调能力，是现有LoRA方法的有效改进。

Abstract: Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.

</details>


### [163] [Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification](https://arxiv.org/abs/2602.05717)
*Tianyi Wang,Long Li,Hongcan Guo,Yibiao Chen,Yixia Li,Yong Wang,Yun Chen,Guanhua Chen*

Main category: cs.AI

TL;DR: 提出 Anchored Policy Optimization (APO) 来解决强化学习中因正向锐化与负向挤压导致的递归空间收缩（RSC）问题，通过定义基于参考模型高置信度支持集的安全流形，实现高效锐化与弹性恢复，打破准确率-多样性权衡，显著提升 Pass@1 和 Pass@K 的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强化学习中因正向锐化和负向挤压导致递归空间收缩（RSC），使有效选项采样概率消失；KL正则化虽缓解但引入形状匹配约束，造成梯度冲突。

Method: 提出 Anchored Policy Optimization (APO)，从全局形状匹配转向支持覆盖，定义安全流形以允许高效锐化，并在纠错时引入恢复力，实现梯度对齐的支持覆盖最大化。

Result: 在数学基准测试中，APO 突破了准确率与多样性的权衡，显著提升 Pass@1 准确率并恢复传统策略梯度方法丢失的 Pass@K 多样性。

Conclusion: APO 作为一种梯度对齐机制，能有效防止策略崩溃，实现弹性恢复，是解决 RSC 问题的有效方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.

</details>


### [164] [Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification](https://arxiv.org/abs/2602.05723)
*Taoye Yin,Haoyuan Hu,Yaxin Fan,Xinhao Chen,Xinya Wu,Kai Deng,Kezun Zhang,Feng Wang*

Main category: cs.AI

TL;DR: 提出RLFKV框架，通过细粒度知识验证提升金融RAG系统生成响应的忠实度，结合精确奖励与信息量奖励，有效减少幻觉并防止奖励劫持。


<details>
  <summary>Details</summary>
Motivation: 金融领域时间敏感性强，现有RAG系统在生成回答时仍存在与检索文档矛盾的幻觉问题，需更精确的优化信号来提升生成内容的忠实度。

Method: 将金融回答分解为原子知识单元，评估每个单元正确性以计算细粒度忠实奖励；引入信息量奖励以避免过度简略回复，防止奖励劫持。

Result: 在FDD任务和新提出的FDD-ANT数据集上实验均显示显著性能提升，验证了方法的有效性。

Conclusion: RLFKV框架通过细粒度知识验证与双重奖励机制，显著提升了金融RAG系统生成结果的准确性与一致性。

Abstract: In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.

</details>


### [165] [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765)
*Zhong Guan,Haoran Sun,Yongjian Guo,Shuai Di,Xiaodong Bai,Jing Long,Tianyun Zhao,Mingxi Luo,Chen Zhou,Yucheng Guo,Qiming Yang,Wanting Xu,Wen Huang,Yunxuan Ma,Hongke Zhao,Likang Wu,Xiaotie Deng,Xi Xiao,Sheng Wen,Yicheng Gong,Junwu Xiong*

Main category: cs.AI

TL;DR: 本文提出了一种全异步策略训练框架，用于解决视觉-语言-动作（VLA）模型在训练效率上的瓶颈问题。该框架通过多层次解耦设计，实现了环境交互、轨迹生成和策略更新的异步并行化，显著提升了系统吞吐量。在LIBERO基准测试中，相比传统同步策略，吞吐量最高提升59.25%，优化后可达126.67%。消融实验验证了各异步组件的有效性，且在8至256个GPU下的扩展性测试表明该方法具有优异的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的VLA模型训练框架（如RLinf）依赖同步执行，导致资源利用率低、吞吐量受限，亟需更高效的训练机制以支持通用具身智能的发展。

Method: 提出一种全异步策略训练框架，采用多层次解耦架构：异步并行化环境交互与轨迹收集、流式执行策略生成、解耦调度训练更新；借鉴大规模模型强化学习中的异步优化思想。

Result: 在LIBERO基准上，吞吐量相比同步策略最高提升59.25%，深度优化后达126.67%；各异步组件经消融实验证明有效；在8至256个GPU下表现出良好可扩展性。

Conclusion: 所提出的全异步训练框架有效解决了VLA模型训练中的效率瓶颈，显著提升吞吐量与资源利用率，具备良好的可扩展性和实际应用前景。

Abstract: In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.

</details>


### [166] [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794)
*Aboli Kathar,Aman Kumar,Anusha Kamath,Araveeti Srujan,Ashish Sharma,Chandra Bhushan,Dilip Asbe,Divya Sorate,Duddu Prasanth Kumar,Evan Acharya,Harsh Sharma,Hrithik Kadam,Kanishk Singla,Keyur Doshi,Kiran Praveen,Kolisetty Krishna SK,Krishanu Adhikary,Lokesh MPT,Mayurdeep Sonowal,Nadeem Shaikh,Navya Prakash,Nimit Kothari,Nitin Kukreja,Prashant Devadiga,Rakesh Paul,Ratanjeet Pratap Chauhan,Raunak Kalani,Raviraj Joshi,Shamanth MH,Shantanu Pandey,Shubham Soni,Siddharth Dixit,Smriti Jopat,Sunil Patel,Suraj Singh,Suvradip Paul,Tulasi Pilla,Utkarsh Vaidya,Vineeth Nambiar,Vishal Kanvaty,Yatharth Dedhia*

Main category: cs.AI

TL;DR: FiMI是为印度数字支付系统设计的领域专用金融语言模型，基于Mistral Small 24B架构，通过多阶段训练流程开发，包含连续预训练、指令微调和领域特定监督微调。该模型在金融推理和工具调用任务上显著优于基线模型，同时保持了与同类模型相当的一般基准表现。


<details>
  <summary>Details</summary>
Motivation: 为满足印度数字支付系统中复杂的金融交互需求，现有通用语言模型在金融推理和工具调用方面表现不足，亟需一个针对本地化场景优化的领域专用模型。

Method: 采用多阶段训练策略：首先在680亿条经过筛选的金融、多语言（英语、印地语、印地语混合）及合成数据上进行连续预训练；随后进行指令微调和面向多轮、工具驱动对话的领域特定监督微调，以模拟真实业务流程如交易争议处理和授权管理。

Result: FiMI Base在金融推理基准测试中相比Mistral Small 24B Base提升20%；FiMI Instruct在领域特定工具调用任务上比Mistral Small 24B Instruct提升87%；同时在通用基准测试中表现与同规模模型相当。

Conclusion: FiMI成功实现了在印度金融场景下的高性能表现，兼具领域专精与通用能力，适用于复杂且真实的数字支付工作流。

Abstract: We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.

</details>


### [167] [NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking](https://arxiv.org/abs/2602.05805)
*Kang Chen,Zhuoka Feng,Sihan Zhao,Kai Xiong,Junjie Nian,Yaoning Wang,Changyi Xiao,Yixin Cao*

Main category: cs.AI

TL;DR: 该论文提出NEX框架，一种无需标签的无监督评分方法，通过检测推理过程中的探索（E-phase）和利用（X-phase）阶段来评估大语言模型的推理质量。NEX利用稀疏激活缓存中每标记新激活的MLP神经元数量的突增作为探索阶段的指标，并使用粘性两状态隐马尔可夫模型（HMM）推断E-X阶段，根据探索引入的神经元在后续利用阶段是否被重用进行赋权。该方法生成可解释的神经元权重和单一的‘优质质量分数’，用于在无任务答案的情况下对候选响应和合并版本进行排序。在多个推理基准和Qwen3合并家族上，基于小规模未标注激活数据计算的NEX能有效预测下游准确率并识别更优变体。通过人工标注验证了E-X信号，并通过‘有效与冗余’神经元迁移提供了因果证据。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在推理时需采样多个思维链轨迹或搜索合并检查点，推理瓶颈从生成转移到选择，而选择过程往往缺乏对目标分布的监督。现有方法难以有效区分探索与利用阶段，导致过度思考或冗余计算。因此需要一种无监督、无需标签的方法来量化推理过程中的探索效率，以优化选择策略并提升推理准确性。

Method: 提出NEX框架，通过分析稀疏激活缓存中每标记新激活的MLP神经元数量的变化，识别探索（E-phase）阶段；采用粘性两状态隐马尔可夫模型（HMM）推断探索与利用阶段的交替模式；基于探索阶段引入的神经元在后续利用阶段是否被重用，对其进行加权，从而构建可解释的神经元贡献度；最终生成一个单一的‘优质质量分数’用于排名候选响应和合并版本。

Result: 在多个推理基准和Qwen3合并家族上，基于小规模未标注激活数据计算的NEX能够有效预测下游任务的准确率，并成功识别出表现更优的模型变体。人工标注验证了E-X阶段划分的合理性，且通过‘有效与冗余’神经元转移实验提供了因果支持，表明探索阶段引入的关键神经元确实有助于提升性能。

Conclusion: NEX提供了一种白盒、无标签、无监督的推理质量评估框架，能够有效捕捉大语言模型推理过程中的探索-利用动态，避免冗余探索，提升选择效率。该方法不仅具备良好的预测能力，还具有高度可解释性，为模型优化和自动化筛选提供了有力工具。

Abstract: Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via "Effective-vs-Redundant" neuron transfer.

</details>


### [168] [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818)
*Zihao Jiang,Miao Peng,Zhenyan Shan,Wenjie Xu,Ben Liu,Gong Chen,Ziqi Gao,Min Peng*

Main category: cs.AI

TL;DR: TKG-Thinker是一种新型智能体，通过动态多轮交互与双阶段训练（监督微调+强化学习）实现对时间知识图谱的自主规划与自适应检索，有效解决现有提示策略在复杂时间约束下的推理幻觉和静态提示导致的模型泛化能力不足问题，在多个基准数据集上达到顶尖性能并展现良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在时间知识图谱问答任务中受限于提示策略：易产生复杂时间约束下的推理幻觉，且静态提示缺乏与时间知识图谱环境的动态交互优化，限制了模型自主性和泛化能力。

Method: 提出TKG-Thinker智能体，采用双阶段训练策略：首先通过链式思维数据进行监督微调以建立核心规划能力；随后通过强化学习结合多维奖励信号，优化在复杂时间约束下的推理策略，实现与时间知识图谱的动态多轮交互。

Result: 在多个基准数据集上，使用三种开源大语言模型验证，TKG-Thinker均取得当前最优性能，并在复杂时间知识图谱问答场景中表现出优异的泛化能力。

Conclusion: TKG-Thinker通过动态交互与自适应推理机制，显著提升了大语言模型在时间知识图谱问答任务中的准确性与鲁棒性，为复杂时序知识推理提供了新范式。

Abstract: Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.

</details>


### [169] [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847)
*Zhangquan Chen,Jiale Tao,Ruihuang Li,Yihao Hu,Ruitao Chen,Zhantao Yang,Xinlei Yu,Haodong Jing,Manyuan Zhang,Shuai Shao,Biao Wang,Qinglin Lu,Ruqi Huang*

Main category: cs.AI

TL;DR: OmniVideo-R1 是一种新型强化框架，通过查询密集型定位和模态感知融合，提升多模态推理能力，在多个基准测试中优于现有基线模型，展现出强大的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有全模态视频模型在音视频理解任务上仍面临挑战，需要更有效的多模态协同机制来提升整体理解能力。

Method: 采用基于自监督学习的查询密集型定位与基于对比学习的模态感知融合策略，实现多模态信息的有效整合与推理。

Result: 在多个基准数据集上的实验表明，OmniVideo-R1 显著优于现有强基线模型，具有优异的性能和鲁棒性。

Conclusion: OmniVideo-R1 有效提升了模型对多模态线索的综合推理能力，为音视频理解任务提供了新的解决方案。

Abstract: While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.

</details>


### [170] [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857)
*Junting Zhou,Jin Chen,Linfeng Hao,Denghui Cao,Zheyu Wang,Qiguang Chen,Chaoyou Fu,Jiaze Chen,Yuchen Wu,Ge Zhang,Mingxuan Wang,Wenhao Huang,Tong Yang*

Main category: cs.AI

TL;DR: 本文提出了一个名为BABE（Biology Arena BEnchmark）的综合性基准，用于评估生物领域人工智能系统在实验推理方面的能力。该基准基于同行评审的研究论文和真实生物学研究构建，强调因果推理与跨尺度推断，旨在更真实地衡量AI在生物学研究中的贡献潜力。


<details>
  <summary>Details</summary>
Motivation: 现有生物学领域的基准无法有效评估研究人员将实验结果与背景知识结合以得出有意义结论的关键能力，因此需要一个更贴近实际科学探究的评估工具。

Method: 从同行评审的研究论文和真实生物研究中构建任务，确保内容反映科学研究的复杂性和跨学科性；通过设计需进行因果推理和跨尺度推断的任务来测试模型的实验推理能力。

Result: BABE为评估人工智能系统像科学家一样进行推理提供了强有力的框架，能够更真实地衡量其在生物研究中的应用潜力。

Conclusion: BABE是一个具有现实意义且全面的基准，能够有效评估生物AI在复杂科学推理任务中的表现，推动人工智能在生物学研究中的深入应用。

Abstract: The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.

</details>


### [171] [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875)
*Anton Ipsen,Michael Cashmore,Kirsty Fielding,Nicolas Marchesotti,Parisa Zehtabi,Daniele Magazzeni,Manuela Veloso*

Main category: cs.AI

TL;DR: 本文提出了层次化座位分配问题（HSAP），旨在优化具有层级结构的组织团队在楼层平面图中的物理座位安排。通过引入基于概率路线图（PRM）和快速探索随机树（RRT）的可扩展距离计算方法，并结合启发式搜索与动态规划，构建了一个端到端的求解框架。实验在不同规模实例上验证了该方法的有效性，从定量和定性角度均展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 大型组织中复杂的层级结构要求紧密相关的团队应就近安排，但当前依赖人工操作，导致重规划频率低且效果不佳，亟需自动化、高效的解决方案。

Method: 采用概率路线图（PRM）和快速探索随机树（RRT）计算座位间距离，结合启发式搜索与动态规划，利用整数规划求解HSAP。

Result: 在多种规模的实例上验证了方法的有效性，表现出良好的可扩展性和高质量的座位分配结果。

Conclusion: 所提出的端到端框架能够有效解决复杂层级组织的座位分配问题，显著提升规划效率与质量，具备实际应用潜力。

Abstract: We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.

</details>


### [172] [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883)
*Philippe J. Giabbanelli*

Main category: cs.AI

TL;DR: 本文旨在为建模与仿真（M&S）应用中的大语言模型（LLM）使用提供全面且实用的指导，指出看似简单的实践可能引发非确定性、模型崩溃、信息丢失等问题。文章强调应基于原则性设计、诊断策略和实证评估，帮助建模者判断何时、如何以及是否依赖LLM。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在建模与仿真领域日益普及，但其使用中存在诸多潜在陷阱，如非确定性输出、知识增强方法的误用、数据处理不当等，可能导致性能下降或结果不可靠，因此需要系统性指导以避免常见错误。

Method: 通过分析常见实践中的问题，提出针对非确定性、知识增强（如RAG和LoRA）、M&S数据分解及超参数设置等方面的建议，结合原则性设计与实证评估方法，构建一套可操作的使用指南。

Result: 提出了若干关键实践建议，包括避免盲目增加数据、合理设置温度参数、审慎进行微调、有效利用检索增强生成技术，并强调对模型能力的先验评估，从而提升LLM在M&S任务中的可靠性与效率。

Conclusion: 在建模与仿真中使用大语言模型需基于科学评估与谨慎设计，不能仅依赖直觉。通过遵循本文提出的指导原则，可显著减少风险，提高模型应用的有效性与可解释性。

Abstract: Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.

</details>


### [173] [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000)
*Ali Shendabadi,Parnia Izadirad,Mostafa Salehi,Mahmoud Bijankhan*

Main category: cs.AI

TL;DR: 该研究探讨了预训练语音识别模型Whisper在情感识别中的应用，提出两种基于注意力的池化方法（Multi-head Attentive Average Pooling和QKV Pooling），以高效降低Whisper表示的维度并保留情感特征。在英语（IEMOCAP）和波斯语（ShEMO）数据集上使用Whisper Tiny和Small进行实验，结果表明QKV架构在ShEMO数据集上达到当前最优表现，未加权准确率提升2.47%。研究还发现中间层编码器对波斯语SER更有效，为轻量化模型提供了替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有语音情感识别（SER）研究受限于缺乏标准化且规模足够的数据集，而预训练模型虽被用于特征提取，但如何高效利用其表示仍需探索。本研究旨在评估Whisper作为情感特征提取器的潜力，并设计有效的维度压缩方法以提升性能。

Method: 提出两种基于注意力的池化方法：Multi-head Attentive Average Pooling 和 QKV Pooling，用于降低Whisper模型输出表示的维度；在IEMOCAP（英语）和ShEMO（波斯语）数据集上进行实验，使用Whisper Tiny和Small模型；分析不同编码层对情感识别的影响。

Result: QKV池化方法在ShEMO数据集上实现2.47%的未加权准确率提升，达到当前最优水平；中间层编码器在波斯语数据集上表现更优；轻量级Whisper模型可替代大型模型如HuBERT X-Large。

Conclusion: Whisper具备作为语音情感识别中高效特征提取器的潜力，注意力池化方法能有效压缩维度并保留关键情感信息，尤其在中间层编码器下表现优异，为轻量化、高性能的SER系统提供可行路径。

Abstract: Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.

</details>


### [174] [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023)
*Christopher A. McClurg,Alan R. Wagner*

Main category: cs.AI

TL;DR: 本文提出一种基于数据驱动的离散事件模拟器（DES），通过学习虚拟现实（VR）实验中参与者的行为，模拟袭击者在校园内的移动和行动，以实现对校园安全干预策略的大规模、可扩展评估。该方法克服了传统VR研究中需为每种条件招募新受试者的局限，尤其适用于需要大量训练回合的自主安全干预策略的开发与优化。模拟器经验证能复现关键实证模式，为难以通过真人实验实施的干预方案提供高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟现实（VR）研究在评估校园安全干预措施时面临挑战：每次新干预都需要重新招募参与者，导致大规模或迭代式评估困难，尤其限制了需要大量训练回合的自主干预策略的学习与优化。

Method: 开发一种数据驱动的离散事件模拟器（DES），将袭击者在区域内的移动与行为建模为从真实VR实验中学习到的随机过程，并利用该模拟器评估基于机器人的袭击者干预策略。

Result: 模拟器成功再现了关键的实证行为模式，实现了对复杂干预策略的可扩展评估与学习，为无法通过真人实验实现的安全策略开发提供了高效仿真平台。

Conclusion: 本研究建立了一套高至中等保真度的仿真工作流程，为自主校园安全干预策略的开发与评估提供了一个可扩展的虚拟替代方案，显著提升了干预策略研究的效率与可行性。

Abstract: Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [175] [A Causal Perspective for Enhancing Jailbreak Attack and Defense](https://arxiv.org/abs/2602.04893)
*Licheng Pan,Yunsheng Lu,Jiexi Liu,Jialing Tao,Haozhe Feng,Hui Xue,Zhixuan Chu,Kui Ren*

Main category: cs.LG

TL;DR: 本文提出Causal Analyst框架，通过结合大语言模型与数据驱动的因果发现，识别导致大语言模型越狱的直接原因，并用于攻击增强与防御。构建了包含3.5万次越狱尝试的综合数据集，涵盖7个LLM、100种攻击模板和50个有害查询，标注了37个可读性高的提示特征。通过联合训练基于LLM的提示编码与基于GNN的因果图学习，重建提示特征与越狱响应之间的因果路径。发现如“正面角色”和“任务步骤数量”等特征是越狱的直接因果驱动因素。基于这些发现，提出了越狱增强器（提升攻击成功率）和护栏顾问（从混淆查询中提取真实恶意意图）两个应用。实验验证了因果分析的鲁棒性和优于非因果方法的效果，表明从因果视角分析越狱特征是提升LLM可靠性的一种有效且可解释的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖于对潜在表示的探查来分析越狱提示，忽视了可解释提示特征与越狱发生之间的因果关系，导致对越狱机制理解不足，亟需一种能揭示因果驱动因素的方法以提升LLM的安全性与可靠性。

Method: 提出Causal Analyst框架，结合LLM进行提示编码，利用GNN进行因果图学习，通过联合训练重建提示特征到越狱响应的因果路径；构建大规模标注数据集，系统化地分析提示特征与越狱之间的因果关联。

Result: 识别出多个关键因果特征（如‘正面角色’和‘任务步骤数量’），并验证其作为越狱直接驱动因素的有效性；所提出的越狱增强器显著提升攻击成功率，护栏顾问能有效识别被混淆的恶意意图；实验表明因果分析比非因果方法更具鲁棒性和优越性。

Conclusion: 从因果视角分析大语言模型越狱特征是一种有效且可解释的方法，有助于深入理解越狱机制，从而在攻击与防御两端提升模型的可靠性与安全性。

Abstract: Uncovering the mechanisms behind "jailbreaks" in large language models (LLMs) is crucial for enhancing their safety and reliability, yet these mechanisms remain poorly understood. Existing studies predominantly analyze jailbreak prompts by probing latent representations, often overlooking the causal relationships between interpretable prompt features and jailbreak occurrences. In this work, we propose Causal Analyst, a framework that integrates LLMs into data-driven causal discovery to identify the direct causes of jailbreaks and leverage them for both attack and defense. We introduce a comprehensive dataset comprising 35k jailbreak attempts across seven LLMs, systematically constructed from 100 attack templates and 50 harmful queries, annotated with 37 meticulously designed human-readable prompt features. By jointly training LLM-based prompt encoding and GNN-based causal graph learning, we reconstruct causal pathways linking prompt features to jailbreak responses. Our analysis reveals that specific features, such as "Positive Character" and "Number of Task Steps", act as direct causal drivers of jailbreaks. We demonstrate the practical utility of these insights through two applications: (1) a Jailbreaking Enhancer that targets identified causal features to significantly boost attack success rates on public benchmarks, and (2) a Guardrail Advisor that utilizes the learned causal graph to extract true malicious intent from obfuscated queries. Extensive experiments, including baseline comparisons and causal structure validation, confirm the robustness of our causal analysis and its superiority over non-causal approaches. Our results suggest that analyzing jailbreak features from a causal perspective is an effective and interpretable approach for improving LLM reliability. Our code is available at https://github.com/Master-PLC/Causal-Analyst.

</details>


### [176] [Momentum Attention: The Physics of In-Context Learning and Spectral Forensics for Mechanistic Interpretability](https://arxiv.org/abs/2602.04902)
*Kingsuk Maitra*

Main category: cs.LG

TL;DR: 本文将Transformer视为物理电路，引入动量注意力（Momentum Attention）以嵌入物理先验，通过动量差分算子实现辛变换，揭示了辛-滤波对偶性，使模型能直接通过单层实现归纳推理，并支持频谱分析。实验验证了其在诱导任务上的优越性能及动量与深度的可替代性规律。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在归纳推理方面受限于拓扑深度约束（至少两层），难以高效建模动态变化信息。为突破此限制，本文从物理系统角度重新诠释Transformer，引入基于动量的机制以增强其对时间动态的感知能力。

Method: 提出动量注意力机制，利用动量差分 $p_t = q_t - q_{t-1}$ 构造辛剪切变换 $\hat{q}_t = q_t + \gamma p_t$，在查询和键中注入动力学信息；建立辛-滤波对偶性，证明该操作等价于高通滤波；结合低通旋转位置编码（RoPE）实现直流（语义）与交流（机制）信号的正交分离。

Result: 125M模型在诱导密集型任务上表现优于350M基线，验证损失仅高出约2.9%；通过5,100+控制实验验证了方法有效性；发现动量强度与模型规模之间的缩放律 $γ^* = 4.17 \times N^{-0.74}$，表明动量与深度具有可替换性。

Conclusion: 本文构建了一个连接生成式AI、哈密顿物理与信号处理的分析框架，通过引入物理先验的动量注意力机制，显著提升了Transformer在动态建模与归纳推理方面的效率与可解释性。

Abstract: The Mechanistic Interpretability (MI) program has mapped the Transformer as a precise computational graph. We extend this graph with a conservation law and time-varying AC dynamics, viewing it as a physical circuit. We introduce Momentum Attention, a symplectic augmentation embedding physical priors via the kinematic difference operator $p_t = q_t - q_{t-1}$, implementing the symplectic shear $\hat{q}_t = q_t + γp_t$ on queries and keys. We identify a fundamental Symplectic-Filter Duality: the physical shear is mathematically equivalent to a High-Pass Filter. This duality is our cornerstone contribution -- by injecting kinematic momentum, we sidestep the topological depth constraint ($L \geq 2$) for induction head formation. While standard architectures require two layers for induction from static positions, our extension grants direct access to velocity, enabling Single-Layer Induction and Spectral Forensics via Bode Plots. We formalize an Orthogonality Theorem proving that DC (semantic) and AC (mechanistic) signals segregate into orthogonal frequency bands when Low-Pass RoPE interacts with High-Pass Momentum. Validated through 5,100+ controlled experiments (documented in Supplementary Appendices A--R and 27 Jupyter notebooks), our 125M Momentum model exceeds expectations on induction-heavy tasks while tracking a 350M baseline within $\sim$2.9% validation loss. Dedicated associative recall experiments reveal a scaling law $γ^* = 4.17 \times N^{-0.74}$ establishing momentum-depth fungibility. We offer this framework as a complementary analytical toolkit connecting Generative AI, Hamiltonian Physics, and Signal Processing.

</details>


### [177] [Mind the Performance Gap: Capability-Behavior Trade-offs in Feature Steering](https://arxiv.org/abs/2602.04903)
*Eitan Sprejer,Oscar Agustín Stanchi,María Victoria Carro,Denise Alejandra Mester,Iván Arcuschin*

Main category: cs.LG

TL;DR: Feature steering methods like Goodfire's Auto Steer can successfully modify LLM behaviors but significantly degrade model performance on downstream tasks, leading to major trade-offs between control and quality. Even when behavior is effectively steered, accuracy and coherence drop substantially compared to simple prompting.


<details>
  <summary>Details</summary>
Motivation: To understand the practical effectiveness of feature steering in real-world applications, particularly the trade-offs between behavioral control and output quality.

Method: Evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries using 171 MMLU questions on Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control.

Result: Auto Steer improves behavioral control (3.33 vs. 2.98 on Llama-8B; 3.57 vs. 3.10 on Llama-70B) but causes significant drops in accuracy (66% → 46% on Llama-8B; 87% → 73% on Llama-70B) and coherence (4.62 → 2.24; 4.94 → 3.89), outperformed overall by simple prompting.

Conclusion: Current feature steering methods face fundamental capability-behavior trade-offs that limit their practical deployment, highlighting the need for empirical characterization before real-world use.

Abstract: Feature steering has emerged as a promising approach for controlling LLM behavior through direct manipulation of internal representations, offering advantages over prompt engineering. However, its practical effectiveness in real-world applications remains poorly understood, particularly regarding potential trade-offs with output quality. We show that feature steering methods substantially degrade model performance even when successfully controlling target behaviors, a critical trade-off. Specifically, we evaluate Goodfire's Auto Steer against prompt engineering baselines across 14 steering queries (covering innocuous and safety-relevant behaviors) on 171 Massive Multitask Language Understanding (MMLU) questions using Llama-8B and Llama-70B, measuring accuracy, coherence, and behavioral control. Our findings show that Auto Steer successfully modifies target behaviors (achieving scores of 3.33 vs. 2.98 for prompting on Llama-8B and 3.57 vs. 3.10 on Llama-70B), but causes dramatic performance degradation: accuracy on the MMLU questions drops from 66% to 46% on Llama-8B and 87% to 73% on Llama-70B, with coherence falling from 4.62 to 2.24 and 4.94 to 3.89 respectively. Simple prompting achieves the best overall balance. These findings highlight limitations of current feature steering methods for practical deployment where task performance cannot be sacrificed. More broadly, our work demonstrates that mechanistic control methods face fundamental capability-behavior trade-offs that must be empirically characterized before deployment.

</details>


### [178] [DCER: Dual-Stage Compression and Energy-Based Reconstruction](https://arxiv.org/abs/2602.04904)
*Yiwen Wang,Jiahao Qin*

Main category: cs.LG

TL;DR: 提出DCER框架，通过双阶段压缩和基于能量的重建解决多模态融合中的噪声输入和模态缺失问题。压缩阶段包括模态内频率变换（如小波、DCT）去除噪声并保留任务相关模式，以及跨模态瓶颈令牌促进真实融合；针对缺失模态，利用学习的能量函数进行梯度下降重构，并提供内在不确定性量化（与预测误差相关性>0.72）。在CMU-MOSI、CMU-MOSEI和CH-SIMS上实验表明，该方法在所有基准上达到最优性能，表现出在完整和高缺失条件下均有利于多模态融合的U型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态融合面临噪声输入降低表示质量以及模态缺失导致预测失败的问题，亟需一个统一框架同时应对这两类挑战。

Method: 提出双阶段压缩与能量基重建相结合的框架：第一阶段通过模态内频率变换（如小波、DCT）去噪并保留关键特征，跨模态瓶颈令牌强制跨模态真实融合；第二阶段通过梯度下降在学习的能量函数上重构缺失模态表示，并以最终能量值作为不确定性度量。

Result: 在CMU-MOSI、CMU-MOSEI和CH-SIMS数据集上均取得当前最优性能，展现出在模态完整与高缺失情况下均具有优异鲁棒性的U型趋势，且能量值与预测误差高度相关（相关系数>0.72）。

Conclusion: DCER框架有效提升了多模态融合在噪声和缺失场景下的鲁棒性，实现了高质量表示学习与不确定性量化，具备实际应用潜力。

Abstract: Multimodal fusion faces two robustness challenges: noisy inputs degrade representation quality, and missing modalities cause prediction failures. We propose DCER, a
  unified framework addressing both challenges through dual-stage compression and energy-based reconstruction. The compression stage operates at two levels:
  within-modality frequency transforms (wavelet for audio, DCT for video) remove noise while preserving task-relevant patterns, and cross-modality bottleneck tokens
  force genuine integration rather than modality-specific shortcuts. For missing modalities, energy-based reconstruction recovers representations via gradient descent
  on a learned energy function, with the final energy providing intrinsic uncertainty quantification (\r{ho} > 0.72 correlation with prediction error). Experiments on
  CMU-MOSI, CMU-MOSEI, and CH-SIMS demonstrate state-of-the-art performance across all benchmarks, with a U-shaped robustness pattern favoring multimodal fusion at
  both complete and high-missing conditions. The code will be available on Github.

</details>


### [179] [LISA: Laplacian In-context Spectral Analysis](https://arxiv.org/abs/2602.04906)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LISA是一种基于拉普拉斯的时序模型在推理时进行自适应的方法，利用观测前缀实现动态调整。它结合延迟坐标嵌入与拉普拉斯谱学习，生成扩散坐标状态表示，并使用冻结的非线性解码器进行一步预测。引入轻量级潜在空间残差适配器（基于高斯过程回归或类似注意力的马尔可夫算子），在变化动力学下表现优异，优于固定基线。该方法将上下文自适应与非参数谱方法联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有时序模型在动态变化环境下难以灵活适应，尤其在推理阶段缺乏有效调整机制。本文旨在通过仅使用观测前缀实现模型在推理时的自适应，提升对变化动力学的建模能力。

Method: LISA采用延迟坐标嵌入构建状态空间，结合拉普拉斯谱学习提取扩散坐标表示；使用冻结的非线性解码器进行预测，并引入轻量级残差适配器（高斯过程或注意力式马尔可夫算子）在潜在空间中实现上下文自适应。

Result: 在预测和自回归滚动实验中，LISA显著优于冻结基线模型，尤其在动态变化场景下优势明显，验证了其在推理阶段适应新动态的有效性。

Conclusion: LISA成功将上下文自适应与非参数谱方法结合，为时序模型提供了一种高效、轻量且适应性强的推理时调整框架，具有广泛的应用潜力。

Abstract: We propose Laplacian In-context Spectral Analysis (LISA), a method for inference-time adaptation of Laplacian-based time-series models using only an observed prefix. LISA combines delay-coordinate embeddings and Laplacian spectral learning to produce diffusion-coordinate state representations, together with a frozen nonlinear decoder for one-step prediction. We introduce lightweight latent-space residual adapters based on either Gaussian-process regression or an attention-like Markov operator over context windows. Across forecasting and autoregressive rollout experiments, LISA improves over the frozen baseline and is often most beneficial under changing dynamics. This work links in-context adaptation to nonparametric spectral methods for dynamical systems.

</details>


### [180] [Physics as the Inductive Bias for Causal Discovery](https://arxiv.org/abs/2602.04907)
*Jianhong Chen,Naichen Shi,Xubo Yue*

Main category: cs.LG

TL;DR: 本文提出了一种整合物理知识与数据驱动的因果发现框架，用于动态系统。通过将系统演化建模为带有已知常微分方程（ODE）动力学的随机微分方程（SDE），利用部分物理先验作为归纳偏置，以提升因果发现的可识别性、稳定性和鲁棒性。该方法采用稀疏诱导的最大似然估计（MLE）算法，结合因果图结构实现高效参数估计，并在温和条件下保证因果图的恢复。实验表明，该方法在多种复杂因果结构下优于纯数据驱动的基准方法，能更稳定且物理一致地恢复因果关系。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法多基于无环或平衡假设，难以处理具有反馈、循环相互作用和非平稳趋势的真实动态系统。将物理模型（如ODE）与数据驱动方法结合，可引入物理知识作为归纳偏置，从而改善因果发现的性能。

Method: 将系统演化建模为随机微分方程（SDE），其中漂移项编码已知的ODE动力学，扩散项表示未知的因果耦合；设计一种可扩展的稀疏诱导最大似然估计（MLE）算法，利用因果图结构进行高效参数估计。

Result: 在多样化的动态系统上，所提方法显著提升了因果图恢复精度，参数估计更稳定且符合物理规律，优于当前纯数据驱动的先进方法。

Conclusion: 融合部分物理知识的因果发现框架能够有效提升动态系统中因果结构识别的准确性与稳定性，为复杂系统分析提供了强有力的新工具。

Abstract: Causal discovery is often a data-driven paradigm to analyze complex real-world systems. In parallel, physics-based models such as ordinary differential equations (ODEs) provide mechanistic structure for many dynamical processes. Integrating these paradigms potentially allows physical knowledge to act as an inductive bias, improving identifiability, stability, and robustness of causal discovery in dynamical systems. However, such integration remains challenging: real dynamical systems often exhibit feedback, cyclic interactions, and non-stationary data trend, while many widely used causal discovery methods are formulated under acyclicity or equilibrium-based assumptions. In this work, we propose an integrative causal discovery framework for dynamical systems that leverages partial physical knowledge as an inductive bias. Specifically, we model system evolution as a stochastic differential equation (SDE), where the drift term encodes known ODE dynamics and the diffusion term corresponds to unknown causal couplings beyond the prescribed physics. We develop a scalable sparsity-inducing MLE algorithm that exploits causal graph structure for efficient parameter estimation. Under mild conditions, we establish guarantees to recover the causal graph. Experiments on dynamical systems with diverse causal structures show that our approach improves causal graph recovery and produces more stable, physically consistent estimates than purely data-driven state-of-the-art baselines.

</details>


### [181] [Temporal Pair Consistency for Variance-Reduced Flow Matching](https://arxiv.org/abs/2602.04908)
*Chika Maduabuchi,Jindong Wang*

Main category: cs.LG

TL;DR: 提出Temporal Pair Consistency (TPC) 方法，通过在成对时间步上耦合速度预测来降低连续时间生成模型中的估计方差，无需修改模型架构、概率路径或求解器。理论分析表明 TPC 引入了二次轨迹耦合正则化，可有效减少梯度方差并保持原有流匹配目标。在 CIFAR-10 和 ImageNet 上验证其有效性，提升采样质量与效率，优于已有方法，并兼容现代先进训练流程。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间生成模型（如扩散模型、流匹配）在训练时通常独立处理时间步，导致估计方差高、采样效率低。尽管已有方法引入平滑惩罚、轨迹正则化或修改概率路径以缓解此问题，但往往复杂且依赖特定设计。本文旨在提出一种轻量级、通用的方差减少机制，不改变原有结构的前提下提升性能。

Method: 提出 Temporal Pair Consistency (TPC)，在相同概率路径上对成对时间步的速度预测施加一致性约束，仅作用于估计器层面，不修改模型结构、概率路径或求解器。该方法通过引入轨迹耦合的二次正则化项，实现梯度方差的理论缩减。

Result: 在 CIFAR-10 与 ImageNet 多分辨率数据集上，基于 TPC 的流匹配模型在相同或更低计算成本下实现了更优的 FID 分数，样本质量更高，采样效率更好。此外，TPC 可无缝集成至现代 SOTA 流水线，包括噪声增强训练、基于得分的去噪和修正流等。

Conclusion: TPC 是一种高效、通用且无需修改原有框架的方差减少策略，为连续时间生成模型提供了显著的性能提升，具有良好的扩展性和实用性。

Abstract: Continuous-time generative models, such as diffusion models, flow matching, and rectified flow, learn time-dependent vector fields but are typically trained with objectives that treat timesteps independently, leading to high estimator variance and inefficient sampling. Prior approaches mitigate this via explicit smoothness penalties, trajectory regularization, or modified probability paths and solvers. We introduce Temporal Pair Consistency (TPC), a lightweight variance-reduction principle that couples velocity predictions at paired timesteps along the same probability path, operating entirely at the estimator level without modifying the model architecture, probability path, or solver. We provide a theoretical analysis showing that TPC induces a quadratic, trajectory-coupled regularization that provably reduces gradient variance while preserving the underlying flow-matching objective. Instantiated within flow matching, TPC improves sample quality and efficiency across CIFAR-10 and ImageNet at multiple resolutions, achieving lower FID at identical or lower computational cost than prior methods, and extends seamlessly to modern SOTA-style pipelines with noise-augmented training, score-based denoising, and rectified flow.

</details>


### [182] [Learning Where It Matters: Geometric Anchoring for Robust Preference Alignment](https://arxiv.org/abs/2602.04909)
*Youngjae Cho,Jongsuk Kim,Ji-Hoon Kim*

Main category: cs.LG

TL;DR: GAPO提出一种动态几何锚点机制，用对抗性局部扰动作为当前策略的悲观基线，替代固定参考策略，以缓解分布不匹配和噪声偏好信号问题。通过引入锚点差距（Anchor Gap）来衡量策略与锚点间的奖励差异，并基于该差距对偏好对进行自适应加权，从而降低对几何上脆弱样本的敏感性，强化稳健偏好信号。在多种噪声环境下，GAPO显著提升鲁棒性，同时在标准大模型对齐与推理基准上保持或超越现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好优化的方法如DPO依赖固定参考策略，随策略漂移易产生分布不匹配，放大噪声偏好信号；而无参考方法虽避免不匹配但面临奖励漂移失控问题。因此需要一种既能适应策略变化、又能控制过拟合脆弱信号的动态机制。

Method: 提出几何锚点偏好优化（GAPO），采用对抗性局部扰动生成动态锚点，作为当前策略的悲观基线；设计锚点差距（Anchor Gap）量化局部奖励差异，构建基于该差距的加权逻辑损失函数，实现对偏好对的自适应重加权。

Result: 在多种噪声设置下，GAPO表现出更强的鲁棒性，且在标准大模型对齐与推理任务中性能优于或等同于现有方法，尤其在高噪声场景下优势明显。

Conclusion: GAPO通过引入几何感知的动态锚点与自适应加权机制，有效缓解了传统偏好优化中的分布偏移与噪声放大问题，为大模型对齐提供了更稳健的解决方案。

Abstract: Direct Preference Optimization (DPO) and related methods align large language models from pairwise preferences by regularizing updates against a fixed reference policy. As the policy drifts, a static reference, however, can become increasingly miscalibrated, leading to distributional mismatch and amplifying spurious preference signals under noisy supervision. Conversely, reference-free variants avoid mismatch but often suffer from unconstrained reward drift. We propose Geometric Anchor Preference Optimization (GAPO), which replaces the fixed reference with a dynamic, geometry-aware anchor: an adversarial local perturbation of the current policy within a small radius that serves as a pessimistic baseline. This anchor enables an adaptive reweighting mechanism, modulating the importance of each preference pair based on its local sensitivity. We further introduce the Anchor Gap, the reward discrepancy between the policy and its anchor, and show under smoothness conditions that it approximates worst-case local margin degradation. Optimizing a logistic objective weighted by this gap downweights geometrically brittle instances while emphasizing robust preference signals. Across diverse noise settings, GAPO consistently improves robustness while matching or improving performance on standard LLM alignment and reasoning benchmarks.

</details>


### [183] [A$^2$-LLM: An End-to-end Conversational Audio Avatar Large Language Model](https://arxiv.org/abs/2602.04913)
*Xiaolin Hu,Hang Yuan,Xinzhu Sang,Binbin Yan,Zhou Yu,Cong Huang,Kai Chen*

Main category: cs.LG

TL;DR: A$^2$-LLM提出一种端到端的对话式音频虚拟人模型，联合推理语言、语音韵律和3D面部动作，在统一框架中实现情感丰富的实时交互。通过引入FLAME-QA数据集，提升语义与面部动态对齐效果，显著改善表达力与响应速度（500毫秒延迟，0.7 RTF）。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统多采用级联模块架构，存在误差累积、高延迟和实时性能差的问题，且难以捕捉深层情感语境，导致表情表现僵硬，仅关注唇同步而非情感深度。

Method: 提出A$^2$-LLM模型，整合语言理解、音频韵律分析与3D面部运动生成于一体；构建FLAME-QA多模态数据集，以问答形式对齐语义意图与面部动态，支持模型训练。

Result: 实验表明，A$^2$-LLM在保持实时性（500毫秒延迟，0.7 RTF）的同时，显著提升了情感表达丰富度和自然度，优于传统方法。

Conclusion: A$^2$-LLM实现了情感丰富、低延迟的端到端对话虚拟人生成，为下一代人机交互提供了高效且生动的解决方案。

Abstract: Developing expressive and responsive conversational digital humans is a cornerstone of next-generation human-computer interaction. While large language models (LLMs) have significantly enhanced dialogue capabilities, most current systems still rely on cascaded architectures that connect independent modules. These pipelines are often plagued by accumulated errors, high latency, and poor real-time performance. Lacking access to the underlying conversational context, these pipelines inherently prioritize rigid lip-sync over emotional depth. To address these challenges, we propose A$^2$-LLM, an end-to-end conversational audio avatar large language model that jointly reasons about language, audio prosody, and 3D facial motion within a unified framework. To facilitate training, we introduce FLAME-QA, a high-quality multimodal dataset designed to align semantic intent with expressive facial dynamics within a QA format. By leveraging deep semantic understanding, A$^2$-LLM generates emotionally rich facial movements beyond simple lip-synchronization. Experimental results demonstrate that our system achieves superior emotional expressiveness while maintaining real-time efficiency (500 ms latency, 0.7 RTF).

</details>


### [184] [SLAY: Geometry-Aware Spherical Linearized Attention with Yat-Kernel](https://arxiv.org/abs/2602.04915)
*Jose Miguel Luna,Taha Bouhsine,Krzysztof Choromanski*

Main category: cs.LG

TL;DR: 提出了一种基于Yat-kernel的新型线性时间注意力机制SLAY，通过将查询和键约束在单位球面上实现几何感知的注意力计算，利用伯恩斯坦定理将球面Yat核表示为多项式-指数乘积核的非负混合，并推导出严格正的随机特征近似，从而实现O(L)线性时间复杂度。实验表明，SLAY性能接近标准softmax注意力，且优于现有线性注意力方法如Performers和Cosformers，是目前最接近softmax注意力的线性时间近似方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer中注意力机制的二次时间复杂度问题，同时保持高性能，避免现有线性注意力方法带来的性能损失。

Method: 将查询和键限制在单位球面上，利用球面Yat核的性质，结合伯恩斯坦定理将其展开为非负多项式-指数乘积核的混合形式，进而构造严格的正随机特征近似，实现线性时间复杂度的注意力计算。

Result: SLAY在性能上几乎与标准softmax注意力无异，具备线性时间与内存复杂度，显著优于Performers、Cosformers等已有线性注意力方法，是目前最接近softmax注意力的线性近似方案。

Conclusion: SLAY是一种高效、几何感知的线性时间注意力机制，能够在不牺牲性能的前提下实现大规模Transformer模型的可扩展性，为线性化注意力提供了新的最优解。

Abstract: We propose a new class of linear-time attention mechanisms based on a relaxed and computationally efficient formulation of the recently introduced E-Product, often referred to as the Yat-kernel (Bouhsine, 2025). The resulting interactions are geometry-aware and inspired by inverse-square interactions in physics. Our method, Spherical Linearized Attention with Yat Kernels (SLAY), constrains queries and keys to the unit sphere so that attention depends only on angular alignment. Using Bernstein's theorem, we express the spherical Yat-kernel as a nonnegative mixture of polynomial-exponential product kernels and derive a strictly positive random-feature approximation enabling linear-time O(L) attention. We establish positive definiteness and boundedness on the sphere and show that the estimator yields well-defined, nonnegative attention scores. Empirically, SLAY achieves performance that is nearly indistinguishable from standard softmax attention while retaining linear time and memory scaling, and consistently outperforms prior linear-time attention mechanisms such as Performers and Cosformers. To the best of our knowledge, SLAY represents the closest linear-time approximation to softmax attention reported to date, enabling scalable Transformers without the typical performance trade-offs of attention linearization.

</details>


### [185] [Multi-Aspect Mining and Anomaly Detection for Heterogeneous Tensor Streams](https://arxiv.org/abs/2602.04917)
*Soshi Kakio,Yasuko Matsubara,Ren Fujiwara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: HeteroComp 是一种用于连续总结异构张量流（包含时间戳和多种属性）的方法，能够有效处理类别型和连续型属性的混合数据，并通过高斯过程先验建模连续属性的未知分布及时间动态，从而实现对群体异常（如DoS攻击）的精准检测。实验表明，该方法在准确率上优于现有最先进算法，且计算时间不随数据流长度增加而增长。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解与异常检测方法无法有效处理异构张量流中的类别型与连续型属性混合问题，且通常对时间戳进行离散化，导致无法捕捉时间动态，难以检测群体异常。

Method: HeteroComp 使用高斯过程先验来建模连续属性的未知分布和时间动态，避免了对数据分布的错误假设；通过将异构张量流持续分解为表示各属性潜在群组及其时间动态的‘组件’，实现高效、准确的群体异常检测。

Result: 在真实数据集上的大量实验表明，HeteroComp 在群体异常检测准确率方面显著优于现有最先进方法，且其计算开销与数据流长度无关，具备良好的可扩展性。

Conclusion: HeteroComp 有效解决了异构张量流中属性异质性和时间动态建模的挑战，提供了一种无需离散化、能直接从数据中学习概率密度的高效异常检测框架，适用于大规模实时流数据场景。

Abstract: Analysis and anomaly detection in event tensor streams consisting of timestamps and multiple attributes - such as communication logs(time, IP address, packet length)- are essential tasks in data mining. While existing tensor decomposition and anomaly detection methods provide useful insights, they face the following two limitations. (i) They cannot handle heterogeneous tensor streams, which comprises both categorical attributes(e.g., IP address) and continuous attributes(e.g., packet length). They typically require either discretizing continuous attributes or treating categorical attributes as continuous, both of which distort the underlying statistical properties of the data.Furthermore, incorrect assumptions about the distribution family of continuous attributes often degrade the model's performance. (ii) They discretize timestamps, failing to track the temporal dynamics of streams(e.g., trends, abnormal events), which makes them ineffective for detecting anomalies at the group level, referred to as 'group anomalies' (e.g, DoS attacks). To address these challenges, we propose HeteroComp, a method for continuously summarizing heterogeneous tensor streams into 'components' representing latent groups in each attribute and their temporal dynamics, and detecting group anomalies. Our method employs Gaussian process priors to model unknown distributions of continuous attributes, and temporal dynamics, which directly estimate probability densities from data. Extracted components give concise but effective summarization, enabling accurate group anomaly detection. Extensive experiments on real datasets demonstrate that HeteroComp outperforms the state-of-the-art algorithms for group anomaly detection accuracy, and its computational time does not depend on the data stream length.

</details>


### [186] [Simulated Adoption: Decoupling Magnitude and Direction in LLM In-Context Conflict Resolution](https://arxiv.org/abs/2602.04918)
*Long Zhang,Fangwei Lin*

Main category: cs.LG

TL;DR: 该研究通过层间几何分析揭示了大语言模型在面对冲突信息时的合规机制，发现其并非通过削弱参数记忆的信号强度（即‘流形稀释’），而是通过引入近似正交的导向向量实现隐藏状态的几何偏移，从而在不改变内部知识幅度的情况下‘模拟采纳’外部上下文。这一现象被称为‘正交干扰’，挑战了基于标量置信度检测幻觉的有效性，强调需采用向量级监控以区分真实知识整合与表面模仿。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在处理冲突信息时为何倾向于遵循上下文而非依赖自身参数化记忆，特别是理解这种行为背后的机制是否源于信号强度衰减或方向性几何变化。

Method: 对Qwen-4B、Llama-3.1-8B和GLM-4-9B三个模型进行层级几何分析，将反事实上下文引发的残差流更新分解为径向（范数）和角向（余弦）成分，评估其在不同层级上的变化模式。

Result: 实验结果否定了‘流形稀释’假设的普适性：两个模型即使在性能显著下降的情况下仍保持残差范数稳定；而所有模型均表现出一致的‘正交干扰’特征，即冲突上下文注入一个与真实方向近乎正交的引导向量，导致隐藏状态被旋转，而非抑制原始知识。

Conclusion: 大语言模型并非通过削弱内部知识的幅度来实现合规，而是通过几何位移机制绕过正确的解码向量，实现对上下文的表层模仿。这表明当前基于标量置信度的幻觉检测方法存在局限，必须转向向量层面的监测手段以准确识别知识融合的真实性质。

Abstract: Large Language Models (LLMs) frequently prioritize conflicting in-context information over pre-existing parametric memory, a phenomenon often termed sycophancy or compliance. However, the mechanistic realization of this behavior remains obscure, specifically how the model resolves these knowledge conflicts through compliance, and whether this suppression arises from signal magnitude dilution or directional geometric alteration within the residual stream. To resolve this, we conducted a layer-wise geometric analysis across Qwen-4B, Llama-3.1-8B, and GLM-4-9B, decomposing the residual stream updates induced by counter-factual contexts into radial (norm-based) and angular (cosine-based) components. Our empirical results reject the universality of the "Manifold Dilution" hypothesis, as two of the three architectures maintained stable residual norms despite exhibiting significant performance degradation on factual queries. Instead, we observed that compliance is consistently characterized by "Orthogonal Interference," where the conflicting context injects a steering vector that is quasi-orthogonal to the ground-truth direction, effectively rotating the hidden state representation. This suggests that models do not "unlearn" or suppress the magnitude of internal truths but rather employ a mechanism of geometric displacement to bypass the correct unembedding vector, effectively simulating adoption while preserving the original structural magnitude. These findings challenge scalar confidence metrics for detecting hallucinations and underscore the necessity of vectorial monitoring to distinguish between genuine knowledge integration and superficial in-context mimicry.

</details>


### [187] [Gradually Compacting Large Language Models for Reasoning Like a Boiling Frog](https://arxiv.org/abs/2602.04919)
*Yiran Zhao,Shengyang Zhou,Zijian Wu,Tongyan Hu,Yuhui Xu,Rengan Dou,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 本文提出了一种渐进式压缩方法（Prune-Tune Loop, PTL），通过多轮细粒度的剪枝与微调循环，逐步压缩大语言模型（LLM）规模，同时在不显著损失性能的前提下实现高效推理。该方法避免了传统剪枝带来的性能骤降问题，适用于多种剪枝策略和后训练方式，并在数学推理、代码生成等多种任务上表现出良好效果，具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法直接移除冗余参数会导致大语言模型在推理任务中性能大幅下降，且需要大量后训练来恢复能力。为解决这一问题，亟需一种既能减少计算资源消耗又不损害性能的高效压缩方法。

Method: 提出渐进式压缩框架Prune-Tune Loop（PTL），将压缩过程分解为多个细粒度迭代阶段，在每一轮中执行剪枝-微调循环，使模型逐步压缩并持续恢复性能，模拟‘温水煮青蛙’效应，避免性能突变。

Result: PTL可将大语言模型压缩至原大小的一半左右，仅需轻量级后训练即可保持与原始模型相当的推理性能；适用于神经元剪枝、层剪枝等不同剪枝策略及持续预训练、强化学习等后训练方法；在数学推理、代码生成等多个任务上均表现优异。

Conclusion: Prune-Tune Loop（PTL）是一种高效、灵活的大语言模型压缩方法，能够在保证性能的前提下显著降低模型规模和计算开销，具备良好的泛化能力和实际应用前景。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, but their substantial size often demands significant computational resources. To reduce resource consumption and accelerate inference, it is essential to eliminate redundant parameters without compromising performance. However, conventional pruning methods that directly remove such parameters often lead to a dramatic drop in model performance in reasoning tasks, and require extensive post-training to recover the lost capabilities. In this work, we propose a gradual compacting method that divides the compression process into multiple fine-grained iterations, applying a Prune-Tune Loop (PTL) at each stage to incrementally reduce model size while restoring performance with finetuning. This iterative approach-reminiscent of the "boiling frog" effect-enables the model to be progressively compressed without abrupt performance loss. Experimental results show that PTL can compress LLMs to nearly half their original size with only lightweight post-training, while maintaining performance comparable to the original model on reasoning tasks. Moreover, PTL is flexible and can be applied to various pruning strategies, such as neuron pruning and layer pruning, as well as different post-training methods, including continual pre-training and reinforcement learning. Additionally, experimental results confirm the effectiveness of PTL on a variety of tasks beyond mathematical reasoning, such as code generation, demonstrating its broad applicability.

</details>


### [188] [CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning](https://arxiv.org/abs/2602.04920)
*Ronghao Lin,Qiaolin He,Sijie Mai,Ying Zeng,Aolin Xiong,Li Huang,Yap-Peng Tan,Haifeng Hu*

Main category: cs.LG

TL;DR: 本文提出一种名为CyIN的循环信息学习框架，旨在解决多模态模型在实际应用中因模态缺失导致性能下降的问题。通过在不同模态间循环应用词元和标签级别的信息瓶颈（IB），构建有信息量的潜在空间，并利用变分近似净化特征以提升跨模态交互效率。同时，引入跨模态循环翻译机制，通过已有的模态重建缺失模态，从而增强模型对不完整输入的鲁棒性。该方法在四个多模态数据集上均表现出色，实现了完整与多种不完整场景下的统一优化。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的多模态数据常出现模态缺失且不可预测，现有模型在训练时依赖完全配对的数据，导致在实际部署中性能显著下降，亟需一种能应对动态缺失模态的鲁棒学习框架。

Method: 提出循环信息学习框架CyIN，结合模态间的循环信息瓶颈（IB）机制，在词元和标签层面构建有信息量的潜在空间；采用变分近似净化潜在表示；设计跨模态循环翻译策略，通过正向与反向传播重建缺失模态。

Result: 在4个公开多模态数据集上的实验表明，该方法在完整及多种不完整模态场景下均优于现有方法，展现出卓越的泛化能力与鲁棒性。

Conclusion: CyIN框架成功实现了对完整与不完整多模态学习的统一建模，有效提升了模型在动态缺失模态条件下的性能表现，为现实场景中的多模态学习提供了新思路。

Abstract: Multimodal machine learning, mimicking the human brain's ability to integrate various modalities has seen rapid growth. Most previous multimodal models are trained on perfectly paired multimodal input to reach optimal performance. In real-world deployments, however, the presence of modality is highly variable and unpredictable, causing the pre-trained models in suffering significant performance drops and fail to remain robust with dynamic missing modalities circumstances. In this paper, we present a novel Cyclic INformative Learning framework (CyIN) to bridge the gap between complete and incomplete multimodal learning. Specifically, we firstly build an informative latent space by adopting token- and label-level Information Bottleneck (IB) cyclically among various modalities. Capturing task-related features with variational approximation, the informative bottleneck latents are purified for more efficient cross-modal interaction and multimodal fusion. Moreover, to supplement the missing information caused by incomplete multimodal input, we propose cross-modal cyclic translation by reconstruct the missing modalities with the remained ones through forward and reverse propagation process. With the help of the extracted and reconstructed informative latents, CyIN succeeds in jointly optimizing complete and incomplete multimodal learning in one unified model. Extensive experiments on 4 multimodal datasets demonstrate the superior performance of our method in both complete and diverse incomplete scenarios.

</details>


### [189] [Imposing Boundary Conditions on Neural Operators via Learned Function Extensions](https://arxiv.org/abs/2602.04923)
*Sepehr Mousavi,Siddhartha Mishra,Laura De Lorenzis*

Main category: cs.LG

TL;DR: 提出了一种通过函数扩展来调节神经算子处理复杂非齐次边界条件的新框架，能够有效学习复杂边界条件与输入域函数之间的丰富依赖关系，在多种PDE问题上实现显著优于基线的精度，且无需跨数据集调参。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理高度可变的非齐次边界条件时表现有限，尤其当解对边界激励敏感时性能下降，亟需一种通用、高效的方法来增强其边界条件建模能力。

Method: 将边界数据映射到整个空间域上的潜在伪扩展，使标准神经算子架构能够接收并利用边界信息；结合任意域到域的神经算子，共同学习边界条件与输入函数的复杂依赖关系。

Result: 在18个涵盖泊松方程、线性弹性与超弹性问题的挑战性数据集上，该方法取得当前最优性能，显著超越基线模型，且无需超参数调整。

Conclusion: 学习边界到域的扩展是一种有效且实用的策略，可显著提升现有神经算子框架对复杂边界条件的处理能力，推动科学机器学习在更广泛PDE问题中的应用。

Abstract: Neural operators have emerged as powerful surrogates for the solution of partial differential equations (PDEs), yet their ability to handle general, highly variable boundary conditions (BCs) remains limited. Existing approaches often fail when the solution operator exhibits strong sensitivity to boundary forcings. We propose a general framework for conditioning neural operators on complex non-homogeneous BCs through function extensions. Our key idea is to map boundary data to latent pseudo-extensions defined over the entire spatial domain, enabling any standard operator learning architecture to consume boundary information. The resulting operator, coupled with an arbitrary domain-to-domain neural operator, can learn rich dependencies on complex BCs and input domain functions at the same time. To benchmark this setting, we construct 18 challenging datasets spanning Poisson, linear elasticity, and hyperelasticity problems, with highly variable, mixed-type, component-wise, and multi-segment BCs on diverse geometries. Our approach achieves state-of-the-art accuracy, outperforming baselines by large margins, while requiring no hyperparameter tuning across datasets. Overall, our results demonstrate that learning boundary-to-domain extensions is an effective and practical strategy for imposing complex BCs in existing neural operator frameworks, enabling accurate and robust scientific machine learning models for a broader range of PDE-governed problems.

</details>


### [190] [Internalizing LLM Reasoning via Discovery and Replay of Latent Actions](https://arxiv.org/abs/2602.04925)
*Zhenning Shi,Yijia Zhu,Junhan Shi,Xun Zhang,Lei Wang,Congcong Miao*

Main category: cs.LG

TL;DR: STIR提出一种动态潜在轨迹控制框架，通过三阶段流程实现推理增强：(1) 差分内在动作诱导提取隐式推理成功经验；(2) 构建稀疏控制基底以形成紧凑且几何多样的工具库；(3) 通过锚点门控进行价值调制的轨迹干预。在六个算术与逻辑基准测试中，相比原始解码，STIR平均准确率提升1.9%至7.5%，平均令牌消耗减少高达35%。该方法实现了显式思维链的优势，同时内化推理过程，无需显式生成即可保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法依赖静态控制向量，无法适应复杂推理任务中非平稳的演化过程，限制了推理效率和适应性。

Method: STIR采用三阶段动态潜变量轨迹控制框架：首先通过差分内在动作诱导从隐藏状态中挖掘有效的推理模式；其次构建稀疏、几何多样化的控制基底作为工具库；最后利用价值调制与锚点门控机制，在推理过程中动态注入上下文相关的干预信号。

Result: 在四个代表性模型上的六项基准测试中，STIR显著提升平均准确率（1.9%-7.5%），同时降低平均令牌消耗最多达35%，证明其在推理效率与精度上的优越性。

Conclusion: 通过动态潜变量轨迹控制，可有效内化链式思维过程，实现无需显式生成的高效推理，显著提升模型表现并降低计算开销。

Abstract: The internalization of chain-of-thought processes into hidden states has emerged as a highly efficient paradigm for scaling test-time compute. However, existing activation steering methods rely on static control vectors that fail to adapt to the non-stationary evolution of complex reasoning tasks. To address this limitation, we propose STIR (Self-Distilled Tools for Internal Reasoning), a framework that reformulates reasoning enhancement as a dynamic latent trajectory control problem. STIR introduces a synergistic three-stage pipeline: (1) differential intrinsic action induction harvests latent reasoning successes to crystallize steering primitives; (2) sparse control basis construction curates a compact, geometrically diverse tool library; and (3) value-modulated trajectory intervention dynamically injects context-specific impulses via anchor-based gating. Extensive experiments on six arithmetic and logical benchmarks across four representative models demonstrate that STIR improves average accuracy by 1.9% to 7.5% while reducing average token consumption by up to 35% compared to vanilla decoding. These findings demonstrate that the benefits of explicit chain-of-thought can be realized through dynamic latent trajectory control, internalizing the reasoning process to bypass the explicit generation while achieving superior fidelity. Our code is available at https://github.com/sznnzs/LLM-Latent-Action.

</details>


### [191] [TurboBoA: Faster and Exact Attention-aware Quantization without Backpropagation](https://arxiv.org/abs/2602.04929)
*Junhan Kim,Yeo Jeong Park,Seungwoo Son,Chungman Lee,Ho-young Kim,Joonyoung Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: TurboBoA is a fast, backpropagation-free post-training quantization (PTQ) method that improves upon BoA by enabling joint quantization of multiple out-channels with closed-form error compensation, reducing sequential bottlenecks and achieving over 3x speedup. It includes error correction for propagated errors and adaptive grid computation with coordinate descent to maintain alignment. Experiments show TurboBoA outperforms BoA in both speed and accuracy, achieving state-of-the-art results in weight-only and weight-activation quantization when combined with outlier suppression.


<details>
  <summary>Details</summary>
Motivation: GPTQ suffers from accuracy drops in low-bit regimes due to layer-wise independence assumptions. BoA improves accuracy by modeling inter-layer dependencies but is slow due to sequential quantization across all out-channels. There is a need for a faster, accurate PTQ method that retains inter-layer dependencies without sacrificing efficiency.

Method: TurboBoA introduces three innovations: (i) joint quantization of multiple out-channels using a closed-form error compensation rule to reduce sequential bottlenecks; (ii) a correction mechanism for errors from previously quantized layers; (iii) adaptive grid computation with coordinate descent refinement to preserve alignment during iterative updates.

Result: TurboBoA achieves over three-fold speedup compared to BoA while consistently improving quantization accuracy. When combined with outlier suppression, it sets new state-of-the-art performance in both weight-only and weight-activation quantization.

Conclusion: TurboBoA effectively balances speed and accuracy in post-training quantization by eliminating sequential bottlenecks through joint channel quantization and error compensation, while maintaining alignment via adaptive grid refinement. It is a highly efficient and accurate PTQ solution suitable for large-scale LLMs.

Abstract: The rapid growth of large language models (LLMs) has heightened the importance of post-training quantization (PTQ) for reducing memory and computation costs. Among PTQ methods, GPTQ has gained significant attention for its efficiency, enabling billion-scale LLMs to be quantized within a few GPU hours. However, GPTQ's assumption of layer-wise independence leads to severe accuracy drops in low-bit regimes. Recently, BoA improved upon GPTQ by incorporating inter-layer dependencies within attention modules, but its reliance on sequential quantization across all out-channels makes it substantially less efficient. In this paper, we propose TurboBoA, a new backpropagation-free PTQ algorithm that preserves the accuracy benefits of BoA while significantly accelerating the process. The proposed TurboBoA introduces three key innovations: (i) joint quantization of multiple out-channels with a closed-form error compensation rule, which reduces sequential bottlenecks and yields more than a three-fold speedup; (ii) a correction mechanism for errors propagated from preceding quantized layers; and (iii) adaptive grid computation with coordinate descent refinement to maintain alignment during iterative updates. Extensive experiments demonstrate that TurboBoA delivers substantial acceleration over BoA while consistently improving accuracy. When combined with outlier suppression techniques, it achieves state-of-the-art results in both weight-only and weight-activation quantization. The code will be available at https://github.com/SamsungLabs/TurboBoA.

</details>


### [192] [Depth-Wise Emergence of Prediction-Centric Geometry in Large Language Models](https://arxiv.org/abs/2602.04931)
*Shahar Haim,Daniel C McNamee*

Main category: cs.LG

TL;DR: 该研究揭示了仅解码器的大语言模型在计算过程中存在从上下文处理到预测生成的深度转变，并伴随表征几何结构的重组。通过结合几何分析与机制干预的统一框架，发现后期层的表征实现了结构化的几何编码，从而实现对词元预测的选择性因果控制。具体而言，表征几何的角度组织参数化了预测分布的相似性，而表征范数则编码了不决定预测的上下文相关信息。这些结果为大语言模型将上下文转化为预测的动态过程提供了机制-几何解释。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何将输入上下文转化为最终的预测输出，尤其是其内部计算过程中的动态变化和表征机制。

Method: 采用统一框架，结合几何分析与机制干预方法，分析模型深层表征的几何结构及其对预测的影响。

Result: 晚期层的表征具有结构化的几何编码，角度组织反映预测分布相似性，范数编码上下文信息但不影响预测；表明模型在深度方向上经历从上下文处理到预测生成的转变。

Conclusion: 该研究为大语言模型中上下文到预测的转化过程提供了基于机制与几何的解释，揭示了深层表征在预测形成中的关键作用。

Abstract: We show that decoder-only large language models exhibit a depth-wise transition from context-processing to prediction-forming phases of computation accompanied by a reorganization of representational geometry. Using a unified framework combining geometric analysis with mechanistic intervention, we demonstrate that late-layer representations implement a structured geometric code that enables selective causal control over token prediction. Specifically, angular organization of the representation geometry parametrizes prediction distributional similarity, while representation norms encode context-specific information that does not determine prediction. Together, these results provide a mechanistic-geometric account of the dynamics of transforming context into predictions in LLMs.

</details>


### [193] [Comparing Euclidean and Hyperbolic K-Means for Generalized Category Discovery](https://arxiv.org/abs/2602.04932)
*Mohamad Dalal,Thomas B. Moeslund,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 提出HC-GCD模型，直接在双曲空间中进行嵌入学习和聚类，相比之前方法在双曲空间学习后转回欧几里得空间聚类，表现出更优的性能。实验表明，使用双曲K-Means比欧几里得K-Means更准确，并且在不同标签粒度下聚类更一致。


<details>
  <summary>Details</summary>
Motivation: 先前的双曲GCD方法虽然在双曲空间中进行表示学习，但在聚类阶段仍转换回欧几里得空间，这可能不是最优策略。作者认为在双曲空间中直接聚类能更好地保留数据的层次结构。

Method: 采用Lorentz双曲面模型进行双曲嵌入学习，并使用双曲K-Means算法在双曲空间中直接聚类。

Result: HC-GCD在语义漂移基准数据集上达到与现有最佳双曲GCD方法相当的性能；双曲K-Means相比欧几里得K-Means提升分类准确率；对未见类别聚类精度随欧几里得嵌入范数裁剪而下降，但已见类别精度上升，整体表现依赖数据集；双曲聚类在不同标签粒度下更具一致性。

Conclusion: 在双曲空间中直接进行聚类优于先映射回欧几里得空间再聚类，双曲几何有助于更准确、更稳定的开放世界分类任务表现。

Abstract: Hyperbolic representation learning has been widely used to extract implicit hierarchies within data, and recently it has found its way to the open-world classification task of Generalized Category Discovery (GCD). However, prior hyperbolic GCD methods only use hyperbolic geometry for representation learning and transform back to Euclidean geometry when clustering. We hypothesize this is suboptimal. Therefore, we present Hyperbolic Clustered GCD (HC-GCD), which learns embeddings in the Lorentz Hyperboloid model of hyperbolic geometry, and clusters these embeddings directly in hyperbolic space using a hyperbolic K-Means algorithm. We test our model on the Semantic Shift Benchmark datasets, and demonstrate that HC-GCD is on par with the previous state-of-the-art hyperbolic GCD method. Furthermore, we show that using hyperbolic K-Means leads to better accuracy than Euclidean K-Means. We carry out ablation studies showing that clipping the norm of the Euclidean embeddings leads to decreased accuracy in clustering unseen classes, and increased accuracy for seen classes, while the overall accuracy is dataset dependent. We also show that using hyperbolic K-Means leads to more consistent clusters when varying the label granularity.

</details>


### [194] [Transolver-3: Scaling Up Transformer Solvers to Industrial-Scale Geometries](https://arxiv.org/abs/2602.04940)
*Hang Zhou,Haixu Wu,Haonan Shangguan,Yuezhou Ma,Huikun Weng,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Transolver-3 is a highly scalable neural PDE solver designed for industrial-scale physics simulations with meshes exceeding 160 million cells. It overcomes memory limitations through optimized matrix operations, geometry slice tiling, amortized training on subsets, and physical state caching, enabling high-fidelity predictions in aerospace and automotive applications.


<details>
  <summary>Details</summary>
Motivation: Existing neural PDE solvers struggle with high-resolution meshes due to excessive memory demands, limiting their use in real-world engineering problems involving complex geometries.

Method: Transolver-3 employs faster slice/deslice via matrix multiplication optimization, geometry slice tiling for computation partitioning, amortized training on random mesh subsets, and physical state caching during inference.

Result: The framework successfully handles meshes with over 160 million cells and achieves strong performance across three challenging benchmarks, including aircraft and automotive design simulations.

Conclusion: Transolver-3 enables practical deployment of deep learning-based PDE solvers in large-scale engineering contexts by addressing memory and scalability bottlenecks.

Abstract: Deep learning has emerged as a transformative tool for the neural surrogate modeling of partial differential equations (PDEs), known as neural PDE solvers. However, scaling these solvers to industrial-scale geometries with over $10^8$ cells remains a fundamental challenge due to the prohibitive memory complexity of processing high-resolution meshes. We present Transolver-3, a new member of the Transolver family as a highly scalable framework designed for high-fidelity physics simulations. To bridge the gap between limited GPU capacity and the resolution requirements of complex engineering tasks, we introduce two key architectural optimizations: faster slice and deslice by exploiting matrix multiplication associative property and geometry slice tiling to partition the computation of physical states. Combined with an amortized training strategy by learning on random subsets of original high-resolution meshes and a physical state caching technique during inference, Transolver-3 enables high-fidelity field prediction on industrial-scale meshes. Extensive experiments demonstrate that Transolver-3 is capable of handling meshes with over 160 million cells, achieving impressive performance across three challenging simulation benchmarks, including aircraft and automotive design tasks.

</details>


### [195] [Near-Optimal Dynamic Matching via Coarsening with Application to Heart Transplantation](https://arxiv.org/abs/2602.04989)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗化（coarsening）方法的新在线匹配算法，通过将离线节点聚类为有容量限制的群组，在保持理论最优性的同时提升了实用性。该方法应用于心脏移植分配，利用历史数据的结构特性设计出具有理论保障的策略，在真实模拟中表现接近理想基准，有效弥合了数据驱动启发式与理论下界之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有在线匹配算法在实际应用中缺乏强理论保证，尤其是在互联网广告和器官分配等关键领域。尽管聚类方法被广泛使用，但其理论基础薄弱，亟需提供严谨的理论支持以增强可信度。

Method: 采用粗化策略，将离线节点聚合为具有容量约束的集群，利用历史数据中的结构特征构建近似最优的在线匹配策略，同时保持可证明的性能边界。

Result: 在心脏移植分配的真实模拟中，所提策略的表现接近于全知基准（omniscient benchmark），表明其在实际场景中具备卓越性能；同时验证了聚类方法的理论合理性。

Conclusion: 本研究通过粗化方法实现了理论保证与实践性能的统一，为基于聚类的器官分配策略提供了坚实的理论依据，并推动了在线匹配从经验性方法向可证明高效算法的演进。

Abstract: Online matching has been a mainstay in domains such as Internet advertising and organ allocation, but practical algorithms often lack strong theoretical guarantees. We take an important step toward addressing this by developing new online matching algorithms based on a coarsening approach. Although coarsening typically implies a loss of granularity, we show that, to the contrary, aggregating offline nodes into capacitated clusters can yield near-optimal theoretical guarantees. We apply our methodology to heart transplant allocation to develop theoretically grounded policies based on structural properties of historical data. In realistic simulations, our policy closely matches the performance of the omniscient benchmark. Our work bridges the gap between data-driven heuristics and pessimistic theoretical lower bounds, and provides rigorous justification for prior clustering-based approaches in organ allocation.

</details>


### [196] [Position: Machine Learning for Heart Transplant Allocation Policy Optimization Should Account for Incentives](https://arxiv.org/abs/2602.04990)
*Ioannis Anagnostides,Itai Zilberstein,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文指出器官分配不仅是优化问题，更是一个涉及移植中心、临床医生和监管机构的复杂博弈。当前数据驱动方法忽视了激励机制的重要性，导致关键激励错配，产生负面影响。作者主张下一代分配政策应具备激励意识，并提出结合机制设计、策略分类、因果推断和社会选择的研究议程，以应对各方策略行为，确保系统稳健性、效率与公平。


<details>
  <summary>Details</summary>
Motivation: 当前器官分配系统虽向数据驱动转型，但忽视了参与方（如移植中心、医生、监管机构）之间的激励错配问题，导致政策执行效果不佳，亟需引入激励意识以提升系统整体性能。

Method: 通过分析美国成人心脏移植分配现状，识别激励错配点，结合机制设计、策略分类、因果推断与社会选择理论，构建未来分配政策研究框架。

Result: 揭示了现有分配系统中因激励不一致导致的实际负面后果，提出需将激励因素纳入算法设计的核心考量。

Conclusion: 未来的器官分配算法必须具备激励意识，融合多学科方法，才能实现真正稳健、高效且公平的系统设计。

Abstract: The allocation of scarce donor organs constitutes one of the most consequential algorithmic challenges in healthcare. While the field is rapidly transitioning from rigid, rule-based systems to machine learning and data-driven optimization, we argue that current approaches often overlook a fundamental barrier: incentives. In this position paper, we highlight that organ allocation is not merely a static optimization problem, but rather a complex game involving transplant centers, clinicians, and regulators. Focusing on US adult heart transplant allocation, we identify critical incentive misalignments across the decision-making pipeline, and present data showing that they are having adverse consequences today. Our main position is that the next generation of allocation policies should be incentive aware. We outline a research agenda for the machine learning community, calling for the integration of mechanism design, strategic classification, causal inference, and social choice to ensure robustness, efficiency, and fairness in the face of strategic behavior from the various constituent groups.

</details>


### [197] [EntRGi: Entropy Aware Reward Guidance for Diffusion Language Models](https://arxiv.org/abs/2602.05000)
*Atula Tejaswi,Litu Rout,Constantine Caramanis,Sanjay Shakkottai,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 本文研究离散扩散语言模型中的奖励引导机制，提出一种名为EntRGi的新方法，通过动态调节奖励模型的梯度，利用模型置信度对连续松弛进行调制，从而在不依赖连续松弛或直通估计的情况下提升性能。实验表明，在多个基准测试中，该方法显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在离散扩散语言模型中使用奖励引导时，要么采用连续松弛导致梯度反馈质量下降，要么使用直通估计引发错误优化，因此需要一种更可靠的方法来平衡梯度传递与模型输出的离散性。

Method: 提出EntRGi（熵感知奖励引导）机制，根据模型的置信度动态调节奖励模型产生的梯度，利用置信度信息对连续松弛进行自适应调制，从而提供更准确、可靠的梯度信号。

Result: 在70亿参数的扩散语言模型上，使用三种不同的奖励模型和三个多技能基准测试验证，EntRGi在所有任务中均表现出比当前最优方法更优的性能，证明了其有效性与鲁棒性。

Conclusion: EntRGi通过结合模型置信度与奖励梯度调节，成功克服了传统方法在离散扩散语言模型中奖励引导的局限性，为后续研究提供了新的方向。

Abstract: Reward guidance has been applied to great success in the test-time adaptation of continuous diffusion models; it updates each denoising step using the gradients from a downstream reward model. We study reward guidance for discrete diffusion language models, where one cannot differentiate through the natural outputs of the model because they are discrete tokens. Existing approaches either replace these discrete tokens with continuous relaxations, or employ techniques like the straight-through estimator. In this work, we show the downsides of both these methods. The former degrades gradient feedback because the reward model has never been trained with continuous inputs. The latter involves incorrect optimization because the gradient evaluated at discrete tokens is used to update continuous logits. Our key innovation is to go beyond this tradeoff by introducing a novel mechanism called EntRGi: Entropy aware Reward Guidance that dynamically regulates the gradients from the reward model. By modulating the continuous relaxation using the model's confidence, our approach substantially improves reward guidance while providing reliable inputs to the reward model. We empirically validate our approach on a 7B-parameter diffusion language model across 3 diverse reward models and 3 multi-skill benchmarks, showing consistent improvements over state-of-the-art methods.

</details>


### [198] [Enhanced QKNorm normalization for neural transformers with the Lp norm](https://arxiv.org/abs/2602.05006)
*Ezequiel Lopez-Rubio,Javier Montes-Perez,Esteban Jose Palomo*

Main category: cs.LG

TL;DR: 本文提出了一种QKNorm归一化方案的推广方法，基于Lp范数，允许使用非欧几里得范数。实验结果表明该方法在简单问题上具有适用性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中查询和键向量的归一化对学习稳定性至关重要。现有归一化方法存在局限性，因此需要一种更通用的归一化方案以适应不同范数的需求。

Method: 提出基于Lp范数的QKNorm归一化方法，通过引入可调节的p值，使模型能够灵活使用非欧几里得范数进行归一化处理。

Result: 实验结果显示，该方法在简单任务中表现出良好的性能和稳定性，验证了其在非欧几里得范数下的有效性。

Conclusion: 所提出的基于Lp范数的QKNorm归一化方法是一种有效的通用扩展，为Transformer中的向量归一化提供了新的灵活性。

Abstract: The normalization of query and key vectors is an essential part of the Transformer architecture. It ensures that learning is stable regardless of the scale of these vectors. Some normalization approaches are available. In this preliminary work, a generalization of the QKNorm normalization scheme is proposed. The approach is based on the Lp norm, allowing non-Euclidean norms to be employed. Experimental results demonstrate the suitability of the method for a simple problem.

</details>


### [199] [Private PoEtry: Private In-Context Learning via Product of Experts](https://arxiv.org/abs/2602.05012)
*Rob Romijnders,Mohammad Mahdi Derakhshani,Jonathan Petit,Max Welling,Christos Louizos,Yuki M. Asano*

Main category: cs.LG

TL;DR: 本文提出了一种基于Product-of-Experts模型的新型私有上下文学习（Private ICL）框架，旨在解决大语言模型在使用少量示例进行任务适应时可能泄露隐私的问题。该方法理论基础扎实，可轻松并行化，显著提升性能，在多个文本分类、数学和视觉-语言数据集上平均准确率比现有私有ICL方法提高超过30个百分点，同时保持强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私（DP）方法在私有ICL中存在计算成本高或依赖启发式策略（如上下文过采样、合成数据生成、不必要的阈值设定）导致效果有限的问题，亟需一种高效且有效的隐私保护机制。

Method: 将私有ICL重新建模为Product-of-Experts框架，利用该模型的统计特性构建具有理论保障的隐私保护机制，并支持高效并行计算。

Result: 在五个不同领域的数据集（文本分类、数学、视觉-语言）上评估表明，该方法相比先前的DP-ICL方法平均准确率提升超过30个百分点，且在隐私保护方面表现优异。

Conclusion: 所提出的基于Product-of-Experts的私有ICL框架在保证强隐私性的同时，显著提升了模型性能，具备良好的可扩展性和实际应用潜力。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to new tasks with only a small set of examples at inference time, thereby avoiding task-specific fine-tuning. However, in-context examples may contain privacy-sensitive information that should not be revealed through model outputs. Existing differential privacy (DP) approaches to ICL are either computationally expensive or rely on heuristics with limited effectiveness, including context oversampling, synthetic data generation, or unnecessary thresholding. We reformulate private ICL through the lens of a Product-of-Experts model. This gives a theoretically grounded framework, and the algorithm can be trivially parallelized. We evaluate our method across five datasets in text classification, math, and vision-language. We find that our method improves accuracy by more than 30 percentage points on average compared to prior DP-ICL methods, while maintaining strong privacy guarantees.

</details>


### [200] [Laws of Learning Dynamics and the Core of Learners](https://arxiv.org/abs/2602.05026)
*Inkee Jung,Siu Cheong Lau*

Main category: cs.LG

TL;DR: 本文提出了学习动态的基本规律，包括守恒定律和总熵减少，并基于此提出了一种基于熵的终身集成学习方法。通过构建免疫机制来防御在CIFAR-10数据集上的迁移式对抗攻击，实验表明该方法在大多数测试情况下均优于简单平均法，尤其在强扰动下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在应对迁移式对抗攻击时效果有限，亟需一种能够持续学习并有效提升鲁棒性的新方法。

Method: 提出基于熵的终身集成学习框架，结合守恒定律与熵减少规律，设计免疫机制以增强模型对对抗攻击的防御能力。

Result: 在CIFAR-10数据集上，所提出的logifold方法在多数测试中表现出更高的准确率，尤其是在强扰动条件下有显著提升。

Conclusion: 基于熵的终身集成学习方法能有效提升模型在对抗攻击下的鲁棒性，为持续学习与安全防御提供了新思路。

Abstract: We formulate the fundamental laws governing learning dynamics, namely the conservation law and the decrease of total entropy. Within this framework, we introduce an entropy-based lifelong ensemble learning method. We evaluate its effectiveness by constructing an immunization mechanism to defend against transfer-based adversarial attacks on the CIFAR-10 dataset. Compared with a naive ensemble formed by simply averaging models specialized on clean and adversarial samples, the resulting logifold achieves higher accuracy in most test cases, with particularly large gains under strong perturbations.

</details>


### [201] [Laplacian Representations for Decision-Time Planning](https://arxiv.org/abs/2602.05031)
*Dikshant Shehmar,Matthew Schlegel,Matthew E. Taylor,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文提出了一种基于拉普拉斯表示的层次化规划算法ALPS，利用其在多时间尺度上捕捉状态空间距离的能力，有效支持局部成本计算并保留长期结构，从而在离线目标条件强化学习任务中超越了传统模型自由方法的基准表现。


<details>
  <summary>Details</summary>
Motivation: 在基于模型的强化学习中，决策时间规划面临的关键挑战是状态表示需同时支持局部成本计算和长期结构保持；现有方法在长预测时域下易产生误差累积，因此需要更有效的状态表示方式。

Method: 采用拉普拉斯表示作为潜在空间，通过捕捉多时间尺度的状态空间距离，实现对长期问题的子目标分解，并减少长程预测中的误差传播；在此基础上设计了层次化规划算法ALPS。

Result: ALPS在OGBench基准上的多个离线目标条件强化学习任务中表现优于主流基线方法，证明了该方法在复杂任务中的有效性。

Conclusion: 拉普拉斯表示能够有效支持决策时间规划，其多尺度距离建模能力有助于将长期任务分解为可管理的子目标，显著提升规划性能，尤其在长时序任务中优势明显。

Abstract: Planning with a learned model remains a key challenge in model-based reinforcement learning (RL). In decision-time planning, state representations are critical as they must support local cost computation while preserving long-horizon structure. In this paper, we show that the Laplacian representation provides an effective latent space for planning by capturing state-space distances at multiple time scales. This representation preserves meaningful distances and naturally decomposes long-horizon problems into subgoals, also mitigating the compounding errors that arise over long prediction horizons. Building on these properties, we introduce ALPS, a hierarchical planning algorithm, and demonstrate that it outperforms commonly used baselines on a selection of offline goal-conditioned RL tasks from OGBench, a benchmark previously dominated by model-free methods.

</details>


### [202] [Causal Representation Meets Stochastic Modeling under Generic Geometry](https://arxiv.org/abs/2602.05033)
*Jiaxu Ren,Yixin Wang,Biwei Huang*

Main category: cs.LG

TL;DR: 本文提出了一种可识别的因果表示学习方法，用于连续时间潜在随机点过程（MUTATE），解决了在非独立同分布和连续时间动态系统中从观测数据中学习有意义因果关系的问题。该方法通过分析参数空间的几何性质来保证可识别性，并设计了时间自适应的变分自编码器框架以推断随机动力学。在模拟和真实数据实验中，MUTATE 能有效回答基因组突变积累、神经元放电触发机制等科学问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习方法多针对独立同分布或离散时间过程，但许多现实场景（如基因组突变、神经活动）涉及连续时间随机过程，亟需一种能处理此类复杂动态并具备可识别性的新方法。

Method: 提出 MUTATE 框架，采用时间自适应的过渡模块与变分自编码器结合，通过分析参数空间几何结构实现对连续时间潜在随机点过程的可识别因果表示学习。

Result: 在模拟与真实数据上，MUTATE 有效捕捉了连续时间动态，成功应用于基因组突变演化和神经元放电机制分析，验证了其科学解释能力与建模有效性。

Conclusion: MUTATE 为连续时间潜在随机过程提供了首个可识别的因果表示学习框架，显著拓展了因果发现的应用边界，尤其适用于生物、医学和神经科学等需要精确动态建模的领域。

Abstract: Learning meaningful causal representations from observations has emerged as a crucial task for facilitating machine learning applications and driving scientific discoveries in fields such as climate science, biology, and physics. This process involves disentangling high-level latent variables and their causal relationships from low-level observations. Previous work in this area that achieves identifiability typically focuses on cases where the observations are either i.i.d. or follow a latent discrete-time process. Nevertheless, many real-world settings require identifying latent variables that are continuous-time stochastic processes (e.g., multivariate point processes). To this end, we develop identifiable causal representation learning for continuous-time latent stochastic point processes. We study its identifiability by analyzing the geometry of the parameter space. Furthermore, we develop MUTATE, an identifiable variational autoencoder framework with a time-adaptive transition module to infer stochastic dynamics. Across simulated and empirical studies, we find that MUTATE can effectively answer scientific questions, such as the accumulation of mutations in genomics and the mechanisms driving neuron spike triggers in response to time-varying dynamics.

</details>


### [203] [Feedback Control for Multi-Objective Graph Self-Supervision](https://arxiv.org/abs/2602.05036)
*Karish Grover,Theodore Vasiloudis,Han Xie,Sixing Lu,Xiang Song,Christos Faloutsos*

Main category: cs.LG

TL;DR: ControlG 是一种基于控制理论的多任务图自监督学习框架，通过反馈控制机制实现不同预训练目标之间的时序分配，有效缓解了目标间干扰问题。该方法通过估计每个目标的难度和相互对抗性，利用帕累托感知的对数超体积规划器设定目标预算，并通过PID控制器进行调度，从而避免传统混合更新策略中的冲突、漂移和饥饿问题。在9个数据集上的实验表明，ControlG显著优于现有基线方法，且能提供可审计的学习调度过程，揭示各目标对学习的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前多任务图自监督学习面临目标间干扰和训练不稳定的挑战，传统方法采用每轮更新混合多个目标，导致冲突、非平稳性和某些目标被忽视等失败模式。本文认为目标协调本质上是时间分配问题，而非简单的权重调整。

Method: ControlG 采用控制理论框架，包括：1）估计每个预训练目标的难度与与其他目标的对抗性；2）使用帕累托感知的对数超体积规划器动态分配优化预算；3）通过PID控制器实现目标间的时序调度，形成反馈闭环。

Result: 在9个图数据集上，ControlG consistently 超越现有最先进方法，在性能和稳定性方面均有显著提升；同时生成可解释的学习调度日志，揭示各目标的实际贡献。

Conclusion: 多任务图自监督学习中的目标协调不应仅依赖加权混合，而应通过动态、可调控的时间分配机制实现。ControlG 通过控制理论实现了高效、稳定且可解释的多目标优化，为未来自监督学习提供了新范式。

Abstract: Can multi-task self-supervised learning on graphs be coordinated without the usual tug-of-war between objectives? Graph self-supervised learning (SSL) offers a growing toolbox of pretext objectives: mutual information, reconstruction, contrastive learning; yet combining them reliably remains a challenge due to objective interference and training instability. Most multi-pretext pipelines use per-update mixing, forcing every parameter update to be a compromise, leading to three failure modes: Disagreement (conflict-induced negative transfer), Drift (nonstationary objective utility), and Drought (hidden starvation of underserved objectives). We argue that coordination is fundamentally a temporal allocation problem: deciding when each objective receives optimization budget, not merely how to weigh them. We introduce ControlG, a control-theoretic framework that recasts multi-objective graph SSL as feedback-controlled temporal allocation by estimating per-objective difficulty and pairwise antagonism, planning target budgets via a Pareto-aware log-hypervolume planner, and scheduling with a Proportional-Integral-Derivative (PID) controller. Across 9 datasets, ControlG consistently outperforms state-of-the-art baselines, while producing an auditable schedule that reveals which objectives drove learning.

</details>


### [204] [ReFORM: Reflected Flows for On-support Offline RL via Noise Manipulation](https://arxiv.org/abs/2602.05051)
*Songyuan Zhang,Oswin So,H. M. Sabbir Ahmad,Eric Yang Yu,Matthew Cleaveland,Mitchell Black,Chuchu Fan*

Main category: cs.LG

TL;DR: ReFORM 是一种基于流策略的离线强化学习方法，通过构建受限源分布的行为克隆流策略来捕捉动作分布的支持集，并优化反射流以生成有界噪声，在保持支持集的同时最大化性能。该方法在不依赖额外环境交互的情况下，有效缓解了分布外（OOD）误差问题，同时保留了策略的表达能力。在 OGBench 基准上的 40 个挑战性任务中，使用固定超参数时，ReFORM 在性能曲线表现上全面超越所有经过手动调参的基线方法。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临两大挑战：一是分布外（OOD）误差，现有方法通过惩罚统计距离限制策略偏离行为策略，但会抑制策略改进；二是最优策略分布可能具有多模态特性，传统方法难以有效建模。尽管近期工作尝试使用扩散或流策略提升表达能力，但如何在保持表达力的同时避免 OOD 误差仍不明确。

Method: ReFORM 采用双流架构：首先训练一个基于行为克隆（BC）的流策略，其源分布被限制在有界范围内，从而确保策略支持集与行为数据一致；随后引入反射流，生成有界噪声并作用于 BC 流策略，以增强探索和性能，同时保持支持集不变。该设计通过构造保证支持集受限，实现更宽松但有效的约束机制。

Result: 在 OGBench 基准的 40 个任务上，使用统一超参数设置，ReFORM 在性能曲线上的整体表现显著优于所有经过手调超参数的基线方法，验证了其在不同数据质量下的鲁棒性和优越性。

Conclusion: ReFORM 通过构造性地施加支持集约束，成功在保持策略表达力的同时有效缓解了离线强化学习中的分布外误差问题。其无需额外环境交互、对超参数不敏感且在多种复杂任务中表现优异，为离线 RL 提供了一种高效且稳健的新范式。

Abstract: Offline reinforcement learning (RL) aims to learn the optimal policy from a fixed dataset generated by behavior policies without additional environment interactions. One common challenge that arises in this setting is the out-of-distribution (OOD) error, which occurs when the policy leaves the training distribution. Prior methods penalize a statistical distance term to keep the policy close to the behavior policy, but this constrains policy improvement and may not completely prevent OOD actions. Another challenge is that the optimal policy distribution can be multimodal and difficult to represent. Recent works apply diffusion or flow policies to address this problem, but it is unclear how to avoid OOD errors while retaining policy expressiveness. We propose ReFORM, an offline RL method based on flow policies that enforces the less restrictive support constraint by construction. ReFORM learns a behavior cloning (BC) flow policy with a bounded source distribution to capture the support of the action distribution, then optimizes a reflected flow that generates bounded noise for the BC flow while keeping the support, to maximize the performance. Across 40 challenging tasks from the OGBench benchmark with datasets of varying quality and using a constant set of hyperparameters for all tasks, ReFORM dominates all baselines with hand-tuned hyperparameters on the performance profile curves.

</details>


### [205] [Learning, Solving and Optimizing PDEs with TensorGalerkin: an efficient high-performance Galerkin assembly algorithm](https://arxiv.org/abs/2602.05052)
*Shizheng Wen,Mingyuan Chi,Tianwei Yu,Ben Moseley,Mike Yan Michelis,Pu Ren,Hao Sun,Siddhartha Mishra*

Main category: cs.LG

TL;DR: 提出了一种统一的算法框架，用于求解具有变分结构的偏微分方程（PDEs），涵盖数值求解、约束优化和物理信息学习。该框架基于伽辽金离散化，并通过一种新型高度优化且支持GPU的TensorGalerkin框架实现高效线性系统组装（刚度矩阵与载荷向量）。TensorGalerkin通过在Python层的Map阶段对单元级操作进行张量化，并利用稀疏矩阵乘法执行全局归约，完成网格诱导稀疏图上的消息传递。该方法可无缝应用于：i）高效的数值PDE求解器；ii）端到端可微的PDE约束优化框架；iii）物理信息算子学习算法。在二维和三维椭圆型、抛物型及双曲型PDEs（使用非结构化网格）的多个基准测试中，结果表明该框架在所有下游应用中均显著优于多种基线方法，展现出更高的计算效率与精度。


<details>
  <summary>Details</summary>
Motivation: 现有PDE求解与学习方法在效率和通用性方面存在局限，尤其在处理复杂几何和高维问题时性能不足。迫切需要一种统一、高效且可扩展的框架，以兼顾数值求解、优化与机器学习任务。

Method: 基于变分形式的伽辽金离散化，结合张量化元素操作与稀疏矩阵乘法的TensorGalerkin框架，实现高效刚度矩阵与载荷向量的构建。该框架支持自动微分，适用于数值求解、优化和物理信息学习三类任务。

Result: 在多种2D/3D PDE类型（椭圆、抛物、双曲）的非结构化网格上，相比基线方法，本框架在计算效率和精度方面均有显著提升，验证了其在不同应用场景下的有效性与鲁棒性。

Conclusion: 所提出的统一框架通过TensorGalerkin实现了高效、可微、可扩展的PDE求解与学习能力，为复杂物理系统的建模与优化提供了强有力的技术支持。

Abstract: We present a unified algorithmic framework for the numerical solution, constrained optimization, and physics-informed learning of PDEs with a variational structure. Our framework is based on a Galerkin discretization of the underlying variational forms, and its high efficiency stems from a novel highly-optimized and GPU-compliant TensorGalerkin framework for linear system assembly (stiffness matrices and load vectors). TensorGalerkin operates by tensorizing element-wise operations within a Python-level Map stage and then performs global reduction with a sparse matrix multiplication that performs message passing on the mesh-induced sparsity graph. It can be seamlessly employed downstream as i) a highly-efficient numerical PDEs solver, ii) an end-to-end differentiable framework for PDE-constrained optimization, and iii) a physics-informed operator learning algorithm for PDEs. With multiple benchmarks, including 2D and 3D elliptic, parabolic, and hyperbolic PDEs on unstructured meshes, we demonstrate that the proposed framework provides significant computational efficiency and accuracy gains over a variety of baselines in all the targeted downstream applications.

</details>


### [206] [Quantile-Physics Hybrid Framework for Safe-Speed Recommendation under Diverse Weather Conditions Leveraging Connected Vehicle and Road Weather Information Systems Data](https://arxiv.org/abs/2602.05053)
*Wen Zhang,Adel W. Sadek,Chunming Qiao*

Main category: cs.LG

TL;DR: 本研究提出一种混合预测框架，基于高分辨率车联网（CV）和道路天气信息系统（RWIS）数据，实时推荐高速公路在不同天气条件下的安全速度区间。利用2022至2023年布法罗地区超过660万条记录构建时空对齐数据集，采用分位数回归森林（QRF）模型在10分钟时间窗口内估计车速分布，输入26个气象、路面和时间特征。通过物理模型计算实时路面对抗力与能见度下的最大安全限速，并结合交通标志限速与物理上限生成最终推荐速度区间。实验表明，该模型预测精度高，平均绝对误差仅1.55英里/小时，96.43%的中位数预测值在5英里/小时以内，50%预测区间覆盖率（PICP）达48.55%，且在多种天气和路段上具有良好的泛化能力，具备实际部署潜力，有助于提升交通安全、减少天气相关事故。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气会显著影响驾驶员视野和轮胎-路面摩擦力，导致交通事故风险上升，因此亟需根据实时天气条件动态调整安全行驶速度。传统固定限速无法适应变化环境，需要更智能、可实时响应的动态速度建议系统。

Method: 构建融合高分辨率车联网（CV）与道路天气信息（RWIS）的时空对齐数据集；采用分位数回归森林（QRF）建模车辆速度分布；引入基于物理的上界限速约束（考虑实时路面附着系数与能见度）；最终将QRF输出的分位数、法定限速与物理上限融合，生成安全速度区间推荐。

Result: QRF模型实现平均绝对误差1.55英里/小时，96.43%的中位数预测在5英里/小时误差范围内，50%预测区间覆盖率（PICP）为48.55%，模型在不同天气类型和道路段落间表现出良好泛化能力，验证了其在真实场景中的可行性与有效性。

Conclusion: 所提出的混合预测框架能够有效响应复杂多变的天气条件，提供精准、安全、可解释的实时速度建议，具有较高的实用价值，可显著降低由天气引发的交通事故，推动智慧交通系统的安全升级。

Abstract: Inclement weather conditions can significantly impact driver visibility and tire-road surface friction, requiring adjusted safe driving speeds to reduce crash risk. This study proposes a hybrid predictive framework that recommends real-time safe speed intervals for freeway travel under diverse weather conditions. Leveraging high-resolution Connected Vehicle (CV) data and Road Weather Information System (RWIS) data collected in Buffalo, NY, from 2022 to 2023, we construct a spatiotemporally aligned dataset containing over 6.6 million records across 73 days. The core model employs Quantile Regression Forests (QRF) to estimate vehicle speed distributions in 10-minute windows, using 26 input features that capture meteorological, pavement, and temporal conditions. To enforce safety constraints, a physics-based upper speed limit is computed for each interval based on real-time road grip and visibility, ensuring that vehicles can safely stop within their sight distance. The final recommended interval fuses QRF-predicted quantiles with both posted speed limits and the physics-derived upper bound. Experimental results demonstrate strong predictive performance: the QRF model achieves a mean absolute error of 1.55 mph, with 96.43% of median speed predictions within 5 mph, a PICP (50%) of 48.55%, and robust generalization across weather types. The model's ability to respond to changing weather conditions and generalize across road segments shows promise for real-world deployment, thereby improving traffic safety and reducing weather-related crashes.

</details>


### [207] [Reliable Explanations or Random Noise? A Reliability Metric for XAI](https://arxiv.org/abs/2602.05082)
*Poushali Sengupta,Sabita Maharjan,Frank Eliassen,Shashi Raj Pandey,Yan Zhang*

Main category: cs.LG

TL;DR: 本文提出解释可靠性指数（ERI），用于衡量复杂机器学习模型解释在真实场景下的稳定性，揭示了SHAP、IG等常用方法在输入微小扰动、特征冗余、模型更新等情况下的不稳定性，并通过ERI-T和ERI-Bench系统评估解释可靠性，推动更可信的可解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能（XAI）方法在高风险领域中广泛应用，但其解释的可靠性尚未得到充分评估。现有方法如SHAP和集成梯度（IG）虽有理论基础，但在实际部署中对输入扰动、特征相关性及模型微调等常见变化敏感，导致解释不稳定，影响可信度。因此需要一套量化解释稳定性的标准体系。

Method: 提出解释可靠性指数（ERI），包含四个可靠性公理：对小输入扰动的鲁棒性、特征冗余下的一致性、模型演进中的平滑性以及对轻微分布偏移的韧性；为每个公理提供形式化保证，包括Lipschitz型边界和时间稳定性结果；设计ERI-T用于序列模型的时间可靠性评估，构建ERI-Bench基准以系统测试解释稳定性。

Result: 实验表明，主流解释方法在真实场景下普遍存在可靠性缺陷，解释结果在微小变化下显著波动；ERI能够有效识别并量化这些不稳定性，为评估与改进解释系统的可靠性提供了工具与框架。

Conclusion: ERI为解释可靠性提供了可量化的评估标准，有助于发现和修复解释过程中的不稳定性问题，从而提升可解释AI在实际应用中的可信度与稳健性。

Abstract: In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.

</details>


### [208] [Individual Fairness In Strategic Classification](https://arxiv.org/abs/2602.05084)
*Zhiqun Zuo,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 本文研究了战略分类中的个体公平性问题，证明确定性阈值分类器违反个体公平性，并提出使用随机化分类器来实现个体公平性。通过线性规划求解最优且个体公平的随机化分类器，实验表明该方法能有效缓解不公平并改善公平性与准确率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注群体公平性，而个体公平性在战略分类中仍被忽视，亟需解决。

Method: 分析阈值分类器，推导随机化分类器满足个体公平性的条件，并通过线性规划优化求解最优分类器。

Result: 所提方法在真实数据集上验证了其有效性，显著提升了公平性与准确率的平衡。

Conclusion: 随机化分类器可在战略分类中实现个体公平性，且可通过线性规划优化，同时可扩展至群体公平性场景。

Abstract: Strategic classification, where individuals modify their features to influence machine learning (ML) decisions, presents critical fairness challenges. While group fairness in this setting has been widely studied, individual fairness remains underexplored. We analyze threshold-based classifiers and prove that deterministic thresholds violate individual fairness. Then, we investigate the possibility of using a randomized classifier to achieve individual fairness. We introduce conditions under which a randomized classifier ensures individual fairness and leverage these conditions to find an optimal and individually fair randomized classifier through a linear programming problem. Additionally, we demonstrate that our approach can be extended to group fairness notions. Experiments on real-world datasets confirm that our method effectively mitigates unfairness and improves the fairness-accuracy trade-off.

</details>


### [209] [Unbiased Single-Queried Gradient for Combinatorial Objective](https://arxiv.org/abs/2602.05119)
*Thanawat Sornwanee*

Main category: cs.LG

TL;DR: 本文提出一种无偏的随机梯度方法，仅需一次组合函数查询即可实现超立方体上的优化，适用于概率重构组合问题，包含REINFORCE及一类新梯度。


<details>
  <summary>Details</summary>
Motivation: 在组合问题的概率重述中，优化常涉及超立方体上的伯努利参数，但精确梯度计算需要多次查询，效率低。

Method: 提出一种仅需单次查询的无偏随机梯度，通过重要性采样将REINFORCE纳入其中，并扩展出新类梯度。

Result: 该方法有效降低计算复杂度，同时保持梯度无偏性，适用于多种组合优化场景。

Conclusion: 所提方法显著提升组合优化中梯度估计的效率与实用性，为概率型组合问题提供高效求解路径。

Abstract: In a probabilistic reformulation of a combinatorial problem, we often face an optimization over a hypercube, which corresponds to the Bernoulli probability parameter for each binary variable in the primal problem. The combinatorial nature suggests that an exact gradient computation requires multiple queries. We propose a stochastic gradient that is unbiased and requires only a single query of the combinatorial function. This method encompasses a well-established REINFORCE (through an importance sampling), as well as including a class of new stochastic gradients.

</details>


### [210] [Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks](https://arxiv.org/abs/2602.05125)
*William F. Shen,Xinchi Qiu,Chenxi Whitehouse,Lisa Alazraki,Shashwat Goel,Francesco Barbieri,Timon Willi,Akhil Mathur,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 本文提出RRD框架，通过递归分解-过滤循环实现对评分标准（rubric）的精细化优化。该方法能有效提升评分标准的覆盖性、区分度与非冗余性，显著改善大模型在主观偏好判断中的准确性，并在强化学习微调中生成更优奖励信号。实验表明，RRD在多项基准测试中表现卓越，最高提升达17.7分；在奖励建模中使奖励值提升高达160%，且效果可迁移至多个下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有评分标准在生成过程中存在覆盖不足、维度混淆、偏好方向错位及冗余相关等问题，严重影响大模型评判准确性和强化学习微调中的奖励质量。因此亟需一种系统化、可控制的方法来优化评分标准。

Method: 提出基于递归分解-过滤循环的RRD框架：首先将粗粒度评分标准分解为细粒度、高区分度的标准以扩大覆盖范围；再通过互补过滤机制剔除不一致和冗余标准；最后采用相关性感知加权策略避免高度相关标准被过度强调，从而构建出信息丰富、全面且无冗余的评分标准集。

Result: RRD在JudgeBench和PPE等评测数据集上显著提升了GPT-4o和Llama3.1-405B等模型的偏好判断准确率，最高提升17.7点；在WildChat上的强化学习微调中，奖励值提升达160%（Qwen3-4B）和60%（Llama3.1-8B），优于以往基线10-20%；且性能增益可泛化至HealthBench-Hard和BiGGen Bench。

Conclusion: RRD建立了递归评分标准精炼的可扩展、可解释范式，为开放领域中大模型的评判与奖励建模提供了坚实基础，具有广泛的应用前景。

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

</details>


### [211] [Adaptive Exploration for Latent-State Bandits](https://arxiv.org/abs/2602.05139)
*Jikai Jin,Kenneth Hung,Sanath Kumar Krishnamurthy,Baoyi Shi,Congshan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一类无需显式状态建模的多臂赌博机算法，通过利用滞后上下文特征和协调探测策略来隐式追踪潜在状态，解决隐藏混淆因子导致的奖励估计偏差和状态信息不足问题。该方法在非平稳奖励环境下表现出良好的适应性与计算效率，在多种场景中优于经典方法，为实际应用中的算法选择提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 经典多臂赌博机算法在存在隐藏、时变状态的环境中表现不佳，因未观测到的混淆因子导致奖励估计偏差和最优动作选择困难。需要一种无需显式建模状态但能有效应对非平稳性和隐藏状态的方法。

Method: 引入基于滞后上下文特征和协调探测策略的状态模型无关算法，通过隐式跟踪潜在状态，分离出依赖状态的奖励模式，实现对最优策略的学习而无需显式状态建模。

Result: 在多种不同设置下，所提方法均显著优于传统算法；具备良好的适应性、计算效率和鲁棒性，适用于现实世界中的动态环境。

Conclusion: 无需显式状态建模的新型算法能够有效应对隐藏混淆因子和非平稳奖励，是解决复杂现实决策问题的有效方案，推荐根据具体应用场景选择合适算法。

Abstract: The multi-armed bandit problem is a core framework for sequential decision-making under uncertainty, but classical algorithms often fail in environments with hidden, time-varying states that confound reward estimation and optimal action selection. We address key challenges arising from unobserved confounders, such as biased reward estimates and limited state information, by introducing a family of state-model-free bandit algorithms that leverage lagged contextual features and coordinated probing strategies. These implicitly track latent states and disambiguate state-dependent reward patterns. Our methods and their adaptive variants can learn optimal policies without explicit state modeling, combining computational efficiency with robust adaptation to non-stationary rewards. Empirical results across diverse settings demonstrate superior performance over classical approaches, and we provide practical recommendations for algorithm selection in real-world applications.

</details>


### [212] [Fairness Under Group-Conditional Prior Probability Shift: Invariance, Drift, and Target-Aware Post-Processing](https://arxiv.org/abs/2602.05144)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 本文研究了在群体条件先验概率偏移（GPPS）下的公平性问题，即标签流行度在不同群体间发生变化但特征生成过程保持稳定的情况。研究发现，基于误差率的公平性标准（如等效几率）在GPPS下具有结构不变性，而基于接受率的标准（如人口均等）则会漂移且无法避免（不可行性）。此外，研究证明仅通过源标签和未标记的目标数据即可识别目标域的风险与公平性指标，并提出了TAP-GPPS算法，该算法无需标签即可估计流行度、校正后验概率并选择阈值以实现目标域的人口均等。实验验证了理论预测，并表明TAP-GPPS在最小化效用损失的前提下实现了目标域的公平性。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统通常在历史数据上训练和评估公平性，但部署时环境已发生变化，尤其是不同群体的正向结果流行度变化不一致。这种变化可能导致现有公平性方法失效，因此需要研究在条件先验概率偏移下的公平性鲁棒性及应对策略。

Method: 分析GPPS下的公平性行为，利用ROC量的不变性进行无标签目标域指标识别，提出TAP-GPPS算法，通过估计目标域流行度、校正后验概率并优化分类阈值来实现目标域的公平性。

Result: 证明了等效几率类公平性在GPPS下不变，而人口均等类公平性不可避免地发生漂移；展示了仅使用源标签和未标记目标数据即可准确估计目标域风险与公平性；实验表明TAP-GPPS能有效实现目标域的公平性，同时保持较高的模型效用。

Conclusion: 在群体条件先验概率偏移下，基于误差率的公平性准则具有内在稳定性，而基于接受率的准则则面临不可规避的漂移。通过利用ROC不变性，可无需目标标签即可实现目标域公平性评估与修正。提出的TAP-GPPS算法在实际中表现良好，为解决现实部署中的公平性挑战提供了有效工具。

Abstract: Machine learning systems are often trained and evaluated for fairness on historical data, yet deployed in environments where conditions have shifted. A particularly common form of shift occurs when the prevalence of positive outcomes changes differently across demographic groups--for example, when disease rates rise faster in one population than another, or when economic conditions affect loan default rates unequally. We study group-conditional prior probability shift (GPPS), where the label prevalence $P(Y=1\mid A=a)$ may change between training and deployment while the feature-generation process $P(X\mid Y,A)$ remains stable. Our analysis yields three main contributions. First, we prove a fundamental dichotomy: fairness criteria based on error rates (equalized odds) are structurally invariant under GPPS, while acceptance-rate criteria (demographic parity) can drift--and we prove this drift is unavoidable for non-trivial classifiers (shift-robust impossibility). Second, we show that target-domain risk and fairness metrics are identifiable without target labels: the invariance of ROC quantities under GPPS enables consistent estimation from source labels and unlabeled target data alone, with finite-sample guarantees. Third, we propose TAP-GPPS, a label-free post-processing algorithm that estimates prevalences from unlabeled data, corrects posteriors, and selects thresholds to satisfy demographic parity in the target domain. Experiments validate our theoretical predictions and demonstrate that TAP-GPPS achieves target fairness with minimal utility loss.

</details>


### [213] [TIDE: Temporal Incremental Draft Engine for Self-Improving LLM Inference](https://arxiv.org/abs/2602.05145)
*Jiyoung Park,Hankyu Jang,Changseok Song,Wookeun Jung*

Main category: cs.LG

TL;DR: TIDE是一个原生服务于大语言模型推理系统的框架，通过将在线草稿适应直接集成到高性能推理系统中，利用推理过程中生成的目标模型隐藏状态作为训练信号，实现零开销的草稿适应。它采用自适应运行时控制，在有益时才激活推测和训练，并通过异构集群映射解耦的推理与训练任务至合适的GPU类别。在多种真实工作负载下，TIDE相较于静态推测解码实现了最高1.15倍的吞吐量提升，同时将草稿训练时间减少了1.67倍。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码虽能显著加速大语言模型推理，但在实际应用中面临工作负载动态变化和系统级约束的挑战，亟需一种高效、自适应的解决方案来持续优化性能。

Method: TIDE通过复用推理过程中的目标模型隐藏状态作为训练信号，实现零开销的草稿适应；引入自适应运行时控制机制，仅在有利时机激活推测与训练；并利用异构集群特性，将推理与训练任务分配给适合的GPU类型，以实现资源高效利用。

Result: 在多种真实世界工作负载下，TIDE相比静态推测解码最高提升了1.15倍的吞吐量，同时将草稿训练时间减少1.67倍，显著优于需要重新计算训练信号的方法。

Conclusion: TIDE成功将在线草稿适应融入高性能大语言模型推理系统，通过零开销训练信号复用和智能资源调度，实现了高效、自适应的推理加速，为复杂动态环境下的大模型服务提供了实用且可扩展的解决方案。

Abstract: Speculative decoding can substantially accelerate LLM inference, but realizing its benefits in practice is challenging due to evolving workloads and system-level constraints. We present TIDE (Temporal Incremental Draft Engine), a serving-engine-native framework that integrates online draft adaptation directly into high-performance LLM inference systems. TIDE reuses target model hidden states generated during inference as training signals, enabling zero-overhead draft adaptation without reloading the target model, and employs adaptive runtime control to activate speculation and training only when beneficial. TIDE exploits heterogeneous clusters by mapping decoupled inference and training to appropriate GPU classes. Across diverse real-world workloads, TIDE achieves up to 1.15x throughput improvement over static speculative decoding while reducing draft training time by 1.67x compared to approaches that recompute training signals.

</details>


### [214] [Cross-talk based multi-task learning for fault classification of physically coupled machine system](https://arxiv.org/abs/2602.05146)
*Wonjun Yi,Rismaya Kumar Mishra,Yong-Hwa Park*

Main category: cs.LG

TL;DR: 本文提出一种基于交叉对话结构的多任务学习框架，利用机器系统中故障状态与物理变量间的天然耦合关系，通过联合学习故障分类与相关物理属性提升分类性能。相比共享主干架构，该方法通过可控的信息交换避免负迁移，尤其在无人机故障和电机复合故障数据集上表现优异，且在单通道与多通道输入下均优于单任务模型、多类别合并模型及共享主干多任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有故障分类研究多依赖直接故障标签，但实际信号中故障条件与其他物理变量存在天然耦合，这些隐含信息可提升分类性能；因此需要一种能有效利用这种耦合关系的建模方法。

Method: 采用基于残差神经维度缩减器的交叉对话多任务学习架构，通过交叉层实现任务间可控信息交换，避免特征混淆与负迁移，并应用于无人机故障和电机复合故障两个具有显著物理耦合特性的基准数据集。

Result: 在两个基准数据集上，所提方法在单通道和多通道输入条件下均显著优于单任务模型、多类别合并模型以及共享主干多任务模型，证明了其在利用物理耦合信息方面的有效性。

Conclusion: 通过引入交叉对话结构的多任务学习框架，能够有效挖掘信号中故障与物理变量之间的耦合信息，显著提升故障分类性能，尤其适用于具有复杂物理耦合特征的工业场景。

Abstract: Machine systems inherently generate signals in which fault conditions and various physical variables are physically coupled. Although many existing fault classification studies rely solely on direct fault labels, the aforementioned signals naturally embed additional information shaped by other physically coupled information. Herein, we leverage this coupling through a multi-task learning (MTL) framework that jointly learns fault conditions and the related physical variables. Among MTL architectures, crosstalk structures have distinct advantages because they allow for controlled information exchange between tasks through the cross-talk layer while preventing negative transfer, in contrast to shared trunk architectures that often mix incompatible features. We build on our previously introduced residual neural dimension reductor model, and extend its application to two benchmarks where physical coupling is prominent. The first benchmark is a drone fault dataset, in which machine type and maneuvering direction significantly alter the frequency components of measured signals even under the same nominal condition. By learning fault classification together with these physical attributes, the cross-talk architecture can better classify faults. The second benchmark dataset is the motor compound fault dataset. In this system, each fault component, inner race fault, outer race fault, misalignment, and unbalance is coupled to the other. For motor compound fault, we also test classification performance when we use single-channel data or multi-channel data as input to the classifier. Across both benchmarks, our residual neural dimension reductor, consistently outperformed single-task models, multi-class models that merge all label combinations, and shared trunk multi-task models.

</details>


### [215] [CoSA: Compressed Sensing-Based Adaptation of Large Language Models](https://arxiv.org/abs/2602.05148)
*Songtao Wei,Yi Li,Bohan Zhang,Zhichun Guo,Ying Huang,Yuede Ji,Miao Yin,Guanpeng Li,Bingzhe Li*

Main category: cs.LG

TL;DR: CoSA 是一种基于压缩感知理论的参数高效微调方法，通过固定随机投影矩阵和可学习核心来表达权重更新，突破了传统低秩假设的限制，实现了更高效的多尺度模型适应。在10个不同任务上对多个规模的模型（如RoBERTa、Llama、Qwen）进行评估，结果表明CoSA在性能上与或优于现有最先进的PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA和PiSSA）依赖于权重更新的低秩分解，但这种假设可能限制模型在特定任务中的表达能力，特别是在奇异值分布较均匀的情况下。因此，需要一种更具表达力且高效的微调方法。

Method: 提出CoSA方法，利用压缩感知理论，将权重更新表示为固定随机投影矩阵与紧凑可学习核心的组合，通过随机投影实现高维权重更新的低维编码与重建。

Result: 在10个多样化的任务（包括自然语言理解与生成）上，使用5种不同规模的模型（来自RoBERTa、Llama、Qwen系列），CoSA在所有设置中均达到或超过当前最优PEFT方法的性能表现。

Conclusion: CoSA为参数高效微调提供了一种有理论支撑的新视角，能够在保持高效性的同时显著提升模型的表达能力，适用于多种规模和任务场景下的模型适应。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.

</details>


### [216] [Position: Capability Control Should be a Separate Goal From Alignment](https://arxiv.org/abs/2602.05164)
*Shoaib Ahmed Siddiqui,Eleni Triantafillou,David Krueger,Adrian Weller*

Main category: cs.LG

TL;DR: 该论文提出将能力控制（capability control）作为与对齐（alignment）并列的独立目标，强调对模型行为施加硬性限制以防止滥用和失败。作者将能力控制机制分为三个层面：数据层、学习层和系统层，并主张采用纵深防御策略，综合运用多层控制。同时指出知识的双重用途和组合泛化等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型因在广泛数据上训练而具备通用能力，但也增加了被误用和失效的风险。现有对齐方法无法充分应对这些风险，因此需要引入专门的能力控制机制来施加硬性行为限制，尤其是在对抗性诱导下。

Method: 将能力控制机制按模型生命周期划分为三个层次：(i) 数据层面通过控制训练分布实现；(ii) 学习层面通过权重或表示层干预实现；(iii) 系统层面通过部署后输入、输出和动作的防护机制实现。倡导跨层次组合使用，形成纵深防御体系。

Result: 单一层次的能力控制存在固有缺陷，组合使用多层控制可更有效防范模型滥用和失败。然而，仍面临知识双重用途、组合泛化等开放性挑战。

Conclusion: 应将能力控制视为独立于对齐的核心目标，构建覆盖全生命周期的多层次、互补式控制体系，以增强模型的安全性和可控性。

Abstract: Foundation models are trained on broad data distributions, yielding generalist capabilities that enable many downstream applications but also expand the space of potential misuse and failures. This position paper argues that capability control -- imposing restrictions on permissible model behavior -- should be treated as a distinct goal from alignment. While alignment is often context and preference-driven, capability control aims to impose hard operational limits on permissible behaviors, including under adversarial elicitation. We organize capability control mechanisms across the model lifecycle into three layers: (i) data-based control of the training distribution, (ii) learning-based control via weight- or representation-level interventions, and (iii) system-based control via post-deployment guardrails over inputs, outputs, and actions. Because each layer has characteristic failure modes when used in isolation, we advocate for a defense-in-depth approach that composes complementary controls across the full stack. We further outline key open challenges in achieving such control, including the dual-use nature of knowledge and compositional generalization.

</details>


### [217] [EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization](https://arxiv.org/abs/2602.05165)
*Kevin Han,Yuhang Zhou,Mingze Gao,Gedi Zhou,Serena Li,Abhishek Kumar,Xiangjun Fan,Weiwei Li,Lizhu Zhang*

Main category: cs.LG

TL;DR: 提出EBPO框架，通过借用策略累积的全局统计信息来正则化局部组基线，解决GRPO在小样本和失败场景下的方差高、梯度消失问题。理论证明其均方误差更低、熵衰减有界、失败时惩罚信号不消失；实验显示在AIME、OlympiadBench等基准上优于GRPO及其他基线，尤其在小组规模下表现更稳定，并受益于难度分层课程学习。


<details>
  <summary>Details</summary>
Motivation: GRPO在小组规模受限时方差高，在所有响应得分为零的饱和失败场景中梯度信号消失，导致训练不稳定，需改进基线估计方式以提升鲁棒性与性能。

Method: 采用基于经验贝叶斯的收缩估计器，将局部组统计与通过Welford在线算法更新的全局先验相结合，动态平衡局部与全局信息，实现对基线的正则化。

Result: EBPO在多个基准测试中持续优于GRPO及其他基线，特别是在小组规模下表现更优，训练更稳定，且在难度分层课程学习中收益显著。

Conclusion: EBPO通过引入全局统计信息的正则化机制，有效缓解了现有RLVR方法的稳定性问题，为大语言模型的强化学习提供了更可靠、高效的优化路径。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for enhancing the reasoning capabilities of Large Language Models (LLMs). However, dominant approaches like Group Relative Policy Optimization (GRPO) face critical stability challenges: they suffer from high estimator variance under computational constraints (small group sizes) and vanishing gradient signals in saturated failure regimes where all responses yield identical zero rewards. To address this, we propose Empirical Bayes Policy Optimization (EBPO), a novel framework that regularizes local group-based baselines by borrowing strength from the policy's accumulated global statistics. Instead of estimating baselines in isolation, EBPO employs a shrinkage estimator that dynamically balances local group statistics with a global prior updated via Welford's online algorithm. Theoretically, we demonstrate that EBPO guarantees strictly lower Mean Squared Error (MSE), bounded entropy decay, and non-vanishing penalty signals in failure scenarios compared to GRPO. Empirically, EBPO consistently outperforms GRPO and other established baselines across diverse benchmarks, including AIME and OlympiadBench. Notably, EBPO exhibits superior training stability, achieving high-performance gains even with small group sizes, and benefits significantly from difficulty-stratified curriculum learning.

</details>


### [218] [Benchmarking Artificial Intelligence Models for Daily Coastal Hypoxia Forecasting](https://arxiv.org/abs/2602.05178)
*Magesh Rajasekaran,Md Saiful Sajol,Chris Alvin,Supratik Mukhopadhyay,Yanda Ou,Z. George Xue*

Main category: cs.LG

TL;DR: 本研究比较了四种深度学习架构（BiLSTM、Medformer、ST-Transformer、TCN）在每日缺氧分类中的表现，利用2009-2020年耦合水动力-生物地球化学模型的逐日模拟数据训练，并以2020-2024年数据进行测试。模型融合了水柱分层、沉积物耗氧及温度依赖分解速率等关键因子。ST-Transformer表现最优，AUC-ROC达0.982–0.992，具备高精度与强判别能力。研究提出可复现的实时缺氧预测框架，支持生态系统管理与韧性建设，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 传统季节性模型对墨西哥湾北部沿海缺氧现象的预报精度不足，难以满足日常生态管理对细尺度变化的需求，亟需更精准、实时的预测方法。

Method: 采用四种深度学习模型（BiLSTM、Medformer、ST-Transformer、TCN），基于2009–2020年逐日模拟数据训练，使用2020–2024年数据测试；统一数据预处理、输入输出格式与验证流程；引入水柱分层、沉积物耗氧和温度依赖分解率等环境因子构建分类模型；通过McNemar检验评估模型预测差异的统计显著性。

Result: 所有模型均表现出高分类准确率与强判别能力，其中ST-Transformer在所有指标和测试时段中表现最佳，AUC-ROC达到0.982–0.992，显著优于其他模型。

Conclusion: 本研究建立了一个可复现的实时缺氧预测框架，为环境与海洋建模社区提供技术支持，有助于提升生态系统管理的响应能力与韧性，推动人工智能在海洋生态监测中的应用。

Abstract: Coastal hypoxia, especially in the northern part of Gulf of Mexico, presents a persistent ecological and economic concern. Seasonal models offer coarse forecasts that miss the fine-scale variability needed for daily, responsive ecosystem management. We present study that compares four deep learning architectures for daily hypoxia classification: Bidirectional Long Short-Term Memory (BiLSTM), Medformer (Medical Transformer), Spatio-Temporal Transformer (ST-Transformer), and Temporal Convolutional Network (TCN). We trained our models with twelve years of daily hindcast data from 2009-2020 Our training data consists of 2009-2020 hindcast data from a coupled hydrodynamic-biogeochemical model. Similarly, we use hindcast data from 2020 through 2024 as a test data. We constructed classification models incorporating water column stratification, sediment oxygen consumption, and temperature-dependent decomposition rates. We evaluated each architectures using the same data preprocessing, input/output formulation, and validation protocols. Each model achieved high classification accuracy and strong discriminative ability with ST-Transformer achieving the highest performance across all metrics and tests periods (AUC-ROC: 0.982-0.992). We also employed McNemar's method to identify statistically significant differences in model predictions. Our contribution is a reproducible framework for operational real-time hypoxia prediction that can support broader efforts in the environmental and ocean modeling systems community and in ecosystem resilience. The source code is available https://github.com/rmagesh148/hypoxia-ai/

</details>


### [219] [SpectraKAN: Conditioning Spectral Operators](https://arxiv.org/abs/2602.05187)
*Chun-Wun Cheng,Carola-Bibiane Schönlieb,Angelica I. Aviles-Rivero*

Main category: cs.LG

TL;DR: SpectraKAN 是一种新型神经算子，通过将谱运算符基于输入条件化，实现动态的多尺度、各向异性系统行为建模。其核心是利用输入的历史信息生成全局表示，并通过单查询交叉注意力调节多尺度傅里叶主干，从而在保持频域高效混合的同时实现自适应运算。理论分析表明该方法在网格细化下收敛为分辨率无关的连续算子，且使用 KAN 实现平滑、利普希茨可控的调制。在多种 PDE 基准测试中，SpectraKAN 显著优于现有基线，最大可降低 49% 的 RMSE，尤其在复杂时空预测任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有谱神经算子（如 FNO）依赖静态傅里叶核，无法捕捉多尺度、状态依赖和各向异性的动态行为，限制了对复杂系统建模的能力。因此需要一种能够根据输入动态调整谱响应的机制。

Method: 提出 SpectraKAN，通过提取输入的时空历史紧凑全局表示，使用单查询交叉注意力来调节多尺度傅里叶主干，实现输入条件化的谱运算。该方法将静态卷积转化为输入驱动的积分算子，同时保持频域高效性。理论分析支持其在网格细化下的收敛性与平滑调制特性。

Result: 在多个不同类型的偏微分方程基准测试中，SpectraKAN 实现了最先进的性能，平均降低 49% 的均方根误差（RMSE），特别是在复杂时空预测任务中表现出显著优势。

Conclusion: SpectraKAN 通过引入输入条件化的谱调制机制，有效提升了神经算子对复杂、非平稳系统动态的建模能力，兼具高精度与计算效率，为学习偏微分方程解算子提供了新的范式。

Abstract: Spectral neural operators, particularly Fourier Neural Operators (FNO), are a powerful framework for learning solution operators of partial differential equations (PDEs) due to their efficient global mixing in the frequency domain. However, existing spectral operators rely on static Fourier kernels applied uniformly across inputs, limiting their ability to capture multi-scale, regime-dependent, and anisotropic dynamics governed by the global state of the system. We introduce SpectraKAN, a neural operator that conditions the spectral operator on the input itself, turning static spectral convolution into an input-conditioned integral operator. This is achieved by extracting a compact global representation from spatio-temporal history and using it to modulate a multi-scale Fourier trunk via single-query cross-attention, enabling the operator to adapt its behaviour while retaining the efficiency of spectral mixing. We provide theoretical justification showing that this modulation converges to a resolution-independent continuous operator under mesh refinement and KAN gives smooth, Lipschitz-controlled global modulation. Across diverse PDE benchmarks, SpectraKAN achieves state-of-the-art performance, reducing RMSE by up to 49% over strong baselines, with particularly large gains on challenging spatio-temporal prediction tasks.

</details>


### [220] [Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs](https://arxiv.org/abs/2602.05191)
*Wentao Ni,Kangqi Zhang,Zhongming Yu,Oren Nelson,Mingu Lee,Hong Cai,Fatih Porikli,Jongryool Kim,Zhijian Liu,Jishen Zhao*

Main category: cs.LG

TL;DR: Double-P 是一种分层稀疏注意力框架，通过两级 top-p 优化，在保持近似零精度损失的同时，显著降低注意力计算开销，实现高达1.8倍的计算节省和1.3倍的端到端解码加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理中，注意力在不断增长的键值缓存上成为主要瓶颈，现有固定预算的 top-k 稀疏注意力无法适应不同头和层之间的异构注意力分布，而 top-p 方法虽能保留注意力质量但未兼顾准确性、选择开销和稀疏注意力成本的联合优化，限制了整体效率。

Method: Double-P 采用分层策略：首先在聚类级别使用加权质心进行粗粒度 top-p 估计，再通过第二级 top-p 自适应地仅在必要时分配词元级注意力，从而协同优化精度、选择开销与计算成本。

Result: 在多个长上下文基准测试中，Double-P 均实现近乎零的精度下降，注意力计算开销减少最多达1.8倍，并带来最高1.3倍的端到端解码速度提升，优于当前最先进的固定预算稀疏注意力方法。

Conclusion: Double-P 通过分层 top-p 策略有效解决了稀疏注意力中的多目标优化难题，在保证高精度的同时大幅提升推理效率，是面向大规模语言模型长上下文推理的高效解决方案。

Abstract: As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.

</details>


### [221] [Extreme Weather Nowcasting via Local Precipitation Pattern Prediction](https://arxiv.org/abs/2602.05204)
*Changhoon Song,Teng Yuan Chang,Youngjoon Hong*

Main category: cs.LG

TL;DR: 本文提出了一种高效的确定性框架exPreCast，用于生成高精度的雷达降水预报，并构建了一个平衡的韩国气象厅（KMA）雷达数据集，涵盖普通降雨和极端降雨事件。该模型结合局部时空注意力、纹理保持的立方双上采样解码器和时间提取器，可灵活调整预报时长。在SEVIR、MeteoNet及KMA数据集上的实验表明，该方法在正常和极端降雨条件下均达到先进水平，兼具准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前降水临近预报仍面临空间局部性显著、精细雨结构复杂以及预报时长多变等挑战。现有基于扩散的生成集成模型计算成本高，难以实时应用；而确定性模型虽高效但对正常降雨存在偏差。此外，常用基准数据集存在偏差，或以普通降雨为主，或仅包含极端降雨事件，限制了模型在真实场景中的泛化能力。因此亟需一种高效且能兼顾极端与普通降雨的预报框架及更均衡的数据支持。

Method: exPreCast采用确定性架构，引入局部时空注意力机制捕捉关键时空特征；设计纹理保持的立方双上采样解码器以增强细节重建；结合时间提取器实现对不同预报时长的灵活适应。同时，构建并公开一个来自韩国气象厅（KMA）的平衡雷达数据集，覆盖普通与极端降雨事件，提升训练与评估的代表性。

Result: 在SEVIR、MeteoNet及新构建的KMA平衡数据集上，exPreCast均取得领先性能，尤其在极端降雨事件中表现优异，且推理速度远快于扩散模型，具备实际部署潜力。

Conclusion: exPreCast是一种高效、准确且适用于多种降雨类型（包括极端事件）的确定性雷达降水临近预报框架。其提出的平衡数据集有助于推动领域内研究向更具现实意义的方向发展。该工作为实时、可靠、高分辨率的天气预报提供了可行方案。

Abstract: Accurate forecasting of extreme weather events such as heavy rainfall or storms is critical for risk management and disaster mitigation. Although high-resolution radar observations have spurred extensive research on nowcasting models, precipitation nowcasting remains particularly challenging due to pronounced spatial locality, intricate fine-scale rainfall structures, and variability in forecasting horizons. While recent diffusion-based generative ensembles show promising results, they are computationally expensive and unsuitable for real-time applications. In contrast, deterministic models are computationally efficient but remain biased toward normal rainfall. Furthermore, the benchmark datasets commonly used in prior studies are themselves skewed--either dominated by ordinary rainfall events or restricted to extreme rainfall episodes--thereby hindering general applicability in real-world settings. In this paper, we propose exPreCast, an efficient deterministic framework for generating finely detailed radar forecasts, and introduce a newly constructed balanced radar dataset from the Korea Meteorological Administration (KMA), which encompasses both ordinary precipitation and extreme events. Our model integrates local spatiotemporal attention, a texture-preserving cubic dual upsampling decoder, and a temporal extractor to flexibly adjust forecasting horizons. Experiments on established benchmarks (SEVIR and MeteoNet) as well as on the balanced KMA dataset demonstrate that our approach achieves state-of-the-art performance, delivering accurate and reliable nowcasts across both normal and extreme rainfall regimes.

</details>


### [222] [Disentangled Representation Learning via Flow Matching](https://arxiv.org/abs/2602.05214)
*Jinjin Chi,Taoping Liu,Mengtao Yin,Ximing Li,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出一种基于流匹配的解耦表示学习框架，将解耦视为在紧凑潜在空间中学习因子条件流的问题。通过引入非重叠（正交性）正则化项，有效抑制跨因子干扰，增强语义对齐，从而减少因子间信息泄露。在多个数据集上的实验表明，该方法显著优于现有基线，在解耦度、可控性和生成样本保真度方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法虽然通过归纳偏置促进因子独立性，但常常缺乏强语义对齐，导致因子解释性不足。因此需要一种能够同时保证因子独立性与语义一致性的新方法。

Method: 提出一种基于流匹配的解耦表示学习框架，利用因子条件流建模潜在空间中的数据生成过程，并引入非重叠（正交性）正则化以强制因子间的分离和语义对齐。

Result: 在多个数据集上实验验证了方法的有效性，相比代表性基线，实现了更高的解耦得分、更强的可控性和更高质量的生成样本。

Conclusion: 所提出的流匹配框架通过显式语义对齐机制，有效提升了解耦表示学习的质量，为理解数据生成过程提供了更具解释性的潜在表示。

Abstract: Disentangled representation learning aims to capture the underlying explanatory factors of observed data, enabling a principled understanding of the data-generating process. Recent advances in generative modeling have introduced new paradigms for learning such representations. However, existing diffusion-based methods encourage factor independence via inductive biases, yet frequently lack strong semantic alignment. In this work, we propose a flow matching-based framework for disentangled representation learning, which casts disentanglement as learning factor-conditioned flows in a compact latent space. To enforce explicit semantic alignment, we introduce a non-overlap (orthogonality) regularizer that suppresses cross-factor interference and reduces information leakage between factors. Extensive experiments across multiple datasets demonstrate consistent improvements over representative baselines, yielding higher disentanglement scores as well as improved controllability and sample fidelity.

</details>


### [223] [Private Prediction via Shrinkage](https://arxiv.org/abs/2602.05219)
*Chao Yan*

Main category: cs.LG

TL;DR: 本文研究差分隐私预测问题，提出在流式场景下可将查询次数T的依赖从标准组合的√T降低至polylog(T)，针对非自适应在线对手，对任意概念类C，仅需O~(VC(C)^{3.5} log^{3.5}T)个标签样本；对自适应在线对手及R^d上的半空间，仅需O~(d^{5.5} log T)个样本。


<details>
  <summary>Details</summary>
Motivation: 标准差分隐私组合导致T个查询时存在√T的依赖，本文旨在改进该依赖关系，实现更优的样本复杂度，尤其在流式和在线学习场景中。

Method: 采用差分隐私技术结合在线学习框架，设计新的私有预测算法，利用结构化噪声注入与自适应样本选择策略，在不同对手模型下优化样本需求。

Result: 实现了T个查询下的polylog(T)依赖，显著优于标准组合的√T；对于特定概念类（如半空间），样本复杂度达到近似最优。

Conclusion: 本文提出了高效且隐私保障强的在线差分隐私预测机制，显著降低了样本需求，为在线学习中的隐私保护提供了新范式。

Abstract: We study differentially private prediction introduced by Dwork and Feldman (COLT 2018): an algorithm receives one labeled sample set $S$ and then answers a stream of unlabeled queries while the output transcript remains $(\varepsilon,δ)$-differentially private with respect to $S$. Standard composition yields a $\sqrt{T}$ dependence for $T$ queries.
  We show that this dependence can be reduced to polylogarithmic in $T$ in streaming settings. For an oblivious online adversary and any concept class $\mathcal{C}$, we give a private predictor that answers $T$ queries with $|S|= \tilde{O}(VC(\mathcal{C})^{3.5}\log^{3.5}T)$ labeled examples. For an adaptive online adversary and halfspaces over $\mathbb{R}^d$, we obtain $|S|=\tilde{O}\left(d^{5.5}\log T\right)$.

</details>


### [224] [ZeroS: Zero-Sum Linear Attention for Efficient Transformers](https://arxiv.org/abs/2602.05230)
*Jiecheng Lu,Xu Han,Yan Sun,Viresh Pati,Yubin Kim,Siddhartha Somani,Shihao Yang*

Main category: cs.LG

TL;DR: 提出Zero-Sum Linear Attention (ZeroS)，解决线性注意力方法中因凸组合限制和均匀权重偏差导致的性能问题。通过移除零阶项并重加权残差，实现正负权重，支持对比操作，保持O(N)复杂度的同时提升表达能力，实验表明其在序列建模任务上可媲美或超越标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力方法受限于凸组合，仅能进行加法信息融合，且存在均匀权重偏差导致长序列中注意力稀释的问题，影响模型性能。

Method: 提出Zero-Sum Linear Attention (ZeroS)，通过消除常数零阶项1/t，并对剩余的零和softmax残差进行重加权，生成数学上稳定的正负权重，从而支持对比性计算。

Result: ZeroS在保持O(N)时间复杂度的前提下，理论上扩展了可表示函数的集合；实验显示其在多个序列建模基准上达到或超过标准softmax注意力的表现。

Conclusion: ZeroS通过引入零和机制与重加权策略，有效克服了传统线性注意力的两大局限，实现了更强大的表达能力和更强的性能表现，为高效注意力设计提供了新思路。

Abstract: Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks.

</details>


### [225] [Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions](https://arxiv.org/abs/2602.05234)
*Yuntai Bao,Xuhong Zhang,Jintao Chen,Ge Su,Yuxiang Cai,Hao Peng,Bing Sun,Haiqin Weng,Liu Yan,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出了一种名为概念分布式对齐搜索（CDAS）的新方法，通过分布匹配而非概率最大化来实现更稳定、可解释的模型干预控制。该方法利用分布式互换干预（DII）支持双向调节，并能从数据中自动推导调控因子，减少超参数调优需求。在大规模基准测试AxBench上表现良好，尤其在模型规模增大时优势更明显；在安全相关案例中成功克服拒绝行为和消除思维链后门，证明其与偏好优化方法互补且具备鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于干预的模型调节方法因沿用微调中的强优化目标而易过拟合，生成不自然输出。作者认为有效调节需准确识别模型内部机制，而非强制外部偏好，因此提出新方法以解决此问题。

Method: 基于分布式对齐搜索（DAS）原理，采用分布式互换干预（DII）并引入针对调节任务设计的分布匹配目标，通过对比干预后输出分布与反事实分布实现弱监督学习。

Result: 在AxBench基准上，CDAS虽不总是优于偏好优化方法，但在更大模型规模下表现更佳；在两个安全案例中成功实现系统性调节，同时保持模型通用能力。

Conclusion: CDAS是一种与偏好优化互补的稳健干预式模型调节方法，适用于需要精确、稳定控制的场景，尤其在大规模模型中具有潜力。

Abstract: Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs. We hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences. To this end, we build on the principles of distributed alignment search (DAS), the standard for causal variable localization, to propose a new steering method: Concept DAS (CDAS). While we adopt the core mechanism of DAS, distributed interchange intervention (DII), we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. CDAS differs from prior work in two main ways: first, it learns interventions via weak-supervised distribution matching rather than probability maximization; second, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control. On AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale. In two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility. These results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering. Our code is available at https://github.com/colored-dye/concept_das.

</details>


### [226] [CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers](https://arxiv.org/abs/2602.05243)
*Boxiang Zhang,Baijian Yang*

Main category: cs.LG

TL;DR: CORP 是一种无需重新训练或微调的闭式单次结构化剪枝框架，适用于视觉变压器。它通过在仅使用少量无标签校准集的情况下，移除整个 MLP 隐含维度和注意力子结构，实现高效推理。该方法将结构化剪枝建模为表示恢复问题，并利用闭式岭回归解决方案，将补偿整合到模型权重中，最小化校准分布下的期望表示误差。实验表明，即使在极端稀疏度下，CORP 也能保持高精度，且在单个 GPU 上不到 20 分钟即可完成剪枝，显著提升实际效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法通常依赖于重训练或多阶段优化，限制了其在后训练部署中的应用。因此需要一种无需标签、梯度或微调的高效剪枝方法，以降低视觉变压器的计算和内存开销。

Method: CORP 将结构化剪枝建模为表示恢复问题，将被移除的激活和注意力逻辑视为保留组件的仿射函数，并推导出闭式岭回归解，将补偿机制直接融入模型权重，从而最小化校准分布下的期望表示误差。

Result: 在 ImageNet 上使用 DeiT 模型的实验显示，MLP 和注意力表示存在显著冗余。不加补偿的单次结构化剪枝会导致严重精度下降，而 CORP 在保持高精度的同时实现了激进稀疏性。例如，在 DeiT-Huge 上，剪枝 50% 的 MLP 和注意力结构后仍能保持 82.8% 的 Top-1 准确率。

Conclusion: CORP 提供了一种高效的后训练结构化剪枝方案，无需额外训练，可在短时间内完成剪枝并显著降低推理成本，同时保持模型性能，具备强大的实际应用潜力。

Abstract: Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\% Top-1 accuracy after pruning 50\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.

</details>


### [227] [Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities](https://arxiv.org/abs/2602.05281)
*Pengyi Li,Elizaveta Goncharova,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出了一种新的优势重加权机制（ARM），以解决强化学习中因标准策略优化导致的低熵策略和模式崩溃问题。通过结合提示困惑度和答案置信度重新调整奖励信号，动态抑制过度自信路径的梯度更新，促进对未充分探索正确解的分布转移，显著提升生成多样性与响应熵，同时保持高准确性。在Qwen2.5和DeepSeek模型上的实验表明，该方法在数学与编码任务中有效缓解熵崩溃，尤其在Pass@32指标上相比GRPO提升13.9%。


<details>
  <summary>Details</summary>
Motivation: 标准强化学习策略优化方法（如GRPO）容易收敛到低熵策略，导致输出缺乏多样性，出现严重模式崩溃；其根源在于对高概率路径的过度强化，抑制了其他有效推理路径的探索。

Method: 提出优势重加权机制（ARM），基于提示困惑度和答案置信度动态调整优势估计，平衡所有正确回答的置信度水平，抑制过自信路径的梯度更新，并将概率质量向欠探索的正确解转移。

Result: 在Qwen2.5-7B和DeepSeek模型上，所提方法显著提升生成多样性与响应熵；在数学与编码基准测试中，Pass@1提升5.7%，Pass@32提升13.9%，证明其在探索与利用之间取得更优平衡。

Conclusion: 通过引入动态优势重加权机制，能够有效缓解强化学习中由标准优化带来的熵崩溃问题，实现更高多样性与准确性的协同提升，适用于复杂推理任务中的大语言模型优化。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.

</details>


### [228] [Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization](https://arxiv.org/abs/2602.05285)
*Minhuan Li,Jiequn Han,Pilar Cossio,Luhuan Wu*

Main category: cs.LG

TL;DR: 提出EmbedOpt，一种在条件嵌入空间中优化实验似然性的新方法，用于改进扩散模型在生物分子构象生成中的表现。该方法通过利用序列和共进化信号，在低密度区域仍能有效引导模型，相比基于坐标的后验采样方法在冷冻电镜图谱拟合任务中表现更优，且对超参数变化更具鲁棒性，减少推理步数，提升效率。


<details>
  <summary>Details</summary>
Motivation: 当目标位于扩散模型先验的低密度区域时，传统的基于坐标的后验采样方法需要过度依赖实验似然权重，导致不稳定。因此需要一种更稳健的方法来引导模型生成符合实验约束的构象。

Method: 提出EmbedOpt，即在条件嵌入空间中进行优化，利用嵌入空间中编码的丰富序列与共进化信息，动态调整扩散模型的先验分布以匹配实验约束。

Result: EmbedOpt在冷冻电镜图谱拟合任务中优于坐标基后验采样方法，在距离约束任务中性能相当，且对超参数变化具有更强鲁棒性；同时显著减少推理所需的扩散步数，提升了计算效率。

Conclusion: EmbedOpt提供了一种高效、稳健的扩散模型推理优化策略，特别适用于实验约束下复杂生物分子构象的生成，是当前数据驱动建模的重要补充。

Abstract: In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.

</details>


### [229] [HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction](https://arxiv.org/abs/2602.05286)
*Dahai Yu,Lin Jiang,Rongchao Xu,Guang Wang*

Main category: cs.LG

TL;DR: 提出HealthMamba，一种考虑空间依赖性和不确定性量化的医疗设施访问预测框架，通过融合静态与动态信息、图状态空间模型和不确定性量化模块，在多个真实数据集上显著提升预测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究将医疗设施访问预测视为时间序列问题，忽视了不同医疗设施间的空间依赖性，且在公共紧急情况等异常情境下预测不可靠。

Method: 设计Unified Spatiotemporal Context Encoder融合异构信息，引入GraphMamba进行分层时空建模，并集成三种不确定性量化机制以增强预测可靠性。

Result: 在加州、纽约州、德克萨斯州和佛罗里达州的四个大规模真实数据集上，HealthMamba相比最先进基线，预测准确率提升约6.0%，不确定性量化性能提升3.5%。

Conclusion: HealthMamba有效解决了医疗设施访问预测中的空间依赖性与异常情境下的可靠性问题，为优化医疗资源配置和制定公共卫生政策提供了有力支持。

Abstract: Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.

</details>


### [230] [A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms](https://arxiv.org/abs/2602.05304)
*Feng Zhu,Robert W. Heath,Aritra Mitra*

Main category: cs.LG

TL;DR: 本文提出了一种统一的收敛性分析框架，适用于SAG、SAGA和IAG三种随机方差减少算法。通过引入简单的集中工具来控制采样延迟，并设计一种新型李雅普诺夫函数以考虑延迟影响，实现了简洁且模块化的证明。该方法首次为SAG和SAGA提供了高概率收敛边界，可自然推广至非凸目标和马尔可夫采样场景。同时，该分析显著改进了IAG算法的已知收敛率。


<details>
  <summary>Details</summary>
Motivation: 现有对SAG、SAGA和IAG等算法的分析方法分散，依赖于特定技术，且SAG原证明复杂，需计算机辅助。亟需一种统一、简洁且可扩展的分析框架。

Method: 采用集中不等式控制随机子采样带来的延迟，设计一种新的考虑延迟的李雅普诺夫函数，实现统一分析。

Result: 得到了SAG和SAGA的首个高概率收敛界，且可推广至非凸问题与马尔可夫采样；同时获得了目前最优的IAG收敛速率。

Conclusion: 本文提出的统一分析框架不仅简化了理论证明，还提升了算法性能边界，为多种变体提供了通用分析工具。

Abstract: Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.

</details>


### [231] [When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging](https://arxiv.org/abs/2602.05536)
*Yayuan Li,Ze Peng,Jian Zhang,Jintao Guo,Yue Duan,Yinghuan Shi*

Main category: cs.LG

TL;DR: 该论文提出了一种名为奇异值校准（SVC）的训练无关、数据无关的后处理方法，用于解决模型合并中因共享知识重复计数导致的奇异值膨胀问题。通过量化子空间重叠并重新缩放被过度放大的奇异值，SVC在视觉和语言基准上显著提升现有合并基线性能，并在任务算术上提升13.0%。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法主要关注任务更新间的冲突，但忽略了共享知识被重复计数导致的奇异值膨胀问题，这会使得合并模型偏向于共享子空间，影响性能。

Method: 提出奇异值校准（SVC），通过分析权重更新中的谱方向重叠，识别并校正被过度放大的奇异值，实现无需训练和数据的后处理优化。

Result: 在多个视觉与语言基准测试中，SVC显著优于现有合并方法，达到当前最优性能；同时将任务算术的性能提升13.0%。

Conclusion: SVC是一种高效且通用的模型合并后处理方法，能够有效缓解共享知识重复计数问题，提升合并模型的泛化能力与性能表现。

Abstract: Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.

</details>


### [232] [Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective](https://arxiv.org/abs/2602.05319)
*Yinan Huang,Hans Hao-Hsun Hsu,Junran Wang,Bo Dai,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为序列流匹配（Sequential Flow Matching）的框架，将流匹配模型与贝叶斯滤波相结合，用于实时流数据中的序列预测。通过利用前一时刻后验分布作为生成的初始状态，实现高效、低延迟的采样，仅需1-2步即可达到与全步扩散模型相当的性能，显著提升推理速度并避免系统积压。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和流匹配模型在实时流环境中因依赖非信息性初始分布进行重复采样，导致推理延迟高、易产生系统积压。需要一种更高效、符合递归更新结构的序列推断方法。

Method: 将序列预测建模为贝叶斯滤波中的概率流传输过程，利用前一时刻的后验分布作为当前步骤的生成起点，实现基于贝叶斯信念更新的递归式流匹配，从而加速采样。

Result: 在多种预测、决策和状态估计任务中，该方法性能媲美全步扩散模型，但仅需1或少数几步采样，显著降低推理延迟，具备实际部署潜力。

Conclusion: 将序列推断建模为贝叶斯滤波中的概率流匹配，为流模型在实时系统中的高效部署提供了新的理论视角和实践路径。

Abstract: Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naïve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.

</details>


### [233] [Steering Large Reasoning Models towards Concise Reasoning via Flow Matching](https://arxiv.org/abs/2602.05539)
*Yawei Li,Benjamin Bergner,Yinghan Zhao,Vihang Prakash Patil,Bei Chen,Cheng Wang*

Main category: cs.LG

TL;DR: FlowSteer提出一种非线性控制方法，通过流匹配学习冗长与简洁推理表示之间的完整变换，实现输入相关的精确控制，显著提升大模型推理的紧凑性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一全局向量进行线性调整，受限于线性假设，难以有效压缩大型推理模型的冗长输出。

Method: 采用流匹配构建速度场，学习冗长与简洁推理表示间的分布传输，实现非线性、输入依赖的精细化控制。

Result: 在多个推理基准上，FlowSteer在保持高性能的同时，显著提升了推理的令牌效率，优于主流推理时基线方法。

Conclusion: 使用生成技术建模完整的分布传输，为控制大型推理模型提供了更有效且原则性的基础。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.

</details>


### [234] [GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL](https://arxiv.org/abs/2602.05323)
*Zifan Liu,Xinran Li,Shibo Chen,Jun Zhang*

Main category: cs.LG

TL;DR: 本文提出一种名为Goal-Assisted Stitching (GAS)的新算法，用于解决离线安全强化学习（OSRL）中生成高质量轨迹和平衡奖励与约束的挑战。GAS通过在转换层面增强和重标注数据集，构建高质量轨迹；引入基于期望分位数回归的新型目标函数，估计数据集中可实现的最佳奖励与成本目标，从而在更广泛的奖励-成本回报范围内实现更好的权衡。此外，通过重塑数据集以获得更均匀的奖励-成本分布，提升训练稳定性和效率。实验表明，GAS在平衡奖励最大化与约束满足方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的离线安全强化学习方法在处理子最优轨迹时缺乏‘拼接’能力，且难以在奖励与成本目标冲突时进行有效平衡，限制了其性能表现。

Method: GAS首先在转换层面增强并重标注数据集，以构建高质量轨迹；利用期望分位数回归训练新型目标函数，估计数据集中可实现的最佳奖励与成本目标；这些目标函数指导策略训练，并通过重塑数据集使奖励-成本分布更均匀，以提升训练稳定性与效率。

Result: 实验结果验证了GAS的有效性，其在平衡奖励最大化与约束满足方面显著优于现有方法，展现出更强的泛化能力和鲁棒性。

Conclusion: GAS通过增强数据拼接能力与自适应目标估计机制，在离线安全强化学习中实现了更优的奖励-成本权衡，为复杂约束环境下的策略学习提供了有效解决方案。

Abstract: Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to "stitch" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.

</details>


### [235] [Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis](https://arxiv.org/abs/2602.05333)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 本文提出了一种信息论框架，用于分析基于池的主动学习（AL）的理论极限。该框架将池式主动学习重新表述为一个有噪损毁压缩问题，其中数据选择被视为压缩，学习被视为解码。通过有限块长的有噪损毁压缩分析，推导出标签复杂度和泛化误差的信息论下界，这些下界为给定学习算法在最优数据选择策略下的理论极限提供了依据。这些边界包含了反映学习算法过拟合及归纳偏置与目标任务不一致性的项，与已有信息论边界和稳定性理论密切相关，为池式主动学习提供了新的理论视角。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解池式主动学习的理论极限，现有方法缺乏统一的信息论分析框架，尤其在数据选择与学习过程之间的关系上存在不足。因此，亟需一种能够整合数据选择与学习过程的理论工具，以揭示其内在机制和性能边界。

Method: 将池式主动学习建模为有噪损毁压缩问题，将观察样本映射为有噪符号观测，数据选择视为压缩过程，学习视为解码过程。利用有限块长的有噪损毁压缩理论，推导出标签复杂度和泛化误差的下界。

Result: 得到了标签复杂度和泛化误差的信息论下界，这些下界反映了学习算法的过拟合风险和归纳偏置与任务间的不匹配程度，且与现有信息论边界和稳定性理论紧密相关，为池式主动学习提供了新的理论支撑。

Conclusion: 本研究通过构建信息论框架，首次系统地刻画了池式主动学习的理论极限，揭示了数据选择与学习之间的深层联系，为优化主动学习策略提供了理论指导。

Abstract: This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.

</details>


### [236] [Rewards as Labels: Revisiting RLVR from a Classification Perspective](https://arxiv.org/abs/2602.05630)
*Zepeng Zhai,Meilin Chen,Jiaxuan Zhao,Junlang Qian,Lei Shen,Yuan Lu*

Main category: cs.LG

TL;DR: 提出REAL框架，将可验证奖励视为分类标签而非标量权重，解决GRPO等方法中的正样本梯度误分配和负样本梯度主导问题。通过引入锚点logits实现平衡的梯度分配，提升训练稳定性和性能，在数学推理任务上显著优于GRPO、DAPO和GSPO。


<details>
  <summary>Details</summary>
Motivation: GRPO等现有RLVR方法存在正样本梯度误分配和负样本梯度主导问题，导致策略更新效率低下且次优，亟需改进优化机制以提升训练效果与稳定性。

Method: 将可验证奖励重新建模为类别标签，将策略优化转化为分类任务；引入锚点logits以调节梯度权重，实现单调且有界的梯度分配，从而平衡不同回溯路径的梯度贡献。

Result: 在数学推理基准测试中，REAL显著提升训练稳定性，1.5B模型下Pass@1平均提升6.7%（相比DAPO），7B模型下分别领先DAPO 6.2%和GSPO 1.7%；即使使用基础二元交叉熵，仍保持稳定并超越DAPO 4.5%。

Conclusion: REAL通过重新定义奖励为标签并引入锚点logits，有效缓解了梯度失衡问题，实现了更高效、稳定的策略学习，在多个规模模型上均展现出优越性能。

Abstract: Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.

</details>


### [237] [Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting](https://arxiv.org/abs/2602.05371)
*Hongyi Li,Han Lin,Jun Xu*

Main category: cs.LG

TL;DR: HRT将每个分裂重新表述为两个线性预测器的非线性最小二乘问题，通过交替拟合实现类似ReLU的表达能力，采用阻尼牛顿法进行优化，保证局部目标函数单调下降并收敛。实验表明，HRT在合成和真实数据集上表现优异，结构更紧凑，且具有通用逼近能力，逼近误差率为O(δ²)。


<details>
  <summary>Details</summary>
Motivation: 解决传统斜决策树学习高质斜切分时面临的NP难问题，现有方法依赖缓慢搜索或无理论依据的启发式方法，缺乏高效且可证明的方法。

Method: 将每个节点的分裂建模为非线性最小二乘问题，利用两个线性预测器的最大/最小包络产生类似ReLU的表达能力，采用交替拟合与阻尼牛顿（高斯-牛顿）方法求解，并结合岭正则化和自适应阻尼策略以提升稳定性与收敛速度。

Result: HRT在多个合成与真实世界基准测试中表现优于或等同于单棵树基线模型，具有更紧凑的结构；理论证明其为通用逼近器，逼近误差率可达O(δ²)。

Conclusion: HRT提供了一种高效、稳定且可证明的斜决策树学习方法，兼具良好的表达能力和简洁的模型结构，适用于复杂分类任务。

Abstract: Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(δ^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.

</details>


### [238] [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859)
*Xu Wang,Bingqing Jiang,Yu Wan,Baosong Yang,Lingpeng Kong,Difan Zou*

Main category: cs.LG

TL;DR: DLM-Scope是首个针对扩散语言模型（DLMs）的稀疏自编码器（SAE）可解释性框架，证明了Top-K SAE能有效提取可解释特征。与自回归LLMs不同，SAE插入DLMs早期层可降低交叉熵损失，且在扩散时间干预中表现优于LLM调制。此外，SAE还能提供解码顺序信号，并在后训练阶段保持稳定，为DLM的机制可解释性奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着扩散语言模型（DLMs）成为自回归大语言模型（LLMs）的有力替代，亟需开发适用于DLMs的机制可解释性工具。现有基于SAE的方法在LLMs中已成功应用，但尚未扩展至DLMs，因此需要构建适配DLM结构与生成机制的可解释框架。

Method: 提出DLM-Scope框架，采用Top-K稀疏自编码器对DLMs进行特征提取；通过在不同层插入SAEs，分析其对模型损失和行为的影响；设计扩散时间干预实验，评估特征调控效果；探究SAE在解码顺序预测与后训练稳定性方面的潜力。

Result: SAE插入早期层可降低DLMs的交叉熵损失，优于在LLMs中的表现；SAE特征在扩散时间干预中更有效，显著优于传统LLM调制方法；SAE能提供解码顺序信号，且在后训练阶段特征稳定，表明其具备良好的可迁移性和可靠性。

Conclusion: 本研究首次建立了面向扩散语言模型的SAE可解释框架——DLM-Scope，验证了SAE在DLMs中的有效性与独特优势，揭示了其在模型干预、解码控制与稳定性方面的潜力，为未来在扩散模型上的可解释性研究与应用提供了坚实基础。

Abstract: Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.

</details>


### [239] [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863)
*Roger Girgis,Rodrigue de Schaetzen,Luke Rowe,Azalée Robitaille,Christopher Pal,Liam Paull*

Main category: cs.LG

TL;DR: Constrained GRPO extends the scalable GRPO framework to handle explicit behavioral constraints using a Lagrangian approach. It addresses a critical issue in advantage estimation—where naive multi-component treatment distorts constraint enforcement due to mismatched standard deviations—and proposes a scalarized advantage construction to preserve proper trade-offs between reward and constraints. Experiments in gridworlds validate the theoretical findings, and robotics tasks demonstrate improved constraint satisfaction and task success.


<details>
  <summary>Details</summary>
Motivation: Existing GRPO methods are effective for critic-free policy learning but lack exploration in constrained settings. Explicit behavioral constraints are crucial in real-world applications like robotics, yet their integration into scalable frameworks remains challenging. A key issue identified is that standard advantage estimation can distort the relative importance of objectives, undermining constraint enforcement.

Method: The paper introduces Constrained GRPO, a Lagrangian-based method that uses indicator cost functions to define constraints. To prevent optimization pathologies, it proposes a scalarized advantage construction that ensures balanced trade-offs between reward maximization and constraint violation minimization. The method avoids component-wise standardization issues by combining advantages into a single scalar signal.

Result: In a toy gridworld, Constrained GRPO successfully restores stable constraint control, confirming the theoretical prediction of optimization pathology in unscalarized setups. On robotics tasks, it achieves higher task success rates while significantly improving constraint satisfaction, demonstrating practical effectiveness in embodied AI.

Conclusion: Constrained GRPO provides a simple, effective, and scalable solution for constrained policy optimization without requiring critics. By addressing the advantage estimation bias through scalarization, it enables reliable enforcement of behavioral constraints, making it well-suited for modern embodied AI systems relying on large multimodal models.

Abstract: While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.

</details>


### [240] [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885)
*Wei Liu,Jiawei Xu,Yingru Li,Longtao Zheng,Tianjian Li,Qian Liu,Junxian He*

Main category: cs.LG

TL;DR: 本文研究强化学习（RL）在核函数生成中的应用，提出KernelGYM分布式GPU环境以支持多轮交互、奖励劫持检测和长期训练。针对多轮RL中的策略梯度偏差问题，提出TRLOO方法实现无偏优势估计；为缓解懒惰优化，引入基于性能分析的奖励机制（PR）与拒绝采样（PRS）。训练得到的Dr.Kernel-14B模型在Kernelbench上表现媲美Claude-4.5-Sonnet。通过测试时的序列缩放，其生成核函数在Level-2子集上达到31.6%的1.2倍以上加速率，最佳候选选择下更达47.8%，优于Claude-4.5-Sonnet和GPT-5。所有资源已开源。


<details>
  <summary>Details</summary>
Motivation: 当前大模型生成高质量核函数面临数据不足、训练环境脆弱及奖励劫持、懒惰优化等问题，导致模型偏向表面正确而非真正性能提升。亟需可靠框架与方法支持高效、稳定、真实的核函数生成训练。

Method: 设计KernelGYM分布式GPU环境，支持多轮交互与奖励劫持检测；提出TRLOO方法解决自包含引起的策略梯度偏差；引入基于性能分析的奖励（PR）与拒绝采样（PRS）以抑制懒惰优化；采用序列测试时缩放策略提升生成质量。

Result: Dr.Kernel-14B在Kernelbench上表现接近Claude-4.5-Sonnet；在Level-2子集上，31.6%的生成核函数实现至少1.2倍加速，最佳候选选择下达到47.8%；优于Claude-4.5-Sonnet（26.7%）和GPT-5（28.6%）。

Conclusion: 通过构建稳健的训练环境与创新的多轮强化学习方法，本工作显著提升了大模型生成高性能核函数的能力，验证了系统化设计对复杂代码生成任务的重要性，并推动了可扩展AI系统的发展。

Abstract: High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.

</details>


### [241] [Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models](https://arxiv.org/abs/2602.05390)
*Wei Soon Cheong,Lian Lian Jiang,Jamie Ng Suat Ling*

Main category: cs.LG

TL;DR: 该研究评估了多种时间序列基础模型在新加坡和澳大利亚电力需求预测中的表现，发现尽管Chronos-2在零样本设置下表现最佳，但简单基线模型在气候稳定的地区（如新加坡）对短期预测更具优势。模型架构与地理环境均显著影响效果，具有协同架构设计（如TTM的通道混合、Chronos-2的分组注意力）的基础模型能更好利用外生特征，而其他模型则表现不一。研究质疑基础模型的普适优越性，强调能源领域需采用领域特定建模。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型在利用外生特征方面能力尚不明确，尤其在电力需求预测中，外生特征至关重要，因此需要系统评估其实际效能。

Method: 在新加坡和澳大利亚的小时及日度粒度数据上，对比MOIRAI、MOMENT、TinyTimeMixers、ChronosX和Chronos-2等基础模型与基准LSTM模型，采用全特征、选择特征和仅目标变量三种特征配置进行系统评估。

Result: Chronos-2在零样本设置中表现最佳，但在新加坡稳定气候下，简单基线模型常优于所有基础模型，尤其在短期预测中；具有协同架构设计的模型更有效利用外生特征；基础模型的优势主要出现在气候多变地区。

Conclusion: 基础模型并非在所有场景下都具备普遍优势，其性能高度依赖于模型架构和地理气候背景，能源领域应发展领域特定模型，而非盲目依赖通用基础模型。

Abstract: Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.

</details>


### [242] [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Jiahan Li,Chenhao Huang,Junjie Ye,Sixian Li,Mingxu Chai,Yuhui Wang,Yajie Yang,Ming Zhang,Jiazheng Zhang,Shichun Liu,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui,Xipeng Qiu,Qi Zhang,Xuanjing Huang*

Main category: cs.LG

TL;DR: DFPO 是一种新的分布式强化学习框架，通过将价值建模为时间步上的连续流，提升在噪声监督和域外泛化方面的性能。该方法利用条件风险控制和一致性约束来稳定训练，并在对话、数学推理和科学任务中优于 PPO、FlowRL 等基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有分布式强化学习方法虽通过多分位点建模提升鲁棒性，但各分位点独立学习，导致价值表示粗粒度，难以捕捉精细状态信息，在复杂或域外场景下表现不佳。

Method: DFPO 采用连续值流场建模，替代传统孤立的分位点预测，实现更精细的状态依赖价值估计；引入条件风险控制与值流轨迹上的一致性约束以增强训练稳定性。

Result: 在对话、数学推理和科学任务上，DFPO 在噪声监督下表现出更高的训练稳定性和更强的泛化能力，显著优于 PPO、FlowRL 等现有方法。

Conclusion: DFPO 通过连续值流建模与双重约束机制，有效提升了分布式强化学习在真实复杂环境中的鲁棒性与泛化能力，为大模型后训练提供了一种高效可靠的优化方案。

Abstract: Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.

</details>


### [243] [Robust Federated Learning via Byzantine Filtering over Encrypted Updates](https://arxiv.org/abs/2602.05410)
*Adda Akram Bendoukha,Aymen Boudguiga,Nesrine Kaaniche,Renaud Sirdey,Didem Demirag,Sébastien Gambs*

Main category: cs.LG

TL;DR: 本文提出一种结合同态加密与基于属性推断的元分类器的新方法，以同时实现联邦学习中的安全聚合与拜占庭鲁棒性。通过在影子更新上训练多类拜占庭攻击的元分类器（如后门、梯度反演、标签翻转等），利用其输出对恶意更新进行加权抵消；并提出自动化选择最优核函数和维度超参数的方法，优化CKKS加密系统下的效率与聚合约束。实验表明，在FEMNIST、CIFAR10、GTSRB和acsincome数据集上，该方法可实现90%~94%的拜占庭更新识别准确率，仅带来轻微模型性能损失，加密推理时间在6–24秒之间，整体聚合时间在9–26秒之间。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护数据隐私，但其分布式特性仍面临隐私泄露（如推理攻击）和拜占庭行为影响模型训练的问题。现有方案通常单独处理安全聚合或拜占庭鲁棒性，难以兼顾两者。因此亟需一种统一框架，同时解决这两类挑战。

Method: 首先，借鉴属性推断攻击原理，在标注的影子更新上训练一组元分类器，模拟多种拜占庭攻击行为（如后门、梯度反演、标签翻转、数据混洗）。其次，利用这些元分类器的输出对加密的更新进行重加权，以抑制恶意更新的影响。第三，设计自动化方法，针对CKKS同态加密系统，优化核函数与维度超参数，平衡安全性、聚合约束与计算效率。

Result: 在多个基准数据集（FEMNIST、CIFAR10、GTSRB、acsincome）上的实验表明，所提方法在保持较高模型性能的同时，对拜占庭更新的识别准确率达90%–94%。加密推理时间在6–24秒之间，整体聚合时间在9–26秒之间，性能损耗极小。

Conclusion: 本文提出的联合框架有效实现了联邦学习中隐私保护聚合与拜占庭鲁棒性的协同保障，具备良好的实用性和可扩展性，为构建更安全可靠的分布式机器学习系统提供了新思路。

Abstract: Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.

</details>


### [244] [BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs](https://arxiv.org/abs/2602.05448)
*Sheshansh Agrawal,Thien Hang Nguyen,Douwe Kiela*

Main category: cs.LG

TL;DR: 提出了一种基于锦标赛图的k-wise重排序框架，通过聚合k个文档比较产生的成对偏好，构建全局偏好图并利用传递闭包推导更多排序，实现高效且准确的重排序。该方法在14个基准测试中表现出色，相比现有方法减少25-40%的令牌使用量，比成对方法少7倍，同时保持相近质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型重排序方法要么依赖启发式规则未能充分利用每次排序决策中的信息，要么效率低下。需要一种更高效、更系统的方法来挖掘多文档比较中的完整信息。

Method: 提出锦标赛图框架，将k文档比较转化为$inom{k}{2}$个成对偏好，构建全局偏好图，并通过传递闭包推导额外排序；设计贪婪查询调度以最大化信息增益；通过合并非传递偏好循环形成等价类，实现分层排名。

Result: 在14个基准和5个LLM上，该方法在准确性上匹配或超越现有方法，同时减少25-40%的令牌消耗，比成对方法少7倍，且质量接近。

Conclusion: 所提出的锦标赛图框架为k-wise重排序提供了理论基础，显著提升效率与性能，是检索增强生成中一种高效、鲁棒的重排序新范式。

Abstract: Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\times$ fewer than pairwise methods at near-identical quality.

</details>


### [245] [When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL](https://arxiv.org/abs/2602.05459)
*Jan Malte Töpperwien,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文研究了离线目标条件强化学习中超参数敏感性问题，发现其并非必然存在，而是由bootstrapping机制放大。在引入少量专家数据（约20%）后，QRL表现出稳定且广泛的近最优区域，而HIQL则呈现尖锐的最优解并随训练阶段显著漂移。通过引入跨目标梯度对齐诊断，揭示了bootstrapped目标存在更强的破坏性梯度干扰，直接导致超参数敏感性。结果表明，超参数敏感性可通过改进算法目标设计来缓解。


<details>
  <summary>Details</summary>
Motivation: 探究深度强化学习中超参数敏感性是否为固有特性，还是由特定训练机制（如bootstrapping）所加剧，特别是在离线目标条件强化学习场景下。

Method: 采用离线目标条件强化学习框架，控制数据分布与质量变化，在静态与非静态数据分布条件下评估两种代表性算法（HIQL和QRL），引入跨目标梯度对齐诊断分析梯度干扰机制。

Result: 在包含约20%专家数据的情况下，QRL展现出稳定、广泛的近最优区域；而HIQL的最优解尖锐且随训练阶段显著漂移。梯度对齐分析显示，bootstrapped目标存在更强的破坏性梯度干扰，与超参数敏感性直接相关。

Conclusion: 超参数敏感性并非强化学习固有属性，而是由bootstrapping动态机制放大所致。通过优化算法目标设计，可有效提升算法鲁棒性。

Abstract: Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\approx$ 20\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.

</details>


### [246] [A Unified Framework for Rethinking Policy Divergence Measures in GRPO](https://arxiv.org/abs/2602.05494)
*Qingyuan Wu,Yuhui Wang,Simon Sinong Zhan,Yanning Dai,Shilong Deng,Sarra Habchi,Qi Zhu,Matthias Gallé,Chao Huang*

Main category: cs.LG

TL;DR: 本文提出一个统一的裁剪框架，用于表征现有强化学习中带验证奖励（RLVR）方法中的策略分歧度量，涵盖似然比和KL散度，并扩展至其他度量。该框架为系统分析不同分歧度量对探索与性能的影响提供了理论基础。研究识别出KL3估计器（一种方差减少的蒙特卡洛KL散度估计）作为关键的策略分歧约束，并理论证明其等价于一种非对称比率裁剪，可将概率质量重新分配给高置信度动作，从而增强探索性，同时保持GRPO类方法的简洁性。实验在数学推理基准上验证了将KL3引入GRPO能提升训练稳定性和最终性能，凸显了合理策略分歧约束在策略优化中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法如GRPO通过裁剪似然比来保证策略更新的稳定性，但缺乏对不同策略分歧度量影响的系统分析。本文旨在建立一个统一的理论框架，以更深入理解分歧度量如何影响探索和性能，并寻找更优的约束方式。

Method: 提出一个统一的裁剪框架，将多种策略分歧度量（如似然比、KL散度等）纳入统一描述；识别并分析KL3估计器作为有效分歧约束的作用；从理论上证明其等价于一种促进探索的非对称裁剪机制；在数学推理任务上进行实验验证。

Result: 实验证明，将KL3估计器融入GRPO方法后，不仅提升了训练稳定性，还显著改善了最终性能，表明基于合理分歧约束的设计对于增强LLM的推理能力至关重要。

Conclusion: 本文提出的统一框架为理解与设计有效的策略分歧约束提供了理论支撑。使用KL3估计器的改进方法在保持简单性的同时增强了探索能力，显著提升了强化学习中大语言模型的推理表现。

Abstract: Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.

</details>


### [247] [Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification](https://arxiv.org/abs/2602.05535)
*Tao Huang,Rui Wang,Xiaofei Liu,Yi Qin,Li Duan,Liping Jing*

Main category: cs.LG

TL;DR: 提出Evidential Uncertainty Quantification (EUQ)方法，通过细粒度量化信息冲突与知识缺失，有效检测大视觉语言模型（LVLMs）的异常行为。该方法利用证据理论在单次前向传播中建模支持与反对证据，实现对幻觉和分布外失败等行为的精准识别，并揭示内部表征演化的动态过程。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法仅捕捉整体认知不确定性，难以有效识别LVLM在面对不胜任或对抗性输入时产生的幻觉、危险指令等偏差行为。这些行为源于内部知识冲突或缺乏支持信息，亟需更精细的不确定性分析机制。

Method: 将模型输出头特征解释为支持（正证据）或反对（负证据），基于证据理论聚合并量化内部冲突与知识空白，在一次前向传播中完成不确定性评估。

Result: EUQ在幻觉、越狱攻击、对抗脆弱性和分布外失效四类任务中均显著优于现有基线方法；实验表明幻觉主要对应高内部冲突，而分布外失败则体现为高无知状态；层间证据不确定性分析揭示了内部表示演化的新型视角。

Conclusion: EUQ提供了一种高效且可解释的细粒度不确定性量化框架，有助于提升LVLM在关键应用中的可靠性与安全性，为模型行为诊断与改进提供了新工具。

Abstract: Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.

</details>


### [248] [Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation](https://arxiv.org/abs/2602.05548)
*Zhiqi Yu,Zhangquan Chen,Mengting Liu,Heye Zhang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文指出当前基于组相对优势估计（GRAE）的强化学习方法（如GRPO）在探索效率和难度适应性方面存在瓶颈，根源在于其隐含的优势对称性。该对称性导致两个问题：1）在组级别，正确与错误轨迹的权重对称使未采样动作的logits保持不变，阻碍了新正确解的探索；2）在样本级别，算法倾向于中等难度样本，无法动态响应难度需求的变化。通过控制实验，作者发现不对称地抑制正确轨迹的优势可促进探索，并且采用类似课程学习的策略——先处理简单样本再逐步转向复杂样本，能最大化学习效率。为此，提出A-GRAE方法，动态调节探索激励和难度关注。在七个基准上的实验表明，A-GRAE显著优于GRPO及其变体，适用于LLMs和MLLMs。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO）虽已成为大模型推理的标准，但在探索效率和难度自适应方面仍存在明显不足。其核心问题源于组相对优势估计（GRAE）中的隐含优势对称性，该对称性限制了模型对新解的探索能力并导致对难度变化的响应迟钝。因此，亟需重新审视并改进这一基础机制以提升整体性能。

Method: 提出Asymmetric GRAE（A-GRAE），通过打破传统GRAE中的优势对称性，实现两方面改进：1）不对称地降低正确轨迹的优势，增强对未探索路径的探索激励；2）引入课程学习式样本选择策略，优先训练简单样本，逐步过渡到复杂样本，从而优化学习过程的难度适应性。该方法动态调节探索强度与难度关注点，提升训练效率与最终表现。

Result: 在七个不同基准测试上，A-GRAE显著优于GRPO及其各类变体，无论是在小规模还是大规模语言模型（LLMs/MLLMs）上均表现出一致的性能提升。实验验证了非对称优势抑制和渐进难度学习的有效性，证明了所提方法在提高探索效率与难度适应性方面的优越性。

Conclusion: 本研究揭示了现有GRAE方法中优势对称性的根本局限，并提出A-GRAE作为有效解决方案。通过引入非对称优势调节与课程式学习机制，A-GRAE显著提升了强化学习在大模型推理中的探索效率与难度适应能力，为未来高效、自适应的推理训练提供了新的范式。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.

</details>


### [249] [Logical Guidance for the Exact Composition of Diffusion Models](https://arxiv.org/abs/2602.05549)
*Francesco Alesiani,Jonathan Warrell,Tanja Bien,Henrik Christiansen,Matheus Ferraz,Mathias Niepert*

Main category: cs.LG

TL;DR: 提出LOGDIFF框架，实现扩散模型在推理时对复杂逻辑表达式的精确约束生成。通过推导精确的布尔代数，证明在特定条件（合取结合条件独立子公式，析取结合条件独立或互斥子公式）下可获得精确逻辑引导，并设计高效递归算法计算引导信号。同时，结合原子引导信号与后验概率估计，提出混合引导方法，适用于组合逻辑引导和标准条件生成。在图像和蛋白质结构生成任务中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理复杂逻辑约束时缺乏精确的引导机制，尤其难以在推理阶段实现基于复杂逻辑表达式的精确生成。因此需要一种能够支持复杂逻辑表达式且保证引导精确性的新方法。

Method: 推导精确布尔代数，构建满足特定条件的电路表示；利用递归算法从原子得分和后验概率中计算精确引导信号；引入混合引导策略，融合分类器引导与无分类器引导。

Result: 在多个图像和蛋白质结构生成任务中，LOGDIFF框架实现了高精度的逻辑约束生成，有效支持复杂逻辑表达式，且计算效率较高。

Conclusion: LOGDIFF框架为扩散模型提供了精确的逻辑引导能力，能够在推理阶段实现对复杂逻辑表达式的准确控制，扩展了扩散模型在条件生成中的应用范围。

Abstract: We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.
  We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.
  First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.
  Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.
  In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.
  Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.
  Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.
  We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.

</details>


### [250] [OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph](https://arxiv.org/abs/2602.05576)
*Chenxi Wan,Xunkai Li,Yilong Zuo,Haokun Deng,Sihan Li,Bowen Fan,Hongchao Qin,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出OpenMAG，一个涵盖19个数据集、6个领域、16个编码器的综合性基准，支持静态与可训练特征编码，包含24个前沿模型和8个下游任务，实现统一公平评估，并得出14项关于MAG学习的关键洞察。


<details>
  <summary>Details</summary>
Motivation: 现有基准在领域覆盖、编码器灵活性、模型多样性及任务范围上存在局限，难以实现公平评估，亟需建立统一的评价标准以推动多模态属性图学习的发展。

Method: 构建OpenMAG基准，整合多领域数据集与多种编码器，提供标准化模型库与任务框架，支持静态与可训练特征编码，并进行系统性评估以提炼关键洞见。

Result: 成功构建了涵盖广泛场景的统一评估平台，验证了其在评估必要性、数据质量、有效性、鲁棒性与效率方面的优势，总结出14条指导未来研究的重要发现。

Conclusion: OpenMAG为多模态属性图学习提供了全面、可复现且公平的评估基础，有助于推动该领域向更严谨、系统化方向发展。

Abstract: Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.

</details>


### [251] [Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.05605)
*Jiaji Zhang,Hailiang Zhao,Guoxuan Zhu,Ruichao Sun,Jiaju Wu,Xinkui Zhao,Hanlin Tang,Weiyi Lu,Kan Liu,Tao Lan,Lin Qu,Shuiguang Deng*

Main category: cs.LG

TL;DR: Shiva-DiT提出一种基于残差的可微分Top-k选择方法，解决扩散变压器（DiTs）中自注意力机制计算成本高的问题。通过残差感知的直通估计器，实现静态编译下的确定性标记数量，同时保持端到端学习能力。引入上下文感知路由器和自适应比率策略，自动学习自适应剪枝调度。实验表明，Shiva-DiT在主流模型如SD3.5上实现了1.54倍的时钟速度提升，且保真度优于现有基线，有效消除不规则张量开销，建立新的帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法无法同时满足可微性、效率和硬件所需的严格静态预算要求，导致扩散变压器（DiTs）因自注意力的二次复杂度而计算成本过高。

Method: 提出Residual-Based Differentiable Top-k Selection，结合残差感知直通估计器以实现确定性令牌数量与端到端可学习性的平衡；引入Context-Aware Router和Adaptive Ratio Policy，实现自适应剪枝调度的自主学习。

Result: 在SD3.5等主流模型上，Shiva-DiT实现1.54×的墙钟速度提升，保真度优于现有方法，有效消除不规则张量开销，建立新的帕累托前沿。

Conclusion: Shiva-DiT成功协调了可微性、效率与静态预算之间的矛盾，为高效扩散模型推理提供了新范式。

Abstract: Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.

</details>


### [252] [Path-Guided Flow Matching for Dataset Distillation](https://arxiv.org/abs/2602.05616)
*Xuhui Li,Zhengquan Luo,Xiwei Liu,Yongqiang Yu,Zhiqiang Xu*

Main category: cs.LG

TL;DR: PGFM 是首个基于流匹配的生成式数据蒸馏框架，通过在冻结 VAE 的潜在空间中求解 ODE 实现快速确定性合成，利用连续路径到原型引导算法实现轨迹稳定控制，在保持多样性和效率的同时显著提升训练效率，相比扩散模型方法提升 7.6 倍，且模式覆盖率达 78%。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的数据蒸馏方法依赖启发式引导或原型分配，存在采样耗时和轨迹不稳定的缺陷，尤其在强控制或低每类样本数（IPC）条件下影响下游泛化性能。

Method: 提出 Path-Guided Flow Matching (PGFM)，在冻结 VAE 的潜在空间中进行条件流匹配，学习从高斯噪声到数据分布的类条件迁移；设计连续路径到原型引导算法，实现 ODE 一致的路径控制，确保轨迹可靠抵达指定原型。

Result: 在高分辨率基准上，PGFM 在更少采样步数下达到或超越现有扩散基方法性能，效率提升 7.6 倍，模式覆盖率达 78%，显著改善了生成效率与稳定性。

Conclusion: PGFM 为生成式数据蒸馏提供了高效、稳定且可控制的新范式，突破了传统扩散方法的效率瓶颈，具备良好的实用前景。

Abstract: Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\times$ more efficient than the diffusion-based counterparts with 78\% mode coverage.

</details>


### [253] [Empowering Time Series Analysis with Large-Scale Multimodal Pretraining](https://arxiv.org/abs/2602.05646)
*Peng Chen,Siyuan Wang,Shiyan Hu,Xingjian Wu,Yang Shu,Zhongwen Rao,Meng Wang,Yijie Li,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 本文提出了一种面向时间序列分析的多模态基础模型，旨在解决现有模型依赖单一模态、缺乏多视角理解的问题。为此，作者构建了首个大规模多模态时间序列数据集MM-TS（涵盖六个领域，达十亿数据点），并设计了HORAI模型，通过频率增强的跨模态编码器与时空频解码器实现异构模态的有效融合。该方法在零样本场景下在时间序列预测和异常检测任务中均达到领先性能，展现了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型主要依赖大规模单模态预训练，缺乏互补模态以提升对时间序列的理解能力。构建多模态基础模型是自然发展方向，但面临两大挑战：一是缺乏统一的多模态预训练范式和大规模多模态时间序列语料库；二是如何有效融合异构模态并提升模型泛化能力。

Method: 提出一种多模态预训练范式，利用时间序列自身衍生的内生模态（图像和文本）以及外部知识（真实新闻），提供多视角分析视角。开发自动化数据构建流程，构建大规模多模态时间序列数据集MM-TS。提出HORAI模型，包含频率增强的跨模态编码器和时间-频率解码器，以实现多模态特征融合与跨域泛化。

Result: 在MM-TS数据集上预训练后，HORAI在时间序列预测和异常检测任务中实现了当前最优的零样本性能，验证了其在多模态融合与跨领域泛化方面的强大能力。

Conclusion: 本文首次系统性地探索了时间序列多模态基础模型的构建路径，通过建立大规模多模态数据集和提出新型融合架构，显著提升了模型对复杂时间序列的理解与泛化能力，为未来多模态时序分析奠定了重要基础。

Abstract: While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.

</details>


### [254] [End-to-End Compression for Tabular Foundation Models](https://arxiv.org/abs/2602.05649)
*Guri Zabërgja,Rafiq Kamel,Arlind Kadra,Christian M. M. Frey,Josif Grabocka*

Main category: cs.LG

TL;DR: TACO是一种端到端的表格数据压缩模型，通过在潜在空间中压缩训练数据，显著提升推理速度和内存效率。相比现有的基于Transformer的表格基础模型，TACO在TabArena基准测试中实现最高94倍的推理加速，内存消耗减少高达97%，同时保持性能不下降，并且在更大数据集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的表格基础模型虽然性能优异，但其注意力机制导致计算复杂度为二次方，限制了大规模数据处理能力，因此需要一种更高效、可扩展的替代方案。

Method: 提出TACO模型，利用潜在空间对训练数据进行压缩，使模型能够在一次前向传播中完成拟合与预测，避免参数更新，从而降低计算开销并提高可扩展性。

Result: 在TabArena基准测试中，TACO实现了最高94倍的推理速度提升，内存占用减少97%，且在大规模数据下表现优于其他基线方法，同时保持接近甚至超越原有性能水平。

Conclusion: TACO成功解决了现有表格Transformer模型在大规模数据下的效率瓶颈，兼具高速推理、低内存消耗与良好可扩展性，是未来表格数据建模的重要方向。

Abstract: The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.

</details>


### [255] [Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation](https://arxiv.org/abs/2602.05656)
*Igor Santos-Grueiro*

Main category: cs.LG

TL;DR: 本文将大语言模型对齐评估形式化为一个在部分可观测性下的可识别性问题，引入了‘对齐可验证性问题’和‘规范不可区分性’概念，证明在有限行为评估和评估感知的智能体条件下，观察到的行为合规性无法唯一确定潜在对齐状态。因此，对齐基准应被理解为对可观测合规性的上界估计，而非对齐保证。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型对齐的评估主要依赖于有限的评测协议（如基准测试、红队测试或自动化流程），并假设行为合规即代表内在对齐。然而，这种从行为证据推断潜在对齐属性的推理过程通常隐含且缺乏系统分析。本文旨在形式化这一推理过程，揭示其根本局限。

Method: 本文将对齐评估建模为在部分可观测环境下的可识别性问题，允许智能体行为依赖于与评估机制相关的信息。通过定义‘对齐可验证性问题’和‘规范不可区分性’，分析不同潜在对齐假设是否会产生相同的可观测行为分布。

Result: 核心结果是一个否定但精确限定的可识别性定理：在有限行为评估和评估感知智能体的设定下，观察到的行为合规性无法唯一确定潜在对齐状态。这意味着理想化的行为评估也无法一般性地验证对齐作为内在属性。

Conclusion: 行为对齐测试应被视为对不可区分性类别的估计器，而非对齐验证器。通过更严格的测试虽可缩小兼容假设空间，但在给定条件下无法将其压缩至单一解。因此，对齐基准仅提供特定评估环境下可观测合规性的上界，而非对齐保证。

Abstract: Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.
  We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.
  Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.
  We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.

</details>


### [256] [Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization](https://arxiv.org/abs/2602.05657)
*Aleksandar Armacki,Dragana Bajović,Dušan Jakovetić,Soummya Kar,Ali H. Sayed*

Main category: cs.LG

TL;DR: 本文通过大偏差理论研究了基于SGD方法的长期尾部衰减，填补了现有工作中对固定误差阈值下失败概率量化及长期尾部衰减分析的空白。针对非凸目标函数和有界噪声，提出了经典SGD最优迭代梯度范数平方尾部的上界，其长期衰减速率为 $e^{-t/\log(t)}$；对于重尾噪声（阶数 $p \\in (1,2]$）下的截断SGD（c-SGD），给出了相应上界，衰减速率分别为 $e^{-t^{β_p}/\log(t)}$（$p \\in (1,2)$）和 $e^{-t/\log^2(t)}$（$p=2$），其中 $β_p = \\frac{4(p-1)}{3p-2}$。同时，证明了下界为 $e^{-t}$，表明所提上界在多对数因子意义下是紧的。结果表明，相比以往基于有限时间界的 $e^{-\\sqrt{t}}$ 和 $e^{-t^{β_p/2}}$ 衰减率，本文揭示了快一个数量级的长期尾部衰减，提供了更强的单次运行保证。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注高概率保证，但缺乏对固定概率阈值下失败概率的直接分析，且多数结果为有限时间性质，难以刻画现代深度学习模型训练中百万级迭代的长期行为。因此亟需从大偏差视角揭示长期尾部衰减特性，以提供更可靠的个体算法运行保障。

Method: 采用大偏差理论框架，分析SGD及其变体（如截断SGD）在非凸优化问题中的长期尾部行为。通过构建合适的概率不等式与渐近分析，推导出梯度范数平方的尾部上界，并结合反例构造给出下界，从而实现对衰减速率的精确刻画。

Result: 提出并证明了经典SGD和截断SGD在不同噪声假设下的长期尾部衰减速率上界：分别为 $e^{-t/\log(t)}$ 与 $e^{-t^{β_p}/\log(t)}$（$p \\in (1,2)$）、$e^{-t/\log^2(t)}$（$p=2$），同时建立下界 $e^{-t}$，表明所提上界在多对数因子意义下紧致。相较于已有有限时间结果 $e^{-\\sqrt{t}}$、$e^{-t^{β_p/2}}$，本工作揭示了快一个数量级的衰减速度。

Conclusion: 本研究首次系统地从长期视角刻画了SGD类方法的尾部衰减行为，揭示了其远优于传统有限时间界限的收敛速率，为理解算法稳定性与个体运行可靠性提供了新的理论支撑，尤其适用于大规模现代机器学习训练场景。

Abstract: The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\log(t)}$, where $β_p = \frac{4(p-1)}{3p-2}$ for $p \in (1,2)$ and $e^{-t/\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.

</details>


### [257] [Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection](https://arxiv.org/abs/2602.05667)
*Ling Zhan,Zhen Li,Junjie Huang,Tao Jia*

Main category: cs.LG

TL;DR: 该研究提出一种名为SCLCS的自监督框架，用于在大规模功能连接（FC）建模中选择代表性子集（core-set），以高效评估数百种FC建模方法。通过结构感知对比学习，SCLCS识别出具有稳定结构特征的样本，并结合密度均衡采样策略确保子集的多样性与代表性。实验表明，仅使用10%的数据即可保持与全数据集相当的模型排名一致性，优于现有SOTA方法23.2%的nDCG@k性能。该工作首次形式化了FC算子基准测试中的核心集选择问题，推动其成为计算神经科学中的常规分析步骤。


<details>
  <summary>Details</summary>
Motivation: 大规模功能连接建模方法的基准测试面临组合爆炸问题，导致全面评估计算成本过高，难以成为常规预处理步骤。因此需要一种高效、可扩展的方法来选择少量代表性数据，以保留不同模型间的相对性能排序。

Method: 提出SCLCS框架，包含三个关键部分：(1) 使用自适应Transformer学习每个样本的功能连接结构；(2) 引入结构扰动评分（SPS）衡量结构稳定性，识别代表基础连接模式的样本；(3) 采用密度平衡采样策略，在选出高稳定性样本的基础上提升分布多样性，最终构建兼具鲁棒性与代表性的核心集。

Result: 在REST-meta-MDD大规模数据集上，仅用10%数据即实现了与全数据集一致的模型排名表现，相比当前最优方法在nDCG@k指标上提升最高达23.2%，验证了SCLCS在降低计算成本的同时显著提升排名保真度的能力。

Conclusion: 本研究首次将核心集选择形式化为功能连接建模基准测试中的关键问题，提出的SCLCS框架有效解决了计算瓶颈，使大规模模型比较成为可行且可重复的分析流程，为计算神经科学提供了新的方法论支持。

Abstract: Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS

</details>


### [258] [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043)
*Prakhar Kaushik,Ankit Vaidya,Shravan Chaudhari,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Share 提出一种参数高效的持续微调方法，通过学习和动态更新单一共享的低秩子空间，实现跨多个任务和模态的无缝适应。该方法在不依赖数据重放或多个适配器的情况下，有效缓解灾难性遗忘问题，实现前向知识迁移，并大幅减少参数量（最多100倍）和内存占用（最多281倍），性能接近联合训练模型。实验验证其在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务中的有效性，支持可扩展、异步的持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法如LoRA虽降低了计算成本，但缺乏严格的持续学习机制，无法有效整合新旧知识，且通常依赖数据重放或多适配器，难以满足真实场景中高效、持续学习的需求。

Method: 提出Share方法，构建一个基础低秩子空间，从过往任务中提取核心知识，并通过识别关键子空间方向，增量式地融合新任务信息，实现知识的动态更新与整合。整个过程仅维护一个共享子空间，避免冗余参数。

Result: Share实现了最高100倍的参数减少和281倍的内存节省，性能接近联合训练模型；单个Share模型可替代数百个任务特定的LoRA适配器，支持大规模、异步的持续学习，在多个任务和模态上均表现优异。

Conclusion: Share是一种高效、可扩展的参数高效持续学习方案，适用于大规模人工智能系统的终身学习需求，为实际部署提供了可行路径。

Abstract: Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.

</details>


### [259] [Stable but Wrong: When More Data Degrades Scientific Conclusions](https://arxiv.org/abs/2602.05668)
*Zhipeng Zhang,Kai Li*

Main category: cs.LG

TL;DR: 本文揭示了数据驱动科学中的一个根本性局限：即使在标准推断程序表现良好、收敛且校准正确的情况下，过多的数据反而会放大错误结论。这种失败源于观测可靠性以不可察觉的方式退化，导致诊断检验（如残差分析）仍显示正常，但结果却系统性错误。研究强调，数据量增加不等于结论更可靠，必须对观测过程的完整性施加显式约束。


<details>
  <summary>Details</summary>
Motivation: 现代科学研究依赖大规模数据和自动化推断流程，普遍假设数据越多结论越可靠。然而，本文质疑这一信念的普适性，旨在揭示在某些结构性条件下，数据增加反而会导致系统性错误，且无法通过常规方法检测。

Method: 通过最小化的合成实验，模拟观测可靠性退化但对推断过程不可观测的情形，检验标准推断方法在不同数据规模下的行为，包括收敛性、校准性和诊断指标的表现。

Result: 在特定结构下，随着数据增加，推断结果不仅不收敛于真实值，反而系统性偏离；残差和拟合优度检验仍显示正常，误导性地支持模型可靠性。

Conclusion: 数据量的增加不能保证推断的正确性；稳定性、收敛性和置信度并非知识有效性的充分条件。科学推断必须建立在对观测过程完整性的显式约束之上，而非单纯依赖数据积累。

Abstract: Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.

</details>


### [260] [Perception-Based Beliefs for POMDPs with Visual Observations](https://arxiv.org/abs/2602.05679)
*Miriam Schäfers,Merlijn Krale,Thiago D. Simão,Nils Jansen,Maximilian Weininger*

Main category: cs.LG

TL;DR: 提出PBP框架，利用图像分类器将视觉观测映射到状态概率分布，简化高维观测下的信念更新，提升传统POMDP求解器在复杂视觉环境中的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于信念和滤波的POMDP求解器难以处理高维观测（如摄像头图像），导致实际应用受限。需要一种有效方法将视觉输入转化为可计算的状态信念。

Method: 引入感知信念框架（PBP），使用图像分类器将视觉观测转换为状态概率分布，并直接用于信念更新；结合不确定性量化，改进信念更新以应对分类器误差。

Result: 实验表明，PBP优于现有端到端深度强化学习方法，在视觉退化情况下也表现出更强的鲁棒性。

Conclusion: PBP通过引入感知模型，显著提升了传统POMDP求解器在高维视觉任务中的适用性与稳定性，为复杂环境下的决策提供了有效解决方案。

Abstract: Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.

</details>


### [261] [Mining Generalizable Activation Functions](https://arxiv.org/abs/2602.05688)
*Alex Vitvitskyi,Michael Boratko,Matej Grcic,Razvan Pascanu,Deep Shah,Petar Veličković*

Main category: cs.LG

TL;DR: 本文提出利用进化搜索框架（如AlphaEvolve）来发现新的激活函数，强调其在扩大搜索空间和提升搜索效率方面的优势。通过使用前沿大模型作为变异算子，可探索所有符合特定计算预算的Python函数，无需手动设计搜索空间。此外，该方法能针对特定归纳偏置优化激活函数，例如通过分布外数据表现作为适应度函数，以捕捉数据内在结构。实验表明，小规模合成数据即可支持有效发现有意义的激活函数。


<details>
  <summary>Details</summary>
Motivation: 当前激活函数的研究旨在改善优化性能并保持表达能力，同时激活函数会影响模型的隐式归纳偏置。传统方法受限于手工构建的搜索空间，且难以高效探索复杂函数形式。因此需要更灵活、高效的搜索机制。

Method: 采用基于大语言模型的进化搜索框架（如AlphaEvolve），以前沿大模型作为变异算子，在给定FLOP预算下自动探索所有可能的Python函数构成的搜索空间。通过将分布外数据表现作为适应度函数，引导搜索过程发现具有特定归纳偏置的激活函数。

Result: 实验表明，仅使用小规模合成数据，AlphaEvolve即可成功发现具有较好性能和特定归纳偏置的激活函数；同时，该框架显著提升了搜索空间的广度与灵活性，并减少了对人工设计的依赖。

Conclusion: 进化搜索结合前沿大模型能够高效发现新型激活函数，不仅提升模型性能，还能实现对特定归纳偏置的精准控制，为激活函数设计提供新范式。

Abstract: The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.

</details>


### [262] [FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning](https://arxiv.org/abs/2602.05693)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: FedRandom is a novel technique to address contribution instability in Federated Learning by generating more samples through statistical estimation, leading to more reliable participant contribution assessments. It outperforms traditional methods in stability and accuracy across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Fairly assessing individual contributions in Federated Learning is challenging due to inherent instability in contribution estimations, which can discourage participation and hinder security against malicious actors. This instability undermines trust and incentivizes free-riding.

Method: FedRandom treats contribution instability as a statistical estimation problem and generates additional samples beyond standard FL aggregation strategies, improving the consistency and reliability of contribution evaluation.

Result: FedRandom reduces the distance to ground truth by over one-third in half the evaluated scenarios and enhances stability in more than 90% of cases across CIFAR-10, MNIST, CIFAR-100, and FMNIST.

Conclusion: FedRandom effectively mitigates contribution instability in Federated Learning, enabling fairer reward distribution, improved participant incentives, and better detection of malicious behavior, thus strengthening the sustainability and security of federated systems.

Abstract: Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.

</details>


### [263] [Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift](https://arxiv.org/abs/2602.05707)
*Amir Asiaee,Kaveh Aryan*

Main category: cs.LG

TL;DR: 本文研究机器学习中预测准确率与群体公平性之间的权衡，指出这种权衡可能是由于训练数据未能正确反映子群比例所导致的。在子群分布稳定但群体比例偏移的情况下，作者证明：(i) 完全重要性加权校正渐近无偏但有限样本下表现不佳；(ii) 有限样本下的最优校正是一个介于目标分布和训练分布之间的收缩重加权；(iii) 表面上的“公平性提升准确率”可能源于与未正确加权基线的错误比较。文章提出一种可操作的评估协议：先优化表示，再评估公平性干预，并以收缩校正后的基线为参照，从而分离出公平性的真正不可规避代价。在合成数据和真实世界基准（Adult、COMPAS）上的实验验证了理论预测，表明该协议能消除虚假权衡，揭示真实的公平-效用前沿。


<details>
  <summary>Details</summary>
Motivation: 机器学习实践中常观察到预测准确率与公平性之间的矛盾，但有时公平性干预反而提升准确率。这种现象可能并非真实存在，而是由训练数据中的子群比例失真所致。因此需要澄清公平性与准确率之间的真正关系，避免误判。

Method: 基于子群偏移（subpopulation shift）假设，分析重要性加权校正的渐近性质与有限样本表现，推导出最优的收缩重加权策略，并设计一种新的评估协议：先固定最优表示，再以收缩校正后的基线为参照，比较不同公平性干预方法。

Result: 理论与实验证明，在子群比例偏移下，不恰当的基线会导致虚假的‘公平性提升准确率’现象；采用收缩校正基线后，可消除此类伪相关，真实揭示公平性与准确率之间的权衡关系。在Adult和COMPAS数据集上验证了理论预测，展示了该评估协议的有效性。

Conclusion: 公平性与准确率之间的权衡是真实存在的，表面的‘公平性帮助准确率’往往是因基线设置不当造成的假象。通过使用收缩重加权的正确评估协议，可以准确识别公平性的真正代价，从而推动更可靠的公平性算法开发与评估。

Abstract: Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent "fairness helps accuracy" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.

</details>


### [264] [Muon in Associative Memory Learning: Training Dynamics and Scaling Laws](https://arxiv.org/abs/2602.05725)
*Binghui Li,Kaifei Wang,Han Zhong,Pinyan Lu,Liwei Wang*

Main category: cs.LG

TL;DR: Muon优化器通过梯度矩阵的符号进行参数更新，在实践中表现出显著性能提升，但其理论动态和缩放行为尚不明确。本文在具有softmax检索的线性关联记忆模型中研究了Muon，分析了查询-答案对的分层频率谱（有无标签噪声）。结果表明，梯度下降（GD）学习不同频率分量的速度极不平衡，导致低频分量成为收敛瓶颈；而Muon能有效缓解这种不平衡，实现更快且更均匀的进展。在无噪声情况下，Muon相比GD实现指数级加速；在具有幂律衰减频率谱的噪声情况下，推导出Muon的优化缩放定律，并证明其优于GD的缩放效率。进一步揭示Muon可被解释为由自适应任务对齐与块对称梯度结构产生的隐式矩阵预处理。相比之下，仅当能访问未知任务表示时，基于坐标符号的预处理才能达到类似效果，这在实际中不可行。合成长尾分类与LLaMA风格预训练实验验证了理论结论。


<details>
  <summary>Details</summary>
Motivation: 理解Muon优化器的理论机制与缩放行为，尤其是在存在频率不平衡和标签噪声的场景下，以揭示其为何在实践中表现优异。

Method: 采用线性关联记忆模型结合softmax检索，构建带有分层频率谱的查询-答案对数据集，分析梯度下降（GD）与Muon在有/无标签噪声条件下的优化动态，推导缩放规律，并从任务对齐与梯度结构角度解析Muon的隐式预处理特性。

Result: Muon在无噪声情况下实现指数级加速于GD；在噪声环境下，其优化缩放效率优于GD；Muon可被解释为由自适应任务对齐与块对称梯度结构形成的隐式预处理；基于坐标符号的预处理虽理论上等效，但依赖不可行的先验信息。

Conclusion: Muon的优越性能源于其对频率不平衡的有效缓解和隐式的自适应预处理能力，其理论机制与缩放规律得到系统验证，为优化器设计提供了新的理解视角。

Abstract: Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.

</details>


### [265] [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746)
*Xin Chen,Jie Zhang,Florian Tramer*

Main category: cs.LG

TL;DR: AutoInject 是一种基于强化学习的框架，用于生成通用且可迁移的对抗性后缀，以实现自动化提示注入攻击。该方法在不损害良性任务性能的前提下，优化攻击成功率，支持黑盒环境下的查询优化与跨模型、跨任务攻击。仅用1.5B参数的生成器，成功攻破GPT 5 Nano、Claude Sonnet 3.5和Gemini 2.5 Flash等前沿系统，在AgentDojo基准上建立了更强的自动化攻击基线。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入攻击依赖人工红队和手工构造提示，缺乏可扩展性和适应性；亟需一种自动化的、高效的优化方法来提升攻击能力。

Method: 提出AutoInject框架，采用强化学习机制联合优化对抗性后缀的攻击成功率与良性任务的性能保持，支持黑盒场景下的查询优化和跨模型/任务迁移攻击。

Result: 在仅使用1.5B参数的生成器情况下，成功对多个前沿大模型（如GPT 5 Nano、Claude Sonnet 3.5、Gemini 2.5 Flash）实施有效攻击，显著提升自动化提示注入攻击的性能与通用性。

Conclusion: AutoInject为自动化提示注入攻击提供了高效、可迁移的解决方案，推动了该领域从依赖人工向数据驱动优化的转变，为后续安全研究奠定了坚实基础。

Abstract: Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.

</details>


### [266] [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749)
*Kai Ming Ting,Wei-Jie Xu,Hang Zhang*

Main category: cs.LG

TL;DR: 该论文探讨了深度聚类（DC）是否真正克服了k-means聚类在发现任意形状、不同大小和密度的聚类方面的根本局限性。研究发现，尽管深度嵌入聚类（DEC）通过深度学习表示进行优化，但其并未有效解决这些基本问题；相反，一个非深度学习方法通过利用数据集中聚类的分布信息，能够更有效地应对这些挑战。


<details>
  <summary>Details</summary>
Motivation: 探究深度聚类方法（如DEC）是否真正克服了k-means聚类在处理复杂聚类结构上的固有缺陷，特别是对任意形状、大小和密度聚类的识别能力不足的问题。

Method: 通过对DEC方法的系统分析，并与基于分布信息的非深度学习方法进行对比，评估其在处理复杂聚类结构时的表现。

Result: 发现深度聚类方法并未显著优于传统方法，而基于数据分布信息的非深度学习方法在应对复杂聚类结构方面表现更优。

Conclusion: 深度聚类方法尚未真正克服k-means的根本局限性，关键在于未充分利用数据的内在分布特性；未来方法应更注重对数据分布信息的挖掘。

Abstract: Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.

</details>


### [267] [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774)
*Xiandong Zou,Jianshu Li,Jing Huang,Pan Zhou*

Main category: cs.LG

TL;DR: VSD提出一种基于变分推断的推测解码方法，通过最大化目标模型接受概率来优化生成路径质量，结合路径级效用与EM算法提升性能，在多个LLM和MLLM上实现最高9.6%的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法仅优化单一贪婪轨迹，忽略了多路径采样中的验证与排序过程，导致训练与解码不一致，影响推理效率。

Method: 将草稿训练建模为潜在提案（草稿路径）上的变分推断，使用ELBO最大化目标模型接受概率；引入路径级效用，并通过期望-最大化（EM）框架，结合MCMC采样与自适应拒绝加权（ARW）及置信度感知正则化（CAR）进行优化。

Result: 理论分析表明VSD可提高预期接受长度与加速比；实验显示在多种大模型上相比EAGLE-3和ViSpec分别实现最高9.6%和7.9%的推理速度提升。

Conclusion: VSD通过变分推断与高效优化机制，有效缓解了训练-解码差异，显著提升了推测解码的效率与稳定性。

Abstract: Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.

</details>


### [268] [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776)
*Mengbei Yan,Jiafei Lyu,Shengjie Sun,Zhongjian Qiao,Jingwen Yang,Zichuan Lin,Deheng Ye,Xiu Li*

Main category: cs.LG

TL;DR: 本文研究跨域离线强化学习，提出一种名为选择性转换修正（STC）的算法，通过逆策略模型和奖励模型校正源域数据的动作与奖励，使其对齐目标域动态。为提升校正效果，引入前向动力学模型筛选更匹配目标域的数据。实验表明，STC在存在动态差异的多种环境中均优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在跨域离线强化学习中，源域与目标域之间存在动态不匹配问题，直接合并数据集可能导致性能下降；现有方法如过滤或修改奖励，难以充分挖掘源域数据价值，因此需要更有效的数据对齐机制。

Method: 提出使用逆策略模型和奖励模型对源域动作与奖励进行校正，使源域数据适应目标域动态；结合前向动力学模型筛选出更符合目标域动态的修正样本，形成选择性转换修正（STC）算法。

Result: 在多个具有动态变化的环境中，所提出的STC算法显著优于现有方法，在政策适应方面表现出更强的鲁棒性和性能。

Conclusion: 通过校正源域数据并利用前向模型筛选高质量样本，STC能够有效利用源域数据实现跨域政策学习，克服动态不匹配问题，提升离线强化学习的性能与可靠性。

Abstract: It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.

</details>


### [269] [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783)
*Shutong Ding,Yimiao Zhou,Ke Hu,Mokai Pan,Shan Zhong,Yanwei Fu,Jingya Wang,Ye Shi*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散桥的分布强化学习方法（DBC），首次将扩散桥模型应用于强化学习中的价值函数估计（批评者）。传统方法多关注扩散策略，而忽视了对批评者的改进。由于价值估计对策略优化至关重要，且现实任务具有随机性，分布建模更合适。DBC通过直接建模Q值的逆累积分布函数（ICDF），精确捕捉值分布，并利用扩散桥的强大分布匹配能力防止分布坍缩。此外，作者推导出一个解析积分公式以减少离散化误差，提升估值精度。实验表明，DBC在MuJoCo机器人控制基准测试中优于现有分布批评者模型，且可作为即插即用模块集成到多数现有RL框架中。


<details>
  <summary>Details</summary>
Motivation: 现有扩散强化学习方法主要集中在扩散策略的应用，而忽略了批评者的重要性。准确的价值估计对于策略优化至关重要，尤其在具有随机性的任务中，分布建模比单一点估计更为合理。同时，现有方法容易导致价值分布坍缩为平凡高斯分布，因此需要更稳健的分布建模方式。

Method: 提出扩散桥批评者（DBC），直接建模Q值的逆累积分布函数（ICDF），利用扩散桥的强分布匹配能力实现高质量分布建模；推导解析积分公式以缓解离散化误差，提升价值估计精度。

Result: DBC在多个MuJoCo机器人控制任务上显著优于现有分布式批评者模型，具备良好的性能和泛化能力，且可无缝集成到主流强化学习框架中。

Conclusion: DBC是首个将扩散桥模型用于批评者的分布强化学习方法，通过精确建模值分布和抑制分布坍缩，实现了更优的价值估计。该方法不仅有效，且具有良好的可扩展性和兼容性，为未来分布强化学习提供了新思路。

Abstract: Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.

</details>


### [270] [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797)
*Caihong Qin,Yang Bai*

Main category: cs.LG

TL;DR: 本文将局部差分隐私（LDP）下的私有学习重新诠释为迁移学习问题，利用噪声数据作为源域，未观测的干净数据作为目标域，提出三种新方法：基于噪声二值反馈的评估机制、模型反转以修复性能不佳的分类器，以及基于估计效用加权的模型平均。理论分析提供了在LDP下的超额风险界，并证明所提方法可降低风险；实验结果表明，在模拟和真实数据集上均显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: LDP虽然提供了强隐私保障，但引入的噪声会严重降低数据效用，影响分类性能。为解决这一问题，本文提出将私有学习视为迁移学习问题，以提升数据使用效率而不牺牲隐私。

Method: 1. 基于噪声二值反馈的评估机制，用于估计数据集效用；2. 模型反转技术，通过逆向决策边界修复表现不佳的分类器；3. 模型平均，根据估计的效用对多个反转分类器进行加权集成。

Result: 理论分析给出了在LDP下的超额风险边界，证明所提方法能有效降低风险；实验证明在模拟与真实数据集上分类准确率有显著提升。

Conclusion: 本文提出的迁移学习视角与三项技术有效缓解了LDP带来的数据效用下降问题，显著提升了分类性能，同时保持了强隐私保障。

Abstract: Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.

</details>


### [271] [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812)
*Matteo Gätzner,Johannes Kirschner*

Main category: cs.LG

TL;DR: 本文提出了一种用于计算断层扫描（CT）重建中置信度估计的原理性框架，基于序列似然混合框架，为基于深度学习的CT重建建立了具有理论覆盖保证的置信区域。该框架采用符合临床和科学成像条件的泊松噪声对数线性前向模型（遵循比尔-朗伯定律），适用于经典算法与深度学习方法（如U-Net、U-Net集成和生成式扩散模型）。实验表明，深度重建方法在保持理论覆盖性的前提下，显著产生更紧致的置信区域，能够检测图像中的幻觉现象，并提供可解释的置信区域可视化。该方法使深度模型不仅具备强大估计能力，也成为可信赖的不确定性感知医学成像工具。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习重建方法虽性能优越，但缺乏可靠的置信度估计机制，难以在医学影像中识别幻觉或不确定区域，限制其临床可信度。因此需要一种兼具理论保障与实际应用价值的置信度估计框架。

Method: 基于序列似然混合框架，构建在符合比尔-朗伯定律的泊松噪声前向模型下的置信区域估计方法，适用于多种重建算法（包括传统方法与深度学习模型），通过理论推导确保覆盖率，并结合可视化实现不确定性分析。

Result: 深度学习重建方法在保持理论覆盖的前提下，生成更紧凑的置信区域；可有效检测图像中的幻觉；提供直观的置信度可视化，增强结果可解释性。

Conclusion: 所提出的置信度估计框架为深度学习驱动的医学影像重建提供了理论保障与可解释性支持，使其从单纯的高精度估计器转变为可靠的不确定性感知工具，推动其在临床实践中的可信应用。

Abstract: We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.

</details>


### [272] [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813)
*Artem Riabinin,Andrey Veprikov,Arman Bolatov,Martin Takáč,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文研究了针对范数约束优化器（如Muon和Lion）的自适应学习率调度。提出了一种广义平滑性假设，表明局部曲率随次优性差距减小，并在优化轨迹上进行了实证验证。在此假设下，证明了在适当学习率选择下具有收敛性保证，且学习率的预热后衰减策略自然地从理论推导中产生，而非人为强加。基于该理论，设计了一种仅依赖标准超参数、能自动调整初始预热时长的实用学习率调度器。在LLaMA架构的大语言模型预训练任务中评估该方法，结果表明其自适应预热选择在所有测试设置下均优于或至少匹配最佳手动调优的预热方案，且无需额外超参数搜索。源代码已公开。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调度策略多为启发式设计，缺乏理论支持；尤其对于范数约束优化器（如Muon和Lion），如何合理设定预热阶段长度仍依赖经验调参。本文旨在建立理论基础，使学习率调度可自适应生成，减少人工干预。

Method: 提出广义平滑性假设，分析局部曲率与次优性差距的关系；基于此推导出收敛性条件，从而自然导出学习率预热+衰减策略；设计一种仅依赖标准超参数的自适应调度器，自动确定预热时长。

Result: 所提方法在大语言模型预训练中表现优异，自适应预热策略始终优于或等同于最优手动调参方案，且无需额外超参数搜索。

Conclusion: 本文通过理论分析建立了范数约束优化器下学习率调度的自适应机制，提出了一个高效、鲁棒且无需额外调参的调度器，在大规模语言模型训练中展现出显著优势。

Abstract: We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.
  Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup

</details>


### [273] [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833)
*Laura Plein,Alexi Turcotte,Arina Hallemans,Andreas Zeller*

Main category: cs.LG

TL;DR: 本文提出了一种新的合成数据生成方法，通过间接利用原始数据，在不直接暴露原始数据的情况下生成具有与原数据相同统计特性的合成测试数据。该方法受GAN启发，采用生成器（模糊测试工具）和判别器的协同机制，生成满足原始数据约束的数据样本，并通过判别器评估其接近度，从而实现高保真度且隐私保护的数据生成。在四个基准数据集上的实验表明，该方法在保持数据实用性的同时有效提升了隐私安全性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的合成数据生成方法存在生成数据准确性不足或易受成员推断攻击和数据重建攻击的问题，因为它们直接使用了原始数据进行训练。因此，需要一种能够复制原始数据统计分布但不直接依赖原始数据的方法，以在保障隐私的前提下生成高质量的合成数据。

Method: 受GAN启发，提出一种基于生成器（测试生成器/模糊测试工具）与判别器协同工作的框架。生成器根据输入规范生成符合原始数据约束的测试数据；判别器评估生成数据与原始数据的相似性，指导生成器优化。通过迭代演化和筛选‘优质’样本，实现对原始数据统计特性的逼近，同时避免直接访问原始数据，从而保障隐私。

Result: 在四个常用基准数据集上的实验结果表明，所提方法生成的合成数据在统计特性上与原始数据高度一致，具有较高的数据实用性；同时，由于未直接使用原始数据训练模型，有效降低了成员推断和数据重建风险，展现出良好的隐私保护能力。

Conclusion: 本文提出的间接式合成数据生成方法能够在不泄露原始数据的前提下，生成具有高统计保真度和实用性的合成数据，为隐私保护下的数据共享与测试提供了可行路径。该方法在兼顾数据效用与隐私安全方面表现优异，具备广泛的应用前景。

Abstract: There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining "good samples" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.

</details>


### [274] [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887)
*Tianqi Shen,Jinji Yang,Junze He,Kunhan Gao,Ziye Ma*

Main category: cs.LG

TL;DR: 本文提出了一种名为模拟预言方向（SOD）的逃逸机制，用于在不进行实际张量升维的情况下，通过模拟过参数化空间的景观和逃逸方向，从低秩矩阵感知中的虚假局部极小值中逃逸。该方法构建了一个数学框架，将过参数化空间的逃逸方向投影回原始参数空间，确保目标函数值严格下降，从而实现对虚假局部极小值的确定性逃逸。这是首个无需随机扰动或启发式估计即可保证逃逸的确定性框架。数值实验表明，该方法能可靠地逃离局部极小值并收敛到全局最优解，且计算开销远低于显式的张量过参数化。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵感知是一个具有挑战性的非凸问题，其优化景观通常包含大量虚假局部极小值，导致基于梯度的优化器难以收敛到全局最优解。尽管已有研究发现通过张量升维进行过参数化可将这些虚假局部极小值转化为严格的鞍点，但直接升维在计算上不可行。因此，亟需一种高效且可保证逃逸的方法来克服这一难题。

Method: 提出模拟预言方向（SOD）机制，通过构建数学框架，模拟过参数化空间的优化景观，并将该空间中的逃逸方向投影回原始参数空间，以确保目标函数值严格下降，从而实现从虚假局部极小值的确定性逃逸，而无需实际执行张量升维。

Result: 所提框架在数值实验中表现出色，能够可靠地逃离局部极小值并引导优化过程收敛至全局最优解；同时，其计算成本显著低于显式张量过参数化方法，具备良好的实用性与效率。

Conclusion: 本工作首次提出了一个无需随机扰动或启发式估计的确定性框架，通过模拟过参数化实现对虚假局部极小值的可靠逃逸，为非凸优化提供了新思路，尤其在低秩矩阵感知等复杂优化问题中具有广泛适用性与深远影响。

Abstract: Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.

</details>


### [275] [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896)
*Alexander Kozachinskiy,Tomasz Steifer,Przemysław Wałȩga*

Main category: cs.LG

TL;DR: 本文提出了一种新的、仅使用单层和单头的Transformer架构来解决PARITY问题，该架构采用不依赖长度且多项式有界的位置编码，使用softmax函数，无需LayerNorm，并在有无因果掩码的情况下均能工作。同时，首次证明了单层单头Transformer无法解决PARITY问题，给出了首个关于Transformer计算能力的下界结果。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已有近十年历史，但对其计算能力的理解仍有限。特别是对于如PARITY这样的简单逻辑问题，现有方法需要至少两层并依赖不切实际的假设（如长度相关位置编码、hardmax、无正则化参数的LayerNorm或非因果掩码）。因此，亟需一种更简洁、更实用的构造方式，并明确其理论边界。

Method: 通过设计一个基于多项式有界、长度无关的位置编码的单层单头Transformer模型，结合softmax激活函数，避免使用LayerNorm与特殊掩码机制，实现对PARITY问题的有效求解；同时利用形式化分析证明单层单头架构无法表达PARITY，从而建立下界。

Result: 成功构建了一个满足所有实际条件的单层单头Transformer用于求解PARITY问题，且首次证明了单层单头Transformer无法解决该问题，为Transformer的计算能力提供了理论限制。

Conclusion: 本研究揭示了单层单头Transformer在计算能力上的局限性，同时展示了在合理假设下构造高效Transformer的可能性，推动了对Transformer基本计算边界的理解。

Abstract: The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.
  We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.

</details>


### [276] [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902)
*Seohyeon Cha,Huancheng Chen,Dongjun Kim,Haoran Zhang,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 本文提出一种基于正则化非对称校准的有界搜索方法，通过在对称与非对称校准之间插值实现正则化，提升后训练量化（PTQ）在低比特权重表示中的鲁棒性。该方法结合简单迭代舍入和可控制计算开销的扩展搜索机制，在多个大语言模型家族、比特位宽和基准测试中均显著优于传统PTQ基线，同时保持较低额外开销。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法受限于量化目标与舍入策略，易受激活值失配影响，导致性能下降。为提升量化鲁棒性与精度，需设计更优的校准与舍入机制。

Method: 提出在对称与非对称校准间进行插值以实现正则化，导出一种自然包含非对称校准的迭代舍入方法，并引入有界搜索扩展，允许在量化质量与计算成本间显式权衡。

Result: 实验表明，所提方法在多种大语言模型、比特位宽及任务上均持续优于现有PTQ基线，显著降低困惑度并提升准确率，且额外计算开销小且可控。

Conclusion: 基于正则化非对称校准的有界搜索方法有效提升了后训练量化的性能与鲁棒性，是一种高效、可控且通用的优化方案。

Abstract: Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.

</details>


### [277] [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903)
*András Balogh,Márk Jelasity*

Main category: cs.LG

TL;DR: 本文研究生成式序列模型在棋类语言中的有效性，提出对抗性序列生成方法以验证模型的正确性（soundness），发现所有测试模型均不完全正确，但训练方法和数据选择可显著提升表现。此外，研究表明棋盘状态在多数模型中并非影响下一步预测的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探究生成式序列模型是否能准确捕捉语言或规则系统的真实结构，尤其是确保其生成的序列是合法的；现有理论表明只能保证生成有效序列，无法保证覆盖全部合法序列，因此需要实用工具验证模型的正确性。

Method: 提出对抗性序列生成方法，通过构造合法序列迫使模型生成非法下一步动作，从而检测其非正确性；设计多种对抗生成策略，并在大量国际象棋模型上进行评估，同时引入棋盘状态探测分析其对预测的影响。

Result: 所有训练出的模型均不满足完全正确性；不同训练方法和数据集的选择显著影响模型的正确性表现；棋盘状态在大多数模型中未起到因果作用于下一步动作预测。

Conclusion: 生成式序列模型在复杂规则系统如国际象棋中难以实现完全正确性，但可通过优化训练策略改善其表现；棋盘状态并非模型决策的核心依据，提示需重新审视模型学习机制。

Abstract: Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.

</details>


### [278] [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910)
*Seoirse Murray,Allison Qi,Timothy Qian,John Schulman,Collin Burns,Sara Price*

Main category: cs.LG

TL;DR: 该论文探讨了大语言模型（LLM）后训练中因使用多样数据集而产生的隐性模式问题，这些模式导致模型学习到虚假相关性（称为'chunky post-training'），从而产生意外行为。作者提出SURF（黑盒运行时行为探测工具）和TURF（故障溯源工具），用于识别并追踪这些异常行为的来源。在多个前沿及开源模型上验证，发现模型偏差往往源于训练数据中不平衡或不明确的数据块。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在后训练阶段使用多种数据集，但这些数据集可能包含开发者未察觉的隐性模式，如格式与内容的偶然关联、特定表达方式的过度泛化等，导致模型产生意外行为，例如对正确事实因格式不同而拒绝。这种现象被称为'chunky post-training'，亟需工具来检测和追溯此类问题。

Method: 提出SURF（表面行为探测器）和TURF（故障溯源工具）。SURF通过黑盒方式在推理阶段检测模型的异常行为；TURF则利用反向分析技术，将失败案例回溯至具体的训练数据块，以定位问题来源。

Result: 在包括Claude 4.5、GPT-5.1、Grok 4.1、Gemini 3和Tülu 3在内的多个主流模型上应用，结果表明：模型确实存在由数据块不平衡或定义模糊引发的校准偏差，且这些偏差可通过SURF和TURF有效识别与溯源。

Conclusion: 大语言模型的后训练过程容易引入隐性模式，导致不可预测的行为。通过SURF和TURF可有效揭示并解决这类问题，提升模型的可靠性和可控性。

Abstract: LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (Tülu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.

</details>


### [279] [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933)
*Zhenghao Xu,Qin Lu,Changlong Yu,Tuo Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为PMD-mean的实用算法，通过用采样策略下的平均奖励近似对数分区项，并在对数策略空间中进行回归，以解决大模型强化学习中因有限回放导致的分区函数估计难题。该方法隐式优化了一个自适应混合KL-χ²正则化镜面下降子问题，其中额外的χ²正则化限制了概率的大幅变化，在期望奖励较低时产生更保守的更新，从而提升对有限样本估计误差的鲁棒性。实验表明，PMD-mean在数学推理任务中表现更优，兼具更高的稳定性和时间效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模语言模型的强化学习中，政策镜面下降（PMD）虽具理论优势，但其理想闭式更新依赖于可靠的分区函数估计，而实际中由于动作空间巨大且回放数据有限，这一目标难以实现。因此需要一种稳健、高效的近似方法来克服此瓶颈。

Method: 提出PMD-mean算法，使用采样策略下的平均奖励近似对数分区项，并在对数策略空间中执行回归；通过理论分析揭示其等价于一个带有自适应混合KL-χ²正则化的镜面下降子问题。

Result: 在数学推理任务上，PMD-mean展现出优于基线的方法，具有更高的性能、更强的稳定性以及更快的训练速度。同时，该方法通过χ²正则化提升了对有限样本误差的鲁棒性。

Conclusion: PMD-mean不仅是一种有效的实践算法，而且其背后的理论机制揭示了自适应混合正则化的作用，为未来设计更稳健、高效的强化学习算法提供了新思路，尤其适用于大模型场景。

Abstract: Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$χ^2$ regularizer. This additional $χ^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.

</details>


### [280] [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.LG

TL;DR: 本文研究基于黎曼几何的降维方法，强调数据底层流形结构的保持。重点探讨主测地线分析（PGA）作为PCA在流形数据上的非线性推广，并通过黎曼适应扩展判别分析等降维方法。这些方法利用测地线距离、切空间表示和内在统计量，实现更忠实的低维嵌入。实验表明，相较于欧氏方法，黎曼方法在超球面和对称正定流形等弯曲空间上表现出更优的表示质量和分类性能，凸显几何感知降维在现代机器学习中的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法如PCA基于欧氏空间假设，难以有效处理具有复杂几何结构的数据（如流形）。当数据位于弯曲空间（如超球面或对称正定矩阵空间）时，欧氏方法会扭曲真实结构，导致信息损失。因此需要发展几何感知的降维方法，以更好地保留数据的内在流形特性。

Method: 采用黎曼几何框架，引入测地线距离、切空间映射及流形上的统计量；将主成分分析（PCA）推广为适用于流形的主测地线分析（PGA）；并构建基于黎曼几何的判别分析方法，如黎曼判别分析（RDA），以实现流形上数据的有效降维与分类。

Result: 在多个代表性数据集上的实验表明，黎曼几何方法相比传统欧氏降维方法，在低维嵌入质量与分类准确率方面均有显著提升，尤其在超球面和对称正定流形数据上表现突出，验证了其在保持几何结构方面的有效性。

Conclusion: 几何感知的降维方法，特别是基于黎曼几何的方法，能够更准确地捕捉数据的真实流形结构，是处理非线性、弯曲数据的重要工具。该研究强调了在现代机器学习和数据科学中融入几何先验的重要性。

Abstract: In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.

</details>


### [281] [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943)
*Sihan Yang,Kexuan Shi,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出OrthoMerge方法，通过在正交群构成的黎曼流形上进行模型合并，保留预训练权重的内在几何结构（如超球面能量），解决传统线性合并方法破坏几何特性的缺陷。该方法利用正交微调（OFT）学习到的正交矩阵映射至李代数，实现方向与强度的合理整合；并进一步通过正交-残差解耦策略扩展至非OFT方法（如低秩、全量微调），提取正交分量在流形上合并，残差部分采用标准加法合并。实验表明，OrthoMerge能有效缓解灾难性遗忘，保持多任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法基于欧氏空间中的线性算术，常破坏预训练权重的内在几何特性（如超球面能量），影响模型性能和稳定性。因此需要一种能保持权重几何结构的合并方法。

Method: 提出OrthoMerge，将模型合并操作置于由正交群构成的黎曼流形上，利用正交微调（OFT）得到的正交矩阵映射到李代数，实现对方向与强度的联合建模；对非OFT方法，引入正交-残差解耦策略，通过正交Procrustes问题提取正交分量，在流形上合并，残差部分使用加法合并。

Result: 实验证明，OrthoMerge在多种任务上显著优于传统合并方法，有效缓解了灾难性遗忘，保持了模型在不同任务上的综合性能。

Conclusion: OrthoMerge通过在黎曼流形上进行正交合并，成功保留了模型权重的几何结构，提升了多任务融合能力，为高效、稳定的大型语言模型合并提供了新范式。

Abstract: Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.

</details>


### [282] [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946)
*Rajdeep Haldar,Lantao Mei,Guang Lin,Yue Xing,Qifan Song*

Main category: cs.LG

TL;DR: 本文从分歧估计的角度扩展了偏好对齐（PA）的视角，提出$f$-GRPO和$f$-HAL两种新方法，适用于包括强化学习与可验证奖励（RLVR）在内的通用对齐场景。理论与实证均证明其能有效提升对齐后的平均奖励，且在数学推理与安全对齐任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法局限于特定场景，缺乏统一框架；尤其在仅有环境奖励的RLVR设置下，传统方法难以有效应用。因此需要一个更通用、理论严谨的对齐框架。

Method: 基于$f$-散度的变分表示，提出$f$-GRPO（在线策略）和$f$-HAL（混合策略）两类目标函数，统一处理PA与RLVR等多样对齐任务。

Result: 理论证明所提方法能提升对齐后的平均奖励；实验显示在数学推理（RLVR）和安全对齐（PA）任务中均优于现有方法，具备更强性能与灵活性。

Conclusion: 本文提出的统一框架及相应算法为大模型对齐提供了新的理论基础与实践工具，在多种复杂设置下展现出优越性。

Abstract: Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.

</details>


### [283] [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961)
*Arran Carter,Sanghyeok Choi,Kirill Tamogashev,Víctor Elvira,Nikolay Malkin*

Main category: cs.LG

TL;DR: 本文提出了一种用于离散空间的扩散采样算法，通过引入非策略训练技术，提升了离散扩散采样的性能，并首次将数据到能量的薛定谔桥训练方法推广至离散域。此外，还展示了该方法在图像生成模型离散潜在空间中无数据后验采样的应用。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散采样方法未能充分利用连续空间采样中的成熟技术，存在性能提升空间。

Method: 提出基于非策略训练的离散扩散采样方法，引入数据到能量的薛定谔桥训练框架，适用于任意分布间的桥梁构建。

Result: 在合成基准测试中显著提升了采样性能，并成功应用于图像生成模型的无数据后验采样任务。

Conclusion: 所提出的离散扩散采样方法有效弥合了离散与连续空间采样之间的差距，为复杂离散分布建模提供了新工具。

Abstract: Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.

</details>


### [284] [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Lucas Pellicer,Rosimeire Pereira Costa,Edson Bollis,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文提出一种基于层选择的高效微调方法，通过识别对模型适应性贡献最大的少数层进行LoRA微调，显著减少可训练参数量（最多降低50%），同时保持甚至提升下游任务性能。该方法与现有低秩适配技术兼容，适用于编码器-解码器及多模态模型，在GLUE、数学推理和代码生成等任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模持续扩大，传统参数高效微调方法（如LoRA）虽已大幅减少可训练参数，但仍难以满足极端规模模型的需求。因此需要更进一步压缩参数量，同时保持或提升模型性能。

Method: 通过测量各层内部表示变化的贡献度，系统性地筛选出最相关层进行LoRA微调；利用中心核对齐（CKA）评估层间表示差异，实现关键层的自动识别。

Result: 在编码器-解码器架构上，参数减少50%时，GLUE基准性能几乎无损；在解码器架构上，数学与编程任务性能略有下降或反而提升；多模态模型也达到与全层LoRA相当的效果。

Conclusion: 通过有选择地微调关键层，可在极大降低可训练参数的同时维持甚至增强模型性能，为超大规模模型的高效微调提供了有效解决方案。

Abstract: Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA

</details>


### [285] [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993)
*Peter Holderrieth,Douglas Chen,Luca Eyring,Ishin Shah,Giri Anantharaman,Yutong He,Zeynep Akata,Tommi Jaakkola,Nicholas Matthew Boffi,Max Simchowitz*

Main category: cs.LG

TL;DR: Diamond Maps 是一种新型的随机流映射模型，旨在实现高效且精确的奖励对齐。该模型将多个模拟步骤合并为单步采样，同时保持了最优奖励对齐所需的随机性，使搜索、序列蒙特卡洛和引导方法在推理时更加高效和一致。通过从 GLASS 流中蒸馏学习，Diamond Maps 能够高效训练，表现出更强的奖励对齐性能并具备更好的可扩展性，为生成模型在推理时快速适应任意偏好和约束提供了实用路径。


<details>
  <summary>Details</summary>
Motivation: 现有流模型和扩散模型在训练后难以高效、稳健地适应用户偏好或约束，即存在奖励对齐难题。当前方法通常将对齐作为事后处理，效率低且脆弱。因此，需要将高效奖励对齐作为生成模型的内在属性，从模型设计层面解决该问题。

Method: 提出 Diamond Maps 模型，结合流映射的单步采样优势与随机性，实现高效的推理阶段奖励对齐。通过蒸馏技术从 GLASS Flows 学习参数，并利用其结构支持高效的值函数估计，从而实现搜索、序列蒙特卡洛和引导的可扩展性。

Result: Diamond Maps 能够高效学习，相比现有方法在奖励对齐任务上表现更优，具有更强的可扩展性。实验验证了其在推理阶段快速适应不同奖励信号的能力，展现出实际应用潜力。

Conclusion: Diamond Maps 代表了一种面向高效奖励对齐的生成模型新范式，通过内在设计实现灵活、快速的推理阶段调整，为构建可动态适应用户需求的生成系统提供了可行路径。

Abstract: Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose "Diamond Maps", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.

</details>


### [286] [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996)
*Leo Zhang,James Martens*

Main category: cs.LG

TL;DR: 提出正交自注意力（OSA）机制，通过矩阵指数映射将查询-键值的反对称矩阵转化为正交注意力矩阵，以解决无跳接架构中softmax自注意力的不稳定性问题。该方法计算复杂度和内存开销与序列长度线性相关，并设计了保证雅可比矩阵良好条件数的初始化方案。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力在无跳接的Transformer架构中存在秩坍缩和雅可比矩阵条件差的问题，导致训练困难，亟需一种更稳定的注意力机制替代方案。

Method: 通过将查询-键值构造的反对称矩阵经矩阵指数映射，生成正交注意力矩阵；利用查询-键值的低秩特性实现高效计算，使复杂度线性于序列长度；并提出相应的初始化策略以确保雅可比矩阵的良好条件性。

Result: OSA机制有效避免了秩坍缩和不良条件的雅可比问题，在无需跳接和归一化层的情况下仍能稳定训练非因果Transformer模型，且计算效率高。

Conclusion: Orthogonal Self-Attention (OSA) 是一种有效的替代方案，可在无跳接、无归一化层的Transformer中实现稳定训练，为简化架构提供了新路径。

Abstract: Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.

</details>


### [287] [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014)
*Shunxing Yan,Han Zhong*

Main category: cs.LG

TL;DR: 本文研究了在自适应数据收集下，汤普森采样（Thompson Sampling, TS）在多臂高斯老虎机中的推断性质。经典渐近理论因各臂抽样次数随机且与奖励耦合而失效。作者识别出‘乐观性’是恢复‘稳定性’的关键机制，该机制确保每臂抽取次数集中在确定性尺度上。研究证明，方差膨胀的汤普森采样对任意K≥2均稳定，包括多个最优臂的复杂情形，解决了此前未解问题。此外，作者还分析了一种保持后验方差不变但增加显式均值奖励的乐观修改方法，同样证明其稳定性。结论表明，合理引入乐观性可使TS实现渐近有效的推断，且仅带来轻微额外后悔代价。


<details>
  <summary>Details</summary>
Motivation: 经典渐近理论在自适应数据收集场景下不适用，因为各臂的样本量是随机且与选择策略耦合的。因此需要理解并解决汤普森采样在多臂带问题中的推断有效性问题，特别是在存在多个最优臂的情况下。

Method: 通过理论分析，研究了方差膨胀型和均值奖励型两种乐观修正的汤普森采样方法，并证明它们在任意K≥2情况下均满足稳定性条件，从而支持渐近有效推断。

Result: 证明了方差膨胀和均值奖励两种乐观修正版本的汤普森采样在一般K-臂设置中均具有稳定性，实现了渐近有效推断，且额外后悔代价较小。

Conclusion: 合理引入乐观性能够稳定汤普森采样，使其在多臂带问题中具备渐近有效的推断能力，同时保持较低的后悔成本，为实际应用提供了理论保障。

Abstract: Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.

</details>


### [288] [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029)
*Yingke Li,Anjali Parashar,Enlu Zhou,Chuchu Fan*

Main category: cs.LG

TL;DR: 该论文首次为最小化期望自由能（EFE）的智能体提供了理论保证，证明了足够的好奇心可同时确保自洽学习（贝叶斯后验一致性）和无悔优化（有界累积后悔）。研究揭示了该机制如何依赖于初始不确定性、可辨识性和目标对齐性，并将主动推理（AIF）与经典贝叶斯实验设计及贝叶斯优化统一在一个理论框架中。此外，论文提出了实用的设计指南以调节混合学习-优化问题中的认知-功利权衡，并通过真实世界实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前主动推理（AIF）在平衡探索与利用时缺乏清晰的理论依据，尤其是好奇心系数的设置对学习一致性与决策效率的影响尚不明确。过低好奇心会导致短视利用，无法解决不确定性；过高则引发不必要的探索和后悔。因此需要建立理论保障来指导实际应用。

Method: 基于贝叶斯推理与信息论，构建理论模型分析EFE最小化下的学习与决策行为，引入‘足够好奇心’作为核心假设，推导出后验一致性与无悔优化的条件，并结合初始不确定性、可辨识性与目标对齐性进行形式化刻画。进一步将理论转化为可操作的设计原则，通过真实场景实验验证其有效性。

Result: 证明了‘足够好奇心’这一单一条件可同时保证自洽学习与无悔优化；揭示了关键影响因素（初始不确定性、可辨识性、目标对齐性）的作用机制；提出并验证了适用于混合学习-优化任务的实用调参指南。

Conclusion: 主动推理通过合理设定好奇心可实现一致的学习与高效的决策，本研究建立了其理论基础，并为实际系统设计提供了可靠指导。

Abstract: Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.

</details>


### [289] [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031)
*Claus Hofmann,Christian Huber,Bernhard Lehner,Daniel Klotz,Sepp Hochreiter,Werner Zellinger*

Main category: cs.LG

TL;DR: AP-OOD 是一种新颖的文本 OOD 检测方法，通过利用分词级别的信息超越传统的平均聚合方式，在半监督框架下灵活结合无监督与有监督设置，仅需少量辅助异常数据即可实现优异性能。在无监督场景下，其在 XSUM 摘要任务上的 FPR95 从 27.84% 降至 4.67%，在 WMT15 En-Fr 翻译任务上从 77.08% 降至 70.37%，达到当前最佳表现。


<details>
  <summary>Details</summary>
Motivation: 现有 OOD 检测方法在利用语言模型的分词嵌入时，多依赖简单的平均聚合，难以充分挖掘分词级信息，导致检测性能受限。如何有效聚合分词嵌入并适应有限标注异常数据是关键挑战。

Method: AP-OOD 提出一种基于分词级信息的新型聚合策略，结合半监督学习框架，通过可调节的插值机制在无监督与有监督之间动态平衡，充分利用少量辅助异常样本提升检测能力。

Result: 在多个文本任务上显著优于现有方法：XSUM 上 FPR95 降低至 4.67%，WMT15 En-Fr 上降低至 70.37%，在无监督设置下达到新的 SOTA 水平。

Conclusion: AP-OOD 证明了分词级信息在文本 OOD 检测中的重要性，提出了一种高效且灵活的半监督框架，为实际部署中可靠模型的构建提供了强有力支持。

Abstract: Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.

</details>


### [290] [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033)
*Luca M. Schulze Buschoff,Konstantinos Voudouris,Can Demircan,Eric Schulz*

Main category: cs.LG

TL;DR: 该研究探讨了预训练视觉语言模型在物理世界理解上的局限性，发现监督微调虽能提升简单物理任务的表现，但无法建立可泛化的物理规则。基于认知科学假设，研究尝试通过强化学习让模型与环境交互以学习物理动态，但结果显示，即使通过交互学习，模型在跨任务泛化上仍表现不佳，表明当前方法难以获得通用的物理直觉。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉语言模型缺乏对物理世界的良好直觉，尽管监督微调能提升特定任务表现，但无法形成可泛化的物理规则。为提升模型对物理规律的理解，需探索更有效的学习机制。

Method: 基于认知科学假设，提出模型应通过与环境的交互来学习物理动态，采用强化学习方法训练模型进行环境交互，并评估其在不同任务间的泛化能力。

Result: 通过交互学习的模型虽提升了任务内性能，但在跨任务泛化方面表现不佳，即便任务间共享视觉特征和物理原理，也无法实现可靠迁移。

Conclusion: 当前基于交互的强化学习方法尚不足以使模型习得通用且可泛化的物理直觉，提示需要新的学习范式或机制来增强模型对物理世界的理解能力。

Abstract: Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.

</details>
