<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了大视觉-语言模型（VLMs）在盲人和低视力（BLV）用户可访问性描述方面的性能，评估了参数量为500M和2.2B的SmolVLM2模型在AVCaps（户外）和Charades（室内）数据集上的表现。提出两种新型评估框架：多上下文BLV框架（评估空间方位、社交互动、动作事件和氛围）与导航辅助框架（聚焦移动关键信息）。系统比较四种提示设计策略，并在智能手机上部署模型，测试FP32与INT8精度版本以评估资源受限设备的实际性能。


<details>
  <summary>Details</summary>
Motivation: 大视觉-语言模型虽在视频描述生成方面表现出色，但其高内存、计算和部署需求限制了实际应用，尤其对依赖详细、上下文感知描述的盲人和低视力（BLV）用户而言。因此需要评估模型规模对可访问性描述质量的影响，并开发适合BLV用户的评估框架与优化方案。

Method: 采用SmolVLM2系列模型（500M和2.2B参数），在两个不同数据集（AVCaps与Charades）上进行评估；设计并应用两个新评估框架——多上下文BLV框架与导航辅助框架；系统测试四种提示设计策略；在移动端部署模型，对比FP32与INT8精度下的性能表现。

Result: 实验表明，较小参数模型在保持较高描述质量的同时显著降低资源消耗，且通过优化提示设计与量化策略，可在手机端实现高效运行。多上下文与导航辅助框架有效识别出对BLV用户至关重要的语义信息，验证了其在评估可访问性描述中的有效性。

Conclusion: 小规模视觉-语言模型结合精心设计的提示与量化技术，能够在资源受限设备上实现对盲人和低视力用户友好的高质量视频描述生成。所提出的评估框架为未来无障碍视频理解系统的设计与评测提供了重要工具。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: Self-Consistency Sampling (SCS) addresses the issue of unfaithful reasoning trajectories in outcome-reward reinforcement learning for multimodal large language models (MLLMs), where incorrect chains of thought that guess the right answer receive the same reward as correct ones. SCS introduces visual perturbations and repeated trajectory sampling to compute a consistency score, which downweights unreliable traces during training. It improves accuracy by up to 7.7% on six benchmarks with minimal computational cost and works across multiple MLLM models.


<details>
  <summary>Details</summary>
Motivation: Existing outcome-reward RL methods in MLLMs fail to distinguish between genuine reasoning and lucky guesses, leading to suboptimal model training. This flaw undermines the reliability of reasoning refinement.

Method: SCS applies small visual perturbations to inputs and repeatedly truncates and resamples initial reasoning trajectories. Consistency among the resulting trajectories is used to compute a differentiable score that penalizes unreliable paths during policy updates.

Result: SCS boosts accuracy by up to 7.7 percentage points on six multimodal benchmarks when integrated into RLOO, GRPO, and REINFORCE++ frameworks. The method requires negligible extra computation and generalizes well across Qwen2.5-VL-7B, Qwen2.5-VL-3B, and InternVL3-8B models.

Conclusion: SCS provides a simple, effective, and general solution to improve the fidelity of reasoning in MLLMs by penalizing inconsistent and unreliable trajectories in outcome-reward RL settings.

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [3] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文提出了一种增强编码器的因果解码器模型架构，该架构在训练效率上表现更优，并在有限硬件条件下实现了比传统因果Transformer更高的语言压缩效果。研究展示了如何对每个词元进行熵估计，并表明当模型训练目标接近但不超过估计的每词元熵时，其泛化能力优于传统最小化损失的目标。这揭示了语言模型性能存在由语言内在信息熵决定的理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在估计语言熵方面计算成本过高，且无法有效反映语言固有的信息熵限制。因此需要一种更高效、可扩展的方法来逼近语言熵的下界，从而提升模型压缩效率与泛化能力。

Method: 引入一种带有编码器增强的因果解码器架构，利用该结构在较低资源消耗下实现更高效的训练，并通过训练过程逼近每词元的语言熵，进而评估模型的压缩与泛化性能。

Result: 所提出的模型在相同硬件条件下实现了更高的语言压缩率；同时，那些被训练以接近而非超过估计熵值的模型展现出更强的泛化能力，验证了熵作为优化目标的有效性。

Conclusion: 语言模型的性能受限于语言本身的信息熵，而通过设计更高效的模型架构并以熵为指导目标，可以在实际应用中实现更优的压缩与泛化效果，为未来高效语言建模提供了新方向。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [4] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 本文提出Socratic Self-Refine（SSR）框架，通过将大语言模型的推理过程分解为可验证的子问题-子答案对，实现细粒度评估与精确修正。该方法通过受控重解和自一致性检查进行步骤级置信度估计，精准定位不可靠推理步骤并迭代优化，显著提升复杂任务上的准确性与可解释性。在五个推理基准和三种LLM上的实验表明，SSR优于现有最先进的迭代自精炼方法。此外，SSR提供了一种系统化的黑箱分析方式，用于理解大模型内部推理机制。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖粗粒度的自我验证与修正，难以有效处理复杂任务，导致推理质量受限。需要一种更精细、可解释且能精准识别错误步骤的推理优化方法。

Method: 将模型输出分解为可验证的子问题-子答案对；通过受控重解和自一致性检查进行步骤级置信度评估；识别低置信度步骤并迭代修正，形成更准确、可解释的推理链。

Result: 在五个推理基准和三种大语言模型上，SSR consistently 超越现有最先进方法，在准确性和可解释性方面均有显著提升。同时提供了有效的黑箱分析工具，用于理解模型内部推理过程。

Conclusion: Socratic Self-Refine（SSR）是一种高效、可解释的推理优化框架，能够通过细粒度评估与精准修正显著提升大语言模型在复杂任务中的表现，并为理解其推理机制提供了新途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [5] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: Instella 是一个完全开源的三亿参数语言模型家族，基于公开数据和代码库训练，使用 AMD Instinct MI300X GPU 完成大规模预训练、通用指令微调和人类偏好对齐。尽管预训练数据量较少，其性能在全开源模型中处于领先地位，并与同类规模的开源模型相当。此外，还发布了两个专用版本：Instella-Long（支持最长 128K tokens 上下文）和 Instella-Math（专精数学推理，通过监督微调和强化学习优化）。整体上，Instella 提供了透明、高效且多功能的开源语言建模解决方案，推动开放可复现研究的发展。


<details>
  <summary>Details</summary>
Motivation: 现有高性能大语言模型多为闭源或部分开源，缺乏透明度和可复现性。为解决这一问题，本文旨在构建一个完全开源、高性能且可复现的语言模型家族，以促进社区研究和应用。

Method: 采用 AMD Instinct MI300X GPU 进行大规模预训练，结合通用指令微调与人类偏好对齐；针对特定任务设计专门变体，如通过监督微调和强化学习提升数学推理能力。所有数据与代码均公开，确保完全透明。

Result: Instella 在全开源模型中达到领先性能，与同规模主流开源模型相比具有竞争力；Instella-Long 支持长达 128K token 的上下文处理，Instella-Math 在数学任务上表现优异，验证了模型的有效性和多样性。

Conclusion: Instella 作为完全开源、高性能、可复现的语言模型家族，为社区提供了透明且强大的替代方案，有效推进了开放语言建模研究的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [6] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: GAD是一种用于黑盒大语言模型蒸馏的新方法，通过将学生模型视为生成器，训练判别器来区分学生与教师模型的输出，形成最小最大博弈。判别器作为共进化且自适应的奖励模型，提供稳定反馈，显著优于传统序列级知识蒸馏。实验表明，使用GAD训练的学生模型（如Qwen2.5-14B-Instruct）在LMSYS-Chat评测中接近甚至媲美教师模型（GPT-5-Chat），证明了该方法的有效性与潜力。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒知识蒸馏方法受限于缺乏内部信息，难以有效捕捉教师模型的隐含知识；同时，传统方法依赖静态目标函数，反馈不够动态和适应性强。因此需要一种能实现在线、自适应学习的蒸馏框架。

Method: 提出生成对抗蒸馏（GAD），将学生模型设为生成器，判别器用于判断输出来源，通过最小最大博弈优化学生模型，使判别器作为共进化奖励模型持续提供高质量反馈。

Result: GAD在多个评估指标上显著优于传统序列级知识蒸馏；特别是Qwen2.5-14B-Instruct在使用GAD后，在LMSYS-Chat自动评测中表现接近甚至媲美其教师模型GPT-5-Chat。

Conclusion: GAD是一种高效且有前景的黑盒大语言模型蒸馏范式，能够实现稳定、自适应的在线学习，为无内部访问条件下的模型压缩提供了新思路。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [7] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: 提出ParoQuant，一种权重仅后训练量化方法，通过独立的Givens旋转和通道缩放均衡通道间幅度，缩小量化组内动态范围，结合优化推理内核，在保持低运行时开销的同时提升推理精度，平均较AWQ在推理任务上提升2.4%准确率，且开销低于10%。


<details>
  <summary>Details</summary>
Motivation: 现有权重仅后训练量化方法难以有效抑制权重和激活中的异常值，导致量化误差大、精度严重下降，尤其在长链推理中误差累积显著；同时现有方法或抑制能力不足，或引入高推理开销。

Method: 采用硬件高效且可优化的独立Givens旋转结合通道级缩放，使各通道幅度趋于一致，压缩每组量化内的动态范围，并协同设计推理内核以充分利用GPU并行性，实现轻量级运行时操作。

Result: 在推理任务上平均比AWQ提升2.4%准确率，运行时开销低于10%，显著提升推理型LLM的部署效率与准确性。

Conclusion: ParoQuant通过创新的旋转与缩放机制及协同内核设计，实现了高效低开销的权重量化，为推理型大模型的高效部署提供了新路径。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种在仿真环境中发现自动驾驶车辆（AV）故障场景后，验证这些场景是否能在真实世界中重现的方法。通过使用Scenic概率编程语言定义抽象场景，并设计查询算法从标注的时间序列传感器数据中识别匹配的场景实例，实现了对仿真故障场景的实证验证。实验表明，该方法在准确性和速度上均优于现有的商业视觉大模型，且可扩展至更长的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 当前仿真测试虽广泛用于验证自动驾驶系统安全性，但存在‘仿真到现实’的差距：仿真中发现的故障场景可能仅由合成传感器数据引起，而非真实世界的实际问题。因此需要一种有效手段来验证这些场景在真实世界中的可重现性。

Method: 提出基于Scenic语言的形式化场景定义方式，将抽象场景表示为场景程序；设计一种查询算法，用于在真实世界标注数据集中匹配并定位与场景程序一致的数据片段。

Result: 所提方法在查询精度和速度上显著优于现有商业视觉大模型，具备良好的可扩展性，能处理长时间序列数据，成功验证了部分仿真故障场景在真实数据中的重现性。

Conclusion: 本研究提供了一种可靠且高效的机制，用于验证仿真中发现的故障场景是否真实存在于现实世界中，有助于缩小仿真与现实之间的差距，提升自动驾驶系统的安全验证能力。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 本文研究了改进型多臂赌博机问题，提出两种新的参数化算法族，通过离线数据界定了学习近优算法的样本复杂度。第一类包含先前工作的最优随机算法，当奖励曲线具有更强的凹性时，可实现对k的最优依赖关系。第二类算法在良好行为实例上保证最优臂识别，在差劲实例上退化为最坏情况保证。从统计学习角度出发，实现了无需验证假设即可获得更强的数据相关保证。


<details>
  <summary>Details</summary>
Motivation: 多臂赌博机问题用于建模不确定性下的资源分配，如技术投资、临床试验和超参数选择。现有算法虽有进展，但最坏情况保证较悲观，存在Ω(k)和Ω(√k)的乘法近似下界。需要更优的算法以适应实际场景中的结构特性。

Method: 提出两类参数化算法族：第一类包含已有最优随机算法，通过调整参数实现对凹性条件的适配；第二类设计兼具强性能与鲁棒性的算法，能在良好实例中识别最优臂，差劲实例中退化至最坏情况。采用统计学习视角，利用离线数据分析并优化算法性能，实现数据依赖的更强保证。

Result: 所提算法族在满足特定凹性条件时，可实现对k的最优依赖关系，显著优于现有最坏情况边界。第二类算法在不同实例上表现出自适应能力，既保证良好行为下的高效学习，又确保差劲情形下的稳健性。整体方法无需显式验证假设即可获得更强的性能保障。

Conclusion: 本工作通过引入参数化算法族和统计学习框架，显著提升了改进型多臂赌博机问题的算法性能，实现了对数据特性的自适应优化，为实际应用提供了更强大且灵活的工具。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
