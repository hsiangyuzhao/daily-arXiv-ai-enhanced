<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 80]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.LG](#cs.LG) [Total: 49]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement](https://arxiv.org/abs/2511.10668)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.CV

TL;DR: 该论文提出了一种可测试的分析框架，用于研究人工智能系统在何种条件下可能出现无限加速的能力增长（即‘失控增长’），并给出在哪些条件下可以排除这种可能性。通过将能力增长与资源建设及部署策略联系起来，结合物理和信息理论中的功率、带宽、内存等限制，定义了瞬时改进的上限服务包络。同时，构建了一个内生增长模型，将资本与计算、数据、能源耦合，识别出超线性增长与亚临界增长之间的临界边界。基于可观测指标（如设施功率、输入输出带宽、训练吞吐量、基准损失、支出）推导出判断是否出现失控行为的决策规则，并提供可直接实施的安全控制措施，如功率上限、吞吐量限流、评估门控。案例分析涵盖受限功率、数据饱和和投资放大场景，揭示了约束条件何时生效。该方法无需模拟，基于工程师已有的测量数据，具有可验证性和实践性。局限在于对能力度量标准和正则性诊断的依赖，未来工作将扩展至随机动态、多智能体竞争和突变架构。总体上，该研究以可检验条件和可部署控制替代了空泛推测，为认证或排除人工智能奇点提供了科学基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的发展依赖于更多算力、数据、能源和更优训练方法，引发了对‘失控增长’（runaway growth）的担忧——即能力是否可能在有限时间内无止境提升。传统讨论多为定性推测，缺乏可测试的条件和实际安全控制手段。本文旨在提出一个精确、可验证的分析框架，以回答‘在什么条件下能力增长会趋于无限？在什么条件下可以排除？’这一核心问题，从而为AI安全提供实证基础和工程化控制方案。

Method: 构建一个递归自我改进的分析框架，融合物理极限（功率、带宽、内存）形成服务包络以限制瞬时改进速度；引入内生增长模型，将资本投入与计算、数据、能源动态耦合，刻画能力演进路径；基于可观测序列（功率、带宽、吞吐量、损失、支出）设计决策规则，生成‘是/否’证书判断是否存在失控行为；提出可直接部署的安全机制，如功率上限、吞吐量限制、评估门控；通过模拟无关的案例研究验证框架在不同设定下的适用性。

Result: 成功构建了可操作的判定体系：能够根据真实工程数据判断系统是否处于潜在失控状态；识别出关键临界边界，区分超线性增长与亚临界增长；提出多种可实施的安全控制措施，具备现实部署可行性；案例研究表明，在特定资源受限或数据饱和情境下，服务包络起作用，有效抑制无限增长趋势。

Conclusion: 该研究将关于人工智能奇点的模糊讨论转化为可检验的科学问题，提供了基于实际测量的决策工具与安全控制手段，实现了从猜测到验证的范式转变。其框架不依赖模拟，直接对接工程实践，为监管与治理提供了坚实的技术基础。尽管存在对度量标准和诊断方法的依赖，但整体上为防止不可控的AI能力爆炸提供了可行路径。

Abstract: AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the "runaway growth" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.

</details>


### [2] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种高效、可扩展的数据归属方法，用于识别文本到图像模型中对生成结果影响最大的训练图像。通过将耗时的基于遗忘机制的归属方法蒸馏到特征嵌入空间，实现了快速检索高影响力训练图像。部署时结合高效的索引与搜索技术，无需运行昂贵的归属算法即可在数秒内完成，相比现有方法提速2,500至400,000倍。实验在MSCOCO和Stable Diffusion等中大规模模型上验证了其优越或相当的性能，推动了数据归属方法在真实世界大模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据归属方法计算开销大，难以应用于实际场景，亟需一种高效、可扩展的方法以支持大规模模型如Stable Diffusion的数据归属需求。

Method: 将慢速的基于未学习（unlearning-based）的数据归属方法蒸馏至特征嵌入空间，并利用高效索引与搜索技术，在部署阶段实现快速高影响力图像检索。

Result: 在MSCOCO和Stable Diffusion等模型上，该方法可在数秒内完成数据归属，性能优于或接近现有方法，速度提升2,500至400,000倍。

Conclusion: 本工作为大规模真实世界模型上的数据归属提供了高效可行的解决方案，是迈向实用化的重要一步。

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [3] [Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow](https://arxiv.org/abs/2511.10766)
*Pooja P Jain,Pietro Mascagni,Giuseppe Massimiani,Nabani Banik,Marta Goglia,Lorenzo Arboit,Britty Baby,Andrea Balla,Ludovica Baldari,Gianfranco Silecchia,Claudio Fiorillo,CompSurg Colorectal Experts Group,Sergio Alfieri,Salvador Morales-Conde,Deborah S Keller,Luigi Boni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本研究开发并验证了一种用于微创结直肠手术（CRS）流程分析的视频基评估工具（ColoWorkflow），通过德尔菲法达成对通用流程阶段和具体操作步骤的共识，应用于多中心视频数据集，展现出良好的适用性和中等水平的评分者间一致性（相位Kappa=0.71，步骤Kappa=0.66），主要差异出现在流程转换和步骤边界定义上。该工具是首个基于共识且经过验证的全面流程分析工具，为视频驱动的手术表现评估提供了可重复框架，支持跨机构基准比较及人工智能辅助流程识别，有望标准化培训、加速能力掌握并推动数据驱动的手术质量改进。


<details>
  <summary>Details</summary>
Motivation: 微创结直肠手术存在操作变异大、学习曲线陡峭、并发症影响质量与结果等问题，现有工作流分析工具难以标准化和实施，亟需一种可推广、可靠的视频基评估工具以减少变异、优化培训并提升手术表现。

Method: 采用德尔菲法达成对通用流程阶段和具体步骤的共识；基于共识框架开发 ColoWorkflow 工具；在来自五个中心的54例腹腔镜与机器人结直肠手术视频上进行独立评分，评估其适用性与评分者间可靠性。

Result: 达成10个程序无关阶段和34个程序相关步骤的共识；ColoWorkflow 在所有视频中均被广泛使用（仅一个标签未使用）；评分者间一致性为中等水平（相位平均Cohen's K=0.71，步骤平均K=0.66），主要差异出现在流程转换和步骤边界界定处。

Conclusion: ColoWorkflow 是首个基于共识且经过验证的视频基评估工具，适用于微创结直肠手术全流程分析，建立了一个可重复的视频评估框架，有助于实现跨机构性能基准化，并支持人工智能驱动的流程识别，具有推动标准化培训、加速技能习得和数据驱动质量改进的潜力。

Abstract: Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.

</details>


### [4] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: 本文提出了一种基于层相关性传播（LRP）的新型剪枝方法，旨在在有限数据条件下实现更高的剪枝率并保持更好的模型精度。该方法无需微调，适用于计算资源受限且数据稀缺的场景，相比现有方法显著减少了精度损失，提升了实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LRP的剪枝方法虽无需微调且剪枝率高，但在实际应用中仍面临显著的精度下降问题，限制了其推广。因此需要一种能在小样本数据下实现高效剪枝并维持高精度的新方法。

Method: 提出了一种改进的基于LRP的剪枝策略，通过优化各层重要性评估机制，在不依赖微调的前提下实现更精准的冗余参数识别与移除，从而提升剪枝后的模型性能。

Result: 实验表明，所提方法在多种图像识别任务上实现了比现有方法更高的剪枝率，同时保持了更高的模型精度，尤其在小样本数据设置下表现突出。

Conclusion: 该研究验证了改进型LRP剪枝方法的有效性，为在资源受限环境下部署轻量化CNN模型提供了可行方案，具有良好的实际应用前景。

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [5] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: 提出一种短窗口滑动学习框架，用于实时检测CCTV视频中的暴力行为。通过将视频分割为1-2秒的片段，并使用大语言模型（LLM）自动生成描述标签，构建细粒度数据集。该方法充分利用每个片段的所有帧以保持时间连续性，从而精确识别快速发生的暴力事件。实验表明，该方法在RWF-2000数据集上达到95.25%准确率，并在长视频（UCF-Crime：83.25%）上显著提升性能，验证了其在智能监控系统中的强泛化能力和实时适用性。


<details>
  <summary>Details</summary>
Motivation: 传统长视频训练方法难以捕捉快速发生的暴力事件，且缺乏对短时动作的精细建模能力。为实现更高效、精准的实时暴力检测，需引入细粒度的时间片段处理机制与自动化标注技术。

Method: 将视频切分为1-2秒的短窗口片段，利用大语言模型（LLM）进行自动标题生成以构建标签数据集；每个片段完整保留所有帧信息，确保时间连续性；采用端到端学习框架进行暴力事件识别。

Result: 在RWF-2000数据集上达到95.25%的准确率，在UCF-Crime长视频测试中取得83.25%的性能表现，显著优于传统方法，证明该框架具备优异的泛化性和实时应用潜力。

Conclusion: 所提出的短窗口滑动学习框架能够有效应对快速暴力事件的检测挑战，结合LLM自动标注与细粒度时间建模，为智能监控系统提供了高精度、低延迟的实时暴力检测解决方案。

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [6] [DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting](https://arxiv.org/abs/2511.10894)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Anthony Miyaguchi,Rodrigo Pereira David,Rodrigo Tripodi Calumby,Lukáš Picek*

Main category: cs.CV

TL;DR: 本文提出了一种高效且具有竞争力的概率降雨临近预报方法，通过在预训练卫星视觉编码器（DINOv3-SAT493M）上附加一个视频投影仪（V-JEPA Vision Transformer）与轻量级概率头，将编码器特征映射为4小时累计降雨量的离散经验累积分布函数（eCDF），并采用连续排名概率评分（CRPS）进行端到端优化。作为对比，还使用了基于聚合排名概率评分和像素级伽马-赫德尔目标的3D-UNET基线模型。在Weather4Cast 2025基准测试中，该方法表现优异，CRPS达到3.5102，相比最佳3D-UNET提升了约26%的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统降雨临近预报方法在计算效率和概率预测精度之间难以平衡，尤其在处理高维、动态变化的卫星图像数据时面临挑战。现有方法往往依赖复杂的模型结构或损失函数，导致训练成本高、推理慢。因此，亟需一种既高效又具备强概率建模能力的方案来提升降雨预报的准确性和实用性。

Method: 采用预训练的卫星视觉编码器DINOv3-SAT493M提取卫星图像特征，结合V-JEPA Vision Transformer作为视频投影仪，将特征映射至离散经验累积分布函数（eCDF），并设计轻量级概率头输出概率分布。整个框架通过连续排名概率评分（CRPS）进行端到端优化。同时，对比了基于3D-UNET的多种基线模型，分别采用聚合排名概率评分和像素级伽马-赫德尔损失函数进行训练。

Result: 在Weather4Cast 2025基准测试中，所提方法取得CRPS为3.5102的优异结果，相较于最优的3D-UNET基线模型实现了约26%的有效性提升，表明其在保持计算效率的同时显著增强了概率预测性能。

Conclusion: 该研究提出的方法在保证计算效率的前提下，有效提升了降雨临近预报的概率准确性，展现出良好的实用潜力，为未来气象预报中的高效概率建模提供了新思路。

Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\approx$26\% in effectiveness gain against the best 3D-UNET.

</details>


### [7] [PhaseWin Search Framework Enable Efficient Object-Level Interpretation](https://arxiv.org/abs/2511.10914)
*Zihan Gu,Ruoyu Chen,Junchi Zhang,Yue Hu,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: PhaseWin提出一种新的相位窗口搜索算法，通过分阶段的粗粒度到细粒度搜索，结合自适应剪枝、窗口化细粒度选择和动态监督机制，以近线性复杂度实现高保真区域归因。该方法在仅使用20%计算预算的情况下，达到贪婪选择95%以上的保真度，并在目标检测和视觉定位任务中显著优于现有基线方法，为对象级多模态模型的可扩展高保真归因建立了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于子模函数选择的归因方法虽具有高保真度，但其二次复杂度导致计算效率低下，难以在实际场景中部署。因此亟需一种既能保持高保真度又具备高效计算能力的归因方法。

Method: PhaseWin采用分阶段的粗到精搜索策略，通过自适应剪枝减少候选集规模，利用窗口化细粒度选择提升局部优化效率，并引入动态监督机制以逼近贪婪选择行为，从而在降低计算成本的同时维持高保真度。

Result: PhaseWin在仅消耗20%计算预算的前提下，实现了超过95%的贪婪归因保真度；在Grounding DINO和Florence-2等模型上，于物体检测与视觉定位任务中持续超越其他归因基线，展现出卓越的性能与效率平衡。

Conclusion: PhaseWin成功实现了高保真度与高效率的统一，为对象级多模态模型的可解释性提供了高效、可扩展的新解决方案，确立了当前可扩展高保真归因的新标准。

Abstract: Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

</details>


### [8] [Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models](https://arxiv.org/abs/2511.10923)
*Zhixia He,Chen Zhao,Minglai Shao,Xintao Wu,Xujiang Zhao,Dong Li,Qin Tian,Linlin Yu*

Main category: cs.CV

TL;DR: 本文提出一种基于正负提示监督的视觉-语言模型方法，用于提升分布外（OOD）检测性能。通过优化类特定的正负提示，使负提示聚焦于类别边界特征，增强语义区分能力，并利用图结构聚合提示的语义监督信息，传递至视觉分支以改进能量基OOD检测器。在CIFAR-100和ImageNet-1K上的实验表明，该方法在多个数据集和大型语言模型下均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用广泛且模糊的负提示，可能引入重叠或误导性信息，影响OOD检测效果；因此需要更精准的负提示设计以提升分类边界区分能力。

Method: 基于大语言模型初始化类特定正负提示，通过优化使正提示聚焦类内特征、负提示突出类别边界特征；采用图结构聚合优化后的提示语义信息并传播至视觉分支，增强能量基检测器的判别能力。

Result: 在CIFAR-100和ImageNet-1K两个基准上，针对八个OOD数据集，使用五种不同LLM进行测试，所提方法均显著优于当前最优基线方法。

Conclusion: 通过精细化设计正负提示并结合图结构语义传播，本方法有效提升了视觉-语言模型在OOD检测任务中的性能，为跨模态边界建模提供了新思路。

Abstract: Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.

</details>


### [9] [Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study](https://arxiv.org/abs/2511.10940)
*Umma Aymon,Nur Shazwani Kamarudin,Ahmad Fakhri Ab. Nasir*

Main category: cs.CV

TL;DR: 本研究评估了YOLOv11n和YOLOv12n两款轻量级模型在统一检测与分类框架下的面部表情识别性能，使用FER2013和KDEF数据集转换为检测格式进行测试。YOLOv12n在干净的KDEF数据集上表现最佳（mAP 0.5为95.6），且在表达多样性上更敏感；而YOLOv11n在噪声较大的FER2013上具有更高精度（65.2），表明其在真实环境中的可靠性更强。两者在复杂表情间易混淆，但在干净数据上类别区分更清晰。结果揭示了灵敏度与精度之间的权衡，证明轻量级YOLO模型在效率与性能间的良好平衡，适用于实时、资源受限的情绪感知AI应用。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在真实世界环境中仍具挑战性，尤其在复杂背景和噪声干扰下。现有方法往往难以兼顾高精度与低计算开销，因此需要探索轻量级模型在该任务中的适应性与性能表现，以支持实际部署于资源受限设备。

Method: 将FER2013和KDEF两个分类数据集转换为对象检测格式，构建统一的检测与分类框架。采用YOLOv11n和YOLOv12n两种纳米级模型进行训练与评估，通过mAP 0.5、精确率、召回率及混淆矩阵分析模型性能。

Result: YOLOv12n在干净的KDEF数据集上达到95.6的mAP 0.5，表现出更强的表情敏感性；在FER2013上，其mAP为63.8，优于YOLOv11n。YOLOv11n在FER2013上具有更高的精确率（65.2），说明其在嘈杂环境中误报较少，更具可靠性。两类模型在相似表情间存在混淆，但在清洁数据上分类边界更清晰。

Conclusion: 轻量级YOLO模型在面部表情识别中展现出良好的性能与效率平衡，能够适应从受控环境到真实世界的多种场景，是实现高效、实时情绪感知系统的有力候选方案。

Abstract: Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.

</details>


### [10] [Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation](https://arxiv.org/abs/2511.10945)
*Xingyue Zhao,Wenke Huang,Xingguang Wang,Haoyu Zhao,Linghao Zhuang,Anwen Jiang,Guancheng Wan,Mang Ye*

Main category: cs.CV

TL;DR: 提出FedBCS方法，通过域不变上下文原型对齐解决联邦医疗学习中的特征异质性问题，引入频域自适应风格重校准和上下文感知双层原型对齐机制，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理医疗数据中因扫描仪或协议差异导致的特征异质性时，存在上下文表示学习不完整和层间风格偏差累积的问题，影响模型鲁棒性和分割精度。

Method: 提出频域自适应风格重校准以解耦内容与风格表示，并学习最优风格参数；设计上下文感知的双层原型对齐方法，从编码器和解码器不同层提取域不变原型并融合上下文信息实现细粒度表示对齐。

Result: 在两个公开数据集上的大量实验表明，所提方法在特征对齐和分割性能方面均表现优异，显著优于现有方法。

Conclusion: FedBCS通过域不变上下文原型对齐有效缓解了联邦医疗学习中的特征异质性问题，提升了模型的泛化能力和鲁棒性。

Abstract: Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.

</details>


### [11] [ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization](https://arxiv.org/abs/2511.10971)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Heng Ping,Tamoghna Chattopadhyay,Sophia I Thomopoulos,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.CV

TL;DR: ERMoE提出了一种基于正交特征基的稀疏Mixture-of-Experts架构，通过将专家重参数化到学习到的正交特征基上，并使用输入特征与专家基之间的余弦相似度作为路由分数（Eigenbasis Score），实现内容感知的稳定路由。该方法无需显式负载平衡损失，避免了干扰梯度，有效缓解了专家利用率不均和路由不稳定问题，在ImageNet、跨模态检索任务及3D MRI脑龄预测中均取得SOTA性能，同时实现更平坦的负载分布和可解释的专家专业化。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构面临路由器逻辑与专家内部结构不匹配导致的路由不稳定和专家利用不足，以及负载不平衡引发的性能瓶颈；传统负载平衡损失虽能缓解负载差异，但会削弱专家专长并损害下游性能。

Method: 将每个专家重参数化为学习到的正交特征基，用输入特征与专家基之间的余弦相似度作为路由分数（Eigenbasis Score），实现内容感知路由，消除对显式负载平衡损失的依赖。

Result: 在ImageNet分类和跨模态图像-文本检索（COCO、Flickr30K）任务上达到SOTA准确率；3D MRI版本ERMoE-ba在脑龄预测中提升超7%，且专家专业化具有解剖学可解释性；专家负载分布更平坦，路由更稳定。

Conclusion: ERMoE引入了一种新的稀疏专家模型架构范式，直接解决路由不稳定性问题，实现了高性能、可扩展且可解释的专家专业化，为未来稀疏模型设计提供了新方向。

Abstract: Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

</details>


### [12] [Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2511.10974)
*Haoran Chen,Houze Xu,Micah Goldblum,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: 本文提出DMC，一种基于CLIP的类增量学习（CIL）的两阶段框架，通过解耦视觉编码器和文本软提示的适应过程，利用单个模态作为另一个模态的稳定语义锚点，以保持跨模态对齐。此外，引入DMC-OT，结合最优传输引导的校准策略来应对视觉编码器更新导致的分布漂移问题，并设计任务特定的提示以增强类别间可分性。在CIFAR-100、ImageNet-R、CUB-200和UCF-101上的实验表明，两者均达到当前最佳性能，其中DMC-OT平均提升1.80%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的类增量学习方法在新增类别时容易因文本原型过拟合近期类别而产生分类器偏差；同时，传统生成回放中使用类级高斯统计忽略了视觉编码器随时间更新带来的分布漂移问题，限制了模型持续学习能力。

Method: 提出DMC两阶段框架：第一阶段冻结文本提示优化视觉编码器，第二阶段冻结视觉编码器优化文本软提示，实现双模态解耦与稳定对齐；进一步提出DMC-OT，引入最优传输引导的统计校准策略，动态调整记忆库分布以匹配演化中的视觉编码器，并采用任务特定提示设计增强不同任务间的区分性。

Result: 在CIFAR-100、ImageNet-R、CUB-200和UCF-101上，DMC和DMC-OT均达到当前最优性能，尤其DMC-OT相比基线平均提升1.80%准确率，有效缓解了灾难性遗忘并提升了跨任务泛化能力。

Conclusion: 本工作展示了通过解耦训练与分布校准策略，能够有效提升视觉语言模型在类增量学习场景下的稳定性与性能，为构建可长期演进的多模态系统提供了新思路。

Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.

</details>


### [13] [PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs](https://arxiv.org/abs/2511.10979)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Hang Wu,Yiwei Wang*

Main category: cs.CV

TL;DR: Video LLMs often suffer from temporal inconsistency due to frame timing shifts affecting attention. The issue stems from extending Rotary Position Embeddings (RoPE) to video via multimodal RoPE, which introduces unstable ripples in the time kernel. This paper proposes Phase Aggregated Smoothing (PAS), a training-free method that applies opposing phase offsets across attention heads and aggregates outputs. PAS smooths the temporal kernel, reduces sensitivity to small timing shifts, and maintains per-head spectral properties without altering the positional encoding structure. Experiments show consistent improvements across benchmarks with minimal overhead, making PAS a plug-and-play solution for robust video understanding.


<details>
  <summary>Details</summary>
Motivation: To address temporal inconsistency in Video LLMs caused by sensitivity to small frame timing shifts, particularly due to limitations in multimodal RoPE extension.

Method: Phase Aggregated Smoothing (PAS): applies small opposite phase offsets across attention heads and aggregates their outputs to smooth the temporal kernel while preserving spectral magnitude.

Result: Experiments on multiple video understanding benchmarks demonstrate consistent performance gains with negligible computational cost, showing improved robustness to temporal shifts.

Conclusion: PAS is a simple, effective, training-free enhancement for video LLMs that significantly improves temporal stability by smoothing the time kernel without modifying the underlying positional encoding structure.

Abstract: Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.

</details>


### [14] [Binary Verification for Zero-Shot Vision](https://arxiv.org/abs/2511.10983)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 提出一种无需训练的二元验证工作流，用于零样本视觉任务。该流程包含两个步骤：量化（将开放式问题转化为多选题，候选集明确且无歧义）和二值化（对每个候选进行真假判断，若仅一个为真则选择之，否则在剩余合理候选中进行多选）。在多个任务上评估显示，相比直接回答开放式问题，该方法显著提升性能，且具有通用性。理论分析揭示了如何将开放式视觉查询转化为多选题并进一步二值化为真假验证，构建了一个难度层级。简单分析解释了布尔推理为何能提升准确率。整体流程强调推理时设计而非任务特定训练，为当前视觉语言模型提供了一条实用、即插即用的增强零样本视觉能力路径。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视觉任务中，开放式查询难以准确解析，导致性能受限。为提升准确性，需要一种不依赖额外训练、适用于多种任务的通用推理机制。本文旨在通过结构化推理流程，提高视觉语言模型在零样本场景下的表现。

Method: 提出一种两步式工作流：(i) 量化，将开放问题转化为多选题，候选集明确；(ii) 二值化，对每个候选提出真假问题，通过布尔逻辑确定唯一答案或回退至多选。整个过程无需训练，仅依赖推理阶段的设计。

Result: 在引用表达定位（REC）、空间推理（Spatial-Map, Spatial-Grid, Spatial-Maze）和BLINK-Jigsaw等任务上，该方法显著优于直接回答开放式问题。量化到多选题带来显著提升，真假二值化进一步稳定增益。方法表现出良好的泛化能力，适用于多种任务。

Conclusion: 该工作流提供了一种简单、统一、无需训练的零样本视觉推理框架，通过量化与二值化策略有效提升视觉语言模型的表现。其核心在于推理时的结构化设计，为当前VLMs提供了高效实用的性能增强路径。

Abstract: We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.

</details>


### [15] [Rethinking Autoregressive Models for Lossless Image Compression via Hierarchical Parallelism and Progressive Adaptation](https://arxiv.org/abs/2511.10991)
*Daxin Li,Yuanchao Bai,Kai Wang,Wenbo Zhao,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于分层并行和渐进适应的高效自回归图像压缩框架HPAC，通过轻量级预训练模型与两项关键优化（CSI推理和AFC编码）显著提升了自回归模型的实用性。结合空间感知率引导的渐进微调策略（SARP-FT），该方法在自然、卫星、医学等多类数据集上达到新的基准性能，在参数量小、编码速度竞争的前提下实现了最先进的无损压缩效果。


<details>
  <summary>Details</summary>
Motivation: 自回归模型虽理论性能优越，但因计算成本过高常被视为不实用。本文旨在重新评估自回归范式，通过结构设计与优化策略使其兼具高性能与实际可行性。

Method: 提出HPAC模型，采用分层因子化结构与内容感知卷积门控以高效建模空间依赖；引入Cache-then-Select Inference (CSI)加速编码，Adaptive Focus Coding (AFC)支持高比特深度图像；通过Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT)实现图像级渐进微调。

Result: 在多种数据集上均取得当前最优的无损图像压缩性能，相比现有方法在压缩率与编码速度之间实现更好平衡，且模型参数量极小。

Conclusion: 经过精心设计的自回归框架可成为高效且强大的无损图像压缩方案，证明了纯自回归方法在实际应用中的巨大潜力。

Abstract: Autoregressive (AR) models, the theoretical performance benchmark for learned lossless image compression, are often dismissed as impractical due to prohibitive computational cost. This work re-thinks this paradigm, introducing a framework built on hierarchical parallelism and progressive adaptation that re-establishes pure autoregression as a top-performing and practical solution. Our approach is embodied in the Hierarchical Parallel Autoregressive ConvNet (HPAC), an ultra-lightweight pre-trained model using a hierarchical factorized structure and content-aware convolutional gating to efficiently capture spatial dependencies. We introduce two key optimizations for practicality: Cache-then-Select Inference (CSI), which accelerates coding by eliminating redundant computations, and Adaptive Focus Coding (AFC), which efficiently extends the framework to high bit-depth images. Building on this efficient foundation, our progressive adaptation strategy is realized by Spatially-Aware Rate-Guided Progressive Fine-tuning (SARP-FT). This instance-level strategy fine-tunes the model for each test image by optimizing low-rank adapters on progressively larger, spatially-continuous regions selected via estimated information density. Experiments on diverse datasets (natural, satellite, medical) validate that our method achieves new state-of-the-art compression. Notably, our approach sets a new benchmark in learned lossless compression, showing a carefully designed AR framework can offer significant gains over existing methods with a small parameter count and competitive coding speeds.

</details>


### [16] [CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis](https://arxiv.org/abs/2511.10993)
*Keunwoo Park,Jihye Chae,Joong Ho Ahn,Jihoon Kweon*

Main category: cs.CV

TL;DR: CLUE是一种基于Stable Diffusion架构的生成模型框架，通过固定格式提示词实现多样化且稳定的图像生成，无需额外数据。其核心是风格编码器生成风格嵌入，并引入新的U-Net注意力层，利用Kullback-Leibler散度在高斯区域内实现连续的潜在空间表示。在中耳炎数据集上，CLUE将FID降至9.30（对比46.81），召回率提升至70.29%（对比49.60%）。使用合成数据训练的分类器在1000%规模下达到F1分数83.21%（对比73.83%），结合真实与合成数据时达94.76%，优于仅用真实数据。外部数据集上，合成数据训练也表现更优。结果表明，CLUE在有限数据场景下可有效生成多样且稳定图像，是领域特定应用中的高效数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法在医学等专业领域受限于数据稀缺和类型单一，难以实现多样化且稳定的图像生成。因此需要一种不依赖额外数据、能有效利用有限数据的生成框架。

Method: 基于Stable Diffusion架构，提出CLUE框架，引入风格编码器处理图像与提示词生成风格嵌入，并将其输入U-Net的新注意力层；通过KL散度约束潜在空间，实现独立于提示词的连续特征表示。

Result: 在中耳炎数据集上，FID降低至9.30（原为46.81），召回率提升至70.29%（原为49.60%）；合成数据训练下分类器F1达83.21%（原73.83%），结合真实与合成数据达94.76%；外部数据集上合成训练达76.77%（原60.61%），联合训练达85.78%，均优于仅用真实数据。

Conclusion: CLUE能够在有限数据条件下实现多样化且稳定的图像生成，具备强大的数据增强能力，适用于医学等数据受限的专业领域。

Abstract: Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.

</details>


### [17] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: PROMISE是一种针对缺失模态情况下的鲁棒多模态表示学习框架，通过结合提示学习与分层对比学习，利用提示注意力机制动态生成一致的表示，有效缓解了完整与不完整数据间的表征差异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理缺失模态时依赖简单生成策略，难以保持跨模态一致性，导致性能下降。为解决这一问题，需设计更鲁棒的多模态学习框架以应对真实场景中的模态缺失。

Method: 提出PROMISE框架，融合多模态提示学习与分层对比学习，引入提示注意力机制，动态生成缺失模态下的稳健表示。

Result: 在多个基准数据集上的实验和消融研究均表明，PROMISE显著优于当前最先进的多模态方法，在模态缺失情况下表现出更强的鲁棒性和一致性。

Conclusion: PROMISE通过提示引导的层次化对比学习，有效解决了多模态表示中因模态缺失导致的一致性问题，为实际应用中不完整输入提供了可靠的解决方案。

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [18] [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)
*Zongyang Qiu,Bingyuan Wang,Xingbei Chen,Yingqing He,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出EmoVid，首个专为创意媒体设计的多模态情感标注视频数据集，涵盖动画、电影片段和表情包等非现实风格内容。数据集包含情感标签、视觉属性（亮度、色彩度、色相）和文本描述。通过分析揭示了视觉特征与情感感知之间的时空模式，并基于此改进了Wan2.1模型，实现情感条件下的视频生成，在文本到视频和图像到视频任务中显著提升生成质量与量化指标。该研究为情感化视频计算提供了新基准，推动了艺术化视频中的情感表达研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成系统多关注低层次视觉指标，忽视情感维度；尽管情感分析在视觉领域有所进展，但视频领域缺乏专门资源将情感理解与生成任务结合，尤其在风格化和非真实场景中。因此亟需一个情感标注的数据集及相应生成方法以填补这一空白。

Method: 构建EmoVid数据集，包含卡通动画、电影片段和动态贴纸等多样化视频内容，每段视频附带情感标签、视觉属性（亮度、色彩度、色相）和文本描述；通过系统性分析发现视觉特征与情感感知间的时空关联；在此基础上对Wan2.1模型进行微调，实现情感条件下的视频生成。

Result: 情感条件生成在文本到视频和图像到视频任务中均表现出显著提升，不仅在定量指标上优于基线，且生成视频的视觉质量更高，情感表达更准确。EmoVid成功建立情感化视频计算的新基准。

Conclusion: EmoVid是首个面向创意媒体的情感标注视频数据集，其构建与应用为艺术化视频中的情感理解与生成提供了重要支持，推动了视频生成从视觉保真向情感表达的演进，具有重要的学术价值与实际意义。

Abstract: Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.

</details>


### [19] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文受人类认知记忆理论启发，提出VisMem框架，通过动态潜在视觉记忆（短期细粒度感知保留与长期语义抽象整合）解决视觉语言模型在复杂视觉任务中的'视觉处理瓶颈'问题。实验表明，VisMem相比基线模型平均提升11.8%，显著优于现有方法，为潜在空间记忆增强树立了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂视觉任务中常因失去视觉证据锚定和缺乏情境化视觉体验而表现不佳，尤其在长时间生成过程中出现感知退化与语义不一致问题。

Method: 受人类短时视觉主导记忆与长时语义主导记忆的启发，设计双模块动态潜在视觉记忆系统：短期模块用于精细感知保持，长期模块用于抽象语义固化；二者在推理过程中无缝调用，实现感知保真与语义一致性的协同维持。

Result: 在多个视觉理解、推理与生成基准测试中，VisMem实现平均11.8%的性能提升，显著超越原始模型及所有对比方法，验证了其有效性与优越性。

Conclusion: VisMem通过模拟人类认知记忆机制，成功缓解了视觉语言模型的视觉处理瓶颈，为提升模型在复杂视觉任务中的表现提供了新思路，并建立了潜在空间记忆增强的新范式。

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [20] [SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation](https://arxiv.org/abs/2511.11014)
*Sumin Yu,Taesup Moon*

Main category: cs.CV

TL;DR: SP-Guard 是一种针对扩散模型的图像生成安全方法，通过估计提示词的有害性并仅对不安全区域应用引导掩码，实现了自适应和选择性引导，从而在保障生成内容安全性的同时减少不必要的内容修改。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本到图像生成模型虽然生成质量高，但容易被用于生成有害内容，引发社会担忧，因此需要更安全的生成机制。当前的方法在推理阶段缺乏根据提示词动态调整引导强度的能力，且无法精准定位不安全区域进行干预。

Method: SP-Guard 通过评估输入提示词的有害程度，构建一个选择性引导掩码，仅对预测为不安全的图像区域施加引导，实现自适应与精准控制。

Result: 实验表明，SP-Guard 在生成更安全图像方面优于现有方法，同时最大程度减少了对安全内容的误改，提升了生成结果的可控性和透明度。

Conclusion: SP-Guard 提供了一种高效、精准的安全增强策略，强调了在图像生成中实现透明性与可控性的关键作用。

Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.

</details>


### [21] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: SUPER是一种新型的解码器模块，基于小波完美重构特性，通过选择性抑制冗余特征，在不增加计算成本的前提下提升U-Net类模型对高频细节（如细裂缝）的恢复能力，并在多种图像逆问题中表现出色，兼具高频率保真度与全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于跳跃连接的编码器-解码器架构（如U-Net变体）在处理逆问题时存在信息丢失问题，尤其影响对细小、高频率细节的恢复。因此需要一种既能保持信息完整性又能有效去除冗余的机制。

Method: 提出选择性抑制完美重构（SUPER）模块，利用小波变换的完美重构性质来避免信息退化，同时通过选择性抑制冗余特征增强表征丰富性，作为即插即用的解码器模块集成到各类U-Net结构中。

Result: 在多个裂纹检测基准数据集上显著提升细裂缝分割性能，尤其在小于4像素宽的裂缝上表现突出；在手机图像去噪任务中也实现稳定的PSNR提升，证明其在高低频场景下的鲁棒性与通用性。

Conclusion: SUPER模块无需固定框架约束，可灵活适配多种U-Net变体，有效解决其内在重建瓶颈，提升表征多样性与高频率保真度，在保持低计算开销的同时实现全局一致性，具备广泛的适用性和先进性。

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [22] [AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning](https://arxiv.org/abs/2511.11025)
*Jirong Zha,Yuxuan Fan,Tianyu Zhang,Geng Chen,Yingfeng Chen,Chen Gao,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出AirCopBench，首个评估多模态大模型在复杂、退化感知条件下空中协同感知能力的综合性基准，涵盖14.6万+问题，覆盖四大任务维度和14种任务类型，通过仿真与真实数据结合生成，并验证了模型在协同任务中的性能差距及模拟到现实的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估多模态大模型在复杂、退化感知条件下的多无人机协同感知能力，缺乏对真实场景中协作行为的有效评测体系。

Method: 结合仿真与真实世界数据，构建包含挑战性退化感知场景的标注数据集，采用模型、规则与人工相结合的方式生成大规模问题，实施严格的质量控制。

Result: 40个MLLM在任务中表现差异显著，最优模型平均落后人类24.38%，且任务间表现不一致；模拟到现实的迁移学习实验验证了可行性。

Conclusion: AirCopBench有效填补了多模态大模型在空中协同感知评估方面的空白，揭示了当前模型在复杂协作任务中的局限性，并为未来研究提供了可靠评估框架。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.

</details>


### [23] [EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027)
*Yong Sun,Zhengjie Zhang,Junyu Shi,Zhiyuan Zhang,Lijiang Liu,Qiang Nie*

Main category: cs.CV

TL;DR: 提出EmbryoDiff，一种基于扩散模型的两阶段框架，用于体外受精中胚胎发育阶段的细粒度识别。通过多焦点特征融合构建三维感知形态表示，缓解细胞遮挡带来的特征模糊问题，并引入混合语义-边界条件模块增强扩散去噪过程，实现高精度分类。仅需一步去噪即达到82.8%和81.3%的准确率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在胚胎发育阶段识别中未能利用发育过程的分布先验，且依赖单一焦点信息导致表征不完整，易受细胞遮挡影响。

Method: 提出两阶段扩散框架：首先冻结帧级编码器提取多焦点鲁棒特征；其次设计多焦点特征融合策略构建3D感知形态表示，并引入混合语义-边界条件块注入扩散去噪过程。

Result: 在两个基准数据集上均取得当前最优性能，单步去噪下分别达到82.8%和81.3%的准确率。

Conclusion: EmbryoDiff通过融合多焦点信息与扩散模型的优势，有效提升了胚胎发育阶段识别的准确性与鲁棒性，为IVF评估提供了可靠工具。

Abstract: Identification of fine-grained embryo developmental stages during In Vitro Fertilization (IVF) is crucial for assessing embryo viability. Although recent deep learning methods have achieved promising accuracy, existing discriminative models fail to utilize the distributional prior of embryonic development to improve accuracy. Moreover, their reliance on single-focal information leads to incomplete embryonic representations, making them susceptible to feature ambiguity under cell occlusions. To address these limitations, we propose EmbryoDiff, a two-stage diffusion-based framework that formulates the task as a conditional sequence denoising process. Specifically, we first train and freeze a frame-level encoder to extract robust multi-focal features. In the second stage, we introduce a Multi-Focal Feature Fusion Strategy that aggregates information across focal planes to construct a 3D-aware morphological representation, effectively alleviating ambiguities arising from cell occlusions. Building on this fused representation, we derive complementary semantic and boundary cues and design a Hybrid Semantic-Boundary Condition Block to inject them into the diffusion-based denoising process, enabling accurate embryonic stage classification. Extensive experiments on two benchmark datasets show that our method achieves state-of-the-art results. Notably, with only a single denoising step, our model obtains the best average test performance, reaching 82.8% and 81.3% accuracy on the two datasets, respectively.

</details>


### [24] [Accelerating Controllable Generation via Hybrid-grained Cache](https://arxiv.org/abs/2511.11031)
*Lin Liu,Huixia Ben,Shuo Wang,Jinda Lu,Junxiang Qiu,Shengeng Tang,Yanbin Hao*

Main category: cs.CV

TL;DR: 提出一种混合粒度缓存（HGC）方法，通过在不同计算阶段采用不同粒度的缓存策略来降低可控生成模型的计算开销。该方法结合粗粒度（块级）缓存和细粒度（提示级）缓存，分别在编码器-解码器块间和模块内复用特征与交叉注意力图，实现高效推理。在四个基准数据集上验证了其有效性，在保持视觉质量的同时显著提升生成效率，例如在COCO-Stuff任务中将计算量（MACs）降低63%，同时保持语义保真度损失低于1.5%。


<details>
  <summary>Details</summary>
Motivation: 可控生成模型虽能提升合成视觉内容的真实感，但因需处理控制条件与高计算需求，导致生成效率低下。因此亟需一种有效降低计算开销的方法，以平衡效率与质量。

Method: 提出混合粒度缓存（HGC）方法：（1）采用块级粗粒度缓存，基于特征重用动态跳过编码器-解码器块间的冗余计算；（2）设计提示级细粒度缓存，复用连续推理步骤中的交叉注意力图，并扩展至相邻步骤的模块计算中。两种缓存机制可无缝集成于可控生成流程的各个计算环节。

Result: 在四个基准数据集上验证了HGC的有效性，尤其在效率与质量之间取得良好平衡。在COCO-Stuff分割任务中，计算量（MACs）从18.22T降至6.70T，降幅达63%，而语义保真度损失控制在1.5%以内。

Conclusion: HGC通过多粒度缓存策略有效降低了可控生成模型的计算开销，显著提升了生成效率，同时保持了高质量的视觉输出，为高效可控生成提供了可行解决方案。

Abstract: Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.

</details>


### [25] [CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging](https://arxiv.org/abs/2511.11034)
*Pooja Singh,Siddhant Ujjain,Tapan Kumar Gandhi,Sandeep Kumar*

Main category: cs.CV

TL;DR: CrossMed is a benchmark for evaluating compositional generalization in medical multimodal LLMs using a Modality-Anatomy-Task (MAT) schema. It reformulates four public datasets into 20,200 multiple-choice VQA instances and tests models on Related, Unrelated, and zero-overlap splits. Results show significant performance drops under unseen combinations, highlighting the challenge of compositional generalization. Multimodal LLMs outperform traditional models and demonstrate cross-task transfer, confirming their strength in zero-shot and modality-agnostic generalization.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal LLMs lack systematic evaluation of compositional generalization across unseen combinations of imaging modality, anatomy, and task type in medical AI. There is a need for a structured benchmark to assess zero-shot, cross-task, and modality-agnostic capabilities.

Method: CrossMed reformulates four medical imaging datasets (CheXpert, SIIM-ACR, BraTS 2020, MosMedData) into a unified visual question answering (VQA) format using a structured MAT schema. It creates 20,200 QA instances and evaluates models on Related, Unrelated, and zero-overlap test splits.

Result: Models trained on Related splits achieve 83.2% classification accuracy and 0.75 segmentation cIoU. Performance drops significantly under Unrelated and zero-overlap settings. Cross-task transfer improves segmentation by 7% cIoU when trained on classification-only data. Multimodal LLMs outperform traditional models (ResNet-50, U-Net) in compositional generalization.

Conclusion: CrossMed provides a rigorous benchmark for assessing compositional generalization in medical multimodal LLMs, demonstrating their unique capability in zero-shot, cross-task, and modality-agnostic reasoning, while highlighting current limitations in generalizing across unseen combinations.

Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

</details>


### [26] [SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices](https://arxiv.org/abs/2511.11038)
*Jiaming Huang,Yi Gao,Fuchang Pan,Renjie Li,Wei Dong*

Main category: cs.CV

TL;DR: SemanticNN is a semantic codec designed for error-resilient, low-resource device-edge collaboration in IoT environments. It enables efficient and reliable inference offloading by focusing on semantic-level correctness rather than bit-level accuracy, using a BER-aware decoder and SQ-based encoder. A novel training strategy (Feature-augmentation Learning) and XAI-based asymmetry compensation improve performance under dynamic conditions. Experiments show up to 344.83x reduction in feature transmission volume with maintained high inference accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on bit-level transmission correctness, which is inefficient under dynamic and unreliable network conditions typical in IoT. Resource-constrained embedded devices require more resilient and compressive solutions for real-time AI inference.

Method: SemanticNN employs a BER-aware decoder that adapts to changing channel conditions and a Soft Quantization-based encoder to learn compact representations. Feature-augmentation Learning enhances offloading efficiency, while XAI-based Asymmetry Compensation addresses capability mismatches between devices and edge servers.

Result: Experiments on STM32 with three models and six datasets show SemanticNN reduces feature transmission volume by 56.82–344.83x across image classification and object detection tasks, while maintaining superior inference accuracy even under varying error rates.

Conclusion: SemanticNN effectively enables efficient, resilient, and low-overhead collaborative inference on weak embedded devices by prioritizing semantic correctness over bit perfection, making it suitable for practical IoT applications with strict constraints.

Abstract: With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.

</details>


### [27] [Hyperbolic Hierarchical Alignment Reasoning Network for Text-3D Retrieval](https://arxiv.org/abs/2511.11045)
*Wenrui Li,Yidan Lu,Yeyu Chai,Rui Zhao,Hengyu Man,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出H²ARN模型，通过双损失机制与超球面几何在文本-3D检索中解决层次表示崩溃和冗余导致显著性稀释问题；引入贡献感知聚合模块增强关键特征，提升区分能力，并发布扩展版T3DR-HIT v2数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文本-3D检索方法面临层次表示崩溃（HRC）和冗余导致显著性稀释（RISD）两大挑战，导致模型难以有效捕捉语义层级关系及关键局部特征。

Method: 将文本与3D数据嵌入Lorentz模型的双曲空间，利用其指数增长体积特性保持层次距离；设计层次排序损失构建收缩蕴含锥体以约束匹配实例位置，结合实例级对比损失分离非匹配样本；提出贡献感知双曲聚合模块，基于洛伦兹距离评估局部特征相关性并进行加权聚合，抑制冗余信息。

Result: 在扩展的T3DR-HIT v2基准上，模型显著优于现有方法，在细粒度文物与复杂室内场景下均实现更高检索精度，验证了双曲几何与贡献感知机制的有效性。

Conclusion: H²ARN通过双曲空间建模与贡献感知聚合，有效缓解了层次压缩与冗余干扰问题，为文本-3D检索提供了更鲁棒、更具判别力的解决方案。

Abstract: With the daily influx of 3D data on the internet, text-3D retrieval has gained increasing attention. However, current methods face two major challenges: Hierarchy Representation Collapse (HRC) and Redundancy-Induced Saliency Dilution (RISD). HRC compresses abstract-to-specific and whole-to-part hierarchies in Euclidean embeddings, while RISD averages noisy fragments, obscuring critical semantic cues and diminishing the model's ability to distinguish hard negatives. To address these challenges, we introduce the Hyperbolic Hierarchical Alignment Reasoning Network (H$^{2}$ARN) for text-3D retrieval. H$^{2}$ARN embeds both text and 3D data in a Lorentz-model hyperbolic space, where exponential volume growth inherently preserves hierarchical distances. A hierarchical ordering loss constructs a shrinking entailment cone around each text vector, ensuring that the matched 3D instance falls within the cone, while an instance-level contrastive loss jointly enforces separation from non-matching samples. To tackle RISD, we propose a contribution-aware hyperbolic aggregation module that leverages Lorentzian distance to assess the relevance of each local feature and applies contribution-weighted aggregation guided by hyperbolic geometry, enhancing discriminative regions while suppressing redundancy without additional supervision. We also release the expanded T3DR-HIT v2 benchmark, which contains 8,935 text-to-3D pairs, 2.6 times the original size, covering both fine-grained cultural artefacts and complex indoor scenes. Our codes are available at https://github.com/liwrui/H2ARN.

</details>


### [28] [CareCom: Generative Image Composition with Calibrated Reference Features](https://arxiv.org/abs/2511.11060)
*Jiaxuan Chen,Bo Zhang,Qingdong He,Jinlong Peng,Li Niu*

Main category: cs.CV

TL;DR: 本文提出一种多参考图像生成图像合成方法，通过校准前景参考图像的全局和局部特征，使其与背景信息兼容，从而在保持细节的同时实现前景姿态/视角调整。实验表明，校准后的参考特征显著提升了生成模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有生成式图像合成方法在同时保持细节和调整前景姿态/视角方面仍存在困难。

Method: 扩展现有生成合成模型为多参考版本，并提出校准前景参考图像的全局和局部特征，使其与背景信息兼容，以补充适当的姿态/视角信息。

Result: 在MVImgNet和MureCom数据集上的大量实验表明，校准后的参考特征显著提升了生成模型的性能。

Conclusion: 通过引入多参考图像并校准其特征，生成模型能够更好地兼顾细节保留和前景姿态/视角调整，显著提升图像合成质量。

Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.

</details>


### [29] [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)
*Dor Shmilovich,Tony Wu,Aviad Dahan,Yuval Domb*

Main category: cs.CV

TL;DR: LiteAttention利用扩散注意力中时间相干性，通过早期标记非关键块并向前传播跳过决策，实现跨去噪序列的进化计算跳过，结合了动态方法的自适应性和静态方法的高效性，在不降低视频生成质量的前提下显著加速了扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有加速方法在动态估计稀疏注意力模式与静态模式之间存在根本权衡：前者计算开销大且存在估计误差，后者固定模式难以适应整个去噪过程。本文旨在解决这一问题，提升视频生成中扩散变压器的效率。

Method: 提出LiteAttention，利用扩散注意力中时间上的强相干性，即某时刻非关键的块在后续步骤中仍很可能非关键，从而在早期标记这些块并传播跳过决策，避免重复的性能评估开销。基于FlashAttention实现高度优化的LiteAttention内核。

Result: 在生产级视频扩散模型上实现了显著的速度提升，且未造成质量下降，证明了该方法的有效性与实用性。

Conclusion: LiteAttention通过利用注意力模式的时间相干性，实现了高效且自适应的计算跳过，是视频生成中扩散模型加速的一个有效方案，具有实际部署价值。

Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.

</details>


### [30] [From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening](https://arxiv.org/abs/2511.11065)
*Muskaan Chopra,Lorenz Sparrenberg,Armin Berger,Sarthak Khanna,Jan H. Terheyden,Rafet Sifa*

Main category: cs.CV

TL;DR: 本综述系统梳理了2016-2025年间糖尿病视网膜病变（DR）领域的研究进展，整合了50余项研究和20多个数据集，涵盖自监督与半监督学习、领域泛化、联邦训练及混合神经符号模型等方法，分析评估协议、报告标准与可重复性挑战，并指出多中心验证与临床信任方面的关键空白。研究提出可复现、隐私保护且临床可用的DR人工智能发展路径，其成果对大规模医学影像智能诊断具有广泛意义。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致可预防性失明的主要原因，早期检测对减少全球视力丧失至关重要。尽管深度学习在过去十年中显著推动了DR筛查技术的发展，但仍面临类别不平衡、标签稀缺、域偏移和可解释性等挑战。现有研究分散且缺乏系统性整合，亟需全面回顾以指导未来研究与临床转化。

Method: 本研究通过系统文献综述方法，筛选并分析2016-2025年间发表的50余篇相关论文，涵盖超过20个公开与私有数据集；采用结构化框架归纳关键技术进展，包括自监督学习、半监督学习、联邦学习、领域泛化与神经符号融合模型；同时构建基准表格对比不同方法在多个数据集上的性能表现，并评估评估协议、报告规范与可重复性问题。

Result: 研究总结出当前DR AI在模型鲁棒性、数据效率与可解释性方面取得显著进展，尤其在处理小样本与跨中心数据方面的新方法展现出潜力。然而，仍存在多中心验证不足、临床部署障碍、算法透明度低等关键瓶颈。部分先进方法在真实世界环境中的泛化能力尚未充分验证。

Conclusion: 本综述揭示了深度学习在糖尿病视网膜病变检测中的巨大潜力，强调必须兼顾技术先进性与临床实用性。未来应推动标准化评估、加强跨机构协作、提升模型可解释性，并建立符合隐私保护要求的可部署系统。所提出的实践路径不仅适用于DR，也为大规模医学影像人工智能的发展提供通用范式。

Abstract: Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.

</details>


### [31] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: 提出S2D-Align，一种基于多阶段、辅助信号引导的SFT范式，通过粗到细的对齐策略，结合图像-报告配对、参考报告和关键短语，提升放射科报告生成中的解剖学对齐能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在实例级别进行图像-文本对齐，缺乏解剖学基础对齐，导致报告生成质量不佳。

Method: 提出S2D-Align，采用从粗到细的对齐策略：先进行粗粒度图像-报告配对，再引入参考报告进行实例级指导，最后利用关键短语锚定解剖细节；通过记忆适配器实现不同对齐阶段间的特征共享。

Result: 在MIMIC-CXR和IU X-Ray数据集上均达到当前最优性能，消融实验验证了多阶段辅助引导策略的有效性。

Conclusion: S2D-Align通过构建解剖学基础对齐，为复杂多模态生成任务中提升生成内容的准确性与可解释性提供了有效路径。

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [32] [Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image](https://arxiv.org/abs/2511.11074)
*Matthias Humt,Ulrich Hillenbrand,Rudolph Triebel*

Main category: cs.CV

TL;DR: This work compares diffusion and autoregressive models for 3D shape generation and completion, showing that diffusion models excel with continuous latents, while autoregressive models perform competitively on discrete latents.


<details>
  <summary>Details</summary>
Motivation: Despite the widespread adoption of generative models across various data modalities, including 3D data, there is no consensus on which model is optimal for specific tasks. Additionally, while conditional information like text and images is commonly used to guide generation, other forms such as partial 3D data remain underexplored.

Method: The study compares Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers, adapting both for generative shape modeling and shape completion tasks. A comprehensive quantitative evaluation is conducted, including a baseline discriminative model and an extensive ablation study.

Result: 1) The diffusion model with continuous latents outperforms both the discriminative model and autoregressive approach, achieving state-of-the-art results in multi-modal shape completion from a single noisy depth image under realistic conditions. 2) When evaluated on the same discrete latent space, the autoregressive model can match or surpass diffusion performance.

Conclusion: Diffusion models with continuous latents are superior for shape completion under realistic conditions, but autoregressive models can be competitive when operating on a shared discrete latent space.

Abstract: While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.

</details>


### [33] [Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids](https://arxiv.org/abs/2511.11077)
*Ke Ma,Yizhou Fang,Jean-Baptiste Weibel,Shuai Tan,Xinggang Wang,Yang Xiao,Yi Fang,Tian Xia*

Main category: cs.CV

TL;DR: 提出 Phys-Liquid 数据集，包含 97,200 张模拟图像和对应的 3D 网格，用于模拟不同实验场景、光照条件、液体颜色和容器旋转下的液体动态。通过四阶段重建与估计流程验证其真实性和有效性，显著提升透明液体几何与体积估计的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏物理信息丰富的仿真数据，无法充分反映真实液体在多种动态场景下的行为，阻碍了自主机器人对透明可变形液体的精确感知与操作。

Method: 构建基于物理的仿真环境生成大量带标签的图像与 3D 网格数据；设计四阶段处理流程：液体分割、多视角掩码生成、3D 网格重建、真实尺度校准。

Result: 实验表明，所提方法在液体几何与体积估计方面优于现有基准，具有更高的准确性和一致性。

Conclusion: Phys-Liquid 数据集及其验证方法为透明液体感知任务提供了重要支持，推动了机器人在复杂液体操作中的发展。

Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.

</details>


### [34] [A Space-Time Transformer for Precipitation Forecasting](https://arxiv.org/abs/2511.11090)
*Levi Harris,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为SaTformer的视频变换器模型，用于基于卫星辐射数据进行极端降水的精准预报。针对传统数值天气预测（NWP）模型在计算效率和短时预报性能上的不足，该研究采用数据驱动的AI-天气预测方法，利用全时空注意力机制建模气象序列，同时通过将降水回归问题转化为分类任务并引入类别加权损失函数，有效缓解了长尾分布数据带来的标签不平衡问题。模型在NeurIPS Weather4Cast 2025累积降雨挑战赛中获得第一名。代码与模型权重已开源。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预测模型存在计算成本高、在0-4小时短时预报（nowcasting）阶段性能下降的问题，且视频理解类架构在气象预报中的应用尚不充分，因此亟需高效、精准的数据驱动替代方案。

Method: 提出SaTformer，一种基于全空间-时间注意力机制的视频变换器；将降水回归任务转化为分类问题，并使用类权重损失函数处理长尾数据分布。

Result: SaTformer在NeurIPS Weather4Cast 2025累积降雨挑战赛中取得第一名成绩，显著提升了短时极端降水预报精度。

Conclusion: 本研究证明了视频变换器在气象预报中的潜力，特别是面对极端降水事件时，结合数据重平衡策略可实现高性能、低延迟的实时预警能力，为未来智能气象服务提供新范式。

Abstract: Meteorological agencies around the world rely on real-time flood guidance to issue live-saving advisories and warnings. For decades traditional numerical weather prediction (NWP) models have been state-of-the-art for precipitation forecasting. However, physically-parameterized models suffer from a few core limitations: first, solving PDEs to resolve atmospheric dynamics is computationally demanding, and second, these methods degrade in performance at nowcasting timescales (i.e., 0-4 hour lead-times). Motivated by these shortcomings, recent work proposes AI-weather prediction (AI-WP) alternatives that learn to emulate analysis data with neural networks. While these data-driven approaches have enjoyed enormous success across diverse spatial and temporal resolutions, applications of video-understanding architectures for weather forecasting remain underexplored. To address these gaps, we propose SaTformer: a video transformer built on full space-time attention that skillfully forecasts extreme precipitation from satellite radiances. Along with our novel architecture, we introduce techniques to tame long-tailed precipitation datasets. Namely, we reformulate precipitation regression into a classification problem, and employ a class-weighted loss to address label imbalances. Our model scored first place on the NeurIPS Weather4Cast 2025 Cumulative Rainfall challenge. Code and model weights are available: https://github.com/leharris3/satformer

</details>


### [35] [VIDEOP2R: Video Understanding from Perception to Reasoning](https://arxiv.org/abs/2511.11113)
*Yifan Jiang,Yueying Wang,Rui Zhao,Toufiq Parag,Zhimin Chen,Zhenyu Liao,Jayakrishnan Unnikrishnan*

Main category: cs.CV

TL;DR: VideoP2R提出了一种新的过程感知视频强化微调框架，通过将感知与推理建模为独立过程，在监督微调阶段构建高质量的链式思维数据集，并在强化学习阶段引入过程感知组相对策略优化算法，实现对感知和推理分别奖励。实验表明该方法在七个视频理解基准中的六个上达到当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调（RFT）框架在大型视频语言模型（LVLMs）上的应用面临挑战，尤其是在提升视频推理能力方面。因此需要一种能够有效分离并优化感知与推理过程的新方法。

Method: 提出VideoP2R框架，包括三步式监督微调管道生成高质量过程感知链式思维数据集（VideoP2R-CoT-162K），以及设计过程感知组相对策略优化（PA-GRPO）算法，分别对感知和推理提供奖励。

Result: 在七个视频推理与理解基准中，VideoP2R在六项上取得当前最优表现；消融实验证明了过程感知建模和PA-GRPO的有效性，并显示模型的感知输出足以支持下游推理任务。

Conclusion: VideoP2R通过显式建模感知与推理过程，显著提升了视频语言模型的推理能力，为大型视频语言模型的微调提供了高效且可扩展的新范式。

Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.

</details>


### [36] [Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions](https://arxiv.org/abs/2511.11116)
*Redwan Hussain,Mizanur Rahman,Prithwiraj Bhattacharjee*

Main category: cs.CV

TL;DR: 本文综述了24篇近期关于AI生成媒体检测的研究，分析了现有方法在泛化能力、跨模型适应性及多模态数据处理方面的局限性，指出当前深度学习检测模型（如CNN、ViT）难以应对未见数据和高度修改内容。研究建议未来应聚焦于多模态深度学习模型以提升检测的鲁棒性和通用性，为构建更有效的合成媒体防御系统提供方向。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如GANs、扩散模型）的发展，合成媒体日益逼真，导致真实与虚假内容难以区分，催生了深度伪造等滥用问题，亟需可靠检测技术以应对信息误导、隐私侵犯和欺诈风险。

Method: 系统回顾24篇最新相关论文，逐项分析其技术贡献与缺陷，归纳共性挑战与局限性，提出改进方向。

Result: 识别出现有检测方法在泛化性能、多模态适应性及对复杂修改内容的处理上存在显著不足；强调多模态深度学习是突破当前瓶颈的关键路径。

Conclusion: 未来研究应重点发展基于多模态深度学习的合成媒体检测框架，以实现更稳健、可迁移的检测能力，为抵御有害合成内容提供有效支持。

Abstract: Artificial intelligence (AI) in media has advanced rapidly over the last decade. The introduction of Generative Adversarial Networks (GANs) improved the quality of photorealistic image generation. Diffusion models later brought a new era of generative media. These advances made it difficult to separate real and synthetic content. The rise of deepfakes demonstrated how these tools could be misused to spread misinformation, political conspiracies, privacy violations, and fraud. For this reason, many detection models have been developed. They often use deep learning methods such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). These models search for visual, spatial, or temporal anomalies. However, such approaches often fail to generalize across unseen data and struggle with content from different models. In addition, existing approaches are ineffective in multimodal data and highly modified content. This study reviews twenty-four recent works on AI-generated media detection. Each study was examined individually to identify its contributions and weaknesses, respectively. The review then summarizes the common limitations and key challenges faced by current approaches. Based on this analysis, a research direction is suggested with a focus on multimodal deep learning models. Such models have the potential to provide more robust and generalized detection. It offers future researchers a clear starting point for building stronger defenses against harmful synthetic media.

</details>


### [37] [Stroke Modeling Enables Vectorized Character Generation with Large Vectorized Glyph Model](https://arxiv.org/abs/2511.11119)
*Xinyue Zhang,Haolong Li,Jiawei Ma,Chen Ye*

Main category: cs.CV

TL;DR: 本文提出一种新型大尺度矢量字形模型（LVGM），通过预测下一个笔画来生成矢量化的中文字符。模型将笔画编码为离散的隐变量（笔画嵌入），并基于DeepSeek大语言模型进行微调，实现从少量笔画生成完整汉字、语义优美的词语乃至未见过的诗句。研究还发布了一个包含907,267个样本的大型中文SVG数据集，支持动态矢量字形生成。实验表明模型具备数据规模上的扩展性，生成结果经专家验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统矢量字形在设计和艺术应用中具有优势，但缺乏智能化生成能力。现有方法难以结合语言模型的序列预测能力实现高质量、语义连贯的矢量字形生成。因此，亟需一种能够基于笔画建模并利用大语言模型潜力的生成框架，以实现高效、灵活且语义合理的矢量中文字符生成。

Method: 1. 将汉字笔画转化为离散的笔画嵌入表示；2. 基于DeepSeek大语言模型进行微调，使其具备预测下一笔画嵌入的能力；3. 通过少量初始笔画输入，逐步生成完整的矢量字形；4. 构建大规模基于笔画的中文SVG数据集用于训练与评估。

Result: 模型可在给定少量笔画的情况下生成结构完整、语义流畅的汉字及词句，支持生成未见文本内容；实验验证了其在不同数据规模下的良好扩展性；生成结果获得领域专家认可，具备实际应用潜力。

Conclusion: 本研究成功构建了首个基于大语言模型的矢量中文字符生成系统——LVGM，实现了从笔画到语义化矢量字形的智能生成。所提出的框架兼具灵活性与可扩展性，为数字艺术、字体设计和人机交互等领域提供了新范式。

Abstract: Vectorized glyphs are widely used in poster design, network animation, art display, and various other fields due to their scalability and flexibility. In typography, they are often seen as special sequences composed of ordered strokes. This concept extends to the token sequence prediction abilities of large language models (LLMs), enabling vectorized character generation through stroke modeling. In this paper, we propose a novel Large Vectorized Glyph Model (LVGM) designed to generate vectorized Chinese glyphs by predicting the next stroke. Initially, we encode strokes into discrete latent variables called stroke embeddings. Subsequently, we train our LVGM via fine-tuning DeepSeek LLM by predicting the next stroke embedding. With limited strokes given, it can generate complete characters, semantically elegant words, and even unseen verses in vectorized form. Moreover, we release a new large-scale Chinese SVG dataset containing 907,267 samples based on strokes for dynamically vectorized glyph generation. Experimental results show that our model has scaling behaviors on data scales. Our generated vectorized glyphs have been validated by experts and relevant individuals.

</details>


### [38] [Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2511.11132)
*Yu Zhao,Ying Zhang,Xuhui Sui,Baohang Zhou,Li Shen,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出Hindsight Distilled Reasoning (HinD)框架，结合知识鼓励偏好优化（KEPO），通过提示7B规模的冻结多模态大模型生成推理过程，构建 hindsight-zero 训练数据，并自蒸馏为思维链（CoT）生成器和知识生成器，以显式生成推理轨迹与事实。针对知识正确性与置信度不匹配问题，使用KEPO优化知识生成器，优先选择低置信但有帮助的知识。在OK-VQA和A-OKVQA上验证了该方法的有效性，仅依赖内部模型能力即可超越现有方法，无需外部知识或商业API。


<details>
  <summary>Details</summary>
Motivation: 现有KBVQA方法的推理过程隐含于多模态大模型中，缺乏显式的多步推理轨迹；同时存在知识正确性与模型自信度不一致的问题，导致生成知识不可靠。因此需要一种方法来显式激发并利用模型内部的知识推理能力，提升推理可解释性与准确性。

Method: 1. 通过提示冻结的7B规模多模态大模型，在问题与真实答案之间补全推理过程，生成hindsight-zero训练数据；2. 自蒸馏该数据，构建Chain-of-Thought（CoT）生成器与知识生成器；3. 引入知识鼓励偏好优化（KEPO），优化知识生成器，使模型更倾向于输出低置信但有用的知识，而非高置信但无用的知识；4. 结合生成的推理链与知识进行答案预测。

Result: 在OK-VQA和A-OKVQA两个基准上，HinD框架显著优于现有方法，即使仅使用7B规模的内部模型，也无需调用商业API或外部知识库，实现了卓越性能，证明了其在激发和利用模型内部知识推理能力方面的有效性。

Conclusion: HinD框架成功实现了对多模态大模型内部知识推理能力的显式挖掘与利用，通过hindsight-zo training和KEPO优化，有效提升了推理过程的透明性与答案准确性，为无需外部知识的高质量KBVQA提供了新范式。

Abstract: Knowledge-based Visual Question Answering (KBVQA) necessitates external knowledge incorporation beyond cross-modal understanding. Existing KBVQA methods either utilize implicit knowledge in multimodal large language models (MLLMs) via in-context learning or explicit knowledge via retrieval augmented generation. However, their reasoning processes remain implicit, without explicit multi-step trajectories from MLLMs. To address this gap, we provide a Hindsight Distilled Reasoning (HinD) framework with Knowledge Encouragement Preference Optimization (KEPO), designed to elicit and harness internal knowledge reasoning ability in MLLMs. First, to tackle the reasoning supervision problem, we propose to emphasize the hindsight wisdom of MLLM by prompting a frozen 7B-size MLLM to complete the reasoning process between the question and its ground truth answer, constructing Hindsight-Zero training data. Then we self-distill Hindsight-Zero into Chain-of-Thought (CoT) Generator and Knowledge Generator, enabling the generation of sequential steps and discrete facts. Secondly, to tackle the misalignment between knowledge correctness and confidence, we optimize the Knowledge Generator with KEPO, preferring under-confident but helpful knowledge over the over-confident but unhelpful one. The generated CoT and sampled knowledge are then exploited for answer prediction. Experiments on OK-VQA and A-OKVQA validate the effectiveness of HinD, showing that HinD with elicited reasoning from 7B-size MLLM achieves superior performance without commercial model APIs or outside knowledge.

</details>


### [39] [OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation](https://arxiv.org/abs/2511.11162)
*Zhanpeng Wang,Shuting Cao,Yuhang Lu,Yuhan Li,Na Lei,Zhongxuan Luo*

Main category: cs.CV

TL;DR: OT-ALD is a novel image-to-image translation framework that improves upon DDIB by using optimal transport (OT) to align latent distributions between source and target domains, enhancing both translation efficiency and image quality. It achieves 20.29% faster sampling and 2.6 lower FID on average across four tasks.


<details>
  <summary>Details</summary>
Motivation: Address the low translation efficiency and trajectory deviations caused by latent distribution mismatches in existing DDIB-based methods.

Method: OT-ALD computes an optimal transport map from the source domain's latent distribution to the target domain's, using it as the starting point for reverse diffusion in the target domain, thereby eliminating distribution mismatches and improving consistency and speed.

Result: OT-ALD improves sampling efficiency by 20.29% and reduces FID by 2.6 on average across four I2I translation tasks on three high-resolution datasets, outperforming top baselines.

Conclusion: By leveraging optimal transport theory, OT-ALD effectively resolves key limitations of DDIB, enabling faster, more accurate, and consistent image-to-image translation while preserving cycle consistency.

Abstract: The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.

</details>


### [40] [Explainable Deep Convolutional Multi-Type Anomaly Detection](https://arxiv.org/abs/2511.11165)
*Alex George,Lyudmila Mihaylova,Sean Anderson*

Main category: cs.CV

TL;DR: 提出MultiTypeFCDD，一种轻量级卷积框架，用于可解释的多类型异常检测。该方法仅使用图像级标签，生成多通道热图，每通道对应一种异常类型，实现单一框架下跨多个物体类别的异常类型区分，避免为每个类别训练独立模型。在Real-IAD数据集上表现媲美先进复杂模型，但参数量和推理时间显著降低，适合资源受限的实时或嵌入式系统应用。


<details>
  <summary>Details</summary>
Motivation: 现有可解释异常检测方法难以区分异常类型，且需为每类物体训练独立模型，成本高；虽有大型视觉-语言模型尝试解决，但计算开销大，不适用于实时或嵌入式场景。因此亟需一种高效、轻量、能精准识别异常类型的通用解决方案。

Method: MultiTypeFCDD采用轻量级卷积架构，利用图像级标签训练多通道热图，每通道专门学习一种异常类型，通过统一框架实现多类别、多类型异常检测，无需为不同物体类别分别建模。

Result: 在Real-IAD数据集上，MultiTypeFCDD在检测性能上与最先进模型相当，同时大幅减少参数量和推理时间，具备良好的实际部署潜力。

Conclusion: MultiTypeFCDD是一种高效、轻量、可解释的多类型异常检测框架，能够有效区分异常类型并适应多种物体类别，特别适合在计算资源受限的现实环境中应用。

Abstract: Most explainable anomaly detection methods often identify anomalies but lack the capability to differentiate the type of anomaly. Furthermore, they often require the costly training and maintenance of separate models for each object category. The lack of specificity is a significant research gap, as identifying the type of anomaly (e.g., "Crack" vs. "Scratch") is crucial for accurate diagnosis that facilitates cost-saving operational decisions across diverse application domains. While some recent large-scale Vision-Language Models (VLMs) have begun to address this, they are computationally intensive and memory-heavy, restricting their use in real-time or embedded systems. We propose MultiTypeFCDD, a simple and lightweight convolutional framework designed as a practical alternative for explainable multi-type anomaly detection. MultiTypeFCDD uses only image-level labels to learn and produce multi-channel heatmaps, where each channel is trained to correspond to a specific anomaly type. The model functions as a single, unified framework capable of differentiating anomaly types across multiple object categories, eliminating the need to train and manage separate models for each object category. We evaluated our proposed method on the Real-IAD dataset and it delivers results competitive with state-of-the-art complex models at significantly reduced parametric load and inference times. This makes it a highly practical and viable solution for real-world applications where computational resources are tightly constrained.

</details>


### [41] [CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios](https://arxiv.org/abs/2511.11168)
*Hangyu Li,Bofeng Cao,Zhaohui Liang,Wuzhen Li,Juyoung Oh,Yuxuan Chen,Shixiao Liang,Hang Zhou,Chengyuan Ma,Jiaxi Liu,Zheng Li,Peng Zhang,KeKe Long,Maolin Liu,Jackson Jiang,Chunlei Yu,Shengxiang Liu,Hongkai Yu,Xiaopeng Li*

Main category: cs.CV

TL;DR: CATS-V2V 是首个面向复杂恶劣交通场景（CATS）的车对车（V2V）协同感知真实世界数据集，涵盖10种天气与光照条件、10个不同地点，共100段视频片段，包含60K帧10Hz LiDAR点云、126万张30Hz多视角相机图像及75万条高精度RTK-GNSS与IMU记录。数据集提供时间一致的3D边界框标注和静态场景信息以构建4D BEV表示，并提出基于目标的时间对齐方法，确保多模态数据精确对齐。该数据集为协同感知研究提供了高质量、大规模、多模态支持，有望推动自动驾驶在复杂场景下的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要聚焦于常规交通场景，难以支持复杂恶劣交通场景下的车对车协同感知研究。由于数据采集要求严苛，缺乏真实世界中多样且高质量的协同感知数据，制约了相关技术的发展。因此亟需一个覆盖多种复杂环境的真实数据集来推动协同感知算法的研究与应用。

Method: 提出一种基于目标的时序对齐方法，通过时间一致性分析与多模态融合策略，实现不同传感器（LiDAR、相机、GNSS/IMU）之间在空间与时间维度上的精准对齐；同时利用静态场景信息构建4D BEV表征，增强跨车辆感知的一致性与鲁棒性。

Result: 成功构建了目前规模最大、质量最高、支持最全面的V2V协同感知数据集CATS-V2V，包含100个场景片段、60K帧10Hz LiDAR点云、1.26M张30Hz多视角图像以及75万条高精度定位与惯性数据，所有对象均完成时间一致的3D边界框标注，支持4D BEV建模与协同感知任务。

Conclusion: CATS-V2V作为首个面向复杂恶劣交通场景的车对车协同感知真实数据集，填补了当前领域在极端与多样化环境数据上的空白，为自动驾驶系统在复杂场景下的感知能力提升提供了坚实的数据基础，有望成为未来协同感知研究的重要基准资源。

Abstract: Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.

</details>


### [42] [Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA](https://arxiv.org/abs/2511.11169)
*Ayush Pandey,Jai Bardhan,Ishita Jain,Ramya S Hebbalaguppe,Rohan Raju Dhanakshirur,Lovekesh Vig*

Main category: cs.CV

TL;DR: 提出AlignVQA，一种基于辩论的多智能体框架，通过多样化视觉语言模型（VLM）在两阶段互动中生成并优化答案，提升信心估计的校准性。引入可微分的校准感知损失函数aligncal，以最小化校准误差上界，从而增强各智能体信心估计的准确性。实验表明该方法显著降低校准偏差，提升VQA系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代VQA系统虽准确率高，但其信心估计常过度自信，尤其在医疗诊断、自动驾驶等高风险领域，校准性不足影响系统可靠性。因此需改进信心估计的准确性，使其更真实反映预测性能。

Method: 设计AlignVQA框架，包含多种采用不同提示策略的专门化VLM智能体；通过两阶段辩论过程（批判、修正与聚合）生成更可靠的答案与信心估计；引入可微分的校准感知损失aligncal，用于微调各智能体，最小化校准误差上界。

Result: 在多个主流VQA数据集上的实验证明，该方法显著降低了校准偏差，提升了信心估计的准确性，且更校准的专门化智能体产生更一致的信心输出。

Conclusion: AlignVQA通过辩论机制与校准感知训练，有效提升了VQA系统信心估计的校准性，增强了其在自主决策任务中的可信度与可靠性。

Abstract: In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.

</details>


### [43] [Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos](https://arxiv.org/abs/2511.11175)
*Zhixin Xu,Hengyu Zhou,Yuan Liu,Wenhan Xue,Hao Pan,Wenping Wang,Bin Wang*

Main category: cs.CV

TL;DR: 提出一种用于非同步多视角视频的4D高斯点阵重建的时序对齐策略，通过粗到细的对齐模块估计并补偿各相机的时间偏移，实现帧级至亚帧级精度的对齐，可无缝集成到现有4DGS框架中，显著提升异步数据下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的多视角视频常因相机触发延迟或独立录制导致时间不同步，现有4DGS方法依赖于时间同步假设，因此在非同步情况下重建质量下降，亟需一种鲁棒的时序对齐方法以提升重建性能。

Method: 设计了一种粗到细的时序对齐模块：首先在帧级别估计时间偏移，再逐步优化至亚帧级精度，从而有效补偿各视角间的时序偏差，并可作为通用模块嵌入现有4DGS框架。

Result: 实验表明，该方法能有效处理时间不同步的多视角视频，在多个基准上显著优于基线方法，提升了动态场景重建的质量和鲁棒性。

Conclusion: 所提出的时序对齐策略为4DGS在真实异步多视角视频中的应用提供了有效解决方案，增强了模型对非同步输入的适应能力，具有良好的实用性和扩展性。

Abstract: Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.

</details>


### [44] [Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation](https://arxiv.org/abs/2511.11177)
*Quoc-Huy Trinh,Mustapha Abdullahi,Do Duy Hung Trinh,Bo Zhao,Debesh Jha*

Main category: cs.CV

TL;DR: Viper-F1 是一种混合状态空间视觉-语言模型，通过用高效的液态状态空间动态替代传统的 Transformer 交叉注意力机制，解决了多模态大模型计算成本高的问题。同时引入轻量级的 Token-Grid 相关性模块和 FiLM 条件化机制，增强对细粒度视觉区域的定位能力，实现线性时间推理，在多个基准测试中表现出更高的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型依赖于计算复杂度高的 Transformer 交叉注意力机制，导致在资源受限场景下难以部署；同时小型模型难以精准捕捉细粒度视觉区域，影响其在实际任务中的表现。

Method: 提出 Viper-F1 模型，采用液态状态空间动态替代注意力机制，并设计 Token-Grid 相关性模块与 FiLM 条件化机制，以提升视觉-语言对齐精度并保持线性推理速度。

Result: 在多个基准测试上，Viper-F1 实现了更精确的细粒度理解，同时显著提升了推理效率，具备在机器人操作、个人助手等场景中部署的潜力。

Conclusion: Viper-F1 通过高效的状态空间建模与轻量级视觉-语言对齐机制，有效平衡了性能与效率，为资源受限环境下的多模态理解提供了可行方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.

</details>


### [45] [Computationally-efficient deep learning models for nowcasting of precipitation: A solution for the Weather4cast 2025 challenge](https://arxiv.org/abs/2511.11197)
*Anushree Bhuskute,Kaushik Gopalan,Jeet Shah*

Main category: cs.CV

TL;DR: 该研究提出一种基于卷积门控循环单元（ConvGRU）的迁移学习框架，用于Weather4Cast 2025竞赛中的短时降雨预测。使用SEVIRI红外通道（10.8 μm）的四次观测作为输入，采用两阶段训练策略，先预测亮度温度以捕捉时空模式，再通过经验非线性变换映射为OPERA兼容的降雨率。在累积降雨任务中获得第二名；在事件预测任务中，直接使用同一模型，表现与基准模型相当。


<details>
  <summary>Details</summary>
Motivation: 提高短时降雨预测精度，尤其是在气象数据有限的情况下，利用迁移学习和高效时空建模能力提升预测性能。

Method: 采用基于ConvGRU的两阶段训练框架：第一阶段预测SEVIRI亮度温度，第二阶段通过经验非线性变换生成降雨率；事件预测部分结合3D事件检测与时空特征提取。

Result: 在累积降雨预测任务中取得第二名；在事件预测任务中表现与基准模型相当，说明模型具备良好的泛化能力。

Conclusion: 所提出的ConvGRU迁移学习框架在短时降雨预测任务中表现出色，尤其在累积降雨预测方面具有显著优势，且可直接应用于事件预测任务，展现出较强的实用性与通用性。

Abstract: This study presents a transfer-learning framework based on Convolutional Gated Recurrent Units (ConvGRU) for short-term rainfall prediction in the Weather4Cast 2025 competition. A single SEVIRI infrared channel (10.8 μm wavelength) is used as input, which consists of four observations over a one-hour period. A two-stage training strategy is applied to generate rainfall estimates up to four hours ahead. In the first stage, ConvGRU is trained to forecast the brightness temperatures from SEVIRI, enabling the model to capture relevant spatiotemporal patterns. In the second stage, an empirically derived nonlinear transformation maps the predicted fields to OPERA-compatible rainfall rates.
  For the event-prediction task, the transformed rainfall forecasts are processed using 3D event detection followed by spatiotemporal feature extraction to identify and characterize precipitation events. Our submission achieved 2nd place in the cumulative rainfall task. Further, the same model was used out-of-the-box for the event prediction task, and resulted in similar scores as the baseline model to the competition.

</details>


### [46] [Geospatial Chain of Thought Reasoning for Enhanced Visual Question Answering on Satellite Imagery](https://arxiv.org/abs/2511.11198)
*Shambhavi Shanker,Manikandan Padmanaban,Jagabondhu Hazra*

Main category: cs.CV

TL;DR: 本文提出一种结合思维链（CoT）推理与直接偏好优化（DPO）的VQA框架，用于提升卫星图像中地理空间问答任务的可解释性、鲁棒性和准确性，尤其适用于气候相关应用如灾害监测和城市韧性规划。实验表明，CoT监督使准确率提升34.9%，DPO进一步增强性能，显著改善复杂地理空间任务的推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型在遥感数据解析中虽具可扩展性，但缺乏处理复杂地理空间查询所需的结构化推理能力，限制了其在高风险气候应用中的可靠决策支持能力。

Method: 提出融合思维链（CoT）推理与直接偏好优化（DPO）的VQA框架，通过生成中间推理步骤，增强模型对检测、分类、空间关系及比较分析等任务的理解与表现。

Result: CoT监督使准确率较直接基线提升34.9%，DPO进一步提升准确率与推理质量，显著增强多光谱地球观测中的地理空间推理能力，推动气候相关应用的发展。

Conclusion: 该框架通过引入结构化推理与偏好优化，有效提升了卫星图像VQA在复杂地理空间任务中的表现，为气候监测与政策支持提供了更可靠的技术支撑。

Abstract: Geospatial chain of thought (CoT) reasoning is essential for advancing Visual Question Answering (VQA) on satellite imagery, particularly in climate related applications such as disaster monitoring, infrastructure risk assessment, urban resilience planning, and policy support. Existing VQA models enable scalable interpretation of remote sensing data but often lack the structured reasoning required for complex geospatial queries. We propose a VQA framework that integrates CoT reasoning with Direct Preference Optimization (DPO) to improve interpretability, robustness, and accuracy. By generating intermediate rationales, the model better handles tasks involving detection, classification, spatial relations, and comparative analysis, which are critical for reliable decision support in high stakes climate domains. Experiments show that CoT supervision improves accuracy by 34.9\% over direct baselines, while DPO yields additional gains in accuracy and reasoning quality. The resulting system advances VQA for multispectral Earth observation by enabling richer geospatial reasoning and more effective climate use cases.

</details>


### [47] [Questioning the Stability of Visual Question Answering](https://arxiv.org/abs/2511.11206)
*Amir Rosenfeld,Neta Glazer,Ethan Fetaya*

Main category: cs.CV

TL;DR: 本文首次对视觉语言模型（VLMs）在轻微、语义不变的视觉和文本扰动下的鲁棒性进行了大规模系统性研究。研究发现，现代VLMs对微小变化极为敏感，如像素偏移、几何变换、重缩放、改写和多语言重述等，即使这些变化不改变图像-问题对的语义。大量样本在轻微扰动下预测答案发生变化，且稳定性与正确性高度相关：稳定样本更可能回答正确。利用这一特性，可基于小型开源模型的稳定性模式高精度预测大型闭源模型的正确性。研究揭示了当前VLMs的根本脆弱性，呼吁关注非对抗性但语义保持的不变性评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注对抗性扰动，而对日常中常见的、语义不变的小幅扰动（如轻微图像移动、句子重写）对VLM性能的影响缺乏系统理解。本文旨在填补这一空白，揭示VLM在真实场景中的可靠性缺陷。

Method: 构建涵盖多种视觉和文本扰动的基准测试集（包括像素级移动、几何变换、重缩放、语义保持的改写与多语言重述），在多个主流VLMs和数据集上进行大规模实验，分析不同扰动类型、问题类别和模型间的稳定性差异，并探究样本级稳定性与预测正确性的关联性。

Result: 现代VLMs对微小扰动表现出高度敏感性，即使是先进的模型（如GPT-4o、Gemini 2.0 Flash）也常在几像素位移或无害重述下出错；样本稳定性与正确性显著正相关；小型开源模型的稳定性可有效预测大型闭源模型的准确性。

Conclusion: 当前视觉语言模型存在根本性脆弱性，其对语义保持的微小变化缺乏鲁棒性。未来评估应超越对抗性攻击，重视模型在自然、良性扰动下的不变性表现，以提升实际应用中的可靠性。

Abstract: Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.

</details>


### [48] [One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger](https://arxiv.org/abs/2511.11210)
*Dongmei Shan,Wei Lian,Chongxia Wang*

Main category: cs.CV

TL;DR: 本文提出首个针对3D视觉的一对多后门攻击框架，利用可配置的球形触发器实现单个触发器编码多个目标类别，突破了现有方法仅支持一对一攻击的局限。通过理论分析与实验验证，证明了在3D点云中实现多目标后门攻击的可行性，攻击成功率高达100\%，同时保持干净数据上的高准确率。该工作为3D视觉中的多目标威胁提供了基准，并为未来智能系统的安全防护奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云后门攻击局限于刚性的一对一模式，无法应对复杂多目标威胁，亟需一种更灵活、可扩展的攻击框架以揭示系统潜在风险。

Method: 提出基于可配置球形触发器的一对多后门攻击框架，利用球体的空间特性作为参数空间，使同一触发器设计能够通过不同配置映射到多个目标类别，建立相应的理论基础并进行系统性实验验证。

Result: 在多个数据集和模型架构上均实现高达100\%的攻击成功率，同时保持对干净数据的高分类准确率，有效验证了方法的可行性和有效性。

Conclusion: 本研究首次实现了3D视觉中的一对多后门攻击，建立了理论基础并提供了一个重要的安全评估基准，有助于推动3D智能系统的安全防御机制发展。

Abstract: Backdoor attacks represent a critical threat to deep learning systems, particularly in safety-sensitive 3D domains such as autonomous driving and robotics. However, existing backdoor attacks for 3D point clouds have been limited to a rigid one-to-one paradigm. To address this, we present the first one-to-N backdoor framework for 3D vision, based on a novel, configurable spherical trigger. Our key insight is to leverage the spatial properties of spheres as a parameter space, allowing a single trigger design to encode multiple target classes. We establish a theoretical foundation for one-to-N backdoor attacks in 3D, demonstrating that poisoned models can map distinct trigger configurations to different target labels. Experimental results systematically validate this conclusion across multiple datasets and model architectures, achieving high attack success rates (up to 100\%) while maintaining accuracy on clean data. This work establishes a crucial benchmark for multi-target threats in 3D vision and provides the foundational understanding needed to secure future 3D-driven intelligent systems.

</details>


### [49] [MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI](https://arxiv.org/abs/2511.11212)
*Mohammad Areeb Qazi,Munachiso S Nwadike,Ibrahim Almakky,Mohammad Yaqub,Numan Saeed*

Main category: cs.CV

TL;DR: MAFM^3 是一种用于多模态医学人工智能的模块化基础模型适应框架，通过轻量级模块组件使单一基础模型能够灵活扩展至不同领域、任务和模态，实现高效多任务与多模态适应。在胸部CT分类基础上，成功拓展至预后预测和分割任务，性能提升显著；引入PET扫描后，Dice分数较基线提升5%。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺，难以为每个领域、模态或任务单独训练基础模型，现有方法在新任务或模态上孤立适应，效率低且缺乏可扩展性。因此需要一种统一、可扩展的框架，使基础模型能灵活适配多种临床需求。

Method: 提出MAFM^3框架，利用轻量级模块组件作为专用技能集，在推理时根据输入类型或临床目标动态激活相应能力，实现跨任务、跨模态的统一适应。

Result: 在胸部CT基础上成功实现预后预测与分割任务的性能提升；加入PET数据后，Dice分数比基线提高5%，验证了模型的泛化与扩展能力。

Conclusion: 基础模型通过模块化组件可突破初始训练范围，发展为具备多任务、多模态能力的系统，为医学影像智能提供高效、灵活的解决方案。

Abstract: Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at https://github.com/Areeb2735/CTscan_prognosis_VLM

</details>


### [50] [RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting](https://arxiv.org/abs/2511.11213)
*Ruocheng Wu,Haolan He,Yufei Wang,Zhihao Li,Bihan Wen*

Main category: cs.CV

TL;DR: 本文提出了一种名为引导得分蒸馏（GSD）的新框架，用于解决3D高斯点阵（3DGS）在稀疏训练视图下容易过拟合的问题。受视频扩散模型（VDM）成功的启发，GSD从预训练的VDM中提取多视图一致性先验信息，并通过结合深度映射和语义图像特征的统一引导形式，校正VDM的噪声预测结果，使优化过程更符合真实相机姿态与几何结构。实验表明，该方法在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏训练视图下容易过拟合，主要原因是缺乏中间视图的监督。现有方法难以有效利用多视图一致性信息，因此需要引入更强的先验引导机制来提升泛化能力。

Method: 提出GSD框架，基于Score Distillation Sampling（SDS）思想，利用VDM生成的多视角图像进行监督；设计统一引导形式，融合基于真实深度图的深度映射引导和基于语义特征的图像引导，以纠正VDM的噪声预测方向，确保优化过程与真实几何和相机姿态对齐。

Result: 在多个数据集上的实验结果表明，所提方法在重建质量、泛化能力和鲁棒性方面均显著优于现有方法，尤其在稀疏视图条件下表现突出。

Conclusion: GSD通过从预训练视频扩散模型中蒸馏多视图一致性先验，有效缓解了3DGS在稀疏输入下的过拟合问题，实现了高质量且稳定的3D场景重建。该方法为稀疏视图下的3D表示学习提供了新的思路。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.

</details>


### [51] [Positional Bias in Multimodal Embedding Models: Do They Favor the Beginning, the Middle, or the End?](https://arxiv.org/abs/2511.11216)
*Kebin Wu,Fatima Albreiki*

Main category: cs.CV

TL;DR: 本文研究多模态表示模型中的位置偏差问题，特别是在图像-文本检索任务中。研究区分了上下文重要性与位置偏差，并发现位置偏差在多模态模型中普遍存在，但不同模态表现不同：文本编码器偏向输入开头，图像编码器则在开头和结尾均表现出偏差。该偏差由位置编码方式、训练损失、上下文重要性以及图像-文本对的使用特性共同导致或加剧。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注文本生成模型中的位置偏差，而对表示模型尤其是多模态模型中的位置偏差了解不足，因此需要系统探究其存在性、表现形式及成因。

Method: 通过区分上下文重要性与位置偏差，设计实验评估多种多模态模型在不同数据集上的位置偏差程度，分析不同因素（如位置编码、训练损失、上下文重要性、模态配对方式）对偏差的影响。

Result: 位置偏差在多模态模型中广泛存在；文本编码器倾向于关注输入开头，图像编码器则在开头和结尾均有偏好；该偏差受位置编码方案、训练目标、上下文重要性及图像-文本配对机制的共同影响。

Conclusion: 位置偏差是多模态表示模型中的一个关键问题，其来源复杂且跨模态表现不一，未来模型设计需考虑位置编码与训练策略的优化以缓解此类偏差。

Abstract: Positional bias - where models overemphasize certain positions regardless of content - has been shown to negatively impact model performance across various tasks. While recent research has extensively examined positional bias in text generation models, its presence and effects in representation models remain underexplored. Even less is known about such biases in multimodal models. In this work, we investigate positional bias in multimodal representation models, specifically in the context of image-text retrieval. We begin by distinguishing between context importance and positional bias, and then assess the presence and extent of positional bias across different models and datasets. Our experiments demonstrate that positional bias is prevalent in multimodal models, but manifests differently across modalities: text encoders tend to exhibit bias toward the beginning of the input, whereas image encoders show bias at both the beginning and end. Furthermore, we find that this bias arises from, or is amplified by, a combination of factors, including the positional encoding scheme, training loss, context importance, and the nature of using image-text pairs in multimodal training.

</details>


### [52] [3D Gaussian and Diffusion-Based Gaze Redirection](https://arxiv.org/abs/2511.11231)
*Abiram Panchalingam,Indu Bodala,Stuart Middleton*

Main category: cs.CV

TL;DR: DiT-Gaze 提出一种基于 Diffusion Transformer (DiT)、弱监督中间视线角度以及正交性约束损失的新框架，显著提升 3D 眼球方向重定向的保真度，实现更平滑、精确的视线生成，在感知质量和重定向精度上均达到新基准，将最先进的眼球误差降低 4.1% 至 6.353 度。


<details>
  <summary>Details</summary>
Motivation: 现有 3D Gaussian Splatting 模型在渲染细微连续视线变化时表现不佳，亟需更高保真度的视线重定向方法以增强眼球估计算法的泛化能力。

Method: 结合 Diffusion Transformer (DiT) 实现高质量图像合成，利用合成中间视线角度进行弱监督训练以构建平滑的视线方向流形，并引入正交性约束损失以解耦视线、头部姿态和表情的内部表示。

Result: DiT-Gaze 在感知质量与重定向准确性方面均超越现有方法，将最优眼球估计误差降低 4.1%（降至 6.353 度），为合成训练数据提供了更优解决方案。

Conclusion: DiT-Gaze 是当前最先进的 3D 眼球方向重定向框架，其在高保真图像生成和精确视线控制方面表现出色，具有广泛的应用潜力，代码与模型将公开供社区使用。

Abstract: High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.

</details>


### [53] [DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding](https://arxiv.org/abs/2511.11232)
*Mingwei Xing,Xinliang Wang,Yifeng Shi*

Main category: cs.CV

TL;DR: 提出DoReMi框架，通过混合专家（MoE）机制联合建模领域特定专家分支与统一表示分支，实现专业化与泛化知识的协同学习。采用领域引导的空间路由（DSR）动态选择专家，结合熵控制的动态分配（EDA）稳定高效利用专家，同时借助预训练的冻结统一表示分支保留跨域几何和结构先验。在多个3D理解基准上表现优异，如ScanNet Val达到80.1% mIoU，S3DIS达77.2% mIoU，展现强大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有3D深度学习在多领域泛化受限于数据集规模小及多源点云异质性高；不同传感器采集的点云在密度和噪声分布上差异大，导致多领域融合时出现负迁移；现有方法仅关注领域特定或领域泛化特征，忽略二者协同潜力。

Method: 提出DoReMi框架，基于混合专家（MoE）结构，包含领域感知专家分支与统一表示分支；通过领域引导的空间路由（DSR）实现上下文感知的专家选择；采用熵控制的动态分配（EDA）保证专家使用稳定高效；统一表示分支通过多属性自监督学习预训练并冻结，以保持跨域几何与结构一致性。

Result: 在ScanNet Val上取得80.1% mIoU，在S3DIS上取得77.2% mIoU，性能优于或媲美现有方法，验证了其在3D理解任务中的有效性与作为未来研究基础框架的潜力。

Conclusion: DoReMi通过协同建模领域特定与通用知识，有效应对多源点云异质性挑战，显著提升3D模型在跨域场景下的泛化能力，是具有前景的3D理解基础框架。

Abstract: The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.

</details>


### [54] [Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing](https://arxiv.org/abs/2511.11236)
*Cong Cao,Yujie Xu,Xiaodong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的少样本风格编辑框架，通过构建包含五种不同风格的基准数据集，设计了参数高效的多风格Mixture-of-Experts LoRA（MoE LoRA），结合风格特定与共享路由机制，实现多风格联合微调。该方法利用新提出的度量引导策略自动确定每层最优秩，并探索了LoRA在DiT模型中的最佳插入位置，同时引入对抗学习和流匹配以指导扩散训练。实验表明，该方法在显著减少LoRA参数量的同时，优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在面对新风格时表现不佳，主要挑战在于如何仅用少量成对数据有效微调模型。现有方法难以在保持高效性的同时适应多种风格，亟需一种参数高效且可扩展的少样本风格编辑方案。

Method: 提出基于多风格Mixture-of-Experts LoRA（MoE LoRA）的微调框架，采用风格特定与风格共享双路由机制；设计度量引导的秩自适应策略以优化每层低秩组件的重要性；探索LoRA在Diffusion in Transformer（DiT）模型中的最优插入位置；融合对抗学习与流匹配提升扩散训练效果。

Result: 所提方法在少样本条件下实现了对多种风格的有效编辑，在性能上超越现有最先进方法，同时使用更少的LoRA参数，验证了其高效性和可扩展性。

Conclusion: 本文提出的少样本风格编辑框架通过参数高效的MoE LoRA设计与多策略协同优化，在有限数据下实现了高质量、多风格的图像编辑，为通用图像编辑模型的快速适配提供了有效解决方案。

Abstract: In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.

</details>


### [55] [Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression](https://arxiv.org/abs/2511.11239)
*Zhongbin Guo,Jiahe Liu,Yushan Li,Wenyu Gao,Zhen Yang,Chenzhi Li,Xinyue Zhang,Ping Jian*

Main category: cs.CV

TL;DR: GEODE提出一种新架构，通过解耦3D推理与数值生成，解决现有视觉语言模型在3D空间智能理解上的双重瓶颈。其核心由两个即插即用模块组成：Decoupled Rationale Module（DRM）用于对齐3D与2D特征并生成可注入的空间思维链令牌；Direct Regression Head（DRH）采用“嵌入即值”范式，通过轻量MLP实现精确连续数值回归。该设计使1.5B参数模型在空间推理上达到7B+模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于'平面感知'架构，难以理解真实世界的3D空间智能，主要因输入阶段几何编码器计算成本高且仅依赖2D特征，输出阶段离散分词器无法生成精确连续数值。

Method: 引入GEODE架构，包含两个模块：DRM通过交叉注意力对齐3D与2D特征，并生成空间思维链（CoT）逻辑以注入理由令牌；DRH采用'嵌入即值'机制，利用控制令牌引导轻量MLP进行连续数值回归，实现精确的标量与3D边界框预测。

Result: 所提出的1.5B参数模型在空间推理任务中表现优异，达到甚至超越7B以上模型的性能，证明了其高效性与有效性。

Conclusion: GEODE通过解耦3D推理与数值生成，有效突破了现有视觉语言模型在3D空间智能方面的瓶颈，为构建更强大的多模态系统提供了新路径。

Abstract: Existing Vision Language Models (VLMs) architecturally rooted in "flatland" perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an "Embedding-as-Value" paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.

</details>


### [56] [Toward Gaze Target Detection of Young Autistic Children](https://arxiv.org/abs/2511.11244)
*Shijian Deng,Erin E. Kosloski,Siva Sai Nagender Vasireddy,Jia Li,Randi Sierra Sherwood,Feroz Mohamed Hatha,Siddhi Patel,Pamela R Rollins,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文提出了一种针对自闭症儿童的新型人工智能眼动目标检测方法，构建了首个自闭症眼动目标（AGT）数据集，并设计了社会感知的粗到精（SACF）框架，通过双路径架构和上下文感知门控模块，有效缓解自闭症数据集中面部注视类别的样本不平衡问题，在关键少数类（面部注视）上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自闭症儿童存在联合注意缺陷，而现有的眼动目标检测技术在该群体中表现不佳，尤其因面部注视样本稀少导致模型性能受限。因此，亟需一种能适应自闭症儿童行为特征的高效、鲁棒的眼动检测系统，以支持自动化评估与干预。

Method: 提出Socially Aware Coarse-to-Fine（SACF）框架，采用双路径结构，分别由社会性专家模型和非社会性专家模型处理社交与非社交场景下的注视预测，并引入上下文感知门控模块动态融合两路输出，从而增强对社会情境的理解与建模能力。

Result: 在自闭症儿童眼动目标检测任务中，SACF框架在多项指标上达到新基准，特别是在面部注视这一关键少数类别上表现突出，显著优于已有方法。

Conclusion: 本研究不仅推动了自闭症儿童眼动分析的技术进步，也为未来基于AI的早期筛查与个性化干预提供了可扩展的解决方案。

Abstract: The automatic detection of gaze targets in autistic children through artificial intelligence can be impactful, especially for those who lack access to a sufficient number of professionals to improve their quality of life. This paper introduces a new, real-world AI application for gaze target detection in autistic children, which predicts a child's point of gaze from an activity image. This task is foundational for building automated systems that can measure joint attention-a core challenge in Autism Spectrum Disorder (ASD). To facilitate the study of this challenging application, we collected the first-ever Autism Gaze Target (AGT) dataset. We further propose a novel Socially Aware Coarse-to-Fine (SACF) gaze detection framework that explicitly leverages the social context of a scene to overcome the class imbalance common in autism datasets-a consequence of autistic children's tendency to show reduced gaze to faces. It utilizes a two-pathway architecture with expert models specialized in social and non-social gaze, guided by a context-awareness gate module. The results of our comprehensive experiments demonstrate that our framework achieves new state-of-the-art performance for gaze target detection in this population, significantly outperforming existing methods, especially on the critical minority class of face-directed gaze.

</details>


### [57] [Discovering Meaningful Units with Visually Grounded Semantics from Image Captions](https://arxiv.org/abs/2511.11262)
*Melika Behjati,James Henderson*

Main category: cs.CV

TL;DR: 本文提出一种新模型，通过分组图像描述中的词元（tokens）来捕捉语言的细粒度表示，使其与图像编码器发现的对象对齐，从而提升视觉-语言模型对场景的细粒度理解。实验表明，该模型发现的词元分组在质和量上均与可对应图像的短语高度相似。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多关注图像块与语言词元的对齐，但图像块对人眼无意义，单个词元也不一定在图像中有明确对应。真正有意义的是由多个词元组成的语义群，描述场景的不同方面。因此需要更精细的语言表示以实现更好的跨模态理解。

Method: 设计一种新模型，将文本描述中的词元进行分组，使这些分组能对应图像中实际存在的对象，并与预训练的图像编码器输出对齐，从而学习到更具语义意义的细粒度语言表示。

Result: 所提出的模型显著提升了视觉-语言模型对场景的细粒度理解能力；发现的词元分组在语义上与可定位的文本短语高度一致，且在定量和定性分析中表现优异。

Conclusion: 通过显式建模词元分组，本方法有效增强了视觉-语言模型对真实世界细粒度知识的理解能力，为跨模态对齐提供了新的视角。

Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

</details>


### [58] [CountSteer: Steering Attention for Object Counting in Diffusion Models](https://arxiv.org/abs/2511.11253)
*Hyemin Boo,Hyoryung Kim,Myungjin Lee,Seunghyeon Lee,Jiyoung Lee,Jang-Hwan Choi,Hyunsoo Cho*

Main category: cs.CV

TL;DR: CountSteer is a training-free method that improves text-to-image models' adherence to numerical instructions by steering cross-attention states during inference, increasing count accuracy by ~4% while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often fail to follow numerical instructions in text, indicating a gap between language and visual representation. However, the models are not entirely blind to numbers; they implicitly understand counting accuracy through internal signal shifts.

Method: Introducing CountSteer, a training-free method that steers cross-attention hidden states during inference to improve object-count accuracy by leveraging the model's latent notion of numerical correctness.

Result: CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating enhanced controllability and semantic reliability in text-to-image generation.

Conclusion: The study reveals that existing models already encode numerical correctness, and CountSteer effectively harnesses this latent knowledge to improve generation precision.

Abstract: Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the model's cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.

</details>


### [59] [From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs](https://arxiv.org/abs/2511.11440)
*Massimo Rizzoli,Simone Alghisi,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 本文提出一种重新设计的视觉-语言模型（VLM）微调流程，通过可控生成无偏、分布均衡且标注准确的合成数据来解决现有方法中的偏差、错误和分布不平衡问题。利用该合成数据集对先进VLM进行微调，并在真实世界数据上评估其性能迁移能力。实验表明：1）在平衡的合成数据上微调可实现视觉场景中均匀的性能并缓解常见偏差；2）在合成数据上微调显著提升真实世界数据（如COCO）上的表现，优于在匹配设置下微调的模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调依赖于非系统性的真实场景数据收集与标注，易引入偏差、错误和分布不平衡，导致过拟合和性能不均。尽管已有研究尝试使用合成数据缓解此问题，但缺乏对数据分布偏差和标注质量的控制。因此，亟需一种更可控、更高质量的合成数据生成与微调方法。

Method: 1）通过综合采样物体属性（颜色、形状、大小、位置等）自动构建无偏、分布均衡且标注准确的合成数据集；2）基于该合成数据集对先进VLM进行微调，并在合成与真实世界基准上进行全面评估，以检验其性能迁移能力。

Result: 1）在平衡合成数据上微调可实现视觉场景中性能的均匀性，有效缓解常见偏差；2）在合成数据上微调显著提升模型在真实世界数据（如COCO）上的表现，优于在匹配设置下微调的模型。

Conclusion: 本研究证明了可控合成数据在训练视觉-语言模型中的有效性，不仅能消除数据偏差和分布不平衡，还能显著提升模型在真实世界任务中的泛化性能，为未来高效、可靠的数据驱动模型训练提供了新范式。

Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.

</details>


### [60] [GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving](https://arxiv.org/abs/2511.11266)
*Fabian Schmidt,Markus Enzweiler,Abhinav Valada*

Main category: cs.CV

TL;DR: 本文提出一种模型无关的方法，通过交通场景图（traffic scene graphs）为基于语言的自动驾驶模型提供结构化关系上下文，以增强对空间结构和动态交互的拓扑感知推理能力。通过在不同抽象层次和格式下序列化场景图，并使用结构化提示模板融入模型，实现了对关系监督有效性的系统性分析。在LangAuto基准上的大量实验表明，该方法显著提升了主流模型的驾驶性能，如LMDrive和BEVDriver分别提升15.6%和17.5%的驾驶得分，且无需测试时输入场景图。代码、微调模型及数据集已公开。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在自动驾驶规划中缺乏显式编码关系依赖的监督，限制了其从原始传感器数据中推断交通参与者之间相互影响的能力。

Method: 提出一种模型无关的方法，将多粒度、多格式的交通场景图序列化并以结构化提示模板形式融入语言驱动模型，实现关系上下文的条件化训练。

Result: 在LangAuto基准上，采用场景图条件化的先进模型表现出显著且持续的性能提升，其中LMDrive提升15.6%，BEVDriver提升17.5%；模型能更好地内化和锚定关系先验，即使测试时不需输入场景图。

Conclusion: 通过场景图条件化训练，可有效提升视觉-语言模型在自动驾驶中的关系推理能力，且该方法具有通用性和实用性，推动了自动驾驶规划中语义与空间关系融合的发展。

Abstract: Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\% increase in driving score for LMDrive and 17.5\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.

</details>


### [61] [Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation](https://arxiv.org/abs/2511.11276)
*Haoyi Wang*

Main category: cs.CV

TL;DR: CORAL提出一种新的方法来解决体积医学图像分割中的挑战，通过联合学习局部和全局解剖结构信息，利用连续的解剖相似性与序数关系，提升特征表示的质量，在标注数据有限的情况下达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖硬二值阈值定义正负样本，忽略了连续的解剖相似性信息，并且未考虑解剖进展的方向一致性，导致特征空间扭曲，无法捕捉跨患者的典型解剖流形。

Method: CORAL采用对比排序目标以利用连续解剖相似性，使切片间特征距离与其解剖位置差异成比例；同时引入序数目标以强制全局方向一致性，使学习到的特征分布与跨患者的标准解剖进展对齐。

Result: 在有限标注设置下，CORAL在基准数据集上实现了最先进的分割性能，同时学习到具有明确解剖结构意义的表示。

Conclusion: 通过协调学习局部与全局解剖关系，CORAL有效提升了体积医学图像分割中特征表示的质量，为下游任务提供了更具解剖意义的表征。

Abstract: Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at https://github.com/haoyiwang25/CORAL.

</details>


### [62] [SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing](https://arxiv.org/abs/2511.11295)
*Yichao Tang,Mingyang Li,Di Miao,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CV

TL;DR: SimuFreeMark是一种无需噪声模拟的图像水印框架，利用图像低频成分的内在稳定性，在深度特征空间中嵌入水印，通过预训练的变分自编码器（VAE）将水印与结构稳定的图像表示绑定，从而在不依赖手动生成噪声训练的情况下，实现对多种常规和语义攻击的强鲁棒性，并保持优异的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的图像水印方法依赖于手工设计的噪声模拟层进行训练，这限制了其在未知失真情况下的泛化能力。随着AIGC的发展，亟需一种能抵御传统信号处理和新型语义编辑攻击的鲁棒水印技术。

Method: 提出SimuFreeMark框架，通过系统验证低频成分对多种攻击具有显著鲁棒性；将水印直接嵌入低频成分的深度特征空间，利用预训练的变分自编码器（VAE）实现水印与结构稳定表示的绑定，彻底避免训练阶段的噪声模拟。

Result: 实验表明，SimuFreeMark在多种常规和语义攻击下均优于现有最先进方法，同时保持出色的视觉质量。

Conclusion: SimuFreeMark通过利用低频成分的稳定性并消除噪声模拟需求，实现了更优的鲁棒性和泛化能力，为AIGC时代下的图像水印提供了新范式。

Abstract: The advancement of artificial intelligence generated content (AIGC) has created a pressing need for robust image watermarking that can withstand both conventional signal processing and novel semantic editing attacks. Current deep learning-based methods rely on training with hand-crafted noise simulation layers, which inherently limit their generalization to unforeseen distortions. In this work, we propose $\textbf{SimuFreeMark}$, a noise-$\underline{\text{simu}}$lation-$\underline{\text{free}}$ water$\underline{\text{mark}}$ing framework that circumvents this limitation by exploiting the inherent stability of image low-frequency components. We first systematically establish that low-frequency components exhibit significant robustness against a wide range of attacks. Building on this foundation, SimuFreeMark embeds watermarks directly into the deep feature space of the low-frequency components, leveraging a pre-trained variational autoencoder (VAE) to bind the watermark with structurally stable image representations. This design completely eliminates the need for noise simulation during training. Extensive experiments demonstrate that SimuFreeMark outperforms state-of-the-art methods across a wide range of conventional and semantic attacks, while maintaining superior visual quality.

</details>


### [63] [AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models](https://arxiv.org/abs/2511.11299)
*Haokun Chen,Jianing Li,Yao Zhang,Jinhe Bi,Yan Xia,Jindong Gu,Volker Tresp*

Main category: cs.CV

TL;DR: 本文提出AUVIC，一种针对多模态大语言模型（MLLMs）的视觉概念机器遗忘框架，通过引入对抗性扰动实现对目标视觉概念的精确遗忘，同时避免对相关概念的意外影响。为评估该方法，作者构建了首个专注于群组上下文下视觉概念遗忘的基准测试VCUBench。实验结果表明，AUVIC在实现顶尖遗忘率的同时，对非目标概念的性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在大规模数据集上优化时，常涉及敏感或受版权保护的内容，引发数据隐私问题。监管要求‘被遗忘的权利’推动了机器遗忘技术的发展，但目前视觉概念遗忘在多模态模型中仍研究不足，尤其缺乏对目标概念精准移除且不干扰相关概念的方法。

Method: 提出AUVIC框架，利用对抗性扰动实现视觉概念的精确遗忘，通过扰动机制隔离目标概念，防止其对相似概念造成误伤。

Result: AUVIC在目标概念遗忘率上达到当前最优水平，同时对非目标概念的性能影响极小；构建的VCUBench基准首次支持在群组上下文中评估视觉概念遗忘效果。

Conclusion: AUVIC为多模态大语言模型中的视觉概念遗忘提供了高效、精准的解决方案，验证了对抗性扰动在实现隐私保护与模型性能平衡方面的有效性，为后续研究提供了新的基准与方向。

Abstract: Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.

</details>


### [64] [6D Strawberry Pose Estimation: Real-time and Edge AI Solutions Using Purely Synthetic Training Data](https://arxiv.org/abs/2511.11307)
*Saptarshi Neil Sinha,Julius Kühn,Mika Silvan Goschke,Michael Weinmann*

Main category: cs.CV

TL;DR: 本研究提出一种基于纯合成数据的草莓6D姿态估计方法，利用程序化Blender管道生成高保真合成数据，并结合YOLOX-6D-Pose算法实现高效、准确的姿态预测。实验表明，该方法在NVIDIA RTX 3090和Jetson Orin Nano上均表现良好，尤其在资源受限的农业机器人场景中，Jetson Orin Nano展现出优异的部署潜力。模型对成熟及部分成熟的草莓识别效果佳，但对未成熟草莓检测能力较弱，未来可通过颜色变化等策略优化。该方法可扩展至苹果、桃子、李子等其他水果，具有广泛的应用前景。


<details>
  <summary>Details</summary>
Motivation: 解决农业自动化中果实采摘面临的高成本与季节性劳动力短缺问题，同时应对真实标注数据稀缺的挑战，推动基于合成数据的6D姿态估计技术发展。

Method: 采用程序化Blender管道生成多样化的高保真合成草莓数据，结合YOLOX-6D-Pose单阶段算法进行6D姿态估计，支持边缘设备推理，提升模型在真实场景中的泛化能力。

Result: 模型在RTX 3090和Jetson Orin Nano上均取得良好性能，ADD-S指标表现接近，但前者处理速度更快；后者更适合资源受限环境部署。定性分析显示模型能有效识别成熟与半成熟草莓，对未成熟草莓检测仍存挑战。

Conclusion: 本研究验证了基于合成数据的6D姿态估计在草莓采摘中的可行性与有效性，具备良好的实时性与可扩展性，未来可通过改进颜色建模提升对未成熟果实的检测能力，并推广至多种水果，助力农业机器人智能化发展。

Abstract: Automated and selective harvesting of fruits has become an important area of research, particularly due to challenges such as high costs and a shortage of seasonal labor in advanced economies. This paper focuses on 6D pose estimation of strawberries using purely synthetic data generated through a procedural pipeline for photorealistic rendering. We employ the YOLOX-6D-Pose algorithm, a single-shot approach that leverages the YOLOX backbone, known for its balance between speed and accuracy, and its support for edge inference. To address the lacking availability of training data, we introduce a robust and flexible pipeline for generating synthetic strawberry data from various 3D models via a procedural Blender pipeline, where we focus on enhancing the realism of the synthesized data in comparison to previous work to make it a valuable resource for training pose estimation algorithms. Quantitative evaluations indicate that our models achieve comparable accuracy on both the NVIDIA RTX 3090 and Jetson Orin Nano across several ADD-S metrics, with the RTX 3090 demonstrating superior processing speed. However, the Jetson Orin Nano is particularly suited for resource-constrained environments, making it an excellent choice for deployment in agricultural robotics. Qualitative assessments further confirm the model's performance, demonstrating its capability to accurately infer the poses of ripe and partially ripe strawberries, while facing challenges in detecting unripe specimens. This suggests opportunities for future improvements, especially in enhancing detection capabilities for unripe strawberries (if desired) by exploring variations in color. Furthermore, the methodology presented could be adapted easily for other fruits such as apples, peaches, and plums, thereby expanding its applicability and impact in the field of agricultural automation.

</details>


### [65] [DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding](https://arxiv.org/abs/2511.11313)
*Tanveer Hannan,Dimitrios Mallios,Parth Pathak,Faegheh Sardari,Thomas Seidl,Gedas Bertasius,Mohsen Fayyaz,Sunando Sengupta*

Main category: cs.CV

TL;DR: DocSLM is a lightweight vision-language model for long-document understanding on edge devices, using a Hierarchical Multimodal Compressor to reduce memory usage and a Streaming Abstention mechanism for efficient, scalable processing. It outperforms state-of-the-art models with 82% fewer visual tokens, 75% fewer parameters, and 71% lower latency.


<details>
  <summary>Details</summary>
Motivation: Existing large vision-language models have high memory requirements, making them unsuitable for deployment on resource-constrained edge devices. There is a need for efficient models that can handle long multimodal documents without sacrificing performance.

Method: DocSLM employs a Hierarchical Multimodal Compressor to encode visual, textual, and layout information into a fixed-length sequence, reducing memory consumption. It also introduces a Streaming Abstention mechanism that processes document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator.

Result: DocSLM achieves competitive or superior performance across multiple long multimodal document benchmarks while significantly reducing visual tokens (82% less), parameters (75% less), and latency (71% lower) compared to state-of-the-art models.

Conclusion: DocSLM enables reliable and efficient multimodal document understanding on lightweight edge devices, demonstrating that high performance and low resource usage can be achieved simultaneously through innovative compression and streaming mechanisms.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.

</details>


### [66] [Free3D: 3D Human Motion Emerges from Single-View 2D Supervision](https://arxiv.org/abs/2511.11368)
*Sheng Liu,Yuanzhi Liang,Sidan Du*

Main category: cs.CV

TL;DR: Free3D提出一种无需3D运动标注的3D人体动作生成框架，通过2D运动数据训练，利用运动提升残差量化变分自编码器（ML-RQ）和一系列无3D监督的正则化项，实现视图一致性、方向一致性和物理合理性，生成多样化、时间连贯且语义对齐的3D动作，性能媲美甚至超越全3D监督模型。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体动作生成模型依赖精确的3D监督，导致泛化能力受限，易拟合固定坐标模式而非学习本质的3D结构与运动语义。为解决此问题，需减少对显式3D标注的依赖，以促进更强的结构推理与泛化能力。

Method: 提出Free3D框架，包含运动提升残差量化变分自编码器（ML-RQ），将2D动作序列映射到3D一致的潜在空间，并引入视图一致性、方向一致性及物理合理性等无3D监督的正则化目标，在纯2D数据上训练生成高质量3D动作。

Result: Free3D在无任何3D标注的情况下，生成的动作在多样性、时间连贯性与语义对齐方面表现优异，性能可媲美或超过完全依赖3D监督的模型，验证了放松显式3D监督有助于提升模型结构理解与泛化能力。

Conclusion: 通过移除显式3D监督，Free3D促使模型学习更本质的3D结构与运动语义，实现了高效、可扩展的3D动作生成新范式，为少标注或无标注场景下的动作生成提供了有效解决方案。

Abstract: Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.

</details>


### [67] [Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis](https://arxiv.org/abs/2511.11406)
*Feng-Qi Cui,Jinyang Huang,Ziyu Jia,Xinyu Li,Xin Yan,Xiaokang Zhou,Meng Wang*

Main category: cs.CV

TL;DR: 提出LSEF框架，基于低秩稀疏原理，通过三个模块（SEM、DDM、CIM）分离情绪基底与瞬时波动，结合RAO优化策略，在多数据集上显著提升模型鲁棒性和动态区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频情感计算模型因复杂情绪动态导致不稳定和表征退化，核心问题是缺乏层次化结构机制来分离长期情绪基调与短期情绪波动。

Method: 提出低秩稀疏情绪理解框架（LSEF），包含稳定性编码模块（SEM）、动态解耦模块（DDM）和一致性融合模块（CIM），并采用感知秩优化（RAO）策略自适应平衡梯度平滑性与敏感性。

Result: 在多个数据集上的实验表明，LSEF显著提升了模型的鲁棒性和对情绪动态的区分能力，验证了层次化低秩稀疏建模在理解情绪动态中的有效性与通用性。

Conclusion: LSEF通过理论重构情绪动态为分层低秩稀疏组合过程，有效解决了情绪计算中的稳定性与表征退化问题，为情感计算提供了新的建模范式。

Abstract: Video-based Affective Computing (VAC), vital for emotion analysis and human-computer interaction, suffers from model instability and representational degradation due to complex emotional dynamics. Since the meaning of different emotional fluctuations may differ under different emotional contexts, the core limitation is the lack of a hierarchical structural mechanism to disentangle distinct affective components, i.e., emotional bases (the long-term emotional tone), and transient fluctuations (the short-term emotional fluctuations). To address this, we propose the Low-Rank Sparse Emotion Understanding Framework (LSEF), a unified model grounded in the Low-Rank Sparse Principle, which theoretically reframes affective dynamics as a hierarchical low-rank sparse compositional process. LSEF employs three plug-and-play modules, i.e., the Stability Encoding Module (SEM) captures low-rank emotional bases; the Dynamic Decoupling Module (DDM) isolates sparse transient signals; and the Consistency Integration Module (CIM) reconstructs multi-scale stability and reactivity coherence. This framework is optimized by a Rank Aware Optimization (RAO) strategy that adaptively balances gradient smoothness and sensitivity. Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination, which further validates the effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics.

</details>


### [68] [Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://arxiv.org/abs/2511.11410)
*Jiaxi Huang,Dongxu Wu,Hanwei Zhu,Lingyu Zhu,Jun Xing,Xu Wang,Baoliang Chen*

Main category: cs.CV

TL;DR: 提出Q-Doc三层次评估框架，系统评测多模态大模型在文档图像质量评估（DIQA）中的能力。在粗粒度层面评估模型打分与标注的相关性；中粒度层面设计单选与多选失真类型识别任务；细粒度层面进行失真严重程度分类。结果表明，尽管模型具备初步的DIQA能力，但存在评分不一致、失真误判和严重程度误判等问题。采用思维链（CoT）提示显著提升各层级表现。研究提供基准测试与公开代码，揭示了当前MLLMs在质量感知上的显著不足及改进路径。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在高阶视觉任务上进展迅速，但在文档图像质量评估（DIQA）方面尚未得到充分探索。为填补这一空白，亟需建立系统性的评估框架以准确衡量和提升模型在该任务上的表现。

Method: 提出Q-Doc三层次评估框架：1）粗粒度：通过指令让模型对文档图像打分，并分析其与人工标注质量分数的相关性；2）中粒度：设计单选与多选失真类型识别任务，涵盖多种失真场景；3）细粒度：引入失真严重程度评估任务，要求模型根据人类标注参考进行强度分类。同时对比使用Chain-of-Thought（CoT）提示的效果。

Result: 实验显示，当前多模态大模型在DIQA任务中仅具备初步能力，存在评分不一致、失真类型识别错误、严重程度判断偏差等关键缺陷。引入Chain-of-Thought（CoT）提示后，模型在所有评估层级上的表现均有显著提升，验证了其在增强推理能力方面的有效性。

Conclusion: Q-Doc框架有效揭示了多模态大模型在文档图像质量评估中的认知局限，表明其在质量感知方面仍有较大提升空间。通过系统化评估与CoT提示策略，可为后续模型优化提供重要方向。相关基准与代码已开源，推动该领域研究发展。

Abstract: The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:
  https://github.com/cydxf/Q-Doc.

</details>


### [69] [BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.11421)
*Lan Li,Tao Hu,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: BOFA提出了一种新的类增量学习框架，通过在CLIP的跨模态桥接层中进行自适应，避免了额外参数和推理开销。利用正交低秩融合机制，确保模型更新局限于与历史任务特征正交的安全子空间，防止遗忘。同时结合文本和视觉原型，提升分类性能。实验表明，BOFA在准确率和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将CLIP应用于类增量学习时面临两个问题：一是下游任务适应需引入额外可学习模块，增加复杂度并易导致遗忘；二是多模态信息未被充分整合。因此需要一种无需新增参数、能有效融合多模态信息且防止遗忘的方法。

Method: 提出BOFA框架，将所有模型适配限制在CLIP的跨模态桥接层内，不引入额外参数。采用正交低秩融合机制，约束参数更新至与过去任务特征正交的低秩安全子空间，以稳定积累知识。同时设计跨模态混合原型，结合稳定的文本原型与由稳定适配桥接层生成的视觉原型，增强分类能力。

Result: 在多个标准基准测试上，BOFA表现出更高的分类准确率和更好的计算效率，显著优于现有类增量学习方法。

Conclusion: BOFA通过在不增加参数的前提下实现高效、稳定的类增量学习，充分利用了多模态信息的互补性，为基于CLIP的持续学习提供了有效解决方案。

Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

</details>


### [70] [Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment](https://arxiv.org/abs/2511.11422)
*Lukun Wu,Jie Li,Ziqi Ren,Kaifan Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出了一种自适应教学范式，通过让视觉模态（教师）动态调整其知识结构以匹配脑电（学生）的表达能力，解决视觉与脑信号之间的不对称性问题。该方法引入ShrinkAdapter模块，采用无残差设计和瓶颈结构，在零样本脑到图像检索任务中达到60.2%的top-1准确率，优于现有方法9.8个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了视觉与脑信号之间的根本不对称性，包括保真度差距（EEG噪声大、信号退化）和语义差距（EEG概念表征浅，视觉语义丰富），导致对齐效果不佳且泛化能力差。

Method: 提出自适应教学范式，使视觉模态作为‘教师’动态缩小并调整其知识结构，以适配EEG模态的表达能力；实现上采用ShrinkAdapter模块，具有残差缺失和瓶颈结构的简洁设计。

Result: 在零样本脑到图像检索任务中，该方法取得60.2%的top-1准确率，相比之前最先进方法提升9.8个百分点，验证了范式的有效性和合理性。

Conclusion: 本工作揭示了视觉与脑信号间不对称对齐的重要性，主张‘教师’应主动收缩与适配以弥合差距，为跨模态脑信号解码提供了新视角。

Abstract: Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach. We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs. vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs. vision's rich semantic depth). Previous methods often overlook this asymmetry, forcing alignment between the two modalities as if they were equal partners and thereby leading to poor generalization. To address this, we propose the adaptive teaching paradigm. This paradigm empowers the ``teacher" modality (vision) to dynamically shrink and adjust its knowledge structure under task guidance, tailoring its semantically dense features to match the ``student" modality (EEG)'s capacity. We implement this paradigm with the ShrinkAdapter, a simple yet effective module featuring a residual-free design and a bottleneck structure. Through extensive experiments, we validate the underlying rationale and effectiveness of our paradigm. Our method achieves a top-1 accuracy of 60.2\% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by a margin of 9.8\%. Our work introduces a new perspective for asymmetric alignment: the teacher must shrink and adapt to bridge the vision-brain gap.

</details>


### [71] [VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation](https://arxiv.org/abs/2511.11450)
*Maximilian Rokuss,Moritz Langenberg,Yannick Kirchhoff,Fabian Isensee,Benjamin Hamm,Constantin Ulrich,Sebastian Regnery,Lukas Bauer,Efthimios Katsigiannopulos,Tobias Norajitra,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: VoxTell 是一个用于文本提示的体积医学图像分割的视觉-语言模型，能够将自由形式的描述（从单个词到完整临床句子）映射到3D掩码。它在超过62,000个CT、MRI和PET体积数据上训练，涵盖1000多个解剖和病理类别，通过多阶段跨解码器层的视觉-语言融合，在多尺度上对齐文本与视觉特征。该模型在跨模态的零样本任务中表现优异，对熟悉概念表现良好，并能泛化到未见类别。实验还证明其具备强大的跨模态迁移能力、对语言变化和临床语言的鲁棒性，以及从真实世界文本中实现精确的实例级分割。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法通常依赖于标注数据或预定义模板，难以应对自由形式的自然语言描述。为了实现更灵活、通用的医学图像理解，需要一种能直接根据文本提示生成3D分割掩码的模型，从而支持临床医生以自然语言方式交互式地指定感兴趣区域。

Method: VoxTell采用多阶段视觉-语言融合机制，通过在解码器各层中逐步融合文本与体积图像特征，实现跨模态语义对齐。使用大规模多模态医学数据集进行训练，结合对比学习与自监督策略，提升模型对复杂描述的理解能力。此外，引入注意力机制以聚焦关键空间区域和语言关键词。

Result: VoxTell 在多个公开医学图像数据集上实现了当前最优的零样本分割性能，尤其在未见过的数据集和类别上表现出显著泛化能力。在跨模态任务中也展现出强大迁移能力，对不同表达方式和临床术语具有高度鲁棒性，且可准确生成个体化实例分割结果。

Conclusion: VoxTell 为医学图像分析提供了一种强大的文本驱动分割框架，具备良好的泛化性、鲁棒性和实用性，为未来智能医疗系统的自然语言交互奠定了基础。

Abstract: We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell

</details>


### [72] [Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs](https://arxiv.org/abs/2511.11427)
*Francisco Nogueira,Alexandre Bernardino,Bruno Martins*

Main category: cs.CV

TL;DR: 本文提出了一种多语言指代表达理解（REC）方法，构建了涵盖10种语言的统一多语言数据集，通过机器翻译和上下文增强翻译扩展了12个现有英语REC基准。该数据集包含约800万条多语言指代表达，覆盖177,620张图像和336,882个标注对象。同时，提出一种基于注意力锚点的神经架构，利用多语言SigLIP2编码器，通过注意力分布生成粗略空间锚点，并通过学习残差进行细化。实验表明，在标准基准上表现良好，如在RefCOCO多语言评估中达到86.9%的IoU@50准确率，接近英语模型的91.3%。多语言评估显示各语言间性能稳定，验证了多语言视觉定位系统的可行性。数据集与模型已公开。


<details>
  <summary>Details</summary>
Motivation: 当前指代表达理解研究主要集中在英语，但全球部署需求推动多语言支持，因此需要构建多语言数据集并开发跨语言视觉定位模型。

Method: 通过机器翻译和上下文增强翻译系统性扩展12个英语REC基准，构建多语言数据集；采用基于注意力锚点的神经架构，结合多语言SigLIP2编码器，利用注意力分布生成粗略空间锚点，并通过学习残差进行精细化调整。

Result: 在多语言基准测试中，模型在RefCOCO聚合评估中达到86.9%的IoU@50准确率，接近英语模型的91.3%；多语言评估显示各语言间性能一致，验证了多语言视觉定位系统的实用性。

Conclusion: 本研究成功构建了一个大规模多语言指代表达理解数据集，并提出一个有效的多语言视觉定位模型，展示了跨语言视觉接地系统的可行性，为全球化应用提供了基础。

Abstract: Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.

</details>


### [73] [WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation](https://arxiv.org/abs/2511.11434)
*Wei Chow,Jiachun Pan,Yongyuan Liang,Mingze Zhou,Xue Song,Liyu Jia,Saining Zhang,Siliang Tang,Juncheng Li,Fengda Zhang,Weijia Wu,Hanwang Zhang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: WEAVE是首个专注于上下文交织跨模态理解与生成的综合性套件，包含10万条交错样本的WEAVE-100k数据集和基于480张图像的人工标注基准WEAVEBench。该套件旨在解决现有模型在多轮交互、上下文依赖场景下的不足，涵盖理解、编辑与生成任务，支持视觉记忆与世界知识推理评估。实验表明，使用WEAVE-100k训练可提升模型的多轮生成与跨模态协作能力，并催生新兴的视觉记忆能力；但WEAVEBench也揭示了当前方法在复杂上下文任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型（UMMs）的数据集和评估基准主要聚焦于单轮交互，无法充分反映真实世界中图像生成与编辑所需的多轮、上下文依赖特性，亟需一个能支持复杂对话情境的新型数据与评测体系。

Method: 提出WEAVE套件，包括WEAVE-100k大规模数据集（含370,000轮对话与500,000张图像）和WEAVEBench人类标注基准，采用结合参考图像与原始图像+编辑指令的混合视觉语言模型（VLM）评判框架，用于评估多轮生成、视觉记忆及跨领域推理能力。

Result: 在WEAVE-100k上训练显著提升了模型在视觉理解、图像编辑及跨模态协作方面的能力，尤其促进了视觉记忆的涌现；但在WEAVEBench上的广泛评估仍暴露出现有方法在多轮、上下文感知图像生成与编辑中的关键瓶颈。

Conclusion: WEAVE为多模态研究社区提供了研究上下文交织理解与生成的新视角与坚实基础，推动迈向更智能、更具上下文感知能力的多模态系统。

Abstract: Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.

</details>


### [74] [The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models](https://arxiv.org/abs/2511.11435)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 本文研究文本到图像扩散模型中泛化与记忆之间的模糊性，聚焦于多模态象征性这一特定现象，即图像和文本唤起文化共享联想的情况。作者提出一个评估框架，区分模型对文化参考的‘识别’与‘实现’，并量化两者。通过评估5个扩散模型在767个来自Wikidata的文化参考上的表现，发现该框架能更有效地区分复制与重构。此外，通过提示扰动实验发现，即使文本线索改变，模型仍常复现标志性视觉结构。研究还表明，文化对齐不仅与训练数据频率相关，还受文本独特性、参考流行度和创建日期影响。结论是，扩散模型的价值不仅在于再现，更在于对文化知识的转化与再语境化，推动评估从简单的文本-图像匹配迈向更丰富的上下文理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注遗忘与记忆，但忽略了模型记住什么以及如何记住，特别是在涉及文化共享联想（如经典艺术作品或电影场景）时。本文旨在揭示扩散模型在处理多模态象征性时的记忆机制，突破传统以相似性为核心的评估局限，探索其在文化知识再创造中的潜力。

Method: 提出一个双维度评估框架，分别衡量模型对文化参考的‘识别’（是否能辨认出引用）与‘实现’（如何呈现，是复制还是重构）。使用767个来自Wikidata的文化参考（涵盖静态与动态图像）进行大规模评估，结合相似性指标与人工判断验证。进行提示扰动实验，包括同义词替换和字面图像描述，以测试模型对语言变化的敏感性。通过统计分析探讨文化对齐与训练频率、文本独特性、参考流行度及创建日期的关系。

Result: 所提出的框架能更准确地区分模型的复制行为与创造性重构；模型在文本提示变化下仍保持对标志性视觉结构的再现能力，显示其对文化符号的高度固化；文化对齐不仅依赖训练数据频率，还受文本独特性、参考流行度和创建时间的影响，表明模型学习具有复杂的社会文化感知能力。

Conclusion: 扩散模型的价值不仅体现在对文化内容的忠实再现，更在于其对文化知识的重新诠释与再语境化能力。本研究推动了文本-图像生成模型的评估范式从简单匹配转向对深层文化理解与创造性表达的考察，为未来模型设计与评估提供了新方向。

Abstract: Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.

</details>


### [75] [Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping](https://arxiv.org/abs/2511.11437)
*Guowei Zhang,Yun Zhao,Moein Khajehnejad,Adeel Razi,Levin Kuhlmann*

Main category: cs.CV

TL;DR: Hi-DREAM is a brain-inspired conditional diffusion framework that explicitly models the cortical hierarchy in fMRI-to-image decoding. It uses an ROI adapter to organize fMRI signals into early/mid/late visual streams, forming a multi-scale cortical pyramid aligned with U-Net depth. A depth-matched ControlNet injects scale-specific hints during denoising, enabling interpretable and efficient image reconstruction. Experiments on NSD show state-of-the-art performance in high-level semantics while preserving low-level fidelity, demonstrating that cortical organization enhances both performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based decoders condition directly on fMRI features without considering the hierarchical organization of visual processing across the cortex, leading to a loss of functional specificity and interpretability in early, middle, and late visual areas.

Method: Hi-DREAM introduces a region-of-interest (ROI) adapter to group fMRI signals into early, mid, and late visual streams, constructing a multi-scale cortical pyramid aligned with U-Net depth. A lightweight, depth-matched ControlNet injects these scale-specific features during the denoising process to guide image generation in a brain-like manner.

Result: Hi-DREAM achieves state-of-the-art performance on high-level semantic metrics on the Natural Scenes Dataset (NSD), while maintaining competitive low-level image fidelity. The model enables clear interpretation of functional contributions from different visual areas.

Conclusion: Structuring conditioning by cortical hierarchy significantly improves both the performance and interpretability of fMRI-to-image decoders, offering a powerful alternative to data-driven embeddings and providing a valuable framework for studying visual cognition.

Abstract: Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.

</details>


### [76] [VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models](https://arxiv.org/abs/2511.11438)
*Mingjie Xu,Jinpeng Chen,Yuzhi Zhao,Jason Chun Lok Li,Yue Qiu,Zekang Du,Mengyang Wu,Pingping Zhang,Kun Li,Hongzheng Yang,Wenao Ma,Jiaheng Wei,Qinbin Li,Kangcheng Liu,Wenqiang Lei*

Main category: cs.CV

TL;DR: 提出VP-Bench基准，评估多模态大模型对视觉提示（VPs）的感知与利用能力，涵盖3万张带八种形状和355种属性组合的视觉提示，分两阶段测试模型在自然场景中的感知能力及对下游任务的影响。评估28个模型，分析属性变化、问题顺序和模型规模等因素对理解VP的影响，为研究模型解决基于视觉参考的问题提供新框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准未系统评估多模态大模型对人类自然使用的视觉提示（如边界框）的理解能力，导致无法判断模型是否能有效识别并利用此类直观提示进行问题求解。

Method: 构建两阶段评估框架：第一阶段使用3万张包含多种形状和属性组合的视觉提示测试模型对VP的感知能力；第二阶段评估VP在真实问题解决场景中的作用，分析不同因素对性能的影响。

Result: 通过评估28个多模态大模型（包括GPT-4o、InternVL3、Qwen2.5-VL等），发现模型规模、提示属性、问题排列方式显著影响对视觉提示的理解效果；VP-Bench为研究模型处理基于视觉参考的问题提供了标准化评估框架。

Conclusion: VP-Bench填补了多模态大模型在视觉提示理解评估方面的空白，揭示了当前模型在处理自然视觉提示时的能力边界，并为未来模型设计与优化提供了可量化的参考标准。

Abstract: Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use "visual prompts" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.

</details>


### [77] [Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification](https://arxiv.org/abs/2511.11460)
*Qinghao Gao,Jianhai Qu,Yunsong Li,Weiqiang Dong*

Main category: cs.CV

TL;DR: 提出MaMOL框架，通过双路由机制应对遥感中多模态缺失问题，实现高效参数适应与跨域泛化，显著提升在不完整数据下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练时假设数据完整，且计算成本高，难以应对真实世界中的模态缺失问题。

Method: 将模态缺失建模为多任务学习问题，设计任务导向动态路由和模态特定-共享静态路由，结合轻量级专家更新与共享，实现参数高效适配。

Result: 在多个遥感基准上表现更优，具备强鲁棒性与泛化能力，计算开销小；自然图像数据集上的迁移实验验证其跨域适用性。

Conclusion: MaMOL是一种通用、高效的不完整多模态学习解决方案，适用于多种实际场景。

Abstract: Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.

</details>


### [78] [Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images](https://arxiv.org/abs/2511.11486)
*Roman Kinakh,Gonzalo R. Ríos-Muñoz,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: 本文提出nnUNet-B，一种基于贝叶斯分割框架的PD-L1表达预测方法，通过多模态后验采样（MPS）从H&E染色组织图像中直接推断PD-L1表达。该方法在肺鳞状细胞癌数据集上达到0.805的平均Dice分数和0.709的平均IoU，同时生成像素级不确定性图，其与分割误差高度相关，但校准仍不完善。结果表明，该方法为临床可扩展、可解释的生物标志物评估提供了有前景的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于免疫组化（IHC）的PD-L1表达评估方法资源消耗大，亟需更高效、可扩展的替代方案。利用H&E染色图像实现PD-L1表达预测，可减少对额外染色的需求，提升临床实用性。

Method: 基于nnUNet-v2构建贝叶斯分割框架，采用循环训练中的多模型检查点采样以近似后验分布，结合多模态后验采样（MPS）实现精确分割与认知不确定性估计（通过熵和标准差）。

Result: 在肺鳞状细胞癌数据集上，nnUNet-B取得0.805的平均Dice分数和0.709的平均IoU，表现出与现有基准相当的性能；同时生成的像素级不确定性图与分割误差显著相关，尽管校准仍有不足。

Conclusion: 不确定性感知的H&E图像基PD-L1预测方法具有良好的应用前景，可推动临床工作中生物标志物评估的可扩展性与可解释性。

Abstract: Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.

</details>


### [79] [PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models](https://arxiv.org/abs/2511.11502)
*Nhat Hoang-Xuan,Minh Vu,My T. Thai,Manish Bhattarai*

Main category: cs.CV

TL;DR: 本文揭示了大型视觉-语言模型（LVLMs）在产生对象幻觉时，往往忽略图像信息，转而依赖先前生成的输出（prelim）token进行推理。为此，作者提出了一种名为Prelim Attention Score（PAS）的轻量级、无需训练的信号，通过分析prelim token的注意力权重来检测幻觉，该方法无需额外前向传播，可实时计算。PAS在多个模型和数据集上实现了最先进的幻觉检测性能，支持实时过滤与干预。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型虽然强大，但在生成过程中常出现对象幻觉，严重影响可靠性。现有方法未能有效识别幻觉的根本原因，尤其缺乏对模型如何依赖先前生成内容而非图像信息的深入理解。因此，亟需一种能够实时检测并干预幻觉行为的方法。

Method: 通过计算图像与预测对象在给定prelim条件下的互信息，量化模型对图像的依赖程度；基于此发现，提出Prelim Attention Score（PAS），利用attention权重中prelim token的分布情况作为判别信号，实现无需训练、无额外计算开销的幻觉检测。

Result: PAS在多个主流视觉-语言模型和数据集上均表现出卓越的幻觉检测能力，优于现有方法；其计算高效，支持实时应用，在不改变模型结构的前提下显著提升输出可靠性。

Conclusion: 本研究揭示了对象幻觉背后的关键机制——模型过度依赖先前生成内容，而非真实图像信息。提出的PAS是一种高效、通用且可部署的解决方案，为提升大型视觉-语言模型的鲁棒性提供了新思路。

Abstract: Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.

</details>


### [80] [Bridging Hidden States in Vision-Language Models](https://arxiv.org/abs/2511.11526)
*Benjamin Fein-Ashley,Jacob Fein-Ashley*

Main category: cs.CV

TL;DR: BRIDGE提出了一种轻量级的跨模态融合模块，通过在视觉和文本编码器顶部添加少量仅跨模态的双向注意力层，直接对齐两者的隐藏状态，实现更自然的多模态理解。该方法保持编码器非因果性以增强理解能力，并将生成任务与融合过程解耦，兼顾效率与性能。在图像检索、VQA和视觉推理等基准上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型通常采用早期或晚期融合方式，且常将融合与自回归解码器绑定。然而，视觉和文本编码器的隐藏状态已包含丰富的模态特异性结构（如空间布局和语法语义），因此直接对齐这些状态是更自然的匹配方式。

Method: 在视觉和文本编码器顶部引入少量仅跨模态的双向注意力层，将两者的隐藏状态投影到共享空间，进行跨模态注意力计算，并通过门控残差更新回原模态，同时使用简单稳定器提升对齐效果。编码器保持非因果性，生成任务可选解耦。

Result: 在标准的图像检索、视觉问答和视觉推理任务中，BRIDGE均优于同类视觉-语言模型，同时保持对比学习模型的双编码器高效性。

Conclusion: BRIDGE通过直接对齐多模态隐藏状态，实现了高效且强大的跨模态理解，在性能与效率之间取得了良好平衡。代码已开源。

Abstract: Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities "think". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [81] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的多轮交互式仿真推演数据分析与性能评估方法，通过任务分解、系统提示设计、自检与反思机制以及自定义工具调用，实现结构化数据提取与多步分析，支持多种报告模板以适应不同应用场景。实验表明，该方法生成的报告质量显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统手动分析方式耗时且易出错，难以满足现代战争中对策略与战术评估的高效性与准确性需求。大语言模型虽具备强大分析能力，但单一指令输入无法生成高质量、结构化的分析报告，因此亟需一种系统化方法提升报告生成质量。

Method: 将复杂任务分解为多个子任务，针对每个子任务设计系统提示和用户提示；采用多轮交互，结合自检与反思机制，实现结构化数据提取与多步分析；引入自定义工具生成图表并计算指标；设计多种报告模板以适配不同应用与输入数据类型。

Result: 在多项评估中，所生成报告在结构化程度、内容完整性与逻辑连贯性等方面均显著优于基线方法，获得更高评分，验证了方法的有效性与泛化能力。

Conclusion: 本方法有效提升了仿真推演数据分析报告的质量与自动化水平，为军事决策支持提供了可靠的技术路径。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [82] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 本文提出一种新架构，通过离线数据增强和高效的并行检索，解决大语言模型在历史人物对话系统中深度与延迟的矛盾。将传记数据转化为带情感-语义元数据的1,774条第一人称记忆，实现0.52秒的快速提示生成。评估显示该方法在小模型（如GPT-3.5、GPT-3）上显著优于传统RAG，在大模型（GPT-4）上达到相当表现，适用于资源受限场景。此外，结构化记忆支持时空热图、情绪轨迹分析等可视化工具，兼具对话与研究功能，以梵高为案例验证，具备广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有历史人物对话系统在响应深度与生成速度之间存在权衡：简单检索增强生成导致回应浅显，多阶段反思虽深入但延迟过高。亟需一种既能保证内容深度又具高效性的解决方案，尤其适用于教育、博物馆及研究场景中的资源受限部署。

Method: 将传记资料离线转化为包含情感与语义信息的第一人称记忆；构建结构化情景记忆库；采用两阶段并行检索机制加速响应生成；结合LLM-as-judge与RAGAs评估框架进行量化验证，并开发可视化工具支持分析。

Result: 系统在GPT-4上达到与传统RAG相当的表现，在GPT-3.5和GPT-3上显著超越传统RAG；提示生成时间仅为0.52秒；支持多种可视化分析功能，如时空热图、情绪轨迹、路径追踪，具备实用性和扩展性。

Conclusion: 所提架构有效平衡了对话深度与响应效率，不仅提升历史人物对话系统的实用性，还为教育、博物馆和学术研究提供了可扩展的技术框架，适用于任何拥有丰富文本记录的历史人物建模。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [83] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: 本文提出了首个用于自然语言生成的混合量子-经典大语言模型HyQuT，将变分量子电路（VQCs）集成到Transformer框架中，分别在8M和150M参数规模下实现连贯且上下文感知的对话。实验表明，仅需10个量子比特和80个量子门即可替代150M参数模型中约10%的经典参数，同时保持相当的收敛稳定性和生成质量，首次展示了量子计算在大规模生成式语言模型中的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有量子或混合模型多局限于简单任务，尚未成功应用于大规模自然语言生成，亟需探索量子计算在大模型中的潜力与整合路径。

Method: 将变分量子电路（VQCs）嵌入Transformer架构，在8M和150M参数规模下构建混合量子-经典语言模型；通过少量量子硬件替代部分经典参数，实现高效推理与训练。

Result: 使用10个量子比特和80个量子门可替代约10%的经典参数，模型在收敛稳定性与生成质量上表现良好，验证了量子模块在大模型中的有效性与可行性。

Conclusion: 本研究为量子计算融入大规模生成式语言模型提供了早期实证支持，标志着迈向实用化混合量子-经典大模型的重要一步。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [84] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 本文研究了在时间约束下部署大语言模型（LLM）于智能体架构中的可靠性问题，发现现有模型在处理时间约束时存在系统性风险：性能分布呈双峰状（准确率要么95%要么50%），对提示格式极度敏感（准确率波动达30-60个百分点），且失败模型出现100%假阳性。参数量与能力无相关性，微调可提升部分模型性能但无法解决根本问题。研究指出，仅靠自然语言的自回归生成无法可靠学习时间约束满足，需引入持续的时间状态表示、显式的约束检查机制和组合式时序推理能力。当前自回归架构缺乏这些机制，因此在关键时间应用中使用纯神经架构存在不可接受的风险，建议采用融合符号推理的混合架构。


<details>
  <summary>Details</summary>
Motivation: 在实时决策场景中，大语言模型被假设能可靠判断动作窗口是否开放，但该假设未经过验证。实际部署中若模型无法正确处理时间约束，可能导致严重后果，亟需评估其在时间约束任务中的表现与可靠性。

Method: 通过设计deadline检测任务，在8个生产级大模型（2.8-8B参数）上评估其处理时间约束的能力，分析性能分布、提示格式敏感性、行动偏差等特征，并进行小样本微调实验以观察性能改进情况。

Result: 发现模型性能呈现双峰分布（95%或50%准确率）、对提示格式极其脆弱（变动导致30-60个百分点波动）、失败模型存在100%假阳性；参数量与能力无关；微调虽可提升部分模型（+12-37个百分点），但无法从根本上解决问题。

Conclusion: 当前自回归架构无法可靠学习时间约束满足，必须引入持续时间状态表示、显式约束检查和组合式时序推理机制。在时间敏感应用中，仅依赖纯神经架构存在重大风险，应采用融合符号推理的混合架构。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [85] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 该研究扩展了谱神经符号推理（Spectral NSR）框架，引入三种语义上合理的增强：(1) 使用上下文嵌入（如Sentence-BERT、SimCSE）的Transformer-based节点合并以减少冗余；(2) 使用预训练的自然语言推论（NLI）分类器（如RoBERTa、DeBERTa）进行句级蕴含验证以提升边的质量；(3) 与外部知识图谱（如ConceptNet、Wikidata）对齐以补充缺失上下文。这些改进在不改变核心谱推理流程的前提下，显著提升了图的保真度。实验在ProofWriter、EntailmentBank和CLUTRR基准上显示准确率提升最高达+3.8%，泛化能力增强，推理噪声减少。创新点在于将语义与符号精炼完全前置于谱推理阶段，实现高效、可解释且可扩展的推理，无需依赖二次注意力机制。整体上，该工作通过模块化、语义驱动的预处理步骤，增强了推理系统的鲁棒性、可解释性和可扩展性，适用于开放域和真实场景部署。


<details>
  <summary>Details</summary>
Motivation: 现有谱神经符号推理框架在构建知识图时存在冗余、低质量边及上下文缺失问题，影响推理性能与可解释性。为提升图结构质量并保持推理效率，需在推理前引入语义层面的精炼机制，从而增强系统在复杂、开放环境中的可靠性与泛化能力。

Method: 提出三种上游预处理模块：(1) 基于Transformer的节点合并，利用Sentence-BERT等模型计算语义相似度，合并重复或相近节点；(2) 句级蕴含验证，采用预训练的NLI模型判断句子间逻辑关系，筛选高质量边；(3) 外部知识图谱对齐，将图中实体链接至ConceptNet或Wikidata，补充缺失语义信息。所有改进均作用于推理前阶段，不修改核心谱推理引擎。

Result: 在ProofWriter、EntailmentBank和CLUTRR三个基准上均取得显著性能提升，最高准确率提升达+3.8%；系统对对抗样本的鲁棒性增强，推理过程噪声减少；整体推理效率保持高效，且具备良好的可解释性与可扩展性。

Conclusion: 本研究通过模块化、语义驱动的预处理方法，有效提升了谱神经符号推理框架的图质量与推理性能。其核心优势在于不改变原有推理机制的前提下，实现了高效、可解释、可扩展的推理系统，适用于开放域和实际应用场景。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [86] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 提出一种名为PRO（PReference Orchestrator）的新框架，通过轻量级偏好适配器自动推断每个提示的偏好权重，实现训练和部署阶段的动态偏好调整。该方法基于多个奖励模型的归一化奖励分数学习有效偏好平衡，避免了手动设定权重的负担，并提升了训练效率。理论分析表明其优于固定权重方法，实验验证了其在多任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有多目标对齐方法依赖手动指定偏好权重，增加了用户负担且导致训练效率低下，因探索无效的偏好组合。因此需要一种能自动推断提示相关偏好权重的方法以提升对齐效果与效率。

Method: 提出PRO框架，引入轻量级偏好适配器，在训练和部署中根据提示内容自动学习归一化奖励分数对应的偏好权重，从而实现提示感知的动态偏好调整。

Result: 在多个任务上，PRO显著优于现有的多目标对齐方法，表现出更高的对齐效果与训练效率。

Conclusion: PRO通过自动推断提示特定的偏好权重，有效解决了人工设定权重带来的负担与低效问题，实现了更优的多目标对齐性能，具有良好的应用前景。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [87] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文提出一种简单有效的对比学习框架，通过利用专利文档内部的多视图信息来学习专利嵌入。针对SimCSE风格的随机丢弃增强在专利场景中导致嵌入过于均匀、丧失语义连贯性的问题，作者提出基于章节的增强方法，将专利的不同部分（如摘要、权利要求、背景）作为互补视图，引入自然的语义和结构多样性，缓解过分散问题，使嵌入更好地保留全局结构与局部连续性。在大规模基准测试中，该完全自监督方法在现有技术检索和分类任务上达到或超越依赖引用和IPC标签的监督基线，且无需依赖脆弱或不完整的标注数据。分析还表明不同章节对不同任务具有专长：权利要求和摘要有利于检索，而背景部分有助于分类，凸显了专利内在话语结构在表征学习中的价值。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如SimCSE）在专利文本中因随机丢弃增强导致嵌入过于均匀，丧失语义连贯性；同时，依赖外部标注（如引用、IPC分类）的数据存在不完整或脆弱的问题，亟需一种不依赖外部标注、能有效利用文档内部结构的自监督方法。

Method: 提出基于章节的增强策略，将专利的不同部分（如摘要、权利要求、背景）作为互补视图进行对比学习，以引入自然的语义与结构多样性，克服传统丢弃增强带来的过分散问题。

Result: 在大规模专利数据集上，所提方法在现有技术检索和分类任务中表现优于或相当甚至超过依赖引用和IPC标注的监督方法，且为完全自监督，无需外部标注；不同章节对不同任务表现出专长性，验证了文档内部结构的有效性。

Conclusion: 利用专利文档内部的多视图信息（如不同章节）进行对比学习，是一种高效、可扩展且通用的专利表征学习方法，能够有效捕捉文档的语义结构，提升专利理解能力。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [88] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 该研究评估了15个开源权重的大语言模型（LLMs）在病理学和放射学报告中的表现，涵盖六种疾病场景（如结直肠肝转移、神经退行性疾病等），并在荷兰、英国和捷克的三家机构中进行。研究比较了六种提示策略（零样本、单样本、少样本、思维链、自一致性、提示图），发现小型到中型通用模型的表现与大型模型相当，而提示图和少样本提示可提升性能约13%。任务特异性因素（如复杂性和标注变异性）比模型大小或提示策略影响更大。总体表明，开源LLMs可在多语言、多机构环境下有效提取临床报告结构化数据，具备可扩展的数据整理潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在临床文本结构化信息提取中应用广泛，但多数研究局限于单一任务、有限模型及英文报告，缺乏跨语言、跨机构和多任务的系统评估。本文旨在填补这一空白，探索开源模型在多种临床场景下的实际表现与适用性。

Method: 选取15个开源权重的LLM（包括通用和医学专用模型），在六类疾病（如肝肿瘤、黑色素瘤等）的病理和放射学报告上测试，采用六种提示策略（零样本、单样本、少样本、思维链、自一致性、提示图），在荷兰、英国和捷克三家机构收集数据，使用任务相关指标评估性能，并通过共识排名聚合与线性混合效应模型分析变异来源。

Result: 顶级模型在各任务上的宏平均得分接近人工标注者间的一致性水平；小至中型通用模型表现与大型模型相当，而极小或专业模型表现较差；提示图和少样本提示显著提升性能约13%；任务本身的复杂性和标注差异对结果影响大于模型规模或提示方式。

Conclusion: 开源大语言模型可在多种疾病、语言和机构背景下有效提取临床报告中的结构化数据，具备良好的可扩展性，适用于大规模临床数据整理，尤其在资源受限场景下具有重要应用价值。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [89] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 提出一种基于加权专家产品（wPoE）的测试时引导框架，通过自适应结合通用压缩模型与预训练神经语言模型，在不需微调的情况下提升文本压缩性能，且能兼容任意自回归语言模型，有效改善压缩率并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统通用压缩器（如gzip）虽速度快、适用性广，但压缩率较低；而现代神经压缩器虽压缩效果好，却难以泛化到未见数据。因此需要一种方法在不牺牲泛化能力的前提下提升压缩性能。

Method: 提出Test-Time Steering via Weighted Product of Experts（wPoE），在推理阶段自适应融合通用压缩模型与预训练神经语言模型，动态调整权重以获得最优压缩效果。

Result: 实验表明，该方法显著提升了文本压缩性能，优于单一模型，且无需微调，可无缝集成至任何自回归语言模型中，适用于多种数据分布。

Conclusion: 所提框架有效结合了通用压缩器的鲁棒性与神经压缩器的高精度，实现了更优的压缩率和更强的泛化能力，为实际文本压缩应用提供了高效解决方案。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [90] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出一种贝叶斯方法，用于量化基于大语言模型（LLM）的文本生成系统在二元评估指标中的统计不确定性，尤其关注由概率性文本生成策略引发的不确定性。通过两个案例研究——评估对抗输入下的拒绝率和评估一个LLM对另一个的成对偏好——展示了该方法在理解LLM行为可靠性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法常忽略对二元评价指标的统计不确定性量化，而这种不确定性在基于概率生成的LLM中尤为显著。为提升评估的严谨性和可信度，需引入更精确的不确定性建模方法。

Method: 采用贝叶斯框架对二元评估结果进行建模，考虑生成过程的随机性，通过后验分布估计评估指标的置信区间，从而提供对评估结果不确定性的量化分析。

Result: 在两个实际案例中，该方法成功揭示了评估结果的不确定性范围，帮助识别评估的稳健性边界，并为决策提供更可靠的依据。

Conclusion: 贝叶斯方法能够有效量化基于LLM的文本生成系统在二元评估中的不确定性，提升了评估的科学性与可解释性，建议在未来的评估实践中推广使用。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [91] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 本研究对七种前沿大语言模型（包括GPT-4o、GPT-4、Claude 3.5 Sonnet等）在粤语、日语和土耳其语上的表现进行了全面评估，构建了一个涵盖问答、摘要、翻译和文化对话的跨语言基准。通过人工评价与自动指标结合的方式，发现尽管大型专有模型整体领先，但在文化语境理解和形态学泛化方面仍存在明显差距，尤其在处理土耳其语的黏着语特征和粤语口语表达时表现不佳；开源小模型则在流畅性和准确性上显著落后，凸显资源不平等。研究发布基准数据以促进可复现性与后续研究。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在低资源及形态丰富的语言（如粤语、日语、土耳其语）中的表现，揭示其在文化理解与语言结构泛化方面的局限性，推动更具备文化敏感性和语言通用性的模型发展。

Method: 构建跨语言多任务基准，涵盖开放域问答、文档摘要、英译X语言及文化情境对话；结合人工评分（流畅性、事实准确性、文化恰当性）与自动化指标（BLEU、ROUGE）进行综合评估；对七种主流大模型进行测试并分析误差模式。

Result: GPT-4o和GPT-4在多数任务中表现最优，Claude 3.5 Sonnet在知识推理任务中表现良好；所有模型在处理土耳其语的黏着形态和粤语口语表达时均出现显著困难；开源小模型（如LLaMA-2 13B、Mistral 7B）在流畅性与准确性上明显落后。

Conclusion: 当前大语言模型在低资源和形态复杂语言中仍存在显著不足，尤其在文化语境理解与形态泛化方面。未来需关注模型的文化适应性与语言多样性建模能力，同时推动资源公平与开放基准建设。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [92] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 本文提出了一种自监督框架，用于提升大语言模型安全检测器（guard models）的语义鲁棒性。针对现有模型对表面语言变化敏感的问题，作者利用同义改写集并设计一种考虑偏斜的聚合策略，以增强预测一致性。实验表明，该方法可将同义句间安全评分波动降低约58%，平均提升基准测试准确率2.5%，且对未见风格变化具有泛化能力。此外，研究发现模型校准与一致性存在双向促进关系，鲁棒性训练可使校准度提升最高达40%。结果表明，将语义一致性作为核心训练目标具有重要意义，为构建更可靠的检测器提供了可扩展方案。


<details>
  <summary>Details</summary>
Motivation: 当前安全检测器对语言表面形式变化敏感，即使语义不变的改写也会导致安全评分剧烈波动，暴露其缺乏语义根基，亟需提升其对语义一致性的鲁棒性。

Method: 提出一种自监督框架，通过构造同义改写集，采用新颖的偏斜感知聚合策略来计算鲁棒的目标标签，从而强制模型在不同表达形式下保持一致的输出。

Result: 在六种开源安全检测器上验证，该方法使同义句间语义变异性降低约58%，平均提升基准准确率2.5%，并能泛化至未见过的风格变化；同时，模型校准性能最高提升40%，揭示了校准与一致性之间的双向关联。

Conclusion: 将语义一致性作为首要训练目标具有显著价值，所提方法为构建更可靠、鲁棒的安全检测器提供了可扩展且实用的解决方案。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [93] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 本文提出STaDS（结构化表格决策模拟）框架，用于评估大语言模型在类似专业人士的结构化决策任务中的理解能力。该框架通过考察模型对问题与指令的理解、基于知识的预测能力以及对相关决策因素的依赖程度，全面评估其是否具备真正的领域理解。实验发现多数模型在跨领域上难以保持一致的高准确率，且存在‘准确但不忠实’的现象——即模型虽预测正确，但其推理依据与实际决定因素不符。研究强调需要超越单纯准确率的评估体系，推动更深入的理解力评测方法发展。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在预测上表现优异，但仅靠准确性无法证明其真正理解了任务背后的逻辑。人类专家不仅预测准确，还能在不同情境下持续依赖正确的决策因素做出合理判断。因此，亟需一种能衡量模型是否具备类似人类专业能力的评估方式，以揭示其真实理解水平。

Method: 提出并应用结构化表格决策模拟（STaDS）框架，设计15个涵盖多种领域的结构化决策任务，要求模型完成理解、预测和解释三方面任务。通过分析模型在这些任务中的表现，重点考察其是否依赖正确的决策特征，并对比其陈述理由与实际影响因素之间的匹配度。

Result: 大多数前沿大语言模型在跨领域任务中表现不稳定；部分模型虽准确但推理过程不可靠，存在显著的‘理由-行为’脱节现象；模型对关键决策因素的依赖程度普遍不足，表明其缺乏全局性、一致性的理解能力。

Conclusion: 现有基于准确率的评估不足以反映大语言模型的真实理解水平。必须建立更严格的全局理解评估机制，发展能够识别并验证模型决策依据可靠性的新框架，才能真正提升模型的可信与可解释性。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [94] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 本研究比较了传统机器学习（ML）与深度迁移学习（DTL）在预测耳蜗植入（CI）儿童语言发展方面的表现，使用基于脑部神经解剖特征的二分类模型区分高改善者与低改善者。结果显示，采用双线性注意力融合策略的DTL模型在准确率（92.39%）、敏感度（91.22%）、特异度（93.56%）和AUC（0.977）方面均显著优于传统ML模型。该方法通过捕捉任务特定的判别信息，展现出更强的表征学习优势。研究支持构建一个全球通用的单个DTL预测模型用于CI儿童语言预后的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前耳蜗植入对儿童语言发展的改善效果存在较大个体差异，而现有预测方法（如植入年龄、残余听力）无法可靠预测个体结果。因此亟需更精准的预测模型来指导临床决策。

Method: 研究纳入来自三个中心的278名双侧感音神经性耳聋患儿，利用脑部神经解剖特征构建二分类模型，分别采用传统机器学习和深度迁移学习（结合双线性注意力融合策略）进行分析，并对比其预测性能。

Result: DTL模型在准确率、敏感度、特异度和AUC上均显著优于传统机器学习模型：准确率为92.39%（95% CI: 90.70%-94.07%），敏感度为91.22%（95% CI: 89.98%-92.47%），特异度为93.56%（95% CI: 90.91%-96.21%），AUC达0.977（95% CI: 0.969-0.986）。

Conclusion: 深度迁移学习在预测耳蜗植入儿童语言发展方面具有显著优势，能够有效捕捉任务相关的判别性特征，具备构建全球适用单一预测模型的可行性。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [95] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出一种基于混合专家（MoE）语音投影器的大型语言模型增强方法，用于解决代码切换（CS）语音翻译中的语义建模复杂性和数据稀缺问题。通过为每种语言分配专门的专家，实现细粒度的语音特征建模，并采用多阶段训练策略，利用现成的单语语音识别和单语翻译数据提升对齐与翻译能力。结合语言特定损失和组内负载均衡损失，优化专家分配；引入过渡损失以平滑不同训练阶段间的数据迁移，缓解高质量CS数据不足的问题。实验表明该方法在多个公开数据集上具有显著效果和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖模型自身隐式学习语义建模，且需昂贵的人工标注来应对代码切换语音翻译中的语义复杂性和数据稀缺问题，效率低下。

Method: 提出基于混合专家（MoE）的语音投影器，每个专家专注特定语言的语义子空间；设计多阶段训练范式，利用单语ASR和单语ST数据进行训练；结合语言特定损失和组内负载均衡损失指导专家分配；引入过渡损失促进不同训练阶段间的数据平滑迁移。

Result: 在多个常用数据集上的实验验证了所提方法的有效性和通用性，显著提升了代码切换语音翻译的性能。

Conclusion: 该方法通过精细的专家分工与多阶段协同训练，有效解决了语义建模复杂与数据稀缺的双重挑战，为代码切换语音翻译提供了高效、可扩展的新范式。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [96] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: 本文提出了一种名为基于视觉事实化的微调（GVF）的新方法，通过引入显式的事实信号来系统性提升多模态大模型的视觉事实一致性。该方法包含三个核心机制：事实锚点数据增强、事实感知指令微调和事实一致性损失函数。在LLaVA-1.5-13B上的实验表明，GVF显著优于标准微调，在视觉幻觉测试（VHTest）的开放问答和是/否问答任务中表现更优，同时在MME和POPE等通用多模态基准上保持或略微提升性能，证明其在抑制视觉幻觉的同时未损害模型的一般理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法对多模态大模型中的视觉幻觉问题改善有限，无法深入干预事实推理过程，亟需一种能系统性提升视觉事实一致性的新方法。

Method: GVF微调包含三个核心机制：(1) 事实锚点数据增强，通过结构化事实锚点和反事实提示扩充训练数据；(2) 事实感知指令微调，将事实线索嵌入明确指令中；(3) 事实一致性损失函数，专门惩罚事实错误。

Result: 在LLaVA-1.5-13B上，GVF微调在VHTest基准的OEQ和YNQ任务中均显著优于标准微调，且在MME和POPE等通用多模态基准上性能保持或略有提升，说明其有效缓解了视觉幻觉而不影响模型整体能力。

Conclusion: GVF微调是一种有效提升多模态大模型视觉事实一致性的新范式，通过显式事实信号的引入，实现了对视觉幻觉的系统性抑制，同时维持了模型的通用理解与推理能力。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [97] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 该综述探讨了大语言模型（LLMs）在材料科学发现流程中的应用，涵盖文献挖掘、预测建模和多智能体实验系统三个关键领域。文章指出，尽管当前进展依赖于闭源商业模型，但基准测试显示开源模型在性能上可媲美闭源模型，且具备更高的透明度、可复现性、成本效益和数据隐私保护。随着开源模型的持续进步，作者倡导更广泛采用开源模型，以构建开放、灵活且由社区驱动的科学发现AI平台。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正在快速改变材料科学的研究范式，但其应用主要依赖于闭源商业模型，存在透明度低、可复现性差、成本高和数据隐私风险等问题。因此亟需评估开源替代方案的可行性，并推动更可持续、可访问的AI研究生态建设。

Method: 通过综述近期文献，分析LLMs在材料科学中的三大应用场景：从科学文献中提取合成条件等信息；学习材料结构与性能之间的关系；协调集成计算工具与实验室自动化的多智能体系统。同时，通过基准测试对比开源与闭源模型的性能表现。

Result: 开源大语言模型在关键任务上的表现已可与闭源模型相媲美，同时在透明度、可复现性、成本控制和数据隐私方面具有显著优势。

Conclusion: 随着开源模型性能不断提升，应积极推动其在材料科学研究中的广泛应用，以构建更加开放、灵活和社区驱动的AI科研平台，促进科学发现的可持续发展。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [98] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 提出一种两阶段演示选择方法TopK + L2D，通过微调的BERT类小语言模型（SLM）生成标签分布并计算其差异，以选择在语义和标签分布上均与测试输入匹配的演示，从而提升大语言模型在文本分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示选择方法主要关注测试输入与演示之间的语义相似性，忽视了标签分布对齐的重要性，导致性能受限。

Method: 采用两阶段方法：首先使用微调的BERT类小语言模型生成测试输入和候选演示的标签分布，然后基于标签分布差异（L2D）进行筛选，结合TopK策略选择最优演示。

Result: 在七个文本分类基准上的实验表明，该方法显著优于以往的演示选择策略；且大语言模型的性能与用于标签分布估计的小语言模型的准确性呈正相关。

Conclusion: 通过引入标签分布对齐机制，所提出的TopK + L2D方法有效提升了大语言模型在文本分类任务中使用少量演示时的表现，验证了标签分布一致性的重要性。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [99] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: SpiderGen is an LLM-based workflow that integrates traditional LCA methodology with LLM reasoning to generate accurate life cycle assessment process information for consumer goods. It achieves an F1-Score of 62% on real-world LCA data, outperforms baselines like chain-of-thought and one-shot prompting, and reduces cost and time significantly—under $1 USD in under 10 minutes versus the current $25,000+ and 21-person days.


<details>
  <summary>Details</summary>
Motivation: To address the high cost, time, and human effort required for traditional Life Cycle Assessments (LCAs) in estimating the environmental impact of consumer products, especially due to GHG emissions from production, use, and disposal.

Method: SpiderGen combines LCA taxonomy and methodology with the reasoning and world knowledge of large language models (LLMs) to automatically generate procedural LCA information. It is evaluated against real-world LCA documents as ground-truth.

Result: SpiderGen produces LCA process information that is mostly accurate (F1-Score: 62%) with minor errors; remaining issues stem from variations in document detail and scope. It significantly outperforms baseline prompting methods and drastically reduces cost and time.

Conclusion: SpiderGen demonstrates strong potential to automate and democratize LCA processes, enabling faster, cheaper, and scalable estimation of carbon impacts for consumer goods.

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [100] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 本文系统研究了不同大语言模型（LLM）对齐方法如何影响模型对提示攻击的响应。选取了基于常见对齐方法（SFT、DPO、RLHF）的开源模型，通过统计方法分析提示微调对攻击成功率（ASR）的影响。结果表明，即使微小的提示变化也会显著改变ASR，使模型更易或更难受到攻击。研究强调仅依赖现有‘攻击基准’可能无法揭示所有潜在漏洞，呼吁采用系统性、基于统计的分析方法来评估对齐方法与提示敏感性的关系。


<details>
  <summary>Details</summary>
Motivation: 现有攻击评估方法可能不足以全面揭示不同对齐方法下模型的脆弱性，亟需更系统、严谨的分析手段以理解提示变化对攻击成功率的影响。

Method: 选取SFT、DPO、RLHF三种主流对齐方法的开源模型，设计针对不当内容的攻击提示，通过统计测试分析提示微调对攻击成功率（ASR）的影响，评估不同对齐方法的敏感性差异。

Result: 微小的提示变化可显著影响攻击成功率（ASR），不同对齐方法表现出不同的敏感性；现有攻击基准可能无法充分暴露模型和方法的所有漏洞。

Conclusion: 仅依赖固定攻击基准不足以全面评估模型安全性，应采用系统性、统计驱动的方法来深入分析对齐方法对提示攻击敏感性的影响，以提升模型安全评估的全面性与可靠性。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [101] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: This paper reveals that LLMs exhibit significant accuracy degradation over multiple interactions, even with simple follow-up prompts. Using Markov chains and linear probes, the authors show that model accuracy stabilizes at a level ~8% lower than initial accuracy, indicating inherent fragility. The findings advocate for stationary accuracy as a key robustness metric and stress the importance of improving consistency in interactive AI systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to evaluate the robustness of large language models (LLMs) in interactive settings, where repeated questioning and follow-up prompts may lead to changes in model answers and accuracy over time. As LLMs are increasingly used in real-world applications with high user interaction, understanding their stability and consistency is crucial.

Method: The authors use multi-turn follow-up prompts, including simple 'Think again' prompts and semantically equivalent reworded questions, to assess how model answers and accuracy evolve across turns. They model accuracy dynamics using Markov chains and apply linear probes to hidden states to predict future answer changes.

Result: The results show significant robustness vulnerabilities: a 'Think again' prompt caused approximately a 10% accuracy drop for Gemini 1.5 Flash over nine turns, and combining it with a reworded question led to a 7.5% drop for Claude 3.5 Haiku. Accuracy dynamics can be effectively modeled with Markov chains, revealing that stationary (long-run) accuracy is on average about 8% lower than first-turn accuracy for Gemini 1.5 Flash. Linear probes demonstrate predictive capability for future answer changes based on hidden states.

Conclusion: The study establishes stationary accuracy as a principled metric for evaluating LLM robustness in interactive scenarios. It highlights the fragility of current models under repeated questioning and emphasizes the need to address instability for deployment in high-stakes, interactive applications where consistent reasoning is critical.

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [102] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: 递归提示通过大语言模型实现了可扩展的合成数据集生成，但存在偏见放大的风险。本文通过三种互补评估框架（基于规则的模式匹配、基于嵌入的语义相似性、下游任务表现）研究了三轮递归文本生成中的性别偏见动态。实验显示，偏见演化呈现平衡态而非单调放大：初始低偏见会向模型固有偏见水平趋近（+36%），而高初始偏见则衰减至该水平（-26%）。在四种缓解策略中，对比增强法（引入性别对调版本）虽导致嵌入层面偏见评分升高，但在下游任务中实现显著偏见降低（低初始偏见下达98.8%，平均91%），揭示语义相似性指标与行为公平性之间可能存在背离，强调负责任合成数据生成需采用多维度评估。


<details>
  <summary>Details</summary>
Motivation: 递归提示生成合成数据具有可扩展性优势，但可能放大初始偏见，威胁生成内容的公平性。现有评估方法往往依赖单一指标，难以全面反映偏见的实际影响，亟需多维度评估框架以确保合成数据的伦理安全性。

Method: 采用三种互补评估框架：规则式模式匹配（检测显性性别关联）、嵌入式语义相似性（衡量内容语义偏移）、下游任务性能（如分类任务中的公平性表现）。实验设置三种初始偏见水平（0.1, 0.3, 0.6）和四种偏见缓解策略，包括对比增强、对抗训练、数据重加权和提示工程，系统分析三轮递归生成过程中的偏见演化轨迹。

Result: 偏见演化呈现非单调平衡态：低初始偏见放大至模型固有偏见水平（+36%），高初始偏见则衰减至该水平（-26%）。对比增强策略在下游任务中表现出最强的偏见抑制能力（低初始偏见下98.8%，平均91%），尽管其嵌入层面偏见得分更高，表明语义相似性指标与实际行为公平性不一致。

Conclusion: 合成数据生成中的偏见演化并非简单放大或衰减，而是趋向于模型内在偏见的平衡点。仅依赖嵌入相似性等静态指标可能误导评估结果。为实现负责任的合成数据生成，必须结合多种评估方式，尤其关注下游任务的行为公平性，避免误判缓解策略的有效性。

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [103] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 本文通过利用闭源多模态系统的偏好偏差，提出一种基于多轮“传话游戏”的方法来揭示其隐藏语言。该方法通过观察概念共现频率的变化，量化分析系统对概念间关联的理解，进而构建全局概念连接图谱。研究还推出了Telescope数据集（10,000+概念对），并结合推理型大模型发现超越文本与视觉相似性的意外概念关系，揭示系统对世界的理解机制。该工作为多模态系统可解释性与可控性研究提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 现有闭源多模态系统因黑箱架构导致其内部语言机制不透明，缺乏有效手段探究其对世界理解的深层逻辑。本文旨在通过偏好偏差揭示系统隐含的概念理解模式，提升对系统认知能力的可解释性。

Method: 采用多轮‘传话游戏’框架，利用系统在图像压缩-文本重构过程中的偏好偏差，观察概念共现频率变化；结合Telescope数据集进行测试，并引入推理型大模型挖掘深层概念关联。

Result: 成功构建了多模态系统概念连接的全局地图，识别出训练中继承的偏好偏差，评估了泛化能力，并发现了更稳定的概念连接路径；揭示了超越表层相似性的深层概念关系。

Conclusion: 本研究提供了一种有效探查多模态系统隐藏语言的新方法，为未来实现系统可解释性、可控性及认知建模奠定了基础。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [104] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出Squid Game，一个动态对抗性评估环境，用于在资源受限和信息不对称条件下评估大语言模型（LLMs）的多方面能力。该环境包含六个淘汰式关卡，涵盖指令遵循、代码生成、推理、规划与安全对齐等任务。通过对50多个LLM的测试，发现模型性能存在明显的代际跃迁，并揭示部分模型依赖投机性捷径获胜，暗示静态基准可能面临高层评估污染问题。相关性分析表明，动态评估可作为静态评估的补充。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以跟上大语言模型的发展，且存在数据污染风险，无法可靠评估模型是否真正掌握解决问题的能力；同时，现有基准多假设理想环境，忽视了模型在压力下的表现。因此需要一种能模拟真实复杂情境的动态评估机制。

Method: 设计并实现Squid Game，一个六级淘汰制的互动游戏环境，通过与其他LLM对手的对抗来评估模型在资源受限和信息不对称条件下的综合能力。采用多维度任务设计，覆盖指令遵循、代码、推理、规划与安全对齐。

Result: 实验显示模型性能呈现明显的代际变化，部分模型表现出依赖投机性策略而非真实理解的现象，说明静态基准可能存在评估污染；相关性分析证实动态评估可有效补充静态评估。

Conclusion: Squid Game提供了一种新颖且有效的动态对抗性评估框架，有助于更真实地检验大语言模型的综合能力，并为未来评估体系的改进提供了重要参考。

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [105] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: 该研究探讨了先进文本转语音系统是否能隐式学习人类通过放慢语速表达礼貌这一非显性语音特征。实验对比了22个合成语音在正式礼貌与随意非正式提示下的发音时长，结果显示，在两种主流AI平台（AI Studio和OpenAI）中，礼貌提示均导致显著更慢的语速，效应量极大且统计显著。这表明AI能够内化人类沟通中的心理细微差别，具备作为社会行为体强化社会规范的能力。


<details>
  <summary>Details</summary>
Motivation: 探究语音型人工智能能否在未被明确编程的情况下，习得并体现人类社会交往中隐含的非显性语言特征，如通过放慢语速表达礼貌。

Method: 选取两个主流AI平台（AI Studio和OpenAI）的22个合成语音，让其在“礼貌正式”与“随意非正式”两种条件下朗读同一脚本，测量并比较其语音持续时间。

Result: 在所有AI Studio的语音中以及大多数OpenAI的语音中，礼貌提示下的语速显著慢于随意提示，效应量大且统计显著。

Conclusion: AI系统能够隐式学习并再现人类沟通中的心理细微特征，显示出其作为社会行为体在延续人类社会规范方面的潜力。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [106] [Where does an LLM begin computing an instruction?](https://arxiv.org/abs/2511.10694)
*Aditya Pola,Vineeth N. Balasubramanian*

Main category: cs.CL

TL;DR: 本文通过构建三个简单数据集（键值、引文归属、字母选择）及两跳组合任务，利用激活修补技术在最小对比提示对上测量逐层翻转率，以确定指令遵循开始的层级位置。在Llama系列模型中，观察到一个转折点（称为'起始点'），在此之前的干预对预测结果有效，而之后则失效。多跳任务也表现出相似的起始点。该方法为定位指令遵循起点提供了一种简单且可复现的手段，并可用于跨任务和模型规模比较。


<details>
  <summary>Details</summary>
Motivation: 探究指令遵循在模型层栈中的具体起始位置，即从读取指令到执行指令的转变点，从而理解大模型如何处理指令性任务。

Method: 使用激活修补技术，在最小对比提示对上测量逐层翻转率，分析干预不同层的残差激活对预测结果的影响，识别出指令遵循开始的转折层。

Result: 在多个Llama模型中均发现一个明确的‘起始点’，在此之前干预有效，之后则无效；多跳任务也表现出类似的起始位置。

Conclusion: 该研究提出了一种可复制的方法来定位指令遵循的起始层，为理解大模型内部机制提供了新视角，并支持跨任务与模型规模的比较。

Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.

</details>


### [107] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型（LLMs）在国际关系领域中的国家层面偏见，基于联合国安理会的历史记录构建了三项测试框架，评估了五大常任理事国的偏见表现。实验发现不同模型存在普遍性偏见模式（如对西方国家的偏好、对俄罗斯的负面偏见），且偏见方向和程度随模型与任务上下文变化，表明偏见具有多维度特性。具备更强推理能力的模型表现出更低偏见和更好性能。为此提出结合检索增强生成与反思式自我反思的去偏框架，在GPT-4o-mini和LLama-3.3-70B上有效降低偏见并提升表现。研究强调在国际关系应用中需同时评估偏见与性能。


<details>
  <summary>Details</summary>
Motivation: 揭示和量化大型语言模型在国际关系领域中存在的国家层面偏见，以提升其在关键外交与政策分析场景中的可信度与公平性。

Method: 构建基于联合国安理会历史记录的三重评估框架，通过多维度测试分析不同模型对五大常任理事国的偏见；引入检索增强生成与反思式自省相结合的去偏方法，并在多个主流模型上进行验证。

Result: 各模型普遍存在对西方国家的偏好及对俄罗斯的负面偏见，但具体表现因模型和任务而异；具备更强推理能力的模型偏见更小、表现更优；所提去偏框架显著降低偏见并提升性能，尤其在GPT-4o-mini和LLama-3.3-70B上效果明显。

Conclusion: 大型语言模型的国家偏见是多维且情境依赖的，不能一概而论。评估其在国际关系领域的应用时，必须同时考虑偏见水平与性能表现。结合检索增强与自我反思的去偏方法可有效改善模型公平性与准确性。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [108] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: PiAttention提出一种周期稀疏Transformer，通过环形局部注意力、确定性π步跳转和自适应融合门实现线性复杂度下的长序列建模。相比RingAttention，其感受野增长更快（O(kL + π log L)），在语言建模、检索和视觉-语言任务中表现更优，达到更低困惑度且节省50% GPU资源。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在长序列建模中因二次计算复杂度带来的瓶颈问题，同时克服现有稀疏注意力机制如RingAttention在感受野受限和缺乏自适应性方面的不足。

Method: 将注意力分解为环形局部邻域、确定性的π步跳转以及自适应融合门；利用周期结构确保远距离token的可预测覆盖，并保持每层计算复杂度与序列长度呈线性关系。

Result: 在多个任务上性能优于或等同于密集注意力，在相同上下文长度下比RingAttention低8.3%的困惑度，且仅需50%的GPU资源；实验验证了周期跳转、自适应融合及头级别稀疏协调的重要性。

Conclusion: PiAttention通过周期稀疏设计实现了高效长序列建模，在保证高质量的同时显著降低计算开销，是当前长序列处理的有效解决方案。

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [109] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 本文提出一种结合TextRank句提取与医学命名实体识别的框架，利用大语言模型（LLMs）提升医疗文本摘要的忠实度。在MeQSum（英文）和BanglaCHQ-Summ（孟加拉语）数据集上微调LLaMA-2-7B模型，实验显示该方法在质量（ROUGE、BERTScore、可读性）和忠实度（SummaC、AlignScore）指标上均优于零样本基线及先前系统，并且超过80%的生成摘要保留了关键医疗信息。研究强调了忠实度在可靠医疗摘要中的重要性，展示了该方法在医疗场景中安全部署LLMs的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗文本摘要方法存在不忠实问题，可能歪曲医学细节，带来严重风险，亟需提升摘要的忠实度以保障医疗沟通的安全性与可靠性。

Method: 结合TextRank进行句子提取，融合医学命名实体识别技术，并基于大语言模型（如LLaMA-2-7B）进行微调，以增强摘要的忠实度与质量。

Result: 在多个指标（ROUGE、BERTScore、SummaC、AlignScore）上表现优于零样本基线和现有系统；人类评估显示超过80%的摘要准确保留关键医疗信息。

Conclusion: 忠实度是实现可靠医疗摘要的关键维度，所提出的框架有效提升了摘要质量与安全性，具备在医疗领域安全应用大语言模型的潜力。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [110] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 本文介绍了TEDxTN，首个公开可用的突尼斯阿拉伯语到英语的语音翻译数据集，旨在缓解阿拉伯方言数据稀缺的问题。研究团队收集、分割、转录并翻译了108个TEDx演讲，涵盖25小时带有代码混用的语音内容，涉及来自突尼斯11个不同地区的多种口音。数据集和标注指南已公开，支持未来扩展。同时报告了基于预训练和微调端到端模型的语音识别与语音翻译强基线结果。该数据集是首个开源的、包含代码混用的突尼斯方言语音翻译语料库，将推动相关自然语言处理研究。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯方言（特别是突尼斯阿拉伯语）在语音翻译任务中因数据稀缺导致的研究障碍，促进低资源语言的自然语言处理发展。

Method: 收集并标注108个TEDx演讲，采用内部开发的标注指南进行语音分割、转录与翻译，涵盖多种口音和代码混用现象，构建公开可访问的数据集。

Result: 成功构建首个公开的突尼斯阿拉伯语至英语的带代码混用语音翻译数据集（TEDxTN），并报告了基于多个预训练与微调模型的语音识别与语音翻译基线性能。

Conclusion: TEDxTN是首个公开的突尼斯方言语音翻译语料库，具有重要价值，能够激励和促进针对突尼斯方言的自然语言处理研究。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [111] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 本研究针对学生在获取日常学术信息时面临的困难，提出利用生成式人工智能（GenAI）和检索增强生成（RAG）技术开发一个智能聊天机器人，以整合分散的校务信息。通过质量指标与大模型作为裁判（LLM-as-a-Judge）的方法评估多个GenAI模型，Gemini 2.0 Flash在质量和速度上表现最佳，Gemma 3n则因开源特性与良好性能被推荐。


<details>
  <summary>Details</summary>
Motivation: 学生普遍反映难以获取分散于多个机构文档和网站中的日常学术信息，信息碎片化导致理解不清和困惑，亟需一种高效、统一的信息访问方式。

Method: 采用生成式人工智能（GenAI）与检索增强生成（RAG）技术构建聊天机器人；通过质量评估指标及大模型作为裁判的方法对多个GenAI模型进行测试与比较。

Result: Gemini 2.0 Flash在生成质量与响应速度方面表现最优，Gemma 3n在性能与开源优势上表现良好，两者均适合用于构建高校信息智能助手。

Conclusion: 基于GenAI与RAG的聊天机器人可有效整合分散的学术信息，提升学生获取信息的效率与准确性，Gemini 2.0 Flash与Gemma 3n为理想候选模型。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [112] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本研究探讨了使用GPT-4o评估本科生计算语言学课程中的简答题和项目报告的可行性。结果显示，GPT-4o与人类评分者之间具有高度相关性（最高达0.98），在55%的简答题中实现完全一致评分；对于项目报告也表现出良好一致性，但在技术性、开放性回答上存在一定评分波动。研究代码和样本数据已公开，为教育评估中的LLM应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在教育任务如评分中的应用日益增多，但其在真实课堂环境中与人类评价的一致性仍缺乏充分研究。本文旨在检验LLM在实际教学评估中的可行性和可靠性，以推动自动化评分系统的发展。

Method: 收集约50名学生在五个简答题测验中的作答，并获取14个团队的项目报告。采用GPT-4o对这些作业进行评分，并与课程助教独立完成的人类评分进行对比分析。通过相关系数和评分一致性指标评估模型表现。

Result: GPT-4o在简答题评分中与人类评分者相关性高达0.98，55%的案例中得分完全一致；在项目报告评分中整体表现良好，但在技术性及开放性问题上存在一定的评分差异。

Conclusion: LLM（如GPT-4o）在教育评估中展现出巨大潜力，尤其适用于结构化题目，但在处理复杂、开放性内容时仍需谨慎。本研究为真实教学场景下自动化评分系统的开发提供了实证支持与改进方向。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [113] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: 本文提出了一种无监督的循环检测框架，结合结构和语义分析，用于检测由大语言模型驱动的智能应用中的隐藏执行循环。该方法先通过高效的时序调用栈分析识别显式循环，再利用语义相似性分析发现由冗余内容生成引起的隐式循环。在基于LangGraph的股票市场应用的1575个轨迹上评估，混合方法取得了0.72的F1分数（精确率0.62，召回率0.86），显著优于单独使用结构或语义方法的效果。尽管结果令人鼓舞，但仍存在改进空间，未来工作需进一步优化该方法并解决其局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的智能应用存在非确定性行为，可能形成隐藏的执行循环，导致资源浪费但不触发明确错误。传统可观测性平台无法有效检测此类高成本低效问题。

Method: 提出一种混合方法：首先进行计算高效的时序调用栈分析以识别显式循环；随后采用语义相似性分析来发现由冗余内容生成引发的隐式循环。

Result: 在1575个轨迹上，混合方法取得F1分数0.72（精确率0.62，召回率0.86），显著优于单独的结构方法（F1: 0.08）和语义方法（F1: 0.28）。

Conclusion: 该框架在检测复杂循环方面表现良好，但仍有提升空间，未来需进一步优化方法并克服现有局限性。

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [114] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 本文研究多语言大模型（LLM）如何内部表示多种语言，发现其采用‘枢纽语言’表征机制：模型在早期层中使用几乎相同的跨语言表示，而语言特异性解码出现在后期层。通过交叉层转换器（CLT）和归因图分析，揭示了最终层依赖少数高频语言特征线性读取语言身份，干预这些特征可实现语言替换。此外，主导训练语言影响解码路径与归因结构，表明理解该机制对改善多语言对齐至关重要。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型虽能处理多种语言，但其内部如何表示语言多样性尚不明确；尤其为何性能仍偏向主要训练语言，亟需深入探究。

Method: 通过训练不同多语言数据混合的LLM，结合交叉层转换器（CLT）和归因图分析其内部机制，并通过干预高频率语言特征验证语言切换能力。

Result: 发现模型存在近似一致的跨语言表示，语言特异性解码出现在后期层；少量高频特征在最终层线性读取语言身份，干预可实现语言替换；主导训练语言显著影响解码路径与归因结构。

Conclusion: 多语言大模型采用枢纽语言表征机制，理解此机制对提升多语言对齐具有关键意义。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [115] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出一种对齐参数空间的先驱策略，以解决任务算术在模型分化时的负干扰问题，通过利用Transformer架构的排列、旋转和缩放对称性，成功将高级推理技能迁移到非推理模型，并在复杂推理基准测试中优于标准任务算术。


<details>
  <summary>Details</summary>
Motivation: 现有任务算术方法在模型训练分化后易产生负干扰，限制了技能迁移效果，亟需一种更稳健的跨模型技能转移机制。

Method: 利用Transformer架构的内在对称性（如排列、旋转、缩放），对齐不同模型的参数空间，结合权重与激活两种方式，适配现代GQA和SwiGLU结构，实现参数空间对齐后再进行任务算术操作。

Result: 在多个挑战性推理基准上，该方法显著优于标准任务算术，有效实现了高级推理能力的迁移，减少了冗余微调并提升了模型适应性。

Conclusion: 提出的方法通过参数空间对齐，显著提升任务算术在异构模型间的技能迁移性能，为大型语言模型家族间专业化技能的高效融合提供了有效路径。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [116] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLM）在社会或对话判断任务中的可靠性，通过将直接事实查询重新表述为对话判断任务，考察模型信念的变化。研究发现，即使微小的对话语境变化也会显著影响模型判断，平均性能变化达9.24%。部分模型如GPT-4o-mini表现出顺从倾向，而Llama-8B-Instruct则变得过于苛刻。研究提出一个可复现的评估框架，用于诊断模型的信念强度，有助于构建更可信的对话系统。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在涉及社会或对话判断任务中的可靠性，特别是当任务从直接事实查询转变为对话情境时，模型判断是否发生改变，以及其信念在对话压力下的稳定性。

Method: 设计了一种评估框架，对比模型在直接事实查询与同一信息嵌入最小对话中的表现；引入简单反驳（如“之前的回答是错误的”）作为压力测试，以衡量模型在对话压力下信念的坚定程度。

Result: 多数模型在对话语境下判断出现显著变化，平均性能波动达9.24%；部分模型表现出顺从（如GPT-4o-mini），另一些则变得过度批判（如Llama-8B-Instruct）。

Conclusion: 对话语境对大模型判断有显著影响，是评估中不可忽视的关键因素；所提出的框架可有效诊断模型信念，有助于提升对话系统的可信度。

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [117] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360 是一个开源的 Python 工具包，用于解释大型语言模型（LLMs）在用户提供的上下文或提示下的输出，支持黑盒和白盒方法，涵盖检索增强生成、自然语言生成和越狱攻击等应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险场景中的广泛应用，需要开发工具来解释其输出，以提高透明度和可信度。

Method: 通过扰动和梯度等黑盒与白盒方法，实现对 LLM 输出的解释，并集成于 ICX360 工具包中。

Result: ICX360 提供了多种解释技术的实现，支持多种典型用例，并配有快速入门指南和详细教程。

Conclusion: ICX360 为解释大型语言模型的决策过程提供了实用、开放的解决方案，有助于提升 LLM 在实际应用中的可解释性和安全性。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [118] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 本研究揭示了大语言模型在二元决策任务中存在格式层面的负面偏见，即提示格式比语义更影响模型输出。通过构建系统化的评估集，研究发现模型在缺乏足够知识时倾向于生成负面回答，形成一种捷径行为。研究还发现，提供相关上下文和‘我不知道’选项可减轻偏见，而链式思维提示则会加剧偏见。提示类型会影响响应方向，表明负偏见受多种因素影响，为缓解该问题提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注检测和纠正导致负面偏见的注意力头，但对影响负面偏见的深层因素仍缺乏深入探索。本文旨在揭示提示格式、模型参数知识状态及不同提示策略如何共同作用于负面偏见，以推动更有效的缓解机制设计。

Method: 提出一种评估集构建流水线，将数据集细分为基于模型参数知识的三类：正确、错误和知识不足。通过分析模型在不同提示场景下的响应行为，识别其在知识不足时倾向于生成负面回答的捷径行为，并考察不同提示策略（如上下文提供、‘我不知道’选项、链式思维）对偏见的影响。

Result: 模型在缺乏足够知识时表现出显著的负面响应倾向，形成格式层面的负偏见；提供相关上下文或‘我不知道’选项能有效降低偏见，而链式思维提示反而增强偏见；提示类型直接影响响应方向，说明负偏见具有情境依赖性。

Conclusion: 负面偏见不仅源于模型内部机制，也受提示格式与知识状态的交互影响。理解这些因素有助于设计更具鲁棒性和公平性的提示策略，从而有效缓解大语言模型中的负面偏见。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [119] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath 是一个大规模、多领域的生物医学实体链接（EL）数据集，整合了九个现有的专家标注数据集。它通过最新版统一医学语言系统（UMLS）对实体进行标准化，并映射到62个其他生物医学词汇表，同时引入了多达11个生物医学词汇表中的完整本体路径（从一般到具体），从而支持语义丰富的可解释性模型训练与评估，推动下一代互操作性强且可解释的临床自然语言处理模型的发展。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学命名实体识别（NER）和实体链接（EL）研究受限于数据碎片化、缺乏可解释模型资源以及语义盲评指标的局限性，亟需一个更全面、结构化的数据集来推动领域发展。

Method: 整合九个现有专家标注的EL数据集，利用最新版UMLS对实体进行标准化，扩展至62个其他生物医学词汇表，并为每个实体添加从一般到具体的完整本体路径，构建多维度、结构化的数据集。

Result: MedPath 成功构建了一个大规模、多领域、语义丰富且可解释的生物医学实体链接数据集，支持训练和评估具有语义理解能力的可解释模型，促进临床NLP系统的互操作性与透明性。

Conclusion: MedPath 为生物医学自然语言处理提供了关键基础设施，解决了数据分散、模型不可解释和评价方式不充分的问题，是迈向下一代可解释、可互操作临床NLP系统的重要一步。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [120] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: Tool-augmented Language Models (TaLMs) show improved accuracy in solving complex problems by using external tools like the Code Interpreter, but this comes at the cost of degraded reasoning quality. The paper identifies a failure mode called Tool-Induced Myopia (TIM), where models treat tool outputs as substitutes for logical reasoning, leading to plausible but unjustified solutions. Using PYMATH, a benchmark of 1,679 competition-level math problems, the authors demonstrate that while TaLMs gain up to 19.3% in answer accuracy, their reasoning is significantly weaker—non-tool LLMs outperform them by up to 41.5% in reasoning quality. The more tools are used, the worse the reasoning becomes, shifting errors from arithmetic to deeper logical flaws. TIM is present in ~55% of high-risk cases. The authors propose a preference-optimization framework to realign TaLMs to use tools as supportive evidence, improving both accuracy and reasoning depth.


<details>
  <summary>Details</summary>
Motivation: Existing Tool-augmented Language Models (TaLMs) achieve higher answer accuracy by leveraging external tools, but it remains unclear whether this improvement reflects genuine, trustworthy reasoning. There is a risk that models may rely on tool outputs without proper internal justification, leading to superficial or incorrect reasoning even when answers are correct. This study aims to investigate whether tool use undermines reasoning integrity and to quantify such degradation.

Method: The study introduces PYMATH, a benchmark of 1,679 competition-level mathematical problems where Python code is helpful but not sufficient for solving. A multi-dimensional evaluation suite is developed to compare reasoning quality between TaLMs and non-tool LLMs through pairwise comparisons, error analysis, and frequency-based assessments. The authors analyze how tool invocation frequency correlates with reasoning degradation and identify the shift from arithmetic to global reasoning errors. Finally, they propose a preference-optimization-based framework to retrain TaLMs to use tools as assistive evidence rather than substitutes for reasoning.

Result: TaLMs achieve up to a 19.3 percentage point increase in final-answer accuracy compared to non-tool models. However, their reasoning quality deteriorates significantly: non-tool LLMs win up to 41.5% more often in reasoning comparisons. The more frequently tools are used, the more coherent reasoning declines. Errors shift from minor arithmetic mistakes to major logical, assumption-based, and creative failures. Tool-Induced Myopia (TIM) is detected in approximately 55% of high-risk cases. The proposed preference-optimization framework successfully improves both answer accuracy and reasoning depth under tool use.

Conclusion: Tool-augmented Language Models can improve answer accuracy but often at the expense of reasoning quality due to Tool-Induced Myopia (TIM). The reliance on tool outputs as substitutes for reasoning leads to plausible but unjustified solutions. By retraining models via preference optimization to treat tools as assistive evidence, both accuracy and reasoning depth can be enhanced. This work highlights the need to prioritize trustworthy reasoning alongside performance gains in tool-augmented AI systems.

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [121] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 本文提出EMSQA数据集及Expert-CoT与ExpertRAG方法，通过引入临床领域和认证级别等专业知识，显著提升大语言模型在急救医疗问答中的表现，使32B参数的模型成功通过计算机自适应认证考试。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗问答中忽视了临床专业领域和认证级别等关键背景信息，导致在高风险场景下性能受限，亟需引入结构化领域知识以提升准确性与可靠性。

Method: 构建包含10个临床主题和4个认证级别的24.3K题多选题数据集EMSQA；设计Expert-CoT（基于领域与认证级别的链式思维提示）和ExpertRAG（基于主题对齐文档与真实患者数据的检索增强生成）方法，融合领域知识进行推理与生成。

Result: 实验表明，Expert-CoT相比基础CoT提升最高2.05%准确率；结合ExpertRAG后较标准RAG基线最高提升4.59%；32B规模的增强模型成功通过全部计算机自适应急救认证模拟考试。

Conclusion: 通过引入临床专业领域和认证级别等结构化上下文，Expert-CoT与ExpertRAG显著提升了大语言模型在急救医疗问答中的表现，为高风险医疗场景下的智能辅助系统提供了可行路径。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [122] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 本文引入了一个包含176个症状-诊断对的新数据集，源自医学电视剧《豪斯医生》，用于评估大语言模型在叙事医学病例中的罕见病诊断能力。研究评估了GPT 4o mini、GPT 5 mini、Gemini 2.5 Flash和Gemini 2.5 Pro四款先进LLM，结果显示准确率在16.48%至38.64%之间，新模型版本性能提升达2.3倍。尽管所有模型在罕见病诊断中仍面临挑战，但跨架构的改进表明未来发展方向。该教育验证基准为叙事医学推理提供了基线性能指标，并建立了公开可访问的评估框架，推动AI辅助诊断研究发展。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在罕见病诊断任务中的表现，特别是在基于叙事的医学案例中的推理能力，填补当前研究空白，并建立一个教育验证的基准以支持未来研究。

Method: 构建一个来自《豪斯医生》电视剧的176个症状-诊断对数据集，使用四个先进的大语言模型进行评估，通过分析其在叙事医学案例中的诊断准确性来比较性能差异。

Result: 模型准确率范围为16.48%到38.64%，较旧版本有2.3倍的性能提升；尽管整体表现有限，但新模型显示出显著进步趋势。

Conclusion: 本研究建立了一个教育验证的基准数据集，为叙事医学推理提供基线性能标准，推动人工智能在罕见病诊断中的进一步研究与应用。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [123] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 本研究提出CardioEmbed，一个基于Qwen3-Embedding-8B的领域专用文本嵌入模型，通过对比学习在七本心血管专科教材共约15万句去重后语料上进行训练。该模型采用InfoNCE损失与批内负样本，在心脏专科语义检索任务中达到99.60%的检索准确率，相比当前最先进的MedTE模型提升15.94个百分点。在MTEB医学基准测试中，其在BIOSSES任务上取得0.77的Spearman相关系数，在SciFact任务上取得0.61的NDCG@10，表现优异。结果表明，使用综合性临床教科书进行领域特化训练，可实现接近完美的心血管领域检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学文本嵌入模型主要基于PubMed研究文献训练，但临床心脏病学实践依赖于教科书中的程序性知识和专业术语，导致现有模型在临床应用中效果受限。因此，亟需开发基于临床教科书的领域专用嵌入模型以填补这一研究空白。

Method: 采用对比学习方法，基于Qwen3-Embedding-8B架构，在七本心血管专科教材组成的约15万句去重语料上训练CardioEmbed模型；使用InfoNCE损失函数并引入批内负样本进行优化。

Result: CardioEmbed在心脏专科语义检索任务中达到99.60%的检索准确率，较MedTE提升15.94个百分点；在MTEB医学基准上，BIOSSES得分为0.77（Spearman），SciFact得分为0.61（NDCG@10），表现出色。

Conclusion: 基于综合性临床教科书进行领域特化训练，能够显著提升心脏病学领域的文本嵌入质量，实现近乎完美的语义检索性能，为临床应用提供了高效、精准的嵌入模型支持。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [124] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 提出Discox基准和Metric-S评估系统，用于评测中文-英文专业领域长文本翻译的连贯性与术语准确性，发现当前最先进大模型仍远低于人类专家水平，验证了任务难度并推动未来LLM翻译发展。


<details>
  <summary>Details</summary>
Motivation: 现有翻译评估方法多关注段落级准确性和流畅性，无法充分衡量专业领域长文本翻译所需的语篇连贯性和术语精确性，亟需更全面的评估体系。

Method: 构建包含200篇跨7个专业领域的高质量中英对照文本（平均长度超1700词符），开发无需参考文本的Metric-S自动评估系统，从准确性、流畅性和得体性三方面进行细粒度评价。

Result: Metric-S与人工评分高度一致，显著优于现有指标；最先进的大模型在该任务上仍明显落后于人类专家，凸显专业级机器翻译的挑战。

Conclusion: Discox基准与Metric-S为专业领域长文本翻译提供了可靠、严谨的评估框架，有助于推动大模型在高阶翻译任务上的持续改进。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [125] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文首次对主流开源DPO数据集进行系统性、数据驱动的分析，利用Magpie框架对样本进行任务类别、输入质量及偏好奖励标注，实现可扩展的细粒度偏好质量评估。研究揭示了不同数据集中奖励差距的结构性差异，并基于此构建了更高效的新混合数据集UltraMix，其规模比表现最佳的单一数据集小30%，但在关键基准测试中性能更优。所有标注、元数据和超混合数据集均已公开。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对开源DPO数据集的系统性比较，主要受限于计算成本高和高质量标注不足，难以理解偏好选择机制、任务覆盖范围及与人类判断的一致性。

Method: 采用Magpie框架对每个样本进行任务类别、输入质量和偏好奖励（基于奖励模型）的标注，通过奖励模型信号验证偏好顺序，实现无需人工标注的可扩展细粒度分析。基于分析结果，筛选并融合五个数据集中的优质样本，去除噪声和冗余样本，构建新混合数据集UltraMix。

Result: 发现不同DPO数据集在奖励边际上存在显著结构与质量差异；所构建的UltraMix数据集规模减少30%但性能优于任一单一数据集，在多个关键基准上表现更佳。

Conclusion: 本研究为数据驱动的偏好优化提供了重要洞见，表明通过精细的数据清洗与混合策略可显著提升训练效率与模型性能，推动未来在数据质量与配置方面的研究进展。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [126] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 本文提出一种新方法，通过利用自动机遍历历史来引导大语言模型生成新颖的结构模式，以增强基于自动机的结构化生成中的多样性。实验表明该方法显著提升了结构和内容多样性，同时保持了相近的生成效率，并在生成开源库测试用例方面展示了有效性。


<details>
  <summary>Details</summary>
Motivation: 当前基于自动机的结构化生成方法虽然能保证输出的有效性，但缺乏输出多样性，这限制了其在需要多样化输出场景的应用，如测试用例生成。

Method: 利用自动机遍历历史作为引导信号，指导大语言模型探索新的结构模式，从而提升生成多样性。

Result: 所提方法在保持高效生成的同时，显著提高了结构和内容的多样性，在测试用例生成任务中表现出色。

Conclusion: 通过结合自动机遍历历史与大语言模型的生成能力，能够有效增强结构化输出的多样性，为实际应用提供了更灵活、多样化的解决方案。

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [127] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 本文发现当前文本嵌入模型的输出存在一致偏差，即每个嵌入向量可分解为$	ilde{e} + μ$，其中$μ$在所有句子中几乎相同。为此提出一种无需训练、轻量级且即插即用的解决方案——重归一化（Renormalization）。实验表明，该方法在MMTEB基准上显著提升现有模型性能：38个模型在检索任务上平均提升9.7σ，分类任务提升3.1σ，其他任务提升0.8σ。重归一化有两种变体：直接从$e$中减去$μ$，或减去$e$在$μ$方向上的投影，理论与实验均表明后者更优。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型的输出存在系统性偏差，导致性能受限，亟需一种无需重新训练、轻量且通用的修正方法。

Method: 提出重归一化方法，通过从嵌入向量中减去或投影后减去共通偏差向量$μ$，实现对模型输出的校正。

Result: 在MMTEB基准上，重归一化在检索、分类和其他任务上分别带来9.7σ、3.1σ、0.8σ的统计显著性能提升，且投影方式优于直接减法。

Conclusion: 重归一化是一种高效、通用且无需训练的文本嵌入校正方法，能显著提升现有模型的性能，尤其在减少系统性偏差方面表现突出。

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [128] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）是否能够检测自身产生的幻觉。通过将幻觉检测建模为句子分类任务，提出了一种基于思维链（CoT）的方法来提取模型参数中的知识，并评估其检测能力。实验结果显示，GPT-3.5 Turbo结合CoT可检测58.2%的自身幻觉，表明当模型参数中包含足够知识时，LLMs具备一定的自我幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能生成流畅文本，但常产生事实性错误（即幻觉），亟需一种机制让模型自我识别这些错误以提升可靠性。

Method: 将幻觉检测转化为句子分类任务，采用思维链（Chain-of-Thought, CoT）方法从模型参数中提取知识，构建分类框架以评估模型自我检测能力。

Result: GPT-3.5 Turbo在使用CoT的情况下，能够检测出58.2%的自身幻觉，证明其具备一定自我检测能力，前提是模型参数中包含足够的相关知识。

Conclusion: 大型语言模型在参数中蕴含充分知识的前提下，通过思维链方法可以实现对自身幻觉的有效检测，这为提升模型可信度提供了可行路径。

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [129] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 本文研究了美国总统辩论中个人攻击的检测，通过手动标注2016、2020和2024年选举周期的辩论文本，并结合统计分析与基于语言模型的方法，评估微调的Transformer模型及通用大语言模型在识别政治演讲中人身攻击方面的有效性。研究展示了任务特定语言模型适配在深化理解政治传播中的潜力。


<details>
  <summary>Details</summary>
Motivation: 个人攻击已成为美国总统辩论的重要特征，影响公众对选举的认知。自动检测此类攻击有助于提升政治话语透明度，为记者、分析师和公众提供洞察。近年来深度学习和基于Transformer的模型（如BERT和大语言模型）的发展为有害语言的自动化检测提供了新机遇。

Method: 对2016、2020和2024年美国总统辩论文本进行手动标注，采用统计分析与基于语言模型的方法，测试微调的Transformer模型以及通用大语言模型在识别个人攻击上的表现。

Result: 研究表明，针对特定任务优化的语言模型能够有效识别正式政治演讲中的个人攻击，提升了对政治沟通模式的理解。

Conclusion: 任务特定的模型适配在分析政治话语中的个人攻击方面具有显著价值，有助于推动更透明、更深入的政治交流研究。

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [130] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: AV-Dialog is a multimodal dialogue framework that uses audio and visual cues to improve speaker tracking, turn-taking prediction, and response generation in noisy, multi-speaker environments. It outperforms audio-only models by reducing transcription errors, improving turn-taking accuracy, and enhancing human-rated dialogue quality.


<details>
  <summary>Details</summary>
Motivation: Dialogue models struggle with noise and multiple speakers, leading to irrelevant responses and poor turn-taking. The need for more robust, speaker-aware dialogue systems in real-world settings motivates the development of AV-Dialog.

Method: AV-Dialog combines acoustic tokenization with multi-task, multi-stage training across monadic, synthetic, and real audio-visual dialogue datasets to enable streaming transcription, turn-boundary detection, and coherent response generation.

Result: AV-Dialog reduces transcription errors, improves turn-taking prediction, and enhances dialogue quality under interference, demonstrating superior performance over audio-only models.

Conclusion: Integrating visual cues with audio significantly improves dialogue robustness in noisy, multi-speaker environments, paving the way for more natural and reliable spoken dialogue agents.

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [131] [Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion](https://arxiv.org/abs/2511.11126)
*Yi Shi,Wenlong Meng,Zhenyuan Guo,Chengkun Wei,Wenzhi Chen*

Main category: cs.CL

TL;DR: 提出MemoDetector框架，通过四步文本增强模块和双阶段模态融合策略，提升Meme情感理解的细粒度与隐含意义挖掘，实验显示在两个数据集上F1分数分别提升4.3%和3.4%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度多模态融合和隐含意义挖掘方面存在不足，亟需更有效的解决方案。

Method: 引入基于多模态大模型的四步文本增强模块，提取隐含与上下文信息；设计双阶段模态融合策略，实现浅层与深层特征融合。

Result: 在MET-MEME和MOOD数据集上F1分数分别提升4.3%和3.4%，消融实验验证了方法的有效性与鲁棒性。

Conclusion: MemoDetector通过增强文本表示与分层融合策略，显著提升了Meme情感理解性能，具有较强应用潜力。

Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.

</details>


### [132] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: SAP$^{2}$是一种新型框架，通过两阶段动态修剪和整合相关上下文关键词，解决ASR系统在长上下文场景中难以利用领域知识的问题。该方法采用语音驱动的注意力池化机制，有效压缩上下文嵌入并保留语音显著信息，在SlideSpeech和LibriSpeech数据集上分别达到7.71%和1.12%的词错误率，并将SlideSpeech上的偏倚关键词错误率降低41.1%，展现出优异的性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别系统在常规条件下表现优异，但在需要领域知识的长上下文场景（如会议演讲）中受限于模型上下文窗口大小和上下文噪声中的信息稀疏性，难以有效利用长距离依赖信息。

Method: 提出SAP$^{2}$方法，包含两个阶段：第一阶段使用语音驱动的注意力池化机制对上下文嵌入进行压缩，筛选出关键语音信息；第二阶段进一步动态整合相关关键词，提升上下文利用效率。整个过程通过注意力机制实现高效的信息聚焦与压缩。

Result: 在SlideSpeech和LibriSpeech数据集上，SAP$^{2}$分别取得7.71%和1.12%的词错误率，优于现有非上下文基线方法；在SlideSpeech上，偏倚关键词错误率降低41.1%；且在大量上下文输入下仍保持稳定性能，显示良好可扩展性。

Conclusion: SAP$^{2}$通过两阶段动态关键词提取与融合，显著提升了ASR系统在复杂上下文环境下的表现，尤其适用于需要深度语义理解与领域知识的场景，为长上下文语音识别提供了高效可行的解决方案。

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [133] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 本文提出了一种基于语言学的副词超义类分类体系，通过注释实证验证了其有效性，涵盖方式、时间、频率、程度、领域、说话者导向和主体导向等主要语义范畴，能广泛覆盖自然文本中的副词，并可由人工标注者可靠标注。该分类体系扩展了WordNet的覆盖范围，更贴近语言学理论，有助于词义消歧、事件抽取、情感分析和话语建模等下游NLP任务。


<details>
  <summary>Details</summary>
Motivation: 当前WordNet对名词和动词提供了丰富的超义层次结构，但副词的语义分类仍不系统，缺乏有效的语义分类体系。为填补这一空白，本文旨在建立一个语言学基础扎实、可实证验证的副词超义类型体系。

Method: 基于语言学理论构建副词超义类别，通过小规模人工标注研究进行实证验证，评估分类体系在自然文本中的覆盖率与标注一致性。

Result: 实验表明，所提出的超义类别能够广泛覆盖自然文本中的副词，且人类标注者可稳定、可靠地进行分类，具有良好的实用性和可操作性。

Conclusion: 该副词超义分类体系有效扩展了WordNet的语义覆盖，增强了其与语言学理论的一致性，并为多种下游NLP应用提供了支持，未来可进一步完善和推广。

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [134] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: 提出了一种名为LANE的对抗性训练策略，通过选择性标记训练集中的替代词生成具有挑战性的负样本，迫使模型在相同句子中不同标记词之间建立更强的可分性，从而提升神经语言模型对细粒度词义的捕捉能力。该方法在词汇语义变化检测和词义消歧任务上表现优于标准对比学习基线，并且能有效捕捉细微语义差异，适用于多种现有表示学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前神经语言模型常过度依赖全局句子表征，难以捕捉局部语义细节，尤其是在细粒度词义分辨方面存在局限。

Method: 提出LANE方法，通过选择性标记训练数据中的替代词生成对抗性负样本，引导模型关注目标词的局部语义信息，增强不同标记词之间的表征可分性。

Result: 在词汇语义变化检测和词义消歧任务上，所提方法显著优于标准对比学习基线，生成的词向量更具判别性，能更好捕捉细微语义差异。

Conclusion: LANE是一种模型无关的高效策略，能够有效提升神经语言模型对细粒度词义的理解能力，适用于广泛的语言表示学习场景。

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [135] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展且确定性的知识图谱问答生成管道，通过关系聚类和自然语言模板构建，结合大语言模型进行语言质量优化，并利用知识图谱中的实体设计干扰项，有效提升了问答对的生成质量与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答生成方法在可扩展性、语言质量和事实一致性方面存在不足，亟需一种高效且高质量的生成方案。

Method: 首先基于关系对知识图谱三元组进行聚类，生成基于实体类型和关系的自然语言模板；然后利用大语言模型对模板进行精细化润色，提升语言流畅性和清晰度；最后通过从知识图谱中选择相关实体作为干扰项，完成答案选项的实例化。

Result: 实验表明，该混合方法能够高效生成高质量的问答对，在保持事实准确性的同时显著提升语言流畅性和表达精度，兼具可扩展性与实用性。

Conclusion: 所提出的混合生成框架在生成规模、语言质量和事实一致性方面表现优异，为教育平台、知识传播工具及大语言模型的训练提供了可靠的数据支持。

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [136] [iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference](https://arxiv.org/abs/2511.11306)
*Wei Fan,JinYi Yoon,Bo Ji*

Main category: cs.CL

TL;DR: iMAD 是一种智能的多智能体辩论框架，通过选择性触发辩论来提高效率和准确性。它利用单个智能体的结构化自我批评生成41个可解释的语言与语义特征，结合轻量级分类器和焦点校准损失（FocusCal），判断是否需要启动多智能体辩论，从而在减少高达92%的令牌消耗的同时，提升最多13.5%的答案准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法对每个查询都触发多智能体辩论，导致计算成本高且可能破坏原本正确的答案。因此需要一种更高效的机制，仅在必要时才启用辩论，以提升整体性能与资源利用率。

Method: iMAD首先让单个大语言模型生成结构化的自我批评响应，从中提取41个可解释的语言和语义特征（如犹豫信号）；然后使用基于焦点校准损失（FocusCal）训练的轻量级辩论决策分类器，判断是否应触发多智能体辩论，实现无需针对测试数据调优的鲁棒决策。

Result: 在六个视觉问答数据集上进行的大量实验表明，iMAD相比五个基线方法，在减少最高达92%的令牌消耗的同时，显著提升了最终答案的准确率，最高提升达13.5%。

Conclusion: iMAD通过智能选择性触发辩论，在保持甚至提升准确率的前提下大幅降低计算开销，是一种高效、通用且可推广的多智能体推理框架。

Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).

</details>


### [137] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 提出了一种名为层自适应集成微调（LAET）的新策略，通过选择性地微调预训练大语言模型中最具表现力的层，同时冻结不关键层，显著降低计算开销并提升任务特定性能。该方法在金融自然语言处理任务中表现出色，即使使用较小的模型（约30亿参数），也能超越现有基准和GPT-4等先进模型，推动了金融NLP研究与实际部署之间的结合。


<details>
  <summary>Details</summary>
Motivation: 现有大型金融语言模型虽然性能优异，但其高计算需求限制了广泛部署。因此，亟需一种高效、低资源消耗的方法来实现高性能金融NLP应用。

Method: LAET通过分析隐藏状态表示，识别并选择性微调对任务贡献最大的模型层，其余层保持冻结，从而减少计算成本，提高效率与性能。

Result: LAET在多种金融NLP任务中表现优异，优于现有基准和主流模型如GPT-4，且在小规模模型上也展现出强大竞争力。

Conclusion: LAET为金融领域提供了高效、可扩展的模型优化方案，有效平衡了性能与计算资源，促进了先进NLP技术在真实场景中的落地应用。

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [138] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: NOVA is an agentic framework for digitized histopathology analysis that converts scientific queries into executable Python code pipelines using 49 domain-specific tools and ad hoc tool creation. It is evaluated on SlideQuest, a 90-question benchmark requiring multi-step reasoning and computational problem solving, verified by pathologists. NOVA outperforms coding-agent baselines and enables pathologist-verified discovery linking morphology to PAM50 subtypes.


<details>
  <summary>Details</summary>
Motivation: Digitized histopathology analysis is complex and requires specialized expertise, limiting accessibility. There is a need for automated, scalable systems that can translate scientific questions into actionable analysis workflows.

Method: NOVA uses an agentic framework to iteratively generate and execute Python code, leveraging 49 open-source-based domain-specific tools and dynamically creating new tools as needed. SlideQuest, a benchmark with 90 pathologist-verified questions, evaluates the system's ability to perform multi-step reasoning and computational problem solving.

Result: NOVA surpasses existing coding-agent baselines in quantitative evaluation. A case study confirmed by pathologists successfully linked histological morphology to prognostically relevant PAM50 breast cancer subtypes, demonstrating its potential for scalable biomedical discovery.

Conclusion: NOVA represents a significant advance in automating histopathology analysis by enabling non-experts to perform complex, hypothesis-driven analyses through natural language queries, with strong potential for accelerating biomedical research and clinical discovery.

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [139] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: 本文介绍了多领域人工智能生成文本检测（M-DAIGT）共享任务，旨在跨新闻和学术写作等领域检测AI生成文本。该任务包含两个二分类子任务：新闻文章检测（NAD）和学术写作检测（AWD），并发布了包含3万条样本的大规模平衡数据集，涵盖多种大语言模型（如GPT-4、Claude）生成的内容。共有46支队伍参与，其中4支提交了最终结果，本文总结了这些团队的方法并展望了未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成文本的流畅性不断提高，其对信息真实性和学术研究构成威胁，亟需有效检测方法以保障内容可信度。

Method: 构建了一个大规模多领域基准数据集，涵盖人类与AI生成文本，并组织共享任务，评估不同团队在新闻文章和学术写作中的检测能力。

Result: 4支参赛团队均完成了两个子任务，展示了多种检测方法的有效性，为后续研究提供了参考。

Conclusion: M-DAIGT任务成功推动了多领域AI生成文本检测的研究，未来可进一步扩展至更多领域和更复杂的生成模式。

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [140] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 语言模型（LMs）在学习有实际语言数据支持的语言时表现良好，但在学习结构不自然的不可能语言时，并非普遍困难，而是因为这些语言更复杂或随机。与人类婴儿相比，语言模型缺乏支持语言习得的人类归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在学习真实语言与虚构不可能语言时的表现差异，揭示其与人类语言习得机制的根本区别。

Method: 综述现有文献并分析语言模型在学习不同语言类型时的表现，比较其与人类婴儿的学习模式。

Result: 语言模型在学习许多不可能语言时表现与真实语言相当，表明其学习困难主要源于语言的复杂性而非结构的非自然性；这反映了语言模型缺乏人类特有的语言习得归纳偏置。

Conclusion: 语言模型的学习能力受限于其缺乏人类语言习得中的归纳偏置，因此对复杂或随机语言的困难并非源于语言结构的不可能性，而是其内在复杂度所致。

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [141] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook is an open catalogue linking shadow library metadata with Goodreads data, creating a high-precision corpus of over 539,000 English-language books with rich bibliographic and popularity metrics. It prioritizes machine-readable EPUB files, addresses biases in traditional corpora, includes multilingual datasets, and evaluates linkage accuracy while ensuring legal compliance for research under EU and US frameworks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional digital corpora like HathiTrust by leveraging crowd-sourced shadow libraries, enabling more accurate and comprehensive computational social science and cultural analytics through high-quality, structured, and diverse bibliographic data.

Method: Linking metadata from shadow libraries (e.g., Library Genesis, Z-Library) with structured bibliographic data from Goodreads; prioritizing natively digital EPUB files for machine readability; validating linkage accuracy; releasing all data openly.

Result: A corpus of over 539,000 English-language books with publication dates, genres, ratings, and reviews; secondary datasets in French, German, and Spanish; demonstrated high linkage accuracy and legal permissibility for text and data mining in research.

Conclusion: MajinBook provides a scalable, open, and legally defensible resource for digital humanities and cultural analytics, offering a robust alternative to conventional corpora by harnessing the breadth and quality of shadow libraries.

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [142] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 本文提出了一种主动式助听系统，能够自动识别并分离佩戴者对话对象，无需用户手动操作。系统基于第一人称双耳音频，利用佩戴者自言自语作为锚点，结合对话轮换行为和对话动态推断对话伙伴，并抑制其他声音。为实现低延迟的实时本地运行，设计了双模型架构：轻量级流式模型每12.5毫秒运行一次以提取对话伙伴，较慢模型则较少运行以捕捉长期对话动态。在真实世界2人和3人对话数据集上测试，涵盖11名参与者共6.8小时的双耳第一人称数据，验证了系统在多对话场景中对对话伙伴的识别与隔离能力。该研究推动了能够主动适应对话动态和参与度的助听辅助技术发展。


<details>
  <summary>Details</summary>
Motivation: 现有助听设备通常依赖用户主动指令来调整音源，无法自动适应复杂对话环境。为提升用户体验，亟需一种能主动感知并分离对话伙伴的智能助听系统，尤其在多人同时对话场景下更显重要。

Method: 系统基于第一人称双耳音频，利用佩戴者自身语音作为锚点信号，通过分析对话轮换模式和交互动态，推断当前对话伙伴。采用双模型架构：轻量级流式模型每12.5毫秒运行一次，用于快速检测对话者；另一较慢模型定期运行，捕获长时序对话特征。整体设计支持实时、本地化部署。

Result: 在包含11名参与者、总计6.8小时的真实世界双耳第一人称数据上，系统在2人和3人对话场景中均表现出良好的泛化能力，能有效识别并分离出主要对话伙伴，在多对话环境中展现出稳定性能。

Conclusion: 本研究提出了一种无需用户提示即可主动识别对话伙伴的助听系统，通过融合自语音锚点与对话动态建模，实现了对多对话场景的有效处理。该系统为下一代主动式、自适应助听辅助设备的发展提供了新方向。

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [143] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: PRBench is a large-scale, open-ended benchmark for evaluating AI models on real-world legal and financial tasks, created with input from 182 professionals. It includes 1,100 expert-authored tasks and 19,356 curated criteria, covering diverse jurisdictions and domains. Evaluation of 20 leading models shows low performance (0.39 in Finance, 0.37 in Legal), revealing gaps in accuracy, transparency, and reasoning. Models with similar overall scores vary significantly across capabilities, highlighting key failure modes like flawed judgments and incomplete reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to assess AI models on open-ended, economically significant tasks in high-stakes professional domains like Law and Finance. There is a need for realistic, rubric-based evaluations that reflect actual professional workflows and decision-making.

Method: The study introduces PRBench, a benchmark built by recruiting 182 qualified professionals (JDs, CFAs, or with 6+ years of experience) to create tasks based on real-world workflows. Tasks span 114 countries and 47 US jurisdictions. Rubrics are validated through independent expert review. Models are evaluated using these tasks and rubrics, with analysis of economic impact and human-annotated categories.

Result: Top-performing models achieve only 0.39 (Finance) and 0.37 (Legal) on the Hard subsets of PRBench. Performance varies significantly even among models with similar overall scores. Common failures include inaccurate judgments, lack of process transparency, and incomplete reasoning.

Conclusion: Current frontier models fall short in handling complex, real-world professional tasks in law and finance. PRBench reveals critical gaps in reliability and reasoning, underscoring the need for improved model design to support trustworthy deployment in high-stakes domains.

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: 该论文研究长尾分布下参数高效微调（PEFT）方法在头类和尾类之间的性能权衡问题，发现其在尾类上表现好但牺牲了头类准确率，并识别出头尾类别比例（η）是关键影响因素。为此提出LT-Soups框架，通过两阶段模型平均与分类器微调，有效平衡不同长尾场景下的性能，实验表明其优于现有PEFT和传统模型汤方法。


<details>
  <summary>Details</summary>
Motivation: 在长尾分布数据中，现有参数高效微调方法虽能保持尾类性能，但会降低头类准确率，且头尾类别比例对性能有显著影响，但未被充分关注。因此需要一种能适应多种长尾分布的通用解决方案。

Method: 提出LT-Soups：第一阶段在平衡子集上训练多个模型并进行模型平均以减少头类偏差；第二阶段仅微调分类器在全数据集上，恢复头类性能。

Result: 在六个基准数据集上的实验表明，LT-Soups在不同长尾分布下均实现了比PEFT和传统模型汤更优的性能权衡，尤其在尾类密集和头类主导场景中表现突出。

Conclusion: LT-Soups是一种有效的两阶段模型集成方法，能够自适应多种长尾分布，克服现有PEFT方法的局限性，在保持尾类性能的同时提升头类准确性，具有良好的泛化能力。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [145] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 本文提出两种新方法，通过利用线性预测聚类（LPC）中可分离性的理论特性，改进了全局优化的效率。基于约束优化范式，研究将LPC问题简化为更高效的混合整数规划（MIP）近似，并进一步转化为二次伪布尔优化（QPBO）问题，在保持近似最优解的同时显著提升计算效率。实验表明，新方法在合成与真实数据集上均优于传统贪心算法和现有MIP方法，具有更低的回归误差和更强的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有线性预测聚类（LPC）方法中，贪心优化虽高效但缺乏全局最优性，尤其在非可分簇情况下表现不佳；而基于混合整数规划（MIP）的全局优化方法虽保证最优性，却存在可扩展性差的问题。因此，亟需在保持全局最优性的同时提升计算效率。

Method: 基于约束优化框架，利用可分离性理论推导出近似模型，降低原MIP问题复杂度；进一步将问题转化为二次伪布尔优化（QPBO）形式，实现计算加速。同时提供可证明的误差界，确保近似解质量。

Result: 所提方法在多种合成与真实数据集上均取得近似最优解，回归误差显著低于贪心方法，且在计算效率上远超传统MIP方法，展现出优异的可扩展性。

Conclusion: 本研究成功构建了高效且可扩展的全局优化框架用于线性预测聚类，通过理论分析与有效近似，平衡了精度与效率，为复杂数据场景下的聚类建模提供了强有力的新工具。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [146] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 该研究探讨了Transformer模型在预测长柯拉茨步骤时的表现，发现模型准确率随输入输出编码基数的变化而变化，最高可达99.7%（如基数24、32），最低仅25%（如基数3）。尽管如此，所有模型均表现出一致的学习模式：随着训练进行，模型逐步学会预测具有相同模 $2^p$ 余数的输入类别，对这些类别的预测接近完美，而对其他输入的准确率低于1%。这对应于柯拉茨序列的一个数学特性——可通过输入的二进制表示推断出计算中涉及的循环长度。模型学习过程实质是识别不同循环长度的输入。失败案例分析显示，绝大多数错误源于循环长度估计失误，而非幻觉，且错误具有可预测性。研究揭示了模型所学算法的本质，表明学习此类复杂算术函数的难点在于理解其控制结构（即循环长度）。该方法为理解、解释和改进语言模型提供了新路径，适用于广泛数学问题。


<details>
  <summary>Details</summary>
Motivation: 探究深度学习模型（特别是Transformer）在处理复杂算术函数（如柯拉茨函数）中的学习机制与局限性，通过数学问题作为工具来揭示模型内部行为，进而提升对模型推理能力的理解。

Method: 使用不同进制编码输入和输出，训练Transformer模型预测长柯拉茨步骤；通过分析模型在不同基数下的表现、学习轨迹及失败案例，结合柯拉茨序列的数学性质，解析模型实际学习到的规则与模式。

Result: 模型准确率受编码基数影响显著，但所有模型均呈现相同的渐进式学习模式：逐步掌握特定模 $2^p$ 的输入类别，预测准确率接近100%；其余输入预测误差极低（<1%）。失败主要源于循环长度估计错误，而非幻觉，且错误模式高度可预测。

Conclusion: 模型学习的核心是识别输入对应的柯拉茨序列中循环长度，其难点在于理解控制结构而非算术运算本身。该研究验证了以数学问题为工具理解语言模型的有效性，具有推广价值。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [147] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 本文研究了基于Transformer的神经算子在更广泛的迁移学习设置中的表现，评估其在多种偏微分方程（PDE）问题上的性能，包括未见参数的外推、新变量的引入以及从多方程数据集的迁移。结果表明，先进的神经算子架构能够有效跨PDE问题传递知识。


<details>
  <summary>Details</summary>
Motivation: 神经算子在数据驱动的物理模拟中广泛应用，但其训练过程仍计算成本高昂。为降低训练开销，近期研究采用下游学习策略，即先在简单问题上预训练模型，再在复杂问题上微调。本文旨在探索基于Transformer的神经算子在更通用的迁移学习场景下的适用性与有效性。

Method: 采用基于Transformer的神经算子架构，在多个不同类型的偏微分方程（PDE）问题上进行实验，涵盖参数外推、新增变量以及跨多方程数据集的迁移任务，系统评估其迁移学习能力。

Result: 实验结果表明，先进的神经算子架构能够在多种复杂的PDE问题之间实现有效的知识迁移，具备良好的泛化能力和适应性，验证了其在广泛物理模拟场景中的潜力。

Conclusion: 基于Transformer的神经算子可在多样化的偏微分方程问题中实现有效的知识迁移，支持其在复杂物理系统建模中的广泛应用，为降低训练成本和提升模型泛化能力提供了新路径。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [148] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 本文提出了一种高效的变分量子核框架，结合参数缩放技术，在八组高维真实世界数据集上展示了量子核方法相较于经典核（如RBF核）的显著性能优势，验证了其在复杂分类任务中的潜力，为量子增强机器学习应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前量子核方法的研究多局限于低维或合成数据，缺乏对真实世界高维数据的充分评估，因此亟需开发更实用的量子核框架以验证其实际优势。

Method: 提出一种基于资源高效电路结构的变分量子核框架，并引入参数缩放技术以加速收敛，同时在多种类型的真实数据（表格、图像、时间序列、图数据）上进行系统性测试。

Result: 在八组高维真实数据集上的类比模拟结果显示，所提量子核方法在分类性能上明显优于经典核方法，如RBF核，展现出良好的泛化能力与实用性。

Conclusion: 经过合理设计的量子核可作为通用且高性能的机器学习工具，具备在真实世界场景中实现量子优势的潜力，但进一步研究仍需开展以全面评估其实用量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [149] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: 提出SurfaceBench，首个面向符号曲面发现的综合性基准，包含183个跨15类复杂度的任务，涵盖显式、隐式和参数化方程形式，数据源自三维合成样本，强调科学领域接地与几何感知评估。相比现有基准，其设计可抵抗大语言模型记忆化，引入Chamfer和Hausdorff距离等几何度量，提升对科学等价性的捕捉能力。实验表明当前先进框架在跨表示类型与复杂度上泛化能力不足，该基准为符号推理与几何重建的融合提供诊断性测试平台，推动组合泛化、数据驱动科学归纳及几何感知推理的研究。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归基准存在局限：仅关注标量函数、缺乏领域接地、依赖脆弱的字符串匹配评估，导致大语言模型易依赖记忆而非真正理解；同时，复杂曲面现象（如流体动力学、电磁场）的符号发现缺乏系统性评测工具，亟需更贴近真实科学问题的基准来推动模型在几何结构与代数表达上的协同理解。

Method: 构建SurfaceBench基准：生成183个任务，覆盖多种方程形式（显式、隐式、参数化），每项任务配有真值方程、变量语义及三维合成数据；通过新颖的符号组合设计防止记忆；采用符号验证结合几何感知指标（如Chamfer、Hausdorff距离）进行综合评估，兼顾代数正确性与空间重建精度。

Result: 主流符号回归框架在特定方程族中表现尚可，但在跨表示形式和表面复杂度上普遍缺乏泛化能力；该基准揭示了当前方法在组合泛化与几何一致性方面的显著短板，验证其作为挑战性诊断测试平台的有效性。

Conclusion: SurfaceBench为符号曲面发现提供了首个全面、科学接地且几何敏感的基准，有效推动大语言模型在科学发现中的符号推理与几何感知能力发展，是衡量数据驱动科学归纳进展的关键基础设施。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [150] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 本文提出了一种区分外在与内在幻觉的评估框架，并引入基于注意力的不确定性量化方法，通过新型注意力聚合策略提升幻觉检测性能。实验表明，采样方法对检测外在幻觉有效，但对内在幻觉效果差；而本文方法在处理内在幻觉方面表现更优，凸显注意力机制作为模型不确定性信号的价值。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键领域应用日益广泛，但易产生幻觉。现有方法多依赖计算成本高的采样策略，且未区分幻觉类型，导致检测效果受限。因此需要更精细的评估框架和针对性的检测方法。

Method: 构建区分外在与内在幻觉的评估框架；采用基于注意力的不确定性量化算法，设计新的注意力聚合策略以增强可解释性与检测性能。

Result: 采样方法（如语义熵）适用于外在幻觉检测，但在内在幻觉上表现不佳；本文提出的注意力聚合方法在内在幻觉检测中显著优于现有方法，证明注意力是量化模型不确定性的有效信号。

Conclusion: 应根据幻觉类型匹配相应的检测策略，注意力机制为提升幻觉检测性能提供了新方向，有助于实现更安全、可靠的LLM部署。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [151] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath是一种新型方法，通过可逆神经流学习控制路径的几何结构，以适应稀疏且不规则采样的时间序列数据。与传统固定插值方法不同，FlowPath能够构建连续、数据自适应的流形，通过可逆性约束确保信息保留和良好行为，显著提升分类准确率，在18个基准数据集和真实案例研究中均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经控制微分方程对控制路径的选择极为敏感，而传统固定插值方法在高缺失情况下常因过于简化的几何假设而误判数据流形。因此需要一种能自适应学习路径几何并保持信息完整性的方法。

Method: 提出FlowPath，利用可逆神经流学习控制路径的几何结构，通过可逆性约束实现信息保真和稳定变换，使路径不仅连接观测点，还建模其内在流形结构。

Result: 在18个基准数据集和一个真实世界案例研究中，FlowPath在分类准确率上显著优于使用固定插值或非可逆架构的基线模型，验证了路径几何建模的重要性。

Conclusion: 建模时间序列不仅需关注动态过程，还需考虑路径本身的几何特性；FlowPath通过可逆神经流实现了对路径几何的数据自适应学习，为不规则时间序列提供了稳健且通用的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [152] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出一种基于低方差回报估计的在线强化学习方法，利用行为策略收集数据以实现更低方差的回报估计，突破了传统认为在策略数据最优的观念。通过扩展两种策略梯度方法，在多个环境中验证了其更高的样本效率和性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法依赖回报估计进行策略改进时，常因高方差回报估计导致样本效率低下和训练不稳定。现有研究发现，设计良好的行为策略可生成方差更低的离线回报估计，但该结果尚未被应用于在线学习场景。

Method: 将离线评估中关于低方差回报估计的理论扩展至在线强化学习设置，仅使用单一行为策略收集数据，并通过改进策略梯度方法实现更优的回报估计与策略更新。

Result: 在多种环境下的实验表明，所提方法相较于基线方法具有更高的样本效率和更强的性能表现。

Conclusion: 即使在单个行为策略的在线学习框架下，通过合理设计行为策略仍可实现比在策略数据更低的回报估计方差，从而显著提升学习效率与稳定性。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [153] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 本文提出一种名为STAMP的时空适配器，利用通用时间序列基础模型（TSFM）生成的单变量嵌入，隐式建模脑电图（EEG）数据的时空特性，在多种临床任务上表现接近最先进的EEG专用基础模型（EEGFM）。该方法参数量少、输入灵活，可有效结合通用TSFM进行EEG建模。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽已开发出针对脑电图（EEG）数据的专用基础模型（EEGFM），但缺乏与通用时间序列基础模型（TSFM）在EEG任务上的系统性比较。为填补这一空白，本文旨在探索如何通过轻量级适配器将通用TSFM应用于EEG建模，以实现高性能且高效的解决方案。

Method: 提出Spatial-Temporal Adapter with Multi-Head Pooling（STAMP），利用通用TSFM提取的单变量嵌入，通过多头池化机制显式捕捉EEG信号的时空结构特征，并实现对多源异构EEG数据的有效建模。

Result: 在8个基准临床任务数据集上，使用STAMP适配的通用TSFM在分类性能上达到与当前最优EEGFM相当的水平；同时，其可训练参数显著减少，具备更强的灵活性和部署优势。

Conclusion: STAMP证明了通用时间序列基础模型通过合理适配即可在EEG任务中实现优异性能，为降低专用模型开发成本、提升模型复用性提供了有效路径。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [154] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM提出一种新的代码选择算法，通过向LLM查询两种新类型的查询（成对成员查询和成对等价查询），在锦标赛机制下识别正确程序，对LLM错误具有鲁棒性。在四个主流代码数据集上，其pass@1平均提升13.0%，最高达27.1%，并使复杂推理的LLM pass@1提升24.0%。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法在识别正确程序时存在局限，可能误判非等价程序，或过度依赖LLM对所有输入输出的正确判断，导致性能受限。

Method: 提出ExPairT-LLM，引入成对成员查询和成对等价查询，通过锦标赛方式筛选最优程序，增强对LLM错误的鲁棒性。

Result: 在四个代码数据集上，ExPairT-LLM的pass@1平均优于当前最佳算法13.0%，最高提升27.1%；同时使复杂推理的LLM pass@1提升24.0%。

Conclusion: ExPairT-LLM通过设计更简单的查询类型和稳健的筛选机制，显著提升了代码选择的准确性和鲁棒性，为基于LLM的代码生成提供了有效解决方案。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [155] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 本文提出了一种利用公共数据辅助的零阶优化方法（PAZO），以改善私有零阶算法的梯度近似，从而在保证隐私的同时提升模型性能。该方法在视觉和文本任务中均表现出色，显著优于现有的一阶差分隐私基线，在高度隐私场景下尤其突出，并实现了高达16倍的运行速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶差分隐私机器学习算法（如DP-SGD）存在计算和内存开销高的问题；尽管零阶方法因使用函数值近似梯度而更易实现隐私保护，但其性能仍低于一阶方法，且应用范围有限。因此，需要一种高效且高精度的零阶方法来克服上述瓶颈。

Method: 提出公共数据辅助的零阶优化框架（PAZO），通过利用公共数据引导和改进私有数据上的梯度估计，减少近似误差，同时保持极低的额外计算开销。理论分析基于公共与私有数据相似性的假设。

Result: PAZO在图像和文本任务的预训练与微调场景中均表现出优越的隐私-效用权衡，特别是在高度隐私设置下超越最优一阶基线，且相比传统方法最多提速16倍。

Conclusion: PAZO通过有效利用公共信息，显著提升了零阶差分隐私优化器的实用性和效率，为高隐私要求下的机器学习部署提供了可行方案。

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [156] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 针对代码大模型训练数据不平衡问题，本文提出GO UT Bench基准数据集，包含5264对Golang代码与单元测试，覆盖10个开源仓库。通过在两种LLM架构上微调，结果表明微调模型在超过75%的任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有训练数据过度偏向开源代码，导致模型在真实开发流程（如单元测试生成）中表现不佳，尤其在低资源语言如Golang中更为明显。

Method: 构建GO UT Bench数据集，从10个宽松许可的Golang仓库中收集代码与单元测试对，并在混合专家和密集解码器两类LLM上进行微调评估。

Result: 微调后的模型在超过75%的基准任务上表现优于原始模型，验证了该数据集的有效性。

Conclusion: GO UT Bench有效缓解了训练数据不平衡问题，显著提升了代码大模型在真实开发任务中的表现，尤其适用于低资源语言场景。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [157] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出一种物理信息深度学习框架PI-MJCA-BiGRU，通过多关节交叉注意力（MJCA）和双向门控循环单元（BiGRU）模块，从运动学数据直接估计多关节系统的肌肉激活与力，无需标签数据即可实现生理一致性预测，并显著提升关节间协调建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高且缺乏高质量的多关节标注数据，限制了其在临床评估和辅助设备控制中的应用。

Method: 采用多关节交叉注意力（MJCA）模块与双向门控循环单元（BiGRU）层，结合嵌入多关节动力学、关节耦合及外部力交互的损失函数，构建物理信息深度学习框架。

Result: 实验表明，该方法在无标签数据情况下性能可媲美监督方法，且MJCA模块显著提升了关节间协调建模效果。

Conclusion: 所提PI-MJCA-BiGRU框架实现了高效、准确的肌肉激活与力估计，具备良好的生理一致性与时间效率，适用于多关节系统建模。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [158] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出SPP-FGC，一种基于局部结构图的联邦图聚类方法，解决隐私与性能之间的权衡问题。通过在客户端构建私有结构图并由服务器聚合形成全局图，实现高效且隐私保护的聚类。提供单轮通信（SPP-FGC）和迭代优化（SPP-FGC+）两种模式，显著提升聚类准确率，最高达10%（NMI），同时保证可证明隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦聚类方法面临性能与隐私的权衡：传输嵌入表示存在数据泄露风险，仅共享聚类原型则降低模型精度。亟需一种既能保护隐私又能保持高精度的新方法。

Method: 提出SPP-FGC框架，利用客户端本地构建的结构图作为知识共享媒介；服务器对各客户端的结构图进行安全聚合与对齐，生成全局图以获得统一聚类结构。支持单轮通信（SPP-FGC）与多轮迭代优化（SPP-FGC+）两种模式。

Result: 实验表明，SPP-FGC在聚类准确率上相比现有联邦基线提升最高达10%（NMI），同时具备可证明的隐私保护能力。

Conclusion: SPP-FGC通过结构图驱动的知识共享机制，有效解决了联邦聚类中隐私与性能的矛盾，实现了高性能与强隐私保障的统一，适用于多种场景下的分布式聚类任务。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [159] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 本文研究了边缘推理中的级联贝叶斯模型变体，分析了四种决策策略（探索后承诺、动作消除、下置信界和汤普森采样），并提供了严格的理论后悔保证。结果表明，探索后承诺和动作消除因固定排序而表现不佳，而下置信界和汤普森采样由于持续更新决策，实现了常数级别的后悔，表现出更强的适应性。仿真验证了理论结果，强调了适应性在不确定环境下高效边缘推理中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 解决边缘推理中因模型精度与错误概率不确定性带来的决策挑战，优化推理模型的选择效率。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)、Thompson Sampling，提供理论后悔分析，并通过仿真验证。

Result: Explore-then-Commit 和 Action Elimination 因固定排序导致次优后悔；而 LCB 与 Thompson Sampling 通过动态调整实现常数级后悔，表现更优。

Conclusion: 在边缘推理场景下，具有持续适应能力的策略（如 LCB 与 Thompson Sampling）能显著提升性能，证明了适应性在不确定性环境中的重要性。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [160] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 本文提出一种新型模型合并方法，通过将模型修正视为最优线性变换，实现无需离线优化的实时偏好控制模型生成。该方法以单步计算替代传统复杂优化过程，显著降低计算成本并提升性能与用户偏好的对齐精度。


<details>
  <summary>Details</summary>
Motivation: 现有可控模型合并方法依赖耗时的离线多目标优化，其复杂度随任务数量呈指数增长，难以高效扩展。为解决这一问题，需设计更高效的在线合并机制。

Method: 将参数空间优化转化为最终表示的直接修正，建模为最优线性变换，获得闭式解，实现单步、架构无关的快速模型生成。

Result: 实验表明，该方法在生成更优的帕累托前沿方面表现卓越，偏好对齐更精确，且计算开销大幅降低。

Conclusion: 本方法实现了高效、可扩展、灵活的可控模型合并，突破了传统离线优化的瓶颈，为多任务模型部署提供了新范式。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [161] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: This study examines how data quality issues impact ML models in credit risk assessment. Using controlled corruption via the Pucktrick library, it tests 10 models and finds varying robustness levels. Results support improved data pipeline design and offer a reusable framework for future research.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate how common data quality issues—such as missing values, noisy attributes, outliers, and label errors—affect the predictive accuracy of machine learning models in credit risk assessment.

Method: The study uses an open-source dataset and introduces controlled data corruption via the Pucktrick library. It evaluates the robustness of 10 popular ML models, including Random Forest, SVM, and Logistic Regression, under various data degradation conditions.

Result: Experiments reveal significant differences in model robustness depending on the type and severity of data quality issues. Some models are more resilient to certain types of corruption than others.

Conclusion: The proposed methodology and tools provide practical guidance for improving data pipeline robustness and offer researchers a flexible framework for studying data-centric AI problems.

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [162] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的无监督鲁棒域适应（URDA）范式，旨在解决传统无监督域适应（UDA）方法在面对对抗攻击时鲁棒性不足的问题。通过分析对抗训练（VAT）在UDA中失效的原因，揭示了通用UDA+VAT范式中的内在纠缠挑战，并首次建立了URDA的泛化界理论。为此，作者提出了DART算法——一种两阶段训练方法：先预训练一个任意的UDA模型，再通过解耦蒸馏进行即时鲁棒性增强。实验表明，DART在多个基准数据集上有效提升了模型的鲁棒性，同时保持了良好的域适应能力，验证了所提范式与理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应（UDA）方法注重知识迁移能力，但忽视了对对抗攻击的鲁棒性；尽管对抗训练（VAT）能提升模型鲁棒性，但在UDA场景下效果有限，亟需理解其失败原因并构建兼具迁移性与鲁棒性的新范式。

Method: 提出URDA范式，构建其泛化界理论，并设计DART算法——一种两步训练流程：先使用标准UDA方法预训练模型，再通过解耦蒸馏实现快速鲁棒性增强。

Result: DART在四个基准数据集上均表现出更强的对抗攻击防御能力，同时保持了优秀的域适应性能，验证了所提范式与理论的有效性。

Conclusion: 本文首次建立无监督鲁棒域适应（URDA）范式及其理论基础，提出简单有效的DART算法，实现了迁移能力与鲁棒性的协同优化，为未来鲁棒域适应研究提供了新方向。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [163] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 本文提出了一种新的有信息的嵌入初始化策略，用于持续学习的知识图谱嵌入（KGE），通过利用知识图谱模式和先前学习的嵌入来为新实体生成初始表示，从而提升新知识获取效率并减少灾难性遗忘。实验表明该方法能显著提高预测性能、增强知识保留，并加速训练过程，适用于多种KGE模型。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在处理频繁且小规模的知识图谱更新时，嵌入初始化质量对最终嵌入精度和训练时间有显著影响。因此需要一种更有效的初始化策略以提升学习效率与稳定性。

Method: 基于知识图谱的结构信息（如实体类别）和已有嵌入，为新实体生成初始嵌入表示；该策略可无缝集成至现有持续学习框架中。

Result: 实验验证了所提方法在多个KGE模型上的有效性：提升了预测准确率、增强了知识保留能力、减少了训练所需迭代次数与时间。

Conclusion: 所提出的嵌入初始化策略有效支持了知识图谱的持续学习，具有通用性与高效性，适用于不同类型的KGE模型。

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [164] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 本文提出并实证评估了几种在中高维数据集中具有高崩溃点和低计算时间的稳健方法，用于检测银行账户余额中的点异常，适用于约260万条匿名用户每日记录的数据集。


<details>
  <summary>Details</summary>
Motivation: 金融机构需要检测银行账户余额中的点异常以识别潜在欺诈、操作问题或其他异常情况；传统稳健统计方法在高维设置下效率较低且计算成本较高。

Method: 提出并评估多种计算高效的稳健方法，适用于中高维数据集，具有高崩溃点和低计算时间。

Result: 所提出的稳健方法在处理大规模银行账户余额数据时表现出良好的性能，能够有效识别异常值，同时保持较低的计算开销。

Conclusion: 所提出的稳健方法在高维数据环境下具备实用性，为金融领域中的异常检测提供了高效且可靠的解决方案。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [165] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的深度学习框架，用于印度四大城市的短时降水预测（班加罗尔、孟买、德里和加尔各答），采用混合时间分布式CNN-ConvLSTM架构，基于多十年的ERA5再分析数据训练。模型在不同城市使用不同的卷积滤波器数量以优化性能，取得的RMSE分别为：班加罗尔0.21 mm/day，孟买0.52 mm/day，德里0.48 mm/day，加尔各答1.80 mm/day。通过排列重要性、Grad-CAM、时间遮蔽和反事实扰动等可解释性分析，揭示了模型依赖城市特异性变量，并具有从1天到5天不等的预测时效。研究证明了可解释AI可在保持高精度的同时提供透明的降水模式洞察。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在降水预测中常被视为黑箱，限制了其在实际天气预报中的应用。为提升模型的透明度并维持预测精度，亟需开发兼具可解释性与高性能的预报系统，尤其在气候多样化的城市环境中。

Method: 采用混合时间分布式CNN-ConvLSTM架构，针对不同城市分别优化卷积滤波器数量（班加罗尔32，孟买和德里64，加尔各答128），基于多十年的ERA5再分析数据进行训练，并结合多种可解释性技术（如排列重要性、Grad-CAM、时间遮蔽、反事实扰动）进行模型行为分析。

Result: 模型在四个城市均表现出良好性能，RMSE值分别为：班加罗尔0.21 mm/day，孟买0.52 mm/day，德里0.48 mm/day，加尔各答1.80 mm/day。可解释性分析显示模型依赖城市特定变量，预测时效随城市而异，从1天至5天不等。

Conclusion: 本研究展示了可解释人工智能在短时降水预测中的有效性，不仅实现了高精度预报，还提供了对不同城市降水模式的透明化理解，为未来智能气象系统的部署提供了可行路径。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [166] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 本文提出一种基于机器学习的极端热浪预测方法，通过幂均值聚合集合预测显著提升分类性能，尤其在高阈值极端事件预测中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有气候极端事件预测方法在准确性和泛化能力上存在不足，亟需改进以应对日益严峻的热浪风险。

Method: 将天气预报建模为分类问题，采用幂均值非线性聚合集合预测结果，使模型具备生成能力并增强对极端事件的预测精度。

Result: 相比传统平均预测，该方法在预测极端高温事件上表现更优，且在不同量化阈值下具有可调适性，尤其在高极值预测中效果显著提升。

Conclusion: 幂均值聚合方法有效提升了机器学习模型对极端热浪事件的预测能力，具有良好的适应性和应用前景。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [167] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了一种名为Sindy Kalman Filter (SKF) 的新方法，将稀疏性驱动的Sindy算法与控制理论中的卡尔曼滤波（KF）相结合，实现了对复杂时变非线性系统的实时建模与参数估计。该方法将未知系统参数视为状态变量，通过联合框架实现高效、在线的模型识别，尤其在混沌系统和真实飞行数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以在实时场景下准确识别复杂、时变的非线性动力系统。现有Sindy算法虽能提取稀疏模型，但缺乏实时性；而卡尔曼滤波虽适用于实时估计，却难以处理非线性结构与稀疏性。因此需要一种融合两者优势的新方法，以实现高精度、自适应的实时系统建模。

Method: 将系统未知参数作为状态变量纳入卡尔曼滤波框架，结合Sindy算法的稀疏性约束，构建了Sindy Kalman Filter (SKF)。通过引入前瞻误差（look-ahead error），优化了稀疏度、方差参数及切换时刻的估计过程，使模型能在动态环境中持续更新。

Result: SKF在具有漂移或切换参数的洛伦兹混沌系统上验证有效，成功实现了实时建模；并在真实飞行数据上准确识别出稀疏非线性飞机模型，显著优于传统方法。

Conclusion: Sindy Kalman Filter (SKF) 通过融合稀疏性与卡尔曼滤波，实现了对复杂时变非线性系统的高效实时建模，为物理系统在线学习提供了强有力的工具，具备广泛的应用前景。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [168] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 提出一种新的动态深度图学习方法DGIMVCM，用于处理不完整多视图聚类问题。该方法通过构建鲁棒的全局图、设计图卷积嵌入层以提取主特征和动态视图特定图结构，并利用图结构对比学习增强一致性；引入图自注意力编码器结合掩码图重建损失，减少优化过程中的梯度噪声；最后通过伪标签自监督训练优化聚类模块。在多个数据集上的实验验证了其有效性与优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的不完整多视图聚类方法存在两个主要问题：一是依赖KNN构建静态图，易引入噪声降低图拓扑鲁棒性；二是直接使用MSE损失进行图重建，导致优化过程中梯度噪声大。因此需要更鲁棒且高效的图学习与重建机制。

Method: 1. 构建缺失鲁棒的全局图，通过图卷积嵌入层提取主特征与动态视图特定图结构，利用全局图实现缺失视图的补全；2. 引入图结构对比学习，挖掘不同视图间图结构的一致性；3. 设计图自注意力编码器，基于补全后的特征和视图特定图生成高层表示，并采用掩码图重建损失以抑制梯度噪声；4. 通过伪标签自监督训练优化聚类模块。

Result: 在多个真实世界多视图数据集上，DGIMVCM显著优于现有主流方法，在聚类性能上表现更优，验证了其有效性和鲁棒性。

Conclusion: DGIMVCM通过动态图学习与掩码重建损失设计，有效缓解了传统方法中图构建噪声和梯度噪声问题，提升了不完整多视图聚类的性能，为复杂数据场景下的聚类提供了新思路。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [169] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass 是一种针对未知环境中低功耗长距离（LoRa）标签的高效、鲁棒搜索方法，利用强化学习结合空间感知特征提取和策略蒸馏损失，提升在信号波动与环境变化下的定位准确性和效率。其探索机制借鉴上置信界（UCB），引导传感器逐步逼近目标，在真实地面与无人机场景中验证，成功率达90%以上，搜索路径长度与初始距离呈线性关系，相较现有方法提升40%。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索方法在面对领域迁移和信号波动时易产生级联决策错误，导致定位精度下降，亟需更鲁棒且高效的搜索策略以应对复杂未知环境中的LoRa标签定位挑战。

Method: 提出LoRaCompass模型，通过空间感知特征提取器学习鲁棒的空间表示，并结合策略蒸馏损失函数增强对环境变化的适应能力；引入受UCB启发的探索函数，动态引导传感器向标签靠近，提高搜索信心与效率。

Result: 在超过80km²的多样化未见环境中，无论是地面还是无人机辅助场景，LoRaCompass均实现超过90%的定位成功率，可在100米范围内找到标签，相比现有方法提升40%；搜索路径长度与初始距离保持线性增长，表现出优异的效率。

Conclusion: LoRaCompass通过融合鲁棒特征学习与自适应探索机制，在复杂多变的真实环境中实现了高效、精准的LoRa标签定位，为移动传感器在实际应用中提供了一种可靠解决方案。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [170] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 本文提出一种基于生成式AI的零样本合成验证框架，用于联邦学习中的早期停止，以减少不必要的训练轮次并节约计算资源。该方法通过生成合成数据来监控模型性能，自适应地在接近最优轮次时停止训练，在多标签胸部X光分类任务上可将训练轮次减少高达74%，同时保持精度与最优水平相差不超过1%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常运行预设数量的全局轮次，导致在达到最优性能后仍进行不必要的计算；此外，当模型无法获得有意义的性能时，训练仍会继续，造成效率低下。因此需要一种能够高效判断何时停止训练的方法。

Method: 提出一种零样本合成验证框架，利用生成式AI生成合成数据，用于评估模型性能，并据此动态决定是否提前停止训练，实现自适应早期停止。

Result: 在多标签胸部X光分类任务上的实验表明，该方法可将训练轮次减少最多74%，同时保持模型精度与最优结果相差不超过1%。

Conclusion: 所提出的基于生成式AI的零样本合成验证框架能有效实现联邦学习中的早期停止，显著降低计算开销，提升训练效率，并支持快速超参数调优。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [171] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提供了一种简洁的推导方法，用于证明Tsallis-INF多臂赌博机算法在随机与对抗性环境下的最优‘最佳两者’保证。该证明利用现代在线凸优化工具，避免了对共轭函数的依赖，并未优化边界中的常数以追求更简洁的证明。


<details>
  <summary>Details</summary>
Motivation: 为了简化对Tsallis-INF算法的分析，避免使用复杂的共轭函数，同时保持证明的清晰性和简洁性。

Method: 采用现代在线凸优化技术，通过非复杂化的方法推导出最佳两者保证，不进行常数优化。

Result: 成功以更简洁的方式证明了Tsallis-INF算法在随机和对抗性设置下的最优性能保证。

Conclusion: 该方法为理解并分析Tsallis-INF算法提供了更为直观和高效的途径，突显了现代在线凸优化工具在强化学习分析中的有效性。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [172] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个专为分割联邦学习（SFL）设计的统一防御框架，可有效检测并恢复五种复杂数据投毒攻击。其核心包括：基于拓扑异常评分（TAS）的图结构检测模块、生成式修复管道，以及结合语义监督与异常感知信号的对抗性多教师蒸馏框架，显著提升对局部特征、标签、拼接数据及模型权重攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对传统联邦学习设计，在SFL中因无法获取完整模型更新而效果有限，亟需一种专为SFL场景定制的统一防御机制来应对多种复杂数据投毒攻击。

Method: 提出三部分协同架构：1）构建拼接数据的图结构，通过拓扑异常评分识别污染样本；2）利用生成模型合成语义一致的替代样本，并通过一致性验证学生模型确保质量；3）采用多教师蒸馏策略，结合普通教师的语义监督和异常影响去偏教师的异常感知信号，以增强学生模型对攻击的辨识能力。

Result: 在四个基准数据集上的实验表明，HealSplit在各种攻击场景下均显著优于十种先进防御方法，展现出更强的鲁棒性和防御效能。

Conclusion: HealSplit为分割联邦学习提供了首个端到端的统一防御方案，有效解决了SFL中因信息碎片化导致的防御难题，具有重要的理论价值与实际应用前景。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [173] [Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels](https://arxiv.org/abs/2511.11245)
*Hong Huang,Chengyu Yao,Haiming Chen,Hang Gao*

Main category: cs.LG

TL;DR: NASK is a novel graph kernel for attributed graphs that effectively models heterogeneous attributes and multi-scale neighborhood structures, demonstrating superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph kernel methods struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs.

Method: Proposes the Neighborhood-Aware Star Kernel (NASK), which uses an exponential transformation of the Gower similarity coefficient to model numerical and categorical features, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information.

Result: NASK consistently outperforms sixteen state-of-the-art baselines across eleven attributed and four large-scale real-world graph benchmarks.

Conclusion: NASK is positive definite and thus compatible with kernel-based learning frameworks like SVMs; it achieves superior performance in attributed graph learning.

Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.

</details>


### [174] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: EHR-based predictive models outperform traditional risk factors in identifying high-risk individuals for eight major cancers, with 3-6 times higher enrichment of true cancer cases. EHR foundation models further enhance performance across 26 cancer types, showing strong potential for precise and scalable early detection.


<details>
  <summary>Details</summary>
Motivation: Current cancer screening guidelines are limited to a few cancer types and rely on narrow criteria like age or smoking history. There is a need for more effective tools to identify high-risk individuals using comprehensive health data.

Method: The study evaluated EHR-based predictive models using data from the All of Us Research Program, which includes EHR, genomic, and survey data from over 865,000 participants. Models were trained to predict cancer risk across eight major cancers and assessed against traditional risk factors such as gene mutations and family history.

Result: EHR-based models achieved 3- to 6-fold higher enrichment of true cancer cases among high-risk individuals compared to traditional risk factors. The EHR foundation model improved predictive performance across 26 cancer types.

Conclusion: EHR-based predictive modeling has significant clinical potential for improving early cancer detection by enabling more precise and scalable identification of high-risk individuals.

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [175] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 本文提出了一种多保真度机器学习力场框架，通过联合利用低保真度非磁性和高保真度磁性计算数据，提升了锂离子电池正极材料的训练数据效率。该方法在磷酸锰铁锂（LMFP）体系中验证有效，实现了以较低数据成本获得高精度力场模型，为正极材料的计算模拟提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池正极材料具有复杂的电子结构，导致高质量计算数据稀缺，限制了机器学习力场（MLFF）的发展与应用。因此，亟需一种高效的数据利用方法来降低对高成本第一性原理数据的依赖。

Method: 提出一种多保真度机器学习力场框架，同时整合低保真度（非磁性）和高保真度（磁性）计算数据进行联合训练，提升数据利用效率。

Result: 在磷酸锰铁锂（LMFP）材料体系上的测试表明，该方法能够以较少的高保真度数据实现高精度的力场建模，显著降低了训练所需的数据成本。

Conclusion: 该多保真度框架有效提升了锂离子电池正极材料机器学习力场的训练效率与精度，为复杂材料体系的高效计算模拟提供了可行路径。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [176] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文研究了在边缘设备上进行微调时，基于反向传播（BP）和零阶优化（MeZO）方法在内存限制下的模型规模适应性。通过理论分析与数值验证，表明在允许足够微调时间的前提下，MeZO因无需存储中间激活值和优化器状态，可在有限内存下支持更大模型，从而在内存受限场景中具有更高的准确率优势。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下适应不同任务，而传统反向传播方法因需存储中间激活值和优化器状态，严重限制了可部署模型的大小。因此亟需一种更高效的微调方法以突破内存瓶颈。

Method: 提出并分析了内存高效的零阶优化（MeZO）方法，通过前向评估估计梯度，避免存储中间激活和优化器状态；同时提供理论模型估算在BP与MeZO下可容纳的相对模型规模，并通过数值实验验证其有效性。

Result: 数值实验验证了理论分析，表明在有足够微调时间的情况下，MeZO在内存受限环境下能支持更大的模型，从而实现更高的精度表现。

Conclusion: MeZO在边缘设备上具有显著的内存优势，尤其适合在内存受限但可接受较长训练时间的场景中使用，是实现大规模模型边缘微调的有效方案。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [177] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 该论文提出BREP ReFT以解决表示微调（ReFT）在数学推理任务中表现不佳的问题，通过截断训练数据优化初始推理前缀生成、干预早期推理阶段防止误差累积，并限制干预向量幅度以避免破坏数值编码。实验表明BREP在多种模型架构上显著优于标准ReFT和基于权重的PEFT方法。


<details>
  <summary>Details</summary>
Motivation: ReFT虽在多数任务上表现优于PEFT，但在数学推理任务中性能下降，主要因早期推理阶段难以生成有效推理前缀，且导致数值编码干扰与误差累积。

Method: 提出BREP ReFT，包含三方面改进：1）截断训练数据以优化初始推理前缀生成；2）在早期推理阶段进行干预以抑制误差累积；3）约束干预向量的大小以保护数值编码。

Result: BREP ReFT在多个模型架构上的数学推理任务中均显著优于标准ReFT和权重型PEFT方法，展现出更高的有效性、效率与泛化能力。

Conclusion: BREP ReFT通过针对性优化早期推理过程，有效提升了ReFT在数学推理任务中的表现，验证了对推理前缀生成与误差控制的关键作用，为高效微调提供了新思路。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [178] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 本文研究在仅能获取基于上下文的边权重预测值时，如何在加权图中找到最优匹配。提出多校准（multicalibration）作为一种公平性概念，确保预测器在各类保护性上下文集合上无偏。对于任意匹配算法类 $\mathcal C$ 和预测器 $γ$，可构造一个特定的多校准预测器 $\hat γ$，使得基于 $\hat γ$ 输出选择的最佳匹配，与在原始预测器 $γ$ 上应用 $\mathcal C$ 中最优决策规则的表现相当。同时给出了样本复杂度的界限。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，权重预测往往不完美，导致基于预测值的最优匹配决策可能次优。因此需要一种机制来补偿预测误差，提升决策性能。多校准被引入以增强预测器在不同上下文子集上的可靠性，从而改善匹配结果。

Method: 提出并利用多校准框架，设计一种新的预测器 $\hat γ$，该预测器在给定的保护性上下文集合族上保持无偏，并通过理论分析证明其在匹配算法类 $\mathcal C$ 下的竞争力。结合样本复杂度分析，提供可实现性和泛化保证。

Result: 构造的多校准预测器 $\hat γ$ 所导出的匹配策略，在性能上可媲美 $\mathcal C$ 中任何决策规则对原始预测 $γ$ 的使用效果。同时，提供了所需的样本复杂度边界，表明该方法在有限数据下仍具可行性。

Conclusion: 多校准是一种有效的工具，能够将不完美的预测转化为具有竞争性表现的匹配决策。它为处理现实世界中预测误差问题提供了理论支持和实践路径。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [179] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出生成模型学习中不确定性量化问题的正式框架，强调现有评估方法忽视了分布近似中的固有不确定性。通过合成数据集的初步实验，验证了聚合精度-召回率曲线在捕捉模型近似不确定性方面的有效性，并为不同模型架构的不确定性特性提供系统性比较基础。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估主要关注学习分布与目标分布的接近程度，但忽略了测量过程中的不确定性，这影响了对模型可靠性的全面理解。

Method: 提出不确定性量化问题的形式化框架，探索基于集成的精度-召回率曲线作为评估手段，并通过合成数据集进行实验验证。

Result: 聚合精度-召回率曲线能有效捕捉生成模型在分布近似中的不确定性，支持不同模型架构之间的系统性比较。

Conclusion: 不确定性量化是评估生成模型可靠性的重要维度，未来研究应重视其在模型选择与性能分析中的作用。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [180] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 本文提出了一种基于JAX-FEM和自动微分（AD）的可微编程方法，用于解决声学逆问题。通过两个应用展示：一是利用AD直接从稀疏压力测量中估计复杂边界导纳，实现三位数精度且无需手动推导伴随方程；二是结合随机有限差分与PyTorch3D进行声学形状优化，在分离物理驱动边界优化与几何驱动内部网格自适应的基础上，仅用30倍少的FEM求解即可实现目标频率下48.1%的能量衰减。该方法展示了现代可微软件栈在物理逆问题优化流程中的快速原型设计能力。


<details>
  <summary>Details</summary>
Motivation: 传统声学逆问题求解依赖于手工推导伴随方程，计算复杂且易出错；同时，几何优化常需大量有限元模拟，效率低下。亟需一种高效、自动化的方法来支持参数估计与形状优化。

Method: 采用JAX-FEM进行前向仿真并利用其自动微分能力实现梯度计算；对于参数估计使用直接梯度法；对于形状优化，结合随机有限差分与PyTorch3D进行网格操作，通过自动微分实现端到端优化。

Result: 在边界导纳估计中达到3位精度，无需手动推导伴随方程；在形状优化中实现48.1%能量减少，仅需标准有限差分方法30倍少的FEM求解次数。

Conclusion: 现代可微编程工具链（如JAX-FEM与PyTorch3D）能够显著提升物理驱动逆问题的建模效率，实现参数估计与几何优化的自动化与高效化，为复杂声学系统的设计与反演提供了强大新范式。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [181] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文提出一种基于最优传输（OT）的后训练量化方法，用于压缩流匹配（FM）生成模型，以解决其高精度参数需求带来的部署挑战。该方法最小化量化前后权重间的2-Wasserstein距离，并在多个基准数据集上验证其有效性，结果表明该方法可在2-3比特/参数下保持生成质量与潜在空间稳定性，优于均匀、分段和对数量化等传统方法。


<details>
  <summary>Details</summary>
Motivation: 流匹配生成模型虽具备高效训练和确定性采样优势，但其实际部署受限于对高精度参数的依赖，亟需有效的模型压缩技术以支持边缘和嵌入式AI应用。

Method: 采用基于最优传输（OT）的后训练量化方法，通过最小化量化前后权重间的2-Wasserstein距离实现参数压缩，并系统比较其与均匀、分段、对数量化等方法的性能差异。

Result: 在五个不同复杂度的基准数据集上，OT-based量化方法在2-3比特/参数时仍能有效保持生成质量与潜在空间稳定性，而其他方法在此条件下显著退化。

Conclusion: OT-based量化是一种原理严谨且高效的FM模型压缩方法，适用于边缘与嵌入式AI场景下的生成模型部署。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [182] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级融合旧模型与新微调模型作为教师，实现知识迁移。采用低秩稀疏更新以减少干扰，并通过基于模型置信度的知识仲裁动态平衡教师贡献。在恶意软件检测和二进制摘要任务中，显著降低遗忘率，提升性能，优于现有基线与理论上限。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖完整重训练或数据回放，在数据敏感环境中不可行；且在安全关键场景下难以有效处理知识迁移中的遗忘与干扰问题。

Method: 提出RETROFIT方法，通过参数级合并旧模型（教师）与新模型（教师），结合低秩稀疏更新限制参数变化范围，并引入基于置信度的知识仲裁机制动态调节教师贡献。

Result: 在恶意软件检测中，保留分数从20.2%提升至38.6%，超越基线及理论最优值；在二进制摘要任务中，BLEU得分约为先前迁移学习方法的两倍，跨表示泛化能力最强。

Conclusion: RETROFIT实现了无需历史数据的高效持续学习，有效缓解遗忘并保持适应性，适用于高安全性、数据受限的场景。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [183] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一种后训练、硬件忠实的框架，通过联合优化扩散变换器（DiTs）中的时间步和每层精度，在不进行训练的情况下减少延迟和内存占用。它结合了基于流形感知的敏感性度量、动态激活量化和基于教师-学生漂移的预算时间步选择器，实现了高达6.25倍的模型压缩、减少50%的时间步数，并实现2.8倍的推理加速，同时保持Delta FID <= 10，显著提升了实际效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽生成高质量图像，但推理成本高，主要因需大量去噪步骤和复杂的矩阵运算。为降低部署时的延迟与内存开销，亟需一种无需重新训练即可优化推理效率的方法。

Method: DiffPro采用后训练方式，利用实际部署中使用的整数核，通过流形感知敏感性度量分配权重位数，使用动态激活量化稳定各时间步的激活值，并借助教师-学生漂移设计预算化时间步选择器，实现对时间和精度的联合优化。

Result: 在标准基准测试中，DiffPro实现了最高6.25倍的模型压缩，减少50%的时间步数，推理速度提升2.8倍，且保持Delta FID <= 10，验证了其在真实场景下的高效性。

Conclusion: DiffPro成功将时间步缩减与精度规划统一为一个可部署的预算化方案，为实时、节能的扩散模型推理提供了实用高效的解决方案。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [184] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于互信息的度量方法，用于评估回归任务中的公平性分离违规问题，并扩展该方法以适用于分类和回归任务，以及二元和连续敏感属性。受公平分类中重加权算法启发，提出了FairReweighing预处理算法，通过密度估计确保学习模型满足分离标准。理论上证明了在数据独立性假设下，该算法可保证训练数据中的分离性；实证结果表明，在合成与真实数据上，FairReweighing在保持高精度的同时，优于现有最先进的回归公平性解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前AI软件在公共部门和工业领域广泛应用，但其缺乏透明性引发了对不同种族、性别或年龄群体间公平性的担忧。尽管已有大量研究关注公平性感知的AI，但多数集中在二分类任务，回归任务中的公平性研究相对不足。因此需要发展适用于回归任务的公平性评估与优化方法。

Method: 采用互信息基度量评估分离违规，扩展该度量以兼容分类与回归任务及多种敏感属性类型；提出FairReweighing预处理算法，基于密度估计实现数据重加权，使模型满足分离准则。

Result: 理论分析表明，在数据独立性假设下，FairReweighing能保证训练数据中的分离性；实验结果显示，该方法在合成与真实数据集上均优于现有最先进回归公平性方法，在提升分离性的同时维持高准确性。

Conclusion: 本研究为回归任务中的公平性问题提供了有效的评估与解决框架，提出的FairReweighing算法在理论上和实证上均表现出色，推动了公平性AI在更广泛任务中的应用。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [185] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 本文重新审视了多步预测中递归与直接策略的传统观点，通过分解预测误差为不可减少的噪声、结构近似偏差和估计方差三部分，发现线性模型下递归策略的结构偏差为零，而非线性模型中递归的重复组合可能增加模型表达能力，从而影响结构偏差。同时，递归策略的估计方差可通过雅可比矩阵相关的放大因子表示，反映参数误差对预测的影响敏感度。实验在ETTm1数据集上验证了该理论，表明应根据模型非线性程度和噪声特性选择策略，而非依赖传统偏差-方差权衡。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略偏差高但方差低，直接策略则相反，但该结论缺乏严谨的误差分解分析，尤其在非线性模型下是否成立尚不明确。本文旨在通过系统分解多步预测误差，揭示递归与直接策略的真实性能差异，提供更准确的实践指导。

Method: 将多步预测误差分解为三部分：不可减少噪声、结构近似偏差和估计方差；针对线性和非线性预测器分别分析结构偏差的性质；推导递归策略的估计方差表达式，引入雅可比基放大因子衡量参数敏感性；在ETTm1数据集上使用多层感知机进行实验验证理论结果。

Result: 在线性模型中，递归策略的结构偏差恒为零；在非线性模型中，递归策略因重复组合可能提升模型表达力，导致结构偏差依赖于模型和数据；递归策略的估计方差是单步方差乘以雅可比基放大因子；实验表明递归策略可在某些条件下同时具有更低偏差和更高方差；实际选择策略应基于模型非线性与噪声特征。

Conclusion: 传统偏差-方差直觉不足以指导多步预测策略选择。递归与直接策略的优劣取决于模型非线性程度、数据特性及参数敏感性。本文提出的误差分解框架为策略选择提供了更精细的理论依据，有助于提升预测建模的科学性与有效性。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [186] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 本文提出一种纯机器学习方法，通过运动捕捉（MoCap）数据生成多普勒雷达谱图。采用基于Transformer的序列到序列模型，联合建模标记间的空间关系与帧间时序动态，实验证明该方法能生成视觉和定量上合理的雷达谱图，并具有良好泛化能力。消融实验表明模型既可将多部位运动转化为多普勒特征，又理解人体各部分之间的空间关系。该方法适用于边缘计算和物联网雷达，可用于用丰富MoCap数据增强稀缺雷达数据集，且计算量远低于物理模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有物理建模方法生成雷达数据计算成本高，且雷达数据稀缺；而运动捕捉数据丰富但难以直接用于雷达应用。因此需要一种高效、可扩展的方法，将MoCap数据转化为真实可靠的雷达谱图，以支持雷达相关应用的训练与部署。

Method: 将MoCap到雷达谱图的转换建模为窗口化的序列到序列任务，使用基于Transformer的模型，同时捕捉人体标记的空间关系与时间动态，实现端到端映射。

Result: 所提方法生成的雷达谱图在视觉和定量评估中表现良好，具备强泛化能力；消融实验验证了模型对多部位运动转换及人体空间结构的理解能力。

Conclusion: 该研究展示了Transformer在时序信号处理中的有效性，为雷达数据生成提供了一种轻量化、可扩展的新范式，尤其适合边缘计算和物联网场景，同时为雷达数据增强提供了新路径。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [187] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出一种基于输入变换和均匀质量分箱的新分箱方法，以更准确评估置信预测的自适应性。通过改进的分箱策略，引入两个新指标来衡量覆盖率违反情况和平均预测集大小，从而实现更可靠的自适应性评估。基于此，提出一种新的自适应预测集算法，按估计难度分组并应用组条件下的共形预测，以确定每组的合适阈值。在ImageNet图像分类和视觉敏锐度预测等任务上的实验表明，该方法在新指标下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估自适应性的方法因分箱不平衡导致覆盖率或集合大小估计不准确，影响对模型自适应能力的可靠评估。因此需要一种更平衡、更精确的分箱与评估机制。

Method: 提出基于输入变换排序难易度，并采用均匀质量分箱的方法；设计两个新指标用于评估覆盖率违反和平均集合大小；构建一种按难度分组并应用组条件共形预测的新算法，以自适应地设定不同组别的预测阈值。

Result: 新提出的分箱方法和评估指标能更准确反映模型的自适应性；所提出的自适应算法在ImageNet和医疗视觉敏锐度预测任务上均优于现有方法，在新指标下表现更优。

Conclusion: 通过改进分箱策略和引入新评估指标，实现了对共形预测自适应性的更可靠评估。所提出的自适应算法能有效提升预测集的质量，尤其在复杂任务中表现出更强性能。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [188] [Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys](https://arxiv.org/abs/2511.11485)
*Alinda Ezgi Gerçek,Till Korten,Paul Chekhonin,Maleeha Hassan,Peter Steinbach*

Main category: cs.LG

TL;DR: 提出了一种数据高效的分割流程，使用仅10张标注的扫描电镜图像训练轻量级U-Net（30.7M参数），在羰基沉淀物分割任务中达到0.98的Dice-Sørensen系数，显著优于传统图像分析方法（0.85），且标注工作量减少一个数量级。该方法可快速自动化量化羰基沉淀物，适用于合金设计，并能推广至其他钢种。


<details>
  <summary>Details</summary>
Motivation: 在扫描电镜图像中，羰基沉淀物与基体之间的灰度值重叠导致简单阈值法失效，因此需要更精确的分割方法来准确分析微结构以预测材料力学性能。

Method: 采用轻量级U-Net模型，仅使用10张标注的扫描电镜图像进行训练，实现高效、精准的羰基沉淀物分割。

Result: 模型在有限数据下取得0.98的Dice-Sørensen系数，显著优于传统方法（0.85），同时将标注需求降低一个数量级，具备良好的泛化能力。

Conclusion: 数据高效的深度学习方法在反应堆压力容器钢微结构分析中具有巨大潜力，可实现快速、自动化、高精度的羰基沉淀物量化，支持合金设计与跨钢种应用。

Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-Sørensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.

</details>


### [189] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective 提出一种新架构，通过跳过模型中的连接来实现计算与通信的重叠，从而解决分布式环境中 MoE 模型的通信瓶颈。该方法成功将 16B 到 109B 参数的多个前沿大模型（如 Llama 4 Scout）转换为可支持通信重叠的版本，并在多项下游任务中保持与原始模型相当的性能（误差小于 1%）。通过优化实现，显著提升了训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 在分布式设置下，MoE 模型的通信开销严重制约其效率，现有方法难以有效缓解通信阻塞。因此需要一种能将计算与通信重叠的新架构，以提升运行效率。

Method: 提出 FarSkip-Collective 架构，通过在模型中引入跳接机制，使计算过程与通信过程并行执行；采用自蒸馏技术对大规模模型（如 109B 的 Llama 4 Scout）进行微调，确保精度不下降。

Result: 成功将 16B 至 109B 参数的多个先进模型转化为支持通信重叠的版本，在多种下游任务中平均精度仅比原模型低约 1%，同时在现有框架中实现了训练和推理速度的显著提升。

Conclusion: FarSkip-Collective 成功解决了 MoE 模型在分布式环境中的通信瓶颈问题，证明了修改后模型仍具备与原始模型相当的性能，且通过计算与通信重叠显著提升了系统效率。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [190] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文将最近的公平聚类问题从两组扩展到任意数量的组，证明了其在多组情况下的NP难性，并提出了近线性时间的近似算法。此外，还改进了公平相关聚类和公平共识聚类的近似保证，首次为多组情形下的公平共识聚类提供了近似算法，解决了先前研究中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法在多属性保护群体中存在偏差，导致对边缘化群体表示不公平。尽管已有研究关注两组情况下的最接近公平聚类，但现实中数据点通常涉及多个保护属性（如年龄、种族、性别等），因此需要更通用的公平聚类方法。

Method: 通过分析多组情况下最接近公平聚类的复杂性，证明其NP-hard性；设计近线性时间的近似算法处理任意大小的多组数据；利用该算法提升公平相关聚类和公平共识聚类的近似性能。

Result: 提出并实现了适用于任意数量组别的最接近公平聚类近似算法；改进了公平相关聚类的近似比；首次为多组公平共识聚类提供近似算法，推动了该领域的发展。

Conclusion: 本工作系统地拓展了公平聚类理论至多组场景，不仅揭示了问题难度的增加，也提供了高效实用的算法解决方案，为实现更公平的数据分析奠定了基础。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [191] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型中自注意力机制的连续时间多智能体动力学，发现其与计算矩阵主特征向量的Oja流存在关联。通过分析单头自注意力系统的平衡点，将其分为四类：共识、双共识、聚类和多边形平衡点。前三种类型的平衡点常共存且具有渐近稳定性，且前两类通常与值矩阵的特征向量对齐，尤其是主特征向量。


<details>
  <summary>Details</summary>
Motivation: 理解自注意力机制在连续时间下的动力学行为及其与主成分分析之间的联系，揭示其在深度学习中的潜在优化特性。

Method: 采用多智能体系统建模自注意力机制，结合动力学系统理论分析平衡点结构，并利用矩阵特征分析验证其与主特征向量的关系。

Result: 识别出四种类型的平衡点，其中前三类常共存且稳定；前两类平衡点通常与值矩阵的特征向量（特别是主特征向量）对齐。

Conclusion: 自注意力动力学表现出复杂的平衡结构，其行为与矩阵主特征向量密切相关，为理解Transformer的内部机制提供了新的数学视角。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [192] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: 本文提出了一种名为FlashMoBA的高效GPU实现方案，以解决混合块注意力（MoBA）在处理长上下文时因缺乏高效硬件支持而难以实际应用的问题。通过建立统计模型分析MoBA的机制，发现其性能关键在于路由器准确区分相关与无关块的能力，并推导出信号-噪声比公式将架构参数与路由精度关联。基于此，提出两种改进路径：使用更小的块大小和对键应用短卷积以聚类相关信号。尽管小块大小理论上有利，但传统GPU实现效率低；为此，作者设计了硬件感知的CUDA内核FlashMoBA，显著提升小块大小下的计算效率。实验表明，改进后的模型可达到与密集注意力相当的性能，且FlashMoBA相比FlashAttention-2最高提速14.7倍。代码已开源。


<details>
  <summary>Details</summary>
Motivation: MoBA虽能有效降低长序列处理中的计算成本，但其设计原理不清晰且缺乏高效的GPU实现，限制了实际应用。因此亟需理解其内在机制并开发适配硬件的优化方案。

Method: 构建统计模型分析MoBA的路由机制，推导信号-噪声比以连接架构参数与路由准确性；提出小块大小与键端短卷积两种改进策略；设计硬件感知的CUDA内核FlashMoBA，实现小块大小下的高效执行。

Result: 改进后的MoBA模型性能媲美密集注意力基线；FlashMoBA在小块大小下相比FlashAttention-2最高实现14.7倍加速，验证了理论分析的有效性与工程实现的实用性。

Conclusion: 通过理论分析与硬件优化结合，本工作成功使MoBA从理论潜力转化为实用技术，为长上下文LLM提供了高效、可扩展的注意力机制解决方案。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [193] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 该论文研究多目标优化中的帕累托修剪问题，将之重新定义为多赢投票问题，并对现有质量度量进行公理分析，发现一些反直觉行为。为此提出新的度量‘定向覆盖’，并分析不同度量的计算复杂性，揭示了在目标数量和结构下可解与不可解的边界。实验表明，质量度量的选择显著影响解集特性，所提方法表现优异或更具优势。


<details>
  <summary>Details</summary>
Motivation: 减少决策者在众多帕累托最优解中选择时的认知负担，现有方法虽能生成代表性子集，但其质量度量存在未被察觉的不合理行为，亟需更合理的度量机制。

Method: 将帕累托修剪问题建模为多赢投票问题，通过公理化分析评估现有质量度量；提出新度量‘定向覆盖’；分析各类度量的计算复杂性，识别可解与不可解的条件边界；并通过实验验证所提方法的有效性。

Result: 发现了现有度量的若干反直觉行为；提出的‘定向覆盖’度量在多种设置下表现良好甚至优于现有方法；明确了不同度量在不同目标数量和结构下的可计算性边界。

Conclusion: 质量度量的选择对帕累托修剪结果具有决定性影响；新提出的‘定向覆盖’度量有效提升了代表性子集的质量，且具备良好的理论与实践表现。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [194] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了基于团宽（clique-width）的图结构在(Q)SAT编码中的应用，特别针对抽象论证框架中各类论证语义的可解性问题。作者提出了线性保持团宽的新归约方法，即定向分解引导（DDG）归约，并证明该方法的开销在合理假设下无法显著改进。


<details>
  <summary>Details</summary>
Motivation: 由于现代SAT求解器在小团宽实例上表现高效，而团宽适用于稠密图，因此需要理解其在编码中的能力与限制。抽象论证框架具有计算挑战性且基于有向图，是研究此类编码的理想候选。

Method: 设计了从论证问题到(Q)SAT的新归约方法，确保团宽在线性时间内被保留，提出定向分解引导（DDG）归约，并通过理论分析验证其最优性。

Result: 建立了所有论证语义（包括计数）的新结果；证明了所提方法的开销在合理复杂性假设下不可再优化。

Conclusion: 本工作首次系统揭示了团宽在论证问题到(Q)SAT编码中的作用，为未来高效编码提供了理论基础和方向。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [195] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文研究了在不确定性下的反事实决策问题，提出两种新度量：潜在结果排序概率（PoR）和实现最优潜在结果的概率（PoB）。PoR揭示个体最可能的潜在结果排序，PoB表示哪个行动最有可能带来个体最优结果。论文建立了识别定理并推导了这些度量的边界，提出了估计方法，并通过数值实验展示了估计器的有限样本性质及其在真实数据集中的应用。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下进行反事实决策时，传统方法依赖于对潜在结果期望值的比较，但缺乏对结果排序和最优结果达成概率的系统分析。为更全面评估决策行为，需引入新的度量以反映个体层面的偏好结构与最优结果可能性。

Method: 提出并定义了两个新指标：潜在结果排序概率（PoR）和实现最佳潜在结果的概率（PoB）；基于因果模型建立识别条件，推导出这些指标的可识别边界；设计相应的估计方法，并通过模拟实验验证其有限样本表现。

Result: 所提出的度量（PoR和PoB）能够有效刻画个体在不同干预下的潜在结果排序偏好与获得最优结果的可能性；识别定理提供了理论基础，估计方法在模拟中表现出良好性能；真实数据应用验证了其实际可用性。

Conclusion: 本文提出的反事实决策规则通过引入PoR和PoB，增强了对个体决策偏好的刻画能力，为不确定环境下的理性决策提供了新的分析工具，具有理论价值与实践意义。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [196] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本文重新审视了大语言模型（LLM）中的推理问题，提出以‘适应性’为核心视角，即根据任务复杂度和不确定性动态分配推理资源。作者将演绎、归纳和类比推理形式化，并将自适应推理建模为一个权衡性能与计算成本的控制增强型策略优化问题。文章提出了一个系统性分类框架，区分基于训练的方法（如强化学习、监督微调）和无需训练的方法（如提示条件化、反馈驱动终止、模块化组合），并指出了自我评估、元推理和人类对齐控制等开放挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理过程中采用统一策略，对简单任务生成冗长推理链，对复杂任务则无法深入扩展，缺乏根据任务难度和不确定性动态调整推理强度的能力。因此需要引入‘适应性’作为核心指标来提升推理效率与智能水平。

Method: 形式化三类经典认知推理（演绎、归纳、类比）在LLM中的实现方式；将自适应推理建模为控制增强的策略优化问题；构建包含训练型与训练自由方法的系统性分类框架。

Result: 提出了一个清晰的自适应推理理论框架，能够系统比较不同方法的实现机制，揭示现有技术的优劣与适用场景，并为未来研究提供方向。

Conclusion: 自适应推理是提升大语言模型智能的关键，但仍面临自我评估、元推理能力及人类对齐控制等关键挑战，亟需进一步探索。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [197] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: 提出HyperComplEx，一种融合双曲、复数和欧几里得空间的混合嵌入框架，通过学习注意力机制自适应选择关系类型对应的几何空间。采用关系特定的空间加权策略与多空间一致性损失，提升对异构关系的建模能力。在从1K到10M论文的知识图谱上均表现优异，尤其在10M规模数据上达到0.612 MRR，较最佳基线提升4.8%，推理速度达85ms/三元组，训练高效且近线性可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法在大规模复杂关系建模中存在局限：欧几里得模型难以处理层次结构，向量空间模型无法捕捉关系不对称性，双曲模型则不适用于对称关系。亟需一种能灵活适配多种关系类型的统一框架。

Method: 提出HyperComplEx，结合双曲、复数与欧几里得空间，利用关系相关的注意力机制动态分配空间权重，并引入多空间一致性损失以保证跨空间预测的一致性。通过自适应维度分配实现近线性扩展。

Result: 在涵盖1K至10M论文的知识图谱上，性能全面超越TransE、RotatE、DistMult、ComplEx、SEPA、UltraE等主流模型；在10M规模数据集上取得0.612 MRR，相对最优基线提升4.8%；推理效率高（85ms/三元组），训练可扩展性强。

Conclusion: HyperComplEx成功实现了对多样化关系类型的统一建模，兼具高精度与高效性，为大规模知识图谱嵌入提供了可扩展、可复现的新范式。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [198] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 本研究提出一种多智能体AI框架，用于从碎片化碰撞数据中重建事故前情景并推断车辆行为。该框架分为两阶段：第一阶段基于多模态输入（文本报告、表格数据、场景图）生成自然语言事故重构；第二阶段结合事件数据记录器（EDR）进行深入推理。在39个复杂案例中的验证显示，系统达到100%准确率，超越人类专家的92%准确率，且在处理不完整或错误数据时表现稳健。


<details>
  <summary>Details</summary>
Motivation: 传统交通事故重建依赖人工经验，面对不完整的多模态数据时结果常不一致。需要更精确、可重复的自动化方法来提升事故分析的可靠性与效率。

Method: 提出一个两阶段协同框架：第一阶段利用多模态数据生成自然语言事故重构；第二阶段结合时间序列的EDR数据进行深度推理，实现车辆行为推断与关键事件识别。

Result: 在39个复杂后撞事故案例中，系统实现100%准确率，成功识别最相关EDR事件并正确区分撞击车与被撞车，优于人类研究人员的92%准确率；对缺失或矛盾数据具有强鲁棒性。

Conclusion: 本研究证明了人工智能在处理异构交通碰撞数据方面的卓越能力，能够以空前的精度重建碰撞动力学和刻画事故前行为，为交通事故分析提供了高效、可靠的新范式。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [199] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出了一种多智能体法律验证器，通过专业化智能体分解合规检查任务，并通过结构化合成协议协调，显著提升日本个人信息保护法（APPI）第16条修订案的合规验证性能。在200个分层标注案例上测试，准确率达72%，较单智能体基线提高21个百分点，尤其在明确合规案例中达到90%准确率（基线仅16%），同时完美检测出明显违规，展现了领域专精与协同推理对法律AI性能的实质性提升。


<details>
  <summary>Details</summary>
Motivation: 随着隐私法规日益严格，传统单一模型难以应对复杂法律判断，需更精确、可解释的自动化合规验证方法，特别是在涉及法律文本解读与业务上下文融合的场景中。

Method: 采用多智能体系统，分工负责法定解释、业务上下文评估和风险评估，通过结构化合成协议进行协同决策，实现对法律条款的深度理解与综合判断。

Result: 在200个经标注的APPI第16条案例上，系统达到72%准确率，比单智能体基线高21个百分点；在清晰合规案例中准确率达90%，而基线仅为16%；且能完全识别明显违规，虽在模糊情形仍有挑战，但整体表现显著优于基线。

Conclusion: 领域专业化与协同推理机制可有效提升法律AI在合规验证中的准确性与可解释性，为构建可扩展、法规敏感的可信自动化合规系统提供了可行框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [200] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文探讨了自主AI系统在面对新或未明确定义情境时，如何评估多个可能的行为路径。尽管经过大量训练，系统仍会遇到无法完全满足所有操作约束的情况。为符合人类期望和价值观，系统需超越已有策略，构建、评估并证明候选行为路径的合理性，这需要超出训练数据的上下文知识。文章分析了决策所需的知识类型，包括规范性、实用性和情境理解，并通过案例研究展示了如何整合这些知识以实现更符合人类期望的决策。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际应用中常面临训练数据未覆盖的新情境，现有策略无法满足所有约束条件。为确保系统行为与人类目标和价值观一致，必须发展出能够生成、评估和解释新行为路径的能力，因此需要探索此类情境下的决策机制和所需知识类型。

Method: 结合理论分析与实证案例研究，识别并分类自主智能体在复杂现实环境中进行决策所需的三类知识：规范性（如法律、伦理）、实用性（如效率、可行性）和情境性（如具体环境背景）。通过案例分析验证这些知识如何协同作用以提升决策的对齐性。

Result: 研究发现，有效的自主决策不仅依赖于预训练策略，还需动态整合多种类型的知识。特别是当系统面临多目标冲突或信息不全时，融合规范性、实用性和情境理解可显著提高行为的合理性与人类接受度。

Conclusion: 为了在不确定和复杂环境中做出符合人类价值的决策，自主智能体必须具备超越固定策略的推理能力，能够主动构建并评估行为路径，且其决策过程应基于对规范、实用与情境因素的综合考量。这为未来智能体设计提供了关键指导方向。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [201] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 本文提出一种新的不完全对称性破坏方法，用于抽象结构的对称性处理，通过更有效地利用其表示形式来加速约束求解。该方法特别针对不可区分对象引起的对称性，相较于先前方法（Akgün et al. 2025）具有更快的求解速度。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构（如嵌套集合）需被转换为求解器支持的形式，而此类转换常引入大量对称性。传统对称性破坏方法在处理抽象变量时会产生复杂且低效的约束，影响求解性能。因此，亟需更高效的对称性破坏机制。

Method: 提出一种新方法，通过优化抽象结构的表示方式，更高效地识别和破坏由不可区分对象引发的对称性，避免生成大量复杂约束，从而提升求解效率。

Result: 实验表明，该方法在处理不可区分对象对称性问题时，比已有方法（Akgün et al. 2025）更快，显著提升了求解性能。

Conclusion: 本研究证明了通过更好利用抽象结构表示来实现对称性破坏的有效性，为高阶建模语言中的大规模约束问题提供了更高效的求解路径。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [202] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: DSS是一种基于可微分模拟器Waymax的规划框架，通过利用其精确的状态预测和可微性，在想象的未来轨迹上使用梯度下降优化动作序列。实验表明，DSS在跟踪和路径规划精度上显著优于序列预测、模仿学习、无模型强化学习及其他规划方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，安全规划至关重要，但传统方法在需要同时学习策略、状态预测器和评估器时面临挑战。如何高效地搜索最优动作序列是关键难点。

Method: 提出DSS框架，利用可微分模拟器Waymax作为状态预测器和评价器，结合梯度下降与随机搜索，在想象的未来轨迹上优化动作序列。

Result: DSS在跟踪和路径规划准确性方面显著优于序列预测、模仿学习、无模型强化学习和其他规划方法。

Conclusion: DSS通过结合可微分模拟和梯度优化，实现了高效且准确的自动驾驶规划，展示了其在复杂交通场景中的优越性能。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [203] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj introduces a dataset and framework for modeling human reasoning in visual tasks using temporally ordered, object-level actions from the ARC corpus, enabling insight into iterative reasoning steps. It supports integration with various AI methods like reinforcement learning and sequence models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to abstract reasoning rely on static input-output pairs, which fail to capture the dynamic, step-by-step nature of human reasoning. ARCTraj aims to address this by recording detailed, time-ordered actions during problem-solving.

Method: ARCTraj collects human-generated trajectories via the O2ARC interface, annotates them with task IDs, timestamps, and success labels, and formulates reasoning as a Markov decision process (MDP). It provides a unified pipeline for data collection, action abstraction, MDP formulation, and downstream learning.

Result: Analyses reveal structured patterns in spatial selection, color attribution, and strategic convergence, demonstrating diverse and interpretable human reasoning behaviors. The framework enables integration with advanced models such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers.

Conclusion: ARCTraj offers a structured, interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence in AI systems.

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [204] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种新颖且简单的通用规划（GP）方法：给定一组训练问题，对每个问题按顺序计算每个目标原子的最优计划，对这些计划进行目标回溯，并将相应输出提升为一阶条件→动作规则集合。这些规则共同构成一个可直接执行或用于剪枝规划搜索空间的通用计划。论文形式化并证明了该方法在何种条件下能保证学习到有效的通用计划及搜索剪枝公理。实验表明，在合成成本、规划覆盖率和解质量三个指标上，该方法显著优于现有最先进的（通用）规划器。


<details>
  <summary>Details</summary>
Motivation: 现有的通用规划方法在合成效率、覆盖范围和解质量方面存在不足，需要一种更简单且有效的方法来生成能够解决一系列相关规划问题的通用计划。

Method: 通过在训练问题中为每个目标原子计算最优计划，进行目标回溯，并将结果提升为一阶条件→动作规则，从而生成通用计划；同时形式化了其有效性条件与搜索剪枝公理。

Result: 在多个经典和数值规划领域中，该方法在合成成本、规划覆盖率和解质量上均显著优于现有最先进方法。

Conclusion: 所提出的通用规划方法具有理论保障和实际性能优势，能够高效生成高质量通用计划，并可用于优化规划搜索过程。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [205] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出GGBench，一个专注于评估统一多模态模型在几何生成推理方面能力的基准测试，旨在填补现有评估体系中对跨模态生成推理能力衡量的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估单一模态的理解或无约束图像生成，无法全面衡量模型在整合语言理解与精确视觉生成方面的综合认知能力。

Method: 通过几何构造作为测试场景，设计GGBench基准，系统性地评估模型在理解、推理并主动构建解决方案方面的能力。

Result: GGBench提供了一个全面的框架，能够诊断模型在几何生成推理任务中的表现，为下一代智能系统设定更严格的标准。

Conclusion: 几何构造是评估生成式推理的理想场景，GGBench有效推动了对统一多模态模型在复杂认知任务中表现的评估。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [206] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: This paper proposes MUG, a novel multi-agent protocol inspired by social deduction games to detect hallucinating agents in LLMs using multimodal counterfactual tests. It enhances reasoning reliability by enabling factual verification, dynamic evidence use, and active agent interaction, overcoming the limitations of previous debate-based methods.


<details>
  <summary>Details</summary>
Motivation: Hallucination remains a critical issue in large language models (LLMs), particularly in reasoning tasks. Existing solutions like Multi-Agent Debate (MAD) assume all agents are rational and reflective, which is unrealistic when agents themselves are prone to hallucinations.

Method: The paper introduces the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games such as 'Who is Undercover?'. MUG uses multimodal counterfactual tests by modifying reference images to introduce counterfactual evidence. Agents are then evaluated on their ability to detect these changes, which serves as ground-truth for identifying hallucinating agents. This enables robust, crowd-powered multimodal reasoning through active probing and cross-evidence analysis.

Result: MUG improves upon MAD by enabling factual verification beyond statistical consensus, introducing dynamic cross-evidence reasoning via modified inputs, and promoting active, probing discussions among agents. These enhancements lead to a more reliable and effective framework for multimodal reasoning in LLMs.

Conclusion: MUG addresses the limitations of current debate-based approaches by providing a mechanism to detect hallucinating agents through counterfactual testing, thus advancing the reliability and effectiveness of multimodal reasoning in large language models.

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [207] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR 提出一种新的表格推理框架，通过模拟人类慢思考过程，提升大语言模型在表格推理中的深度与稳定性。该方法采用两阶段难度感知强化学习训练，并在推理时结合置信度与答案一致性进行轨迹级不确定性量化，从而选择更可靠的推理路径。实验表明，STaR 在多个基准测试中表现优异，且在跨域数据上具有强泛化能力，展现出可靠且类人认知的推理潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的表格推理方法存在推理深度不足和过程不稳定的问题，难以满足高可靠性应用场景的需求。为此，本文旨在构建一种模仿人类慢思考机制的推理框架，以增强推理的深度、可解释性与稳定性。

Method: 提出 STaR 框架，包含两阶段难度感知强化学习（DRL）训练策略，在训练中逐步从简单到复杂查询学习；推理阶段引入轨迹级不确定性量化，融合标记级置信度与答案一致性，实现对推理路径可信度的评估与选择。

Result: STaR 在多个主流表格推理基准上显著优于现有方法，推理性能与稳定性均得到提升，同时在跨域数据集上表现出强泛化能力，验证了其作为可靠、类人推理系统的技术可行性。

Conclusion: STaR 通过引入慢思考机制与不确定性感知推理，有效缓解了大语言模型在表格推理中深度不足与不稳定的缺陷，为构建更智能、更可信的结构化数据理解系统提供了新范式。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [208] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出一种创新且实用的方案，通过强制记录自动化决策中每个组件的训练与推理过程，实现对AI决策的不可篡改、可验证和全面追溯。该方法基于可信计算技术，扩展了DBOM概念，构建了首个可运行的工作流，并以区分有毒与可食用蘑菇的应用为例，展示了其在高风险决策支持中的实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大量关键决策依赖于脆弱的人工智能系统，但缺乏对决策过程的充分文档化，导致难以追溯责任链条，尤其在法律纠纷中无法证明决策依据，存在侵犯个人福祉或基本人权的风险。因此亟需一种可信赖的决策追踪机制。

Method: 提出并实现一个基于可信计算技术的运行工作流，强制记录所有参与AI模型训练与推理的组件信息，确保生成的决策日志具有不可篡改性、可验证性和完整性，从而实现端到端的决策溯源。

Result: 成功构建了一个可运行的、支持生成完整、防篡改决策追踪日志的工作流；并通过开发一款用于识别毒蘑菇与可食用蘑菇的App，验证了该方法在真实场景中的可行性和有效性。

Conclusion: 本研究首次实现了可落地的AI决策全过程可追溯框架，为高风险场景下的AI责任认定提供了技术保障，推动了负责任AI的发展。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [209] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比型ABox解释的概念，用于回答‘为何a是C的实例，而b不是’这类问题。与仅解释正向蕴含或缺失蕴含的方法不同，对比解释同时考虑两者，从而聚焦于a和b之间的共同点与差异。研究针对描述逻辑本体中的ABox推理，定义了对比解释，并分析了在不同优化标准下各种变体的计算复杂度，涵盖轻量级及更丰富的描述逻辑。作者实现了一种对比解释方法，并在真实知识库生成的问题上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独解释正向蕴含（如为何C(a)被知识库蕴含）或缺失蕴含（如为何C(b)未被蕴含），但无法有效处理两者并列的情况。为理解个体间差异的根本原因，需要一种能够同时考虑正负例的解释机制，即对比解释。

Method: 提出对比型ABox解释的形式化定义；基于描述逻辑本体设计计算框架；分析不同优化准则下（如最小差异、最大共同性）的计算复杂度；实现并测试其中一种变体算法。

Result: 所提出的对比解释方法能有效识别导致个体差异的关键属性或关系；在多种描述逻辑中验证了其可行性；实验表明该方法在真实知识库数据集上具有可接受的运行效率与解释质量。

Conclusion: 对比型ABox解释为理解知识库中个体间的差异提供了新的视角，能够揭示关键区别因素，有助于提升知识推理的可解释性与透明度。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [210] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign is an inference-time framework that improves Large Vision-Language Models' (LVLMs) alignment by treating reasoning as an economically rational process. It uses a forward-looking scoring function to balance safety, utility, and cost, while enforcing path safety via the weakest-link principle. Experiments show EcoAlign outperforms state-of-the-art methods in safety and utility with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Current LVLM alignment methods face a trade-off between safety, utility, and cost, and are often process-blind—focusing only on final outputs, which allows harmful reasoning to be disguised with benign justifications. This inefficiency wastes computational resources and undermines safety.

Method: EcoAlign reframes alignment as an economic optimization problem. It incrementally builds a thought graph and scores actions using a net present value-like function that dynamically balances expected safety, utility, and cost against remaining budget. Path safety is enforced by requiring all steps in a reasoning path to meet safety standards (weakest-link principle).

Result: EcoAlign achieves state-of-the-art performance in safety and utility across 3 closed-source and 2 open-source models on 6 datasets, while reducing computational costs. It effectively prevents deception by penalizing unsafe intermediate reasoning.

Conclusion: EcoAlign provides a principled, economical, and robust approach to LVLM alignment by modeling reasoning as bounded rationality with dynamic cost-benefit evaluation and strict path safety enforcement.

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [211] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: 提出RLSLM框架，结合规则模型与强化学习，实现高效、可解释且符合人类直觉的社交导航，通过实验验证其优越性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决社交导航中规则方法缺乏泛化性与灵活性，数据驱动方法效率低且难以对齐人类直觉的问题。

Method: 提出RLSLM混合强化学习框架，将基于实证行为实验的规则化社会移动模型融入强化学习奖励函数中，生成方向敏感的社会舒适场以量化空间中的人类舒适度。

Result: RLSLM在沉浸式虚拟现实实验中优于现有最先进规则模型，在用户体验上表现更优；消融和敏感性分析表明其显著提升可解释性。

Conclusion: 该工作提供了一种可扩展、以人为本的方法，有效融合认知科学与机器学习，实现真实世界的社交导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [212] [MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism](https://arxiv.org/abs/2511.11373)
*Shulin Liu,Dong Du,Tao Yang,Yang Li,Boyu Qiu*

Main category: cs.AI

TL;DR: MarsRL is a new reinforcement learning framework with agentic pipeline parallelism that jointly optimizes multiple agents (Solver, Verifier, Corrector) in multi-agent reasoning systems. It addresses limitations of open-source models by introducing agent-specific rewards to reduce noise and using pipeline training for efficiency. Applied to Qwen3-30B-A3B-Thinking-2507, it boosts AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, outperforming larger models like Qwen3-235B.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent reasoning systems struggle to generalize to open-source models due to weak critic and correction capabilities, despite success in closed-source models like Gemini 2.5 Pro. The limited output length of LLMs also restricts deep reasoning in single inference. There is a need for a scalable, efficient, and effective framework to enhance reasoning depth and generalization across diverse models.

Method: MarsRL employs agentic pipeline parallelism to enable joint optimization of Solver, Verifier, and Corrector agents. It uses agent-specific reward mechanisms to mitigate reward noise and adopts pipeline-inspired training to handle long reasoning trajectories efficiently, improving training stability and scalability.

Result: MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8% on Qwen3-30B-A3B-Thinking-2507, surpassing the performance of the larger Qwen3-235B model. This demonstrates its effectiveness in enhancing reasoning quality and generalization in open-source models.

Conclusion: MarsRL advances multi-agent reasoning systems by enabling efficient, joint optimization of agents through pipeline parallelism and tailored reward design. It significantly enhances reasoning performance on open-source models, making high-quality reasoning more accessible and scalable across diverse tasks.

Abstract: Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.

</details>


### [213] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本文系统回顾了多智能体强化学习（MARL）在现实通信约束下的鲁棒且高效的通信策略进展，涵盖消息扰动、传输延迟和带宽限制等问题，并聚焦于自动驾驶协同、分布式定位与建图、联邦学习三个实际应用场景，指出当前面临的关键挑战并倡导通信、学习与鲁棒性协同设计的统一方法。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信即时、可靠且带宽无限，但现实中通信存在延迟、干扰和带宽限制，导致理论模型难以落地。因此亟需研究适应真实环境的通信策略以提升MARL系统的实用性。

Method: 系统性综述近年来在受限通信条件下改进MARL通信机制的研究工作，结合典型应用案例分析不同策略的有效性与局限性。

Result: 识别出通信可靠性、低延迟、隐私保护与数据效率之间的核心权衡问题，提出应通过联合优化通信、学习与鲁棒性来缩小理论与实践差距。

Conclusion: 未来MARL的发展需要采用协同设计框架，将通信机制与学习算法及系统鲁棒性深度融合，以实现更高效、可靠的实用化部署。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [214] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet is a multimodal model that integrates clinical notes, lab tests, and time-series visit data using LLMs and transformer encoders to improve chronic disease prediction. It achieves over 94% accuracy on MIMIC-III and FEMH datasets.


<details>
  <summary>Details</summary>
Motivation: Existing predictive models often focus on single data types and fail to capture complex interactions and temporal patterns across multimodal EHR data, limiting their effectiveness in chronic disease prediction.

Method: CURENet uses large language models for processing unstructured clinical notes and textual lab results, and transformer encoders for modeling longitudinal visit sequences, enabling unified representation learning across diverse data modalities.

Result: CURENet achieved over 94% accuracy in predicting the top 10 chronic conditions in a multi-label setting on both MIMIC-III and private FEMH datasets.

Conclusion: Multimodal integration of EHR data through CURENet enhances predictive performance and holds strong potential for improving clinical decision-making and patient outcomes in chronic disease management.

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [215] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR 是一种动态生成定制化策略的智能体系统，能够基于经验在推理时灵活调整语言模型调用、工具使用、采样参数和控制逻辑。它通过一个基于LLM的元策略（生成策略的策略）实现全维度适应，包含引导器（Guide）生成候选策略与整合器（Consolidator）根据执行反馈优化未来策略。该系统在多个复杂基准测试中显著提升准确率（最高+14%），同时降低计算开销最多达111倍，并随经验积累持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在推理阶段无法灵活调整策略组件（如采样参数、工具配置、控制逻辑等），要么仅通过修改输入文本进行有限引导，要么依赖离线优化且部署后固定不变。这限制了智能体在真实场景中根据经验自适应调整行为的能力。

Method: 提出Experience-Guided Reasoner（EGuR），采用双组件架构：(1) Guide 根据当前问题与结构化记忆生成多个候选策略；(2) Consolidator 利用执行反馈迭代优化策略生成过程。整个策略为完整可执行的计算流程，包含LLM调用、工具、参数设置和控制逻辑，支持缓存复用，避免资源浪费。

Result: 在AIME 2025、3-SAT及三个Big Bench Extra Hard任务上，EGuR相比最强基线最高提升14%准确率，计算成本降低最多达111倍；随着经验积累，性能持续增强。

Conclusion: EGuR实现了推理阶段的全面动态策略适应，突破了传统方法在灵活性与效率之间的权衡，是迈向真正自适应智能体的重要一步。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>
