{"id": "2511.10627", "categories": ["cs.AI", "cs.CV", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10627", "abs": "https://arxiv.org/abs/2511.10627", "authors": ["Edward Kim", "Devan Shanker", "Varun Bharadwaj", "Hongbeen Park", "Jinkyu Kim", "Hazem Torfah", "Daniel J Fremont", "Sanjit A Seshia"], "title": "Querying Labeled Time Series Data with Scenario Programs", "comment": null, "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4eff\u771f\u73af\u5883\u4e2d\u53d1\u73b0\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u6545\u969c\u573a\u666f\u540e\uff0c\u9a8c\u8bc1\u8fd9\u4e9b\u573a\u666f\u662f\u5426\u80fd\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u91cd\u73b0\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4f7f\u7528Scenic\u6982\u7387\u7f16\u7a0b\u8bed\u8a00\u5b9a\u4e49\u62bd\u8c61\u573a\u666f\uff0c\u5e76\u8bbe\u8ba1\u67e5\u8be2\u7b97\u6cd5\u4ece\u6807\u6ce8\u7684\u65f6\u95f4\u5e8f\u5217\u4f20\u611f\u5668\u6570\u636e\u4e2d\u8bc6\u522b\u5339\u914d\u7684\u573a\u666f\u5b9e\u4f8b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eff\u771f\u6545\u969c\u573a\u666f\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u5546\u4e1a\u89c6\u89c9\u5927\u6a21\u578b\uff0c\u4e14\u53ef\u6269\u5c55\u81f3\u66f4\u957f\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u4eff\u771f\u6d4b\u8bd5\u867d\u5e7f\u6cdb\u7528\u4e8e\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u4f46\u5b58\u5728\u2018\u4eff\u771f\u5230\u73b0\u5b9e\u2019\u7684\u5dee\u8ddd\uff1a\u4eff\u771f\u4e2d\u53d1\u73b0\u7684\u6545\u969c\u573a\u666f\u53ef\u80fd\u4ec5\u7531\u5408\u6210\u4f20\u611f\u5668\u6570\u636e\u5f15\u8d77\uff0c\u800c\u975e\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9645\u95ee\u9898\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u624b\u6bb5\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u573a\u666f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u91cd\u73b0\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eScenic\u8bed\u8a00\u7684\u5f62\u5f0f\u5316\u573a\u666f\u5b9a\u4e49\u65b9\u5f0f\uff0c\u5c06\u62bd\u8c61\u573a\u666f\u8868\u793a\u4e3a\u573a\u666f\u7a0b\u5e8f\uff1b\u8bbe\u8ba1\u4e00\u79cd\u67e5\u8be2\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u6570\u636e\u96c6\u4e2d\u5339\u914d\u5e76\u5b9a\u4f4d\u4e0e\u573a\u666f\u7a0b\u5e8f\u4e00\u81f4\u7684\u6570\u636e\u7247\u6bb5\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u67e5\u8be2\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5546\u4e1a\u89c6\u89c9\u5927\u6a21\u578b\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u5904\u7406\u957f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6210\u529f\u9a8c\u8bc1\u4e86\u90e8\u5206\u4eff\u771f\u6545\u969c\u573a\u666f\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u91cd\u73b0\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u673a\u5236\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4eff\u771f\u4e2d\u53d1\u73b0\u7684\u6545\u969c\u573a\u666f\u662f\u5426\u771f\u5b9e\u5b58\u5728\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2511.10618", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10618", "abs": "https://arxiv.org/abs/2511.10618", "authors": ["Benjamin L. Badger", "Matthew Neligeorge"], "title": "Know Your Limits: Entropy Estimation Modeling for Compression and Generalization", "comment": null, "summary": "Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7f16\u7801\u5668\u7684\u56e0\u679c\u89e3\u7801\u5668\u6a21\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5728\u6709\u9650\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u56e0\u679cTransformer\u66f4\u9ad8\u7684\u8bed\u8a00\u538b\u7f29\u6548\u679c\u3002\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5bf9\u6bcf\u4e2a\u8bcd\u5143\u8fdb\u884c\u71b5\u4f30\u8ba1\uff0c\u5e76\u8868\u660e\u5f53\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\u63a5\u8fd1\u4f46\u4e0d\u8d85\u8fc7\u4f30\u8ba1\u7684\u6bcf\u8bcd\u5143\u71b5\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u6700\u5c0f\u5316\u635f\u5931\u7684\u76ee\u6807\u3002\u8fd9\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u5b58\u5728\u7531\u8bed\u8a00\u5185\u5728\u4fe1\u606f\u71b5\u51b3\u5b9a\u7684\u7406\u8bba\u4e0a\u9650\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f30\u8ba1\u8bed\u8a00\u71b5\u65b9\u9762\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u8bed\u8a00\u56fa\u6709\u7684\u4fe1\u606f\u71b5\u9650\u5236\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u903c\u8fd1\u8bed\u8a00\u71b5\u7684\u4e0b\u754c\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u538b\u7f29\u6548\u7387\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u5e26\u6709\u7f16\u7801\u5668\u589e\u5f3a\u7684\u56e0\u679c\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5229\u7528\u8be5\u7ed3\u6784\u5728\u8f83\u4f4e\u8d44\u6e90\u6d88\u8017\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u8fc7\u7a0b\u903c\u8fd1\u6bcf\u8bcd\u5143\u7684\u8bed\u8a00\u71b5\uff0c\u8fdb\u800c\u8bc4\u4f30\u6a21\u578b\u7684\u538b\u7f29\u4e0e\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u76f8\u540c\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bed\u8a00\u538b\u7f29\u7387\uff1b\u540c\u65f6\uff0c\u90a3\u4e9b\u88ab\u8bad\u7ec3\u4ee5\u63a5\u8fd1\u800c\u975e\u8d85\u8fc7\u4f30\u8ba1\u71b5\u503c\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u71b5\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u8bed\u8a00\u672c\u8eab\u7684\u4fe1\u606f\u71b5\uff0c\u800c\u901a\u8fc7\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u5e76\u4ee5\u71b5\u4e3a\u6307\u5bfc\u76ee\u6807\uff0c\u53ef\u4ee5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u66f4\u4f18\u7684\u538b\u7f29\u4e0e\u6cdb\u5316\u6548\u679c\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.10621", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10621", "abs": "https://arxiv.org/abs/2511.10621", "authors": ["Haizhou Shi", "Ye Liu", "Bo Pang", "Zeyu Leo Liu", "Hao Wang", "Silvio Savarese", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz"], "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning", "comment": "Preprint; work in progress", "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSocratic Self-Refine\uff08SSR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b50\u95ee\u9898-\u5b50\u7b54\u6848\u5bf9\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4e0e\u7cbe\u786e\u4fee\u6b63\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53d7\u63a7\u91cd\u89e3\u548c\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\u8fdb\u884c\u6b65\u9aa4\u7ea7\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u7cbe\u51c6\u5b9a\u4f4d\u4e0d\u53ef\u9760\u63a8\u7406\u6b65\u9aa4\u5e76\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u548c\u4e09\u79cdLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSSR\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8fed\u4ee3\u81ea\u7cbe\u70bc\u65b9\u6cd5\u3002\u6b64\u5916\uff0cSSR\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u9ed1\u7bb1\u5206\u6790\u65b9\u5f0f\uff0c\u7528\u4e8e\u7406\u89e3\u5927\u6a21\u578b\u5185\u90e8\u63a8\u7406\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6846\u67b6\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u81ea\u6211\u9a8c\u8bc1\u4e0e\u4fee\u6b63\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u5bfc\u81f4\u63a8\u7406\u8d28\u91cf\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u3001\u53ef\u89e3\u91ca\u4e14\u80fd\u7cbe\u51c6\u8bc6\u522b\u9519\u8bef\u6b65\u9aa4\u7684\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u6a21\u578b\u8f93\u51fa\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b50\u95ee\u9898-\u5b50\u7b54\u6848\u5bf9\uff1b\u901a\u8fc7\u53d7\u63a7\u91cd\u89e3\u548c\u81ea\u4e00\u81f4\u6027\u68c0\u67e5\u8fdb\u884c\u6b65\u9aa4\u7ea7\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff1b\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u6b65\u9aa4\u5e76\u8fed\u4ee3\u4fee\u6b63\uff0c\u5f62\u6210\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u94fe\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u548c\u4e09\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cSSR consistently \u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9ed1\u7bb1\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u7406\u89e3\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "Socratic Self-Refine\uff08SSR\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4e0e\u7cbe\u51c6\u4fee\u6b63\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e3a\u7406\u89e3\u5176\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.10628", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10628", "abs": "https://arxiv.org/abs/2511.10628", "authors": ["Jiang Liu", "Jialian Wu", "Xiaodong Yu", "Yusheng Su", "Prakamya Mishra", "Gowtham Ramesh", "Sudhanshu Ranjan", "Chaitanya Manem", "Ximeng Sun", "Ze Wang", "Pratik Prabhanjan Brahma", "Zicheng Liu", "Emad Barsoum"], "title": "Instella: Fully Open Language Models with Stellar Performance", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.", "AI": {"tldr": "Instella \u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u4e09\u4ebf\u53c2\u6570\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u57fa\u4e8e\u516c\u5f00\u6570\u636e\u548c\u4ee3\u7801\u5e93\u8bad\u7ec3\uff0c\u4f7f\u7528 AMD Instinct MI300X GPU \u5b8c\u6210\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3001\u901a\u7528\u6307\u4ee4\u5fae\u8c03\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u8f83\u5c11\uff0c\u5176\u6027\u80fd\u5728\u5168\u5f00\u6e90\u6a21\u578b\u4e2d\u5904\u4e8e\u9886\u5148\u5730\u4f4d\uff0c\u5e76\u4e0e\u540c\u7c7b\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u5e03\u4e86\u4e24\u4e2a\u4e13\u7528\u7248\u672c\uff1aInstella-Long\uff08\u652f\u6301\u6700\u957f 128K tokens \u4e0a\u4e0b\u6587\uff09\u548c Instella-Math\uff08\u4e13\u7cbe\u6570\u5b66\u63a8\u7406\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff09\u3002\u6574\u4f53\u4e0a\uff0cInstella \u63d0\u4f9b\u4e86\u900f\u660e\u3001\u9ad8\u6548\u4e14\u591a\u529f\u80fd\u7684\u5f00\u6e90\u8bed\u8a00\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u5f00\u653e\u53ef\u590d\u73b0\u7814\u7a76\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u5927\u8bed\u8a00\u6a21\u578b\u591a\u4e3a\u95ed\u6e90\u6216\u90e8\u5206\u5f00\u6e90\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u3001\u9ad8\u6027\u80fd\u4e14\u53ef\u590d\u73b0\u7684\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u91c7\u7528 AMD Instinct MI300X GPU \u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u7ed3\u5408\u901a\u7528\u6307\u4ee4\u5fae\u8c03\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff1b\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\u4e13\u95e8\u53d8\u4f53\uff0c\u5982\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002\u6240\u6709\u6570\u636e\u4e0e\u4ee3\u7801\u5747\u516c\u5f00\uff0c\u786e\u4fdd\u5b8c\u5168\u900f\u660e\u3002", "result": "Instella \u5728\u5168\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u4e0e\u540c\u89c4\u6a21\u4e3b\u6d41\u5f00\u6e90\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff1bInstella-Long \u652f\u6301\u957f\u8fbe 128K token \u7684\u4e0a\u4e0b\u6587\u5904\u7406\uff0cInstella-Math \u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u548c\u591a\u6837\u6027\u3002", "conclusion": "Instella \u4f5c\u4e3a\u5b8c\u5168\u5f00\u6e90\u3001\u9ad8\u6027\u80fd\u3001\u53ef\u590d\u73b0\u7684\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u6548\u63a8\u8fdb\u4e86\u5f00\u653e\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.10643", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10643", "abs": "https://arxiv.org/abs/2511.10643", "authors": ["Tianzhu Ye", "Li Dong", "Zewen Chi", "Xun Wu", "Shaohan Huang", "Furu Wei"], "title": "Black-Box On-Policy Distillation of Large Language Models", "comment": null, "summary": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.", "AI": {"tldr": "GAD\u662f\u4e00\u79cd\u7528\u4e8e\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b66\u751f\u6a21\u578b\u89c6\u4e3a\u751f\u6210\u5668\uff0c\u8bad\u7ec3\u5224\u522b\u5668\u6765\u533a\u5206\u5b66\u751f\u4e0e\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5f62\u6210\u6700\u5c0f\u6700\u5927\u535a\u5f08\u3002\u5224\u522b\u5668\u4f5c\u4e3a\u5171\u8fdb\u5316\u4e14\u81ea\u9002\u5e94\u7684\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u4f9b\u7a33\u5b9a\u53cd\u9988\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e8f\u5217\u7ea7\u77e5\u8bc6\u84b8\u998f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528GAD\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\uff08\u5982Qwen2.5-14B-Instruct\uff09\u5728LMSYS-Chat\u8bc4\u6d4b\u4e2d\u63a5\u8fd1\u751a\u81f3\u5ab2\u7f8e\u6559\u5e08\u6a21\u578b\uff08GPT-5-Chat\uff09\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5185\u90e8\u4fe1\u606f\uff0c\u96be\u4ee5\u6709\u6548\u6355\u6349\u6559\u5e08\u6a21\u578b\u7684\u9690\u542b\u77e5\u8bc6\uff1b\u540c\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u76ee\u6807\u51fd\u6570\uff0c\u53cd\u9988\u4e0d\u591f\u52a8\u6001\u548c\u9002\u5e94\u6027\u5f3a\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5b9e\u73b0\u5728\u7ebf\u3001\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u84b8\u998f\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5bf9\u6297\u84b8\u998f\uff08GAD\uff09\uff0c\u5c06\u5b66\u751f\u6a21\u578b\u8bbe\u4e3a\u751f\u6210\u5668\uff0c\u5224\u522b\u5668\u7528\u4e8e\u5224\u65ad\u8f93\u51fa\u6765\u6e90\uff0c\u901a\u8fc7\u6700\u5c0f\u6700\u5927\u535a\u5f08\u4f18\u5316\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u5224\u522b\u5668\u4f5c\u4e3a\u5171\u8fdb\u5316\u5956\u52b1\u6a21\u578b\u6301\u7eed\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53cd\u9988\u3002", "result": "GAD\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e8f\u5217\u7ea7\u77e5\u8bc6\u84b8\u998f\uff1b\u7279\u522b\u662fQwen2.5-14B-Instruct\u5728\u4f7f\u7528GAD\u540e\uff0c\u5728LMSYS-Chat\u81ea\u52a8\u8bc4\u6d4b\u4e2d\u8868\u73b0\u63a5\u8fd1\u751a\u81f3\u5ab2\u7f8e\u5176\u6559\u5e08\u6a21\u578bGPT-5-Chat\u3002", "conclusion": "GAD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u524d\u666f\u7684\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u8303\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u7a33\u5b9a\u3001\u81ea\u9002\u5e94\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u4e3a\u65e0\u5185\u90e8\u8bbf\u95ee\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.10645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10645", "abs": "https://arxiv.org/abs/2511.10645", "authors": ["Yesheng Liang", "Haisheng Chen", "Song Han", "Zhijian Liu"], "title": "ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference", "comment": null, "summary": "Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.", "AI": {"tldr": "\u63d0\u51faParoQuant\uff0c\u4e00\u79cd\u6743\u91cd\u4ec5\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u72ec\u7acb\u7684Givens\u65cb\u8f6c\u548c\u901a\u9053\u7f29\u653e\u5747\u8861\u901a\u9053\u95f4\u5e45\u5ea6\uff0c\u7f29\u5c0f\u91cf\u5316\u7ec4\u5185\u52a8\u6001\u8303\u56f4\uff0c\u7ed3\u5408\u4f18\u5316\u63a8\u7406\u5185\u6838\uff0c\u5728\u4fdd\u6301\u4f4e\u8fd0\u884c\u65f6\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u7cbe\u5ea6\uff0c\u5e73\u5747\u8f83AWQ\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u63d0\u53472.4%\u51c6\u786e\u7387\uff0c\u4e14\u5f00\u9500\u4f4e\u4e8e10%\u3002", "motivation": "\u73b0\u6709\u6743\u91cd\u4ec5\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6291\u5236\u6743\u91cd\u548c\u6fc0\u6d3b\u4e2d\u7684\u5f02\u5e38\u503c\uff0c\u5bfc\u81f4\u91cf\u5316\u8bef\u5dee\u5927\u3001\u7cbe\u5ea6\u4e25\u91cd\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u8bef\u5dee\u7d2f\u79ef\u663e\u8457\uff1b\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u6216\u6291\u5236\u80fd\u529b\u4e0d\u8db3\uff0c\u6216\u5f15\u5165\u9ad8\u63a8\u7406\u5f00\u9500\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u9ad8\u6548\u4e14\u53ef\u4f18\u5316\u7684\u72ec\u7acbGivens\u65cb\u8f6c\u7ed3\u5408\u901a\u9053\u7ea7\u7f29\u653e\uff0c\u4f7f\u5404\u901a\u9053\u5e45\u5ea6\u8d8b\u4e8e\u4e00\u81f4\uff0c\u538b\u7f29\u6bcf\u7ec4\u91cf\u5316\u5185\u7684\u52a8\u6001\u8303\u56f4\uff0c\u5e76\u534f\u540c\u8bbe\u8ba1\u63a8\u7406\u5185\u6838\u4ee5\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u6027\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u8fd0\u884c\u65f6\u64cd\u4f5c\u3002", "result": "\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u6bd4AWQ\u63d0\u53472.4%\u51c6\u786e\u7387\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u4f4e\u4e8e10%\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u578bLLM\u7684\u90e8\u7f72\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "conclusion": "ParoQuant\u901a\u8fc7\u521b\u65b0\u7684\u65cb\u8f6c\u4e0e\u7f29\u653e\u673a\u5236\u53ca\u534f\u540c\u5185\u6838\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f4e\u5f00\u9500\u7684\u6743\u91cd\u91cf\u5316\uff0c\u4e3a\u63a8\u7406\u578b\u5927\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2511.10615", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.10615", "abs": "https://arxiv.org/abs/2511.10615", "authors": ["Shruti Singh Baghel", "Yash Pratap Singh Rathore", "Sushovan Jena", "Anurag Pradhan", "Amit Shukla", "Arnav Bhavsar", "Pawan Goyal"], "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals", "comment": "8 pages", "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\uff08BLV\uff09\u7528\u6237\u53ef\u8bbf\u95ee\u6027\u63cf\u8ff0\u65b9\u9762\u7684\u6027\u80fd\uff0c\u8bc4\u4f30\u4e86\u53c2\u6570\u91cf\u4e3a500M\u548c2.2B\u7684SmolVLM2\u6a21\u578b\u5728AVCaps\uff08\u6237\u5916\uff09\u548cCharades\uff08\u5ba4\u5185\uff09\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u63d0\u51fa\u4e24\u79cd\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff1a\u591a\u4e0a\u4e0b\u6587BLV\u6846\u67b6\uff08\u8bc4\u4f30\u7a7a\u95f4\u65b9\u4f4d\u3001\u793e\u4ea4\u4e92\u52a8\u3001\u52a8\u4f5c\u4e8b\u4ef6\u548c\u6c1b\u56f4\uff09\u4e0e\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\uff08\u805a\u7126\u79fb\u52a8\u5173\u952e\u4fe1\u606f\uff09\u3002\u7cfb\u7edf\u6bd4\u8f83\u56db\u79cd\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff0c\u5e76\u5728\u667a\u80fd\u624b\u673a\u4e0a\u90e8\u7f72\u6a21\u578b\uff0c\u6d4b\u8bd5FP32\u4e0eINT8\u7cbe\u5ea6\u7248\u672c\u4ee5\u8bc4\u4f30\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5b9e\u9645\u6027\u80fd\u3002", "motivation": "\u5927\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u867d\u5728\u89c6\u9891\u63cf\u8ff0\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u90e8\u7f72\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u5bf9\u4f9d\u8d56\u8be6\u7ec6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63cf\u8ff0\u7684\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\uff08BLV\uff09\u7528\u6237\u800c\u8a00\u3002\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u89c4\u6a21\u5bf9\u53ef\u8bbf\u95ee\u6027\u63cf\u8ff0\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u9002\u5408BLV\u7528\u6237\u7684\u8bc4\u4f30\u6846\u67b6\u4e0e\u4f18\u5316\u65b9\u6848\u3002", "method": "\u91c7\u7528SmolVLM2\u7cfb\u5217\u6a21\u578b\uff08500M\u548c2.2B\u53c2\u6570\uff09\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\uff08AVCaps\u4e0eCharades\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b\u8bbe\u8ba1\u5e76\u5e94\u7528\u4e24\u4e2a\u65b0\u8bc4\u4f30\u6846\u67b6\u2014\u2014\u591a\u4e0a\u4e0b\u6587BLV\u6846\u67b6\u4e0e\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\uff1b\u7cfb\u7edf\u6d4b\u8bd5\u56db\u79cd\u63d0\u793a\u8bbe\u8ba1\u7b56\u7565\uff1b\u5728\u79fb\u52a8\u7aef\u90e8\u7f72\u6a21\u578b\uff0c\u5bf9\u6bd4FP32\u4e0eINT8\u7cbe\u5ea6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8f83\u5c0f\u53c2\u6570\u6a21\u578b\u5728\u4fdd\u6301\u8f83\u9ad8\u63cf\u8ff0\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u4e14\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bbe\u8ba1\u4e0e\u91cf\u5316\u7b56\u7565\uff0c\u53ef\u5728\u624b\u673a\u7aef\u5b9e\u73b0\u9ad8\u6548\u8fd0\u884c\u3002\u591a\u4e0a\u4e0b\u6587\u4e0e\u5bfc\u822a\u8f85\u52a9\u6846\u67b6\u6709\u6548\u8bc6\u522b\u51fa\u5bf9BLV\u7528\u6237\u81f3\u5173\u91cd\u8981\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bc4\u4f30\u53ef\u8bbf\u95ee\u6027\u63cf\u8ff0\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c0f\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4e0e\u91cf\u5316\u6280\u672f\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u53cb\u597d\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u63cf\u8ff0\u751f\u6210\u3002\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u65e0\u969c\u788d\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2511.10619", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10619", "abs": "https://arxiv.org/abs/2511.10619", "authors": ["Avrim Blum", "Marten Garicano", "Kavya Ravichandran", "Dravyansh Sharma"], "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem", "comment": "25 pages", "summary": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $\u03a9(k)$ and $\u03a9(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6539\u8fdb\u578b\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\uff0c\u901a\u8fc7\u79bb\u7ebf\u6570\u636e\u754c\u5b9a\u4e86\u5b66\u4e60\u8fd1\u4f18\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002\u7b2c\u4e00\u7c7b\u5305\u542b\u5148\u524d\u5de5\u4f5c\u7684\u6700\u4f18\u968f\u673a\u7b97\u6cd5\uff0c\u5f53\u5956\u52b1\u66f2\u7ebf\u5177\u6709\u66f4\u5f3a\u7684\u51f9\u6027\u65f6\uff0c\u53ef\u5b9e\u73b0\u5bf9k\u7684\u6700\u4f18\u4f9d\u8d56\u5173\u7cfb\u3002\u7b2c\u4e8c\u7c7b\u7b97\u6cd5\u5728\u826f\u597d\u884c\u4e3a\u5b9e\u4f8b\u4e0a\u4fdd\u8bc1\u6700\u4f18\u81c2\u8bc6\u522b\uff0c\u5728\u5dee\u52b2\u5b9e\u4f8b\u4e0a\u9000\u5316\u4e3a\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002\u4ece\u7edf\u8ba1\u5b66\u4e60\u89d2\u5ea6\u51fa\u53d1\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9a8c\u8bc1\u5047\u8bbe\u5373\u53ef\u83b7\u5f97\u66f4\u5f3a\u7684\u6570\u636e\u76f8\u5173\u4fdd\u8bc1\u3002", "motivation": "\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\u7528\u4e8e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8d44\u6e90\u5206\u914d\uff0c\u5982\u6280\u672f\u6295\u8d44\u3001\u4e34\u5e8a\u8bd5\u9a8c\u548c\u8d85\u53c2\u6570\u9009\u62e9\u3002\u73b0\u6709\u7b97\u6cd5\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u8f83\u60b2\u89c2\uff0c\u5b58\u5728\u03a9(k)\u548c\u03a9(\u221ak)\u7684\u4e58\u6cd5\u8fd1\u4f3c\u4e0b\u754c\u3002\u9700\u8981\u66f4\u4f18\u7684\u7b97\u6cd5\u4ee5\u9002\u5e94\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u7ed3\u6784\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\uff1a\u7b2c\u4e00\u7c7b\u5305\u542b\u5df2\u6709\u6700\u4f18\u968f\u673a\u7b97\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u5b9e\u73b0\u5bf9\u51f9\u6027\u6761\u4ef6\u7684\u9002\u914d\uff1b\u7b2c\u4e8c\u7c7b\u8bbe\u8ba1\u517c\u5177\u5f3a\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u7684\u7b97\u6cd5\uff0c\u80fd\u5728\u826f\u597d\u5b9e\u4f8b\u4e2d\u8bc6\u522b\u6700\u4f18\u81c2\uff0c\u5dee\u52b2\u5b9e\u4f8b\u4e2d\u9000\u5316\u81f3\u6700\u574f\u60c5\u51b5\u3002\u91c7\u7528\u7edf\u8ba1\u5b66\u4e60\u89c6\u89d2\uff0c\u5229\u7528\u79bb\u7ebf\u6570\u636e\u5206\u6790\u5e76\u4f18\u5316\u7b97\u6cd5\u6027\u80fd\uff0c\u5b9e\u73b0\u6570\u636e\u4f9d\u8d56\u7684\u66f4\u5f3a\u4fdd\u8bc1\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u65cf\u5728\u6ee1\u8db3\u7279\u5b9a\u51f9\u6027\u6761\u4ef6\u65f6\uff0c\u53ef\u5b9e\u73b0\u5bf9k\u7684\u6700\u4f18\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u574f\u60c5\u51b5\u8fb9\u754c\u3002\u7b2c\u4e8c\u7c7b\u7b97\u6cd5\u5728\u4e0d\u540c\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u65e2\u4fdd\u8bc1\u826f\u597d\u884c\u4e3a\u4e0b\u7684\u9ad8\u6548\u5b66\u4e60\uff0c\u53c8\u786e\u4fdd\u5dee\u52b2\u60c5\u5f62\u4e0b\u7684\u7a33\u5065\u6027\u3002\u6574\u4f53\u65b9\u6cd5\u65e0\u9700\u663e\u5f0f\u9a8c\u8bc1\u5047\u8bbe\u5373\u53ef\u83b7\u5f97\u66f4\u5f3a\u7684\u6027\u80fd\u4fdd\u969c\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u5f15\u5165\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\u548c\u7edf\u8ba1\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6539\u8fdb\u578b\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\u7684\u7b97\u6cd5\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u636e\u7279\u6027\u7684\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2511.10648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.10648", "abs": "https://arxiv.org/abs/2511.10648", "authors": ["Jiahao Wang", "Weiye Xu", "Aijun Yang", "Wengang Zhou", "Lewei Lu", "Houqiang Li", "Xiaohua Wang", "Jinguo Zhu"], "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling", "comment": "Accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.", "AI": {"tldr": "Self-Consistency Sampling (SCS) addresses the issue of unfaithful reasoning trajectories in outcome-reward reinforcement learning for multimodal large language models (MLLMs), where incorrect chains of thought that guess the right answer receive the same reward as correct ones. SCS introduces visual perturbations and repeated trajectory sampling to compute a consistency score, which downweights unreliable traces during training. It improves accuracy by up to 7.7% on six benchmarks with minimal computational cost and works across multiple MLLM models.", "motivation": "Existing outcome-reward RL methods in MLLMs fail to distinguish between genuine reasoning and lucky guesses, leading to suboptimal model training. This flaw undermines the reliability of reasoning refinement.", "method": "SCS applies small visual perturbations to inputs and repeatedly truncates and resamples initial reasoning trajectories. Consistency among the resulting trajectories is used to compute a differentiable score that penalizes unreliable paths during policy updates.", "result": "SCS boosts accuracy by up to 7.7 percentage points on six multimodal benchmarks when integrated into RLOO, GRPO, and REINFORCE++ frameworks. The method requires negligible extra computation and generalizes well across Qwen2.5-VL-7B, Qwen2.5-VL-3B, and InternVL3-8B models.", "conclusion": "SCS provides a simple, effective, and general solution to improve the fidelity of reasoning in MLLMs by penalizing inconsistent and unreliable trajectories in outcome-reward RL settings."}}
