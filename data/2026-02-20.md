<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 42]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.LG](#cs.LG) [Total: 80]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 提出一种基于高斯点阵（GS）的数字孪生方法，用于实现土木基础设施中三维损伤的精确可视化。该方法结合多尺度重建策略，在保证效率的同时提升损伤细节表现，并支持随时间演化的损伤更新，显著优于传统2D图像识别与常规摄影测量技术。


<details>
  <summary>Details</summary>
Motivation: 传统2D图像损伤识别无法满足现代数字孪生对精准3D损伤可视化的需求，而现有3D重建方法在处理无纹理区域和效率方面存在局限，亟需更高效、高质量的三维表示技术。

Method: 采用高斯点阵（Gaussian Splatting, GS）进行3D重建，将2D损伤分割结果映射至3D空间；设计多尺度重建策略以平衡计算效率与细节保留；支持动态更新，适应损伤随时间演变的过程。

Result: 在开源地震后检测合成数据集上验证，该方法有效提升了3D损伤可视化的精度与效率，减少了分割误差，实现了损伤随时间演化的动态数字孪生更新。

Conclusion: 所提出的GS驱动的数字孪生方法为土木基础设施的三维损伤可视化提供了高效、准确且可更新的解决方案，具有广泛的应用前景。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了一个大规模的多维度视频质量评估数据集UltraVQA，涵盖用户生成内容（UGC）在运动质量、运动幅度、美学质量、内容质量和清晰度质量五个维度上的标注。每段视频由超过3名人类评分者进行评分，并附有基于GPT生成的解释性理由。为此，作者提出分析性评分优化（ASO），一种理论驱动的后训练目标，将质量评估建模为正则化决策过程，获得闭式解以自然捕捉人类评分的序数特性，确保与人类排序偏好对齐。实验表明，该方法优于多数基线模型，包括闭源API和开源模型，同时降低了质量预测的平均绝对误差（MAE）。


<details>
  <summary>Details</summary>
Motivation: 传统视频质量评估依赖单一的平均意见分数，难以全面反映视频的多方面质量特征。随着用户生成内容（UGC）的普及，亟需更丰富、多维度的评估体系，同时需要可解释的标注和与人类偏好一致的评估模型。

Method: 提出UltraVQA数据集，包含五维标注及基于GPT生成的解释性理由；引入分析性评分优化（ASO）作为后训练目标，将质量评估视为正则化决策问题，推导出闭式解以保持人类评分的序数结构。

Result: 所提方法在多维度视频质量评估任务中显著优于现有基线模型，包括闭源API和开源模型，在降低平均绝对误差（MAE）的同时提升了与人类排序偏好的一致性。

Conclusion: 多维度、可解释的标注与基于强化学习的对齐机制对于推动视频质量评估的发展至关重要，本研究为未来高质量、可解释的VQA系统提供了坚实基础。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 本文提出DODO，首个利用块离散扩散模型的视觉-语言模型，用于光学字符识别（OCR），通过分块生成缓解全局扩散中的同步误差，实现近顶尖准确率的同时推理速度提升达3倍。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在OCR中依赖自回归解码，导致长文档处理时计算成本高、速度慢；而OCR是高度确定性任务，理论上可采用并行扩散模型加速，但现有掩码扩散模型因结构不稳定性无法有效应用。

Method: 提出DODO模型，采用块离散扩散机制，将生成过程分解为多个块，降低全局扩散带来的同步错误，从而实现高效并行解码。

Result: 实验表明，DODO在保持接近当前最优准确率的同时，推理速度相比自回归基线最高提升3倍。

Conclusion: DODO首次成功将块离散扩散应用于OCR任务，克服了传统扩散模型在严格匹配需求下的缺陷，显著提升了生成效率，为高精度、高速度的文本生成提供了新范式。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 提出 StereoAdapter-2，通过新型 ConvSS2D 模块替代传统 ConvGRU，实现单步高效长距离视差传播；构建大规模合成水下立体数据集 UW-StereoDepth-80K，结合动态 LoRA 适配，显著提升水下零样本深度估计性能，在 TartanAir-UW 和 SQUID 上分别提升 17% 和 7.2%，并在 BlueROV2 平台上验证了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水下立体深度估计方法受光衰减、散射和折射导致的域偏移影响严重，且基于 GRU 的迭代更新机制存在计算效率低、长距离传播能力弱的问题，尤其在大视差和无纹理区域表现不佳。

Method: 采用基于选择性状态空间模型的 ConvSS2D 模块替代 ConvGRU，引入四向扫描策略以契合对极几何并捕捉垂直结构一致性，实现线性复杂度下的单步长程传播；构建两阶段生成式管道合成大规模水下立体数据集 UW-StereoDepth-80K；结合动态 LoRA 适配实现零样本迁移。

Result: 在 TartanAir-UW 和 SQUID 基准上分别取得 17% 和 7.2% 的性能提升，达到当前最优的零样本水下深度估计效果，并在真实水下机器人 BlueROV2 平台上验证了实际应用中的鲁棒性。

Conclusion: StereoAdapter-2 通过创新的 ConvSS2D 模块与大规模合成数据训练，有效缓解了水下视觉的域偏移问题，实现了高效、精准的立体深度估计，为水下机器人感知提供了强有力的技术支持。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 本文提出一种名为SemCovNet的新模型，旨在解决视觉模型中因语义覆盖不均（SCI）导致的偏差问题。该模型通过语义描述图（SDM）、描述符注意力调制（DAM）模块和描述符-视觉对齐（DVA）损失，显式学习纠正语义覆盖差异，显著降低覆盖差异指数（CDI），提升模型可靠性与公平性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型依赖丰富的语义表示，但数据集存在语义覆盖不均（SCI）问题，即罕见但重要的语义在训练中被忽视，影响模型学习与推理的公平性，亟需有效方法进行纠正。

Method: 提出SemCovNet模型，包含语义描述图（SDM）用于建模语义表征，描述符注意力调制（DAM）动态调整视觉与概念特征权重，以及描述符-视觉对齐（DVA）损失以增强语义一致性。

Result: 在多个数据集上的实验表明，SemCovNet显著降低了覆盖差异指数（CDI），提升了模型的公平性与可靠性，验证了其在缓解语义覆盖不均方面的有效性。

Conclusion: 本文首次将语义覆盖不均（SCI）定义为可度量、可校正的偏差，为实现更公平、可解释的视觉学习提供了新框架。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一个统一的视觉模型架构，用于大规模图像和视频理解，基于来自 Facebook 和 Instagram 的超大规模社交媒体数据进行训练。它利用超过 150 亿张图像-文本对和 100 亿个视频-标签对，通过稳健的数据清洗流程减少噪声并提升语义多样性。采用三阶段训练策略：自监督 MAE、半监督标签分类和 CLIP 风格对比学习，联合优化图像与视频模态。模型基于 Vision Transformer 架构，并引入高效标记重组（EViT）以提高计算效率。在 ImageNet、Kinetics、HMDB51、MSCOCO 等多个基准上表现优异，具备强鲁棒性，尤其在对抗扰动和领域偏移下表现稳定。进一步将大语言模型作为文本编码器（LLM2CLIP）显著提升了跨模态检索性能和泛化能力。该模型在可扩展性、多模态理解和效率方面树立了新标准。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在处理大规模图像和视频理解任务时面临数据规模、模态融合效率及跨域泛化能力的挑战。为实现更高效、更具泛化能力的多模态理解系统，亟需构建一个基于真实世界海量数据、具备强大语义表达能力和计算效率的统一模型架构。

Method: 采用三阶段训练流程：1）自监督掩码建模（MAE）预训练；2）基于视频-标签对的半监督分类微调；3）使用 CLIP 式对比学习联合优化图像与视频表征。模型基于 Vision Transformer，结合高效标记重组（EViT）结构以降低计算开销。同时探索大语言模型作为文本编码器的集成方式（LLM2CLIP），增强语义对齐能力。

Result: Xray-Visual 在 ImageNet、Kinetics、HMDB51 和 MSCOCO 等多个主流基准上均达到或超越当前最优水平，表现出卓越的图像分类、视频理解与跨模态检索性能。模型在面对域偏移和对抗攻击时具有较强鲁棒性。引入 LLM2CLIP 后，跨模态检索准确率显著提升，尤其在复杂现实场景中表现突出。整体模型在精度与计算效率之间取得良好平衡，支持工业级部署。

Conclusion: Xray-Visual 成功构建了一个面向大规模社会媒体数据的统一多模态视觉模型，实现了高精度、强鲁棒性与高效计算的统一。其三阶段训练范式与 EViT 结构设计为未来可扩展的视觉理解系统提供了有效路径。引入大语言模型作为文本编码器进一步拓展了模型在真实世界应用中的潜力，标志着多模态视觉理解迈向新阶段。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: 提出HSI-SC-NeRF，一种基于静态相机的多通道神经辐射场框架，用于高通量高光谱三维重建，适用于农产品后收获检测。通过在特氟龙成像舱内旋转物体并使用静态相机采集多视角高光谱数据，结合ArUco标记和模拟姿态变换实现标准NeRF训练。采用联合优化所有高光谱波段的复合光谱损失及两阶段训练策略，在三种农产品上验证了高空间精度和强光谱保真度，支持其集成到自动化农业流程中。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱成像与3D重建融合方法依赖复杂硬件，难以适应自动化表型分析系统；传统NeRF需移动相机，限制了农业环境中的吞吐量与可重复性。

Method: 设计静态相机多视角高光谱数据采集系统，利用特氟龙成像舱实现均匀光照；通过ArUco标记估计物体姿态，并经模拟变换映射至相机坐标系；采用多通道NeRF模型与复合光谱损失，结合两阶段训练（几何初始化+辐射度精炼）实现高效重建。

Result: 在三种农产品样本上实现了高空间重建精度和可见光至近红外波段的强光谱保真度，证明该方法适合自动化农业工作流集成。

Conclusion: HSI-SC-NeRF克服了传统方法在硬件复杂性和动态采集方面的限制，为高通量、高精度的农产品质量与表型分析提供了可行且高效的解决方案。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 提出动态分块策略，根据内容复杂度和去噪步数动态调整图像和视频生成中的块大小，在不损失生成质量的前提下显著降低计算成本，实现高达3.52倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器（DiTs）在图像和视频生成中表现优异，但因固定分块方式导致计算开销大，尤其在不同去噪阶段对细节需求差异未被考虑，因此需要一种更高效的测试时策略来适应内容复杂度变化。

Method: 提出动态分块方法，依据内容复杂度与去噪时间步动态调整块大小：早期阶段使用较大块以建模全局结构，后期使用较小块以细化局部细节，并在推理阶段实时重分配块尺寸。

Result: 在FLUX-1.Dev和Wan 2.1上分别实现最高3.52×和3.2×的加速，同时保持生成质量与提示遵循性不变。

Conclusion: 动态分块策略有效提升了扩散变换器在图像和视频生成中的效率，证明了在不同去噪阶段自适应调整分块大小的可行性与优越性。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO 是一种监督的潜在变量填补模型，用于在多模态学习中量化缺失模态的预测影响。它通过潜在变量建模缺失模态与已知模态之间的关系，利用所有可用数据（完整或部分）进行训练，并在推理时通过采样生成多个可能的缺失模态补全，以获得边际预测分布并分析各模态对单个实例预测的影响。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型通常假设训练和推理时所有模态都可用，但现实中模态常缺失、异步或不完整。因此需要一种能处理不完整多模态数据的方法，以充分利用所有可用样本并评估缺失模态的预测贡献。

Method: PRIMO 使用监督的潜在变量模型，将缺失模态建模为与观测模态相关的潜在变量，基于上下文预测目标。在推理阶段，通过从学习到的分布中采样多个潜在补全，计算预测方差以衡量每个模态的预测影响，并可视化不同补全下的合理标签结果。

Result: PRIMO 在合成 XOR 数据集、Audio-Vision MNIST 和 MIMIC-III 数据集上表现良好：当某一模态完全缺失时性能接近单模态基线；当所有模态均存在时性能接近多模态基线。此外，它能够以实例级方差指标量化缺失模态的预测影响，并可视化多种合理的预测结果。

Conclusion: PRIMO 有效应对了多模态数据中的不完整性问题，不仅提升了模型对不完整数据的利用率，还提供了对缺失模态预测贡献的可解释性分析，具有实际应用价值。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG 是一个检索增强框架，通过整合外部部件数据库与扩散变换器，实现单图像3D生成中的部件级结构控制。它采用分层对比检索模块，从1236个标注部件资产中检索多样化、物理合理的示例，提升生成多样性与一致性；同时引入掩码式部件级编辑器，在共享规范空间中支持部件替换、属性优化和组合更新，无需重生成整个对象，保持非目标部件和多视角一致性。在Objaverse、ShapeNet和ABO上表现优异，显著降低点云距离（Chamfer Distance从0.1726降至0.1528），提升F-Score（从0.7472升至0.844），推理时间38秒，交互编辑仅需5-8秒。生成结果在部件边界、细长结构和可动物体上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有单图像3D生成方法在部件级结构建模方面存在两大挑战：一是学习到的先验难以覆盖部件几何的长尾分布，且难以维持多视角一致性；二是缺乏对精确、局部化编辑的支持。因此需要一种能够结合外部知识库并支持精细编辑的新框架。

Method: 提出PartRAG框架，包含两个核心模块：1）分层对比检索模块（Hierarchical Contrastive Retrieval），将图像密集块与3D部件潜在表示在部件与对象层面进行对齐，从1236个标注部件资产中检索物理合理且多样化的部件实例；2）掩码式部件级编辑器，基于共享规范空间实现部件替换、属性调整与组合更新，无需重生成整体对象，保持非目标部分一致性和多视角一致性。

Result: 在Objaverse、ShapeNet和ABO数据集上均取得先进性能，Chamfer Distance下降至0.1528（原为0.1726），F-Score提升至0.844（原为0.7472）；推理耗时38秒，交互编辑仅需5-8秒；生成结果在部件边界清晰度、细长结构还原度及可动物体处理方面显著优于基线模型。

Conclusion: PartRAG通过引入外部部件数据库与可编辑的部件表示，有效解决了单图像3D生成中部件多样性不足与局部编辑困难的问题，实现了高质量、可控且高效的3D内容生成，具备良好的应用前景。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [11] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 提出了一种高效的压缩框架，将60层的双流MMDiT架构Qwen-Image转化为轻量级模型，无需从头训练。基于此框架，提出了Amber-Image系列模型，其中Amber-Image-10B通过时间步敏感的深度剪枝策略实现压缩，保留层通过局部权重平均重初始化，并经逐层蒸馏和全参数微调优化；Amber-Image-6B进一步引入混合流架构，将深层双流转为单流，由图像分支初始化，再通过渐进式蒸馏和轻量微调优化。该方法减少70%参数，无需大规模数据工程，整个压缩与训练流程仅需少于2000 GPU小时，显著降低成本。在DPG-Bench和LongText-Bench等基准测试中，Amber-Image实现高保真生成和优异文本渲染能力，性能媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变压器（DiT）在文生图（T2I）生成中计算成本高昂和部署困难的问题，提升模型效率与实用性。

Method: 采用时间步敏感的深度剪枝策略，结合局部权重平均重初始化、层间蒸馏、全参数微调及混合流架构设计，实现高效模型压缩与性能保持。

Result: 成功构建Amber-Image-10B和Amber-Image-6B模型，参数减少70%，训练仅需少于2000 GPU小时，性能媲美更大模型，在多个基准上表现优异。

Conclusion: 所提出的压缩框架有效实现了高性能文生图模型的轻量化，兼具低计算成本与高生成质量，具备良好的实用价值与推广潜力。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [12] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: 提出StructCore，一种无需训练的结构感知图像级评分方法，通过计算异常得分图的低维结构描述符phi(S)，捕捉分布和空间特征，并利用来自正常样本的对角马氏距离校准进行图像级评分，超越了传统最大池化方法。


<details>
  <summary>Details</summary>
Motivation: 传统最大池化方法依赖单一极端响应，忽略异常证据在图像中的分布与结构信息，导致正常与异常得分重叠，影响检测性能。

Method: StructCore通过构建低维结构描述符phi(S)来捕获异常得分图的分布与空间特性，并采用基于训练正常样本估计的对角马氏校准进行图像级评分，无需修改像素级定位。

Result: 在MVTec AD上达到99.6%的图像级AUROC，在VisA上达到98.4%，显著优于传统方法，证明其能有效利用最大池化遗漏的结构信号。

Conclusion: StructCore是一种高效、无需训练的结构感知方法，能够充分挖掘异常得分图中的结构信息，提升无监督异常检测的图像级判别能力。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [13] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 提出Cholec80-port数据集和标准化操作流程（SOP），用于精确标注腹腔镜手术中的穿刺器端口，排除中心开口以保证几何一致性，提升下游任务如图像拼接、3D重建和视觉SLAM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有公开手术数据集中缺乏明确的穿刺器端口标注，且多数标注违反几何一致性，例如遮盖中心孔洞，导致动态或非解剖异常点影响图像对齐与跟踪稳定性。

Method: 基于Cholec80数据集构建高保真穿刺器端口分割数据集Cholec80-port，定义并实施统一的标注SOP，排除端口中心开口；同时清洗并统一其他公开数据集的标注以符合该标准。

Result: 实验表明，几何一致性的标注显著提升了跨数据集的鲁棒性，其效果超越单纯增加数据量所带来的改进。

Conclusion: 几何一致的穿刺器端口标注对提升视觉导航与重建系统的性能至关重要，本工作为未来研究提供了高质量、标准化的数据基础。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [14] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD是一种双分支框架，通过交叉伪标签实现弱监督视频异常检测，能够在仅有视频级标签的情况下实现异常定位和异常类别识别，实验表明其在异常检测和类别分类上均达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视频异常检测方法在仅使用视频级标签时难以同时实现精确的异常定位和准确的异常类别识别，因此需要一种能够结合时间精度与语义判别的方法。

Method: 提出双分支框架CPL-VAD，包含二值异常检测分支（专注于片段级异常定位）和类别分类分支（利用视觉-语言对齐识别异常事件类别），通过伪标签交换实现两分支间的互补信息传递。

Result: 在XD-Violence和UCF-Crime数据集上的实验结果表明，CPL-VAD在异常检测和异常类别分类任务中均达到了当前最优性能。

Conclusion: CPL-VAD通过双分支协同与交叉伪标签机制，有效提升了弱监督视频异常检测中异常定位与类别识别的能力，为该领域提供了新的解决方案。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [15] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet is a hybrid deep learning framework that combines direct reconstruction and image-based denoising to improve gamma-ray burst (GRB) localization under low photon statistics and high background noise. It outperforms existing methods in accuracy across challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Detecting faint GRBs from the distant universe is crucial for understanding early star formation, but difficult due to low photon counts and strong background noise. Existing machine learning models struggle to balance statistical robustness and noise suppression.

Method: ComptonUNet integrates raw data processing with image reconstruction, leveraging the statistical efficiency of direct models and the denoising power of image-based architectures. It was evaluated using realistic simulations of GRB-like events in low-Earth orbit background environments.

Result: ComptonUNet achieves significantly improved localization accuracy compared to existing approaches, especially in low-statistic and high-background scenarios.

Conclusion: ComptonUNet provides a robust and effective solution for GRB localization under extreme observational challenges, enabling more reliable detection of faint GRBs and advancing studies of early star formation.

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [16] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 本文提出一种融合射频（RF）传感与3D高斯点云渲染的多模态框架，以提升在恶劣环境下的3D场景重建性能。通过利用汽车雷达等RF信号在复杂条件下（如低光照、遮挡、恶劣天气）的鲁棒性，实现稀疏RF深度测量下的高效深度预测，生成高质量点云用于初始化高斯函数，从而增强传统视觉驱动的高斯溅射（GS）方法的稳定性与精度。实验表明，该方法显著提升了场景重建质量，尤其在视觉线索不可靠时表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的3D高斯溅射（GS）方法依赖大量相机视角进行初始化和参数训练，在低光照、遮挡或恶劣天气等条件下性能下降，且初始化成本高。为克服这一局限，需引入对环境变化不敏感的感知模态。射频（RF）信号具有强鲁棒性，可穿透遮挡、不受光照影响，因此将其融入GS框架，有望提升系统在真实复杂场景中的适应能力。

Method: 提出一种多模态融合框架，将射频（如车载雷达）深度数据与3D高斯溅射（GS）渲染结合。首先利用稀疏的RF深度测量进行高效深度估计，构建高质量3D点云；随后用该点云作为初始条件，引导高斯函数的空间分布与参数初始化，支持多种GS架构。整个流程无需依赖密集视觉输入，增强了初始化阶段的鲁棒性与效率。

Result: 在多种复杂环境下（包括低光照、部分遮挡、雨雾天气），所提方法均实现了高质量的3D场景重建，渲染保真度显著优于纯视觉GS方法。特别是当视觉信息缺失或退化时，仍能保持稳定的结构重建能力，验证了RF信息在提升系统鲁棒性方面的有效性。

Conclusion: 融合射频传感的多模态3D高斯溅射框架，能够有效克服纯视觉方法在极端条件下的局限性，实现更稳健、高效的3D场景重建与渲染。该方法为自动驾驶、机器人导航等实际应用提供了可靠的技术路径，具有广泛的应用前景。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [17] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: B$^3$-Seg 是一种无需训练、无相机依赖的快速3D高斯溅射（3DGS）分割方法，通过贝塔-伯努利贝叶斯更新和分析期望信息增益（EIG）主动选择视角，实现开集词汇下的实时交互分割。其理论保证了信息增益的单调性和次模性，可获得最优采样策略的(1−1/e)近似解。实验表明，该方法在多个数据集上达到与高成本监督方法相当的效果，且能在数秒内完成端到端分割，具备信息效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS分割方法依赖预设相机视角、真值标签或昂贵重训练，难以满足影视和游戏制作中对低延迟实时编辑的需求。亟需一种无需训练、不受相机限制、能快速响应的交互式分割方法。

Method: 将3DGS分割建模为序列贝塔-伯努利贝叶斯更新过程，利用分析形式的期望信息增益（EIG）主动选择最优视角，通过贪婪算法实现$(1{-}1/e)$近似最优视图采样策略。

Result: 在多个数据集上，B$^3$-Seg实现了与高成本监督方法相当的分割性能，且可在数秒内完成端到端分割，显著降低延迟，支持实时交互。

Conclusion: B$^3$-Seg 提供了一种理论完备、高效实用的开集3DGS交互分割方案，具备信息效率和可扩展性，适用于真实场景中的快速资产编辑。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [18] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++ is a unified framework addressing stealthiness and persistence in backdoor attacks on multimodal contrastive learning models. It introduces a semantic-fusion QR micro-trigger for imperceptible, task-relevant patterns and uses target-aligned subset selection to strengthen signals at low poisoning rates. For persistence, it employs radius shrinkage, centroid alignment, curvature control, and elastic weight consolidation to stabilize trigger embeddings and model parameters. Theoretical analysis shows co-directional gradients within a trust region, ensuring stable attack performance under fine-tuning. Experiments show 99.99% ASR with 0.3% poisoning, outperforming baselines by 11.4 points; maintains >99.90% ASR across 19 defenses with <0.8% clean accuracy drop; achieves 65.03% success in physical attacks and resists watermark removal.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks on multimodal contrastive learning models suffer from poor stealthiness due to cross-modal inconsistency exposing triggers, and weak persistence due to gradient dilution at low poisoning rates, leading to rapid backdoor forgetting under detection or fine-tuning. These issues remain inadequately addressed.

Method: BadCLIP++ proposes a semantic-fusion QR micro-trigger embedded near task-relevant regions to preserve clean-data statistics and produce compact trigger distributions. It uses target-aligned subset selection to enhance signal strength at low injection rates. For persistence, it applies radius shrinkage and centroid alignment to stabilize trigger embeddings, and curvature control and elastic weight consolidation to maintain model parameters in a low-curvature wide basin resistant to fine-tuning.

Result: With only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across 19 defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. It also achieves 65.03% success in physical attacks and demonstrates robustness against watermark removal defenses.

Conclusion: BadCLIP++ effectively addresses both stealthiness and persistence in backdoor attacks on multimodal contrastive learning models through a unified design that combines imperceptible trigger embedding, signal enhancement, and parameter stabilization. Theoretical insights confirm gradient co-directionality under fine-tuning, enabling sustained attack performance. Experimental results validate its high effectiveness and robustness across diverse settings.

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [19] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM提出一种基于3D高斯点云的单目非刚性SLAM系统，用于内窥镜场景，通过引入可学习的变形概率来解决相机运动与组织变形之间的耦合模糊问题，并采用贝叶斯自监督策略优化。系统包含可变形跟踪和映射模块，实现鲁棒的粗到精位姿估计与高效变形更新，同时结合几何先验提升重建质量。在多个公开数据集上表现优于现有方法，显著降低位姿误差并生成更逼真的三维重建结果。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景中软组织持续变形破坏了刚性假设，导致相机运动与形变之间存在强耦合模糊，现有单目非刚性SLAM方法缺乏有效解耦机制，依赖稀疏或低保真场景表示，造成跟踪漂移和重建质量差。

Method: 提出基于3D高斯点云的非刚性SLAM框架，引入变形感知的3D高斯地图，每个高斯原语附加可学习的变形概率；采用贝叶斯自监督策略进行优化；设计可变形跟踪模块，优先处理低变形区域进行粗到精位姿估计；开发可变形映射模块，逐步扩展与优化地图；引入统一的鲁棒几何损失，融合外部几何先验以缓解单目非刚性SLAM的病态性问题。

Result: 在多个公开内窥镜数据集上，NRGS-SLAM实现了高达50%的位姿估计均方根误差（RMSE）降低，生成更高保真度、更逼真的三维重建结果，且消融实验验证了各组件的有效性。

Conclusion: NRGS-SLAM通过引入变形感知的3D高斯表示与自监督优化机制，有效解耦相机运动与组织形变，显著提升了单目非刚性SLAM在内窥镜场景中的定位精度与重建质量，为未来自主内窥导航提供了有力支持。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [20] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出一种名为视觉信息收益（VIG）的困惑度基度量，用于衡量视觉输入带来的预测不确定性降低程度。VIG可在样本和词元层面进行细粒度分析，有效识别颜色、空间关系等视觉相关元素。基于此，作者设计了一种VIG引导的选择性训练方案，优先处理高VIG样本和词元，从而提升视觉对齐能力并缓解语言偏差，在显著减少监督需求的同时实现更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型常受语言偏差影响，依赖文本而非视觉证据生成答案；尽管已有方法尝试通过解码策略、架构修改或指令数据优化来缓解该问题，但缺乏对训练样本或词元实际受益于图像的量化评估，因此亟需一种可衡量视觉贡献的方法。

Method: 提出视觉信息收益（VIG），基于困惑度计算视觉输入对预测不确定性的减少量；利用VIG对训练样本和词元进行评分，并设计选择性训练策略，仅保留高VIG内容进行训练。

Result: 所提方法在减少标注数据使用量的前提下，显著提升了模型的视觉对齐能力，降低了语言偏差，且在多个基准上取得优于基线的性能表现。

Conclusion: VIG为理解视觉输入在模型决策中的作用提供了可量化工具，其引导的选择性训练机制能高效利用视觉信息，推动更鲁棒、更可靠的视觉语言模型发展。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [21] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 提出基于矩阵熵的熵坍缩层（ECL）概念，设计EntropyPrune框架，通过量化视觉令牌的信息价值实现高效无注意力图依赖的剪枝，利用双格拉姆矩阵谱等价性降低熵计算复杂度，理论加速达64倍；在多个基准上显著优于现有方法，在LLaVA-1.5-7B上实现68.2% FLOPs减少且保持96.0%性能，具备良好泛化性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌剪枝方法多依赖静态、经验选择的层，缺乏可解释性和跨模型迁移能力，亟需一种更普适、可解释的剪枝准则以提升多模态大模型推理效率。

Method: 提出矩阵熵视角，识别出信息量骤降的熵坍缩层（ECL），构建基于矩阵熵的剪枝框架EntropyPrune，利用双格拉姆矩阵的谱等价性加速熵计算，实现无需注意力图的高效冗余令牌剪枝。

Result: 在多种多模态基准上持续优于现有剪枝方法；于LLaVA-1.5-7B上实现68.2% FLOPs降低，性能保留率达96.0%；对高分辨率图像和视频模型也表现出强泛化性与可扩展性。

Conclusion: EntropyPrune通过引入矩阵熵机制，为多模态大模型提供了可解释、可迁移的剪枝策略，显著提升推理效率，具备广泛的应用前景。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [22] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 本文提出一种基于几何视角的文本到图像生成多样性增强方法——几何感知球面采样（GASS）。不同于依赖熵引导的传统方法，GASS通过分解CLIP嵌入中的多样性成分，分别控制与提示相关的语义变化和与提示无关的独立变化（如背景），并沿两个正交方向扩展生成轨迹上的投影分布，从而提升生成图像的多样性。实验表明，该方法在多种冻结的T2I模型（U-Net、DiT，扩散与流模型）和基准测试中均有效提升了多样性，同时对图像保真度和语义对齐影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型尽管在语义对齐上表现良好，但生成结果缺乏多样性，限制了用户选择并可能放大社会偏见。因此需要一种更有效的机制来增强生成多样性，同时保持图像质量与语义一致性。

Method: 提出几何感知球面采样（GASS），通过将CLIP嵌入中的多样性分解为与提示相关的文本嵌入方向和一个正交的提示无关方向（如背景），并在此基础上扩展生成轨迹上的几何投影分布，引导采样过程以增加多样性。

Result: 在多种冻结的T2I模型（包括U-Net、DiT，扩散与流模型）和多个基准测试中，GASS显著提升了生成图像的多样性，同时对图像保真度和语义对齐的影响微乎其微。

Conclusion: 通过引入几何感知的多样性控制机制，GASS实现了对提示相关与无关变化的有效解耦与增强，为提高文本到图像生成的多样性提供了新的有效路径，且无需牺牲生成质量。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [23] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文提出了一种基于基础设施的多模态货运车辆检测系统，融合激光雷达（LiDAR）与摄像头传感器，部署于信号交叉口和路段中，通过无线通信实现同步数据传输。系统采用混合感知架构，结合基于聚类与深度学习的检测方法，并使用卡尔曼滤波进行跟踪，实现高时空分辨率的车辆类型、位置与速度感知。通过地理参考系注册LiDAR数据，支持车道级定位与一致跟踪。实地评估表明系统在实时性与可靠性方面表现优异，为货运信号优先（FSP）应用提供了可落地的技术方案与实践启示。


<details>
  <summary>Details</summary>
Motivation: 货运车辆在通过信号交叉口时需要可靠的检测与运动估计以支持基础设施驱动的货运信号优先（FSP）策略。准确及时地感知车辆类型、位置和速度是实现有效优先控制的关键前提。现有感知技术在复杂交通环境下的精度与稳定性仍不足，亟需一种鲁棒、高精度的多模态感知系统来支撑智能交通管理。

Method: 采用融合LiDAR与摄像头的多模态传感架构，分为路口安装的子系统与路段中的子系统，通过无线通信实现数据同步；感知流程包含基于聚类与深度学习的检测算法，结合卡尔曼滤波实现目标跟踪；所有LiDAR数据均注册至地心坐标系，以实现车道级定位与连续追踪。

Result: 系统在实际道路环境中实现了对货运车辆运动状态的高时空分辨率可靠监测，具备良好的实时性与稳定性。部署与评估结果验证了该系统在支持货运信号优先应用方面的可行性与有效性，为未来智慧交通基础设施建设提供了实用设计范式。

Conclusion: 本研究成功设计并部署了一套基于多模态传感器的货运车辆感知系统，能够满足信号优先控制对车辆信息感知的高精度与低延迟需求。系统在真实场景中表现出良好性能，为后续城市交通智能化管理提供了关键技术支撑与实践经验。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [24] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: 提出EA-Swin模型，一种基于分块窗口注意力设计的嵌入无关型Swin Transformer，直接在预训练视频嵌入上建模时空依赖关系，兼容通用ViT风格的补丁编码器。同时构建了包含13万视频的EA-Video基准数据集，涵盖多种商业与开源生成器，并支持跨分布评估。实验表明，该方法在主流生成器上达到0.97-0.99准确率，优于现有最先进方法5-20%，且具备强泛化能力，为现代AI生成视频检测提供了可扩展、鲁棒的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成检测方法受限于浅层嵌入轨迹、图像适应或计算量大的多模态大模型，难以应对Sora、Veo等新型高保真视频生成系统带来的挑战，亟需更高效、通用且具备强泛化能力的检测机制。

Method: 提出EA-Swin模型，采用因子化窗口注意力机制，在预训练视频嵌入上直接建模时空依赖；结合通用的ViT式补丁编码器，实现对多种生成器输出的统一建模；同时构建大规模、多样化的EA-Video数据集，支持跨生成器分布评估。

Result: 在13万视频的EA-Video数据集上，EA-Swin在主要生成器上实现0.97-0.99的检测准确率，显著优于此前最先进方法（0.8-0.9），提升5-20%；并在未见过的生成器分布上保持良好泛化性能。

Conclusion: EA-Swin结合创新架构与高质量基准数据，成为当前最先进的可扩展、鲁棒的AI生成视频检测方案，为应对日益复杂的生成内容威胁提供了有效工具。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [25] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 本文提出了一种名为'附着锚点'（attachment anchors）的结构化表示方法，用于编码结直肠手术中组织与其解剖附着点之间的局部几何与力学关系。该方法通过将手术场景归一化到一致的局部参考系，降低了抓取点预测的不确定性。实验基于90例结直肠手术数据集表明，该方法在图像仅输入基线基础上显著提升了抓取点预测性能，尤其在未见手术类型和术者等分布外场景中表现突出，证明其作为学习驱动组织操作的中间表示具有有效性。


<details>
  <summary>Details</summary>
Motivation: 结直肠手术复杂且耗时长，目前研究中代表性不足，但因其重复性组织操作特性，是实现自主、机器学习辅助手术的理想切入点。准确预测抓取点是实现自主组织操作的关键挑战，现有方法在复杂多变场景中表现受限。

Method: 提出并利用'附着锚点'这一结构化表示，捕捉组织与解剖附着点间的局部几何与机械关系；从腹腔镜图像中预测附着锚点，并将其融入基于机器学习的抓取框架中。

Result: 在90例结直肠手术数据集上的实验表明，使用附着锚点可显著提升抓取点预测精度，尤其在未见过的手术程序和外科医生场景下效果更优，表现出良好的泛化能力。

Conclusion: 附着锚点是一种有效的中间表示，能有效降低抓取点预测的不确定性，推动学习型组织操作在结直肠手术中的应用，具备较强的实际潜力。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [26] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini 是一种基于解剖结构的鲁棒且多功能的图像配准框架，利用深度学习提供的预训练分割模型提取解剖特征点（通过区域质心），实现高效全局与局部仿射匹配，生成从仿射到多项仿射可调平滑度的变换，具有微分同胚性质。该方法在结构对齐方面优于主流基于强度的配准方法，并能有效提升后续非线性配准的初始化性能，具备快速、鲁棒、准确的特点，适用于医学图像处理流程集成。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像配准多依赖基于强度的方法，虽有效但缺乏解剖学意义；而基于特征的方法因难以可靠提取特征而被忽视。随着深度学习的发展，高质量的解剖分割已可即时获得，为构建真正基于解剖结构的配准方法提供了可能。

Method: 利用预训练分割模型获取解剖区域，通过提取各区域质心建立1对1对应关系，采用闭式解法进行全局与局部仿射匹配，结合日志欧几里得框架构建可调节平滑度的多项仿射变换，确保变换的微分同胚性。

Result: Polaffini 在结构对齐精度上优于现有强度基配准方法，显著改善了非线性配准的初始状态，同时具备高效、鲁棒、准确的优势，适合嵌入临床图像处理流程。

Conclusion: Polaffini 充分利用深度学习带来的精确解剖分割能力，实现了高效、可靠且具有解剖学意义的图像配准，为医学图像分析提供了一种强大而灵活的新工具。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [27] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 本文提出了南美洲首个10米分辨率的树作物地图，利用融合Sentinel-1和Sentinel-2遥感影像时序数据的多模态时空深度学习模型生成。该地图识别出约1100万公顷的树作物，其中23%与2000-2020年间森林覆盖损失相关。研究发现，现有支持欧盟零毁林法规（EUDR）的监管地图常将小规模农林业误判为“森林”，可能导致虚假毁林警报和对小农户的不公平处罚。本研究通过提供高分辨率基线数据，助力制定更有效、包容且公平的保护政策。


<details>
  <summary>Details</summary>
Motivation: 当前零毁林政策如欧盟的《无毁林产品法规》（EUDR）面临缺乏高分辨率数据的问题，难以区分农业系统与森林，尤其在小规模农林业区域易产生误判。这导致可能对小农户造成不公，并影响政策执行效果。因此亟需一种高精度、可区分树作物与森林的监测方法。

Method: 采用多模态、时空深度学习模型，结合Sentinel-1（雷达）和Sentinel-2（光学）卫星影像的时间序列数据，实现南美洲10米分辨率的树作物制图。模型通过训练学习不同地表覆盖类型的时序特征，精准识别树作物分布。

Result: 生成了南美洲首张10米分辨率树作物地图，识别出约1100万公顷树作物；其中23%与2000–2020年森林损失相关；同时发现现行监管地图常将小规模农林业误标为森林，存在重大误判风险。

Conclusion: 本研究提供的高分辨率树作物地图可有效减少监管误判，为零毁林政策提供科学依据，确保政策在保护环境的同时兼顾小农户权益，推动更加公平、可持续的农业与森林管理。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [28] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR是一种基于Retentive Networks（RetNet）的解码器仅模型，用于手写文本识别（HTR）。相比同等规模的Transformer解码器，其推理速度提升1.6-1.9倍，内存占用减少38-42%，且准确率无损失。通过用无softmax的保留机制替代注意力，并引入多尺度序列先验，避免了键值缓存的增长，使解码在时间和内存上均呈线性增长。为恢复注意力的局部到全局归纳偏置，提出分层伽马缩放，逐步扩大深层的有效保留范围，使浅层捕捉短程依赖，深层建模长程上下文，缓解移除softmax带来的灵活性差距。该模型在IAM-A（en）、RIMES（fr）、Bentham（en）上分别取得2.26%、1.81%、3.46%的最低字符错误率，在READ-2016（de）上达到4.21%，表现优异，证明了解码器仅的RetNet可在保持Transformer级精度的同时，显著提升解码效率和内存性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的手写文本识别系统普遍采用Transformer架构，但其不断增长的键值缓存导致解码过程缓慢且内存消耗大。因此，亟需一种既能保持高精度又能显著提升解码效率与内存利用率的新模型。

Method: 提出DRetHTR，一种基于Retentive Networks（RetNet）的解码器仅模型。采用无softmax的保留机制替代传统的softmax注意力，避免键值缓存增长；引入多尺度序列先验以增强建模能力；设计分层伽马缩放策略，逐步扩大深层的有效保留范围，从而恢复注意力机制中的局部到全局归纳偏置。

Result: DRetHTR在多个标准数据集上实现了领先的字符错误率：IAM-A（en）为2.26%，RIMES（fr）为1.81%，Bentham（en）为3.46%，在READ-2016（de）上达到4.21%。同时，推理速度提升1.6-1.9倍，内存使用减少38-42%，且未牺牲准确性。

Conclusion: DRetHTR表明，基于解码器仅的RetNet架构可以在保持Transformer级别手写文本识别精度的同时，实现显著更优的解码速度与内存效率，是高效高精度HTR系统的有效方案。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [29] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD is a novel, efficient multimodal approach for Generalized Category Discovery (GCD) that leverages CLIP's cross-modal image-concept similarities to create unified semantic representations. It uses a task-agnostic concept dictionary and introduces Spectral Filtering to retain only relevant concepts via a cross-modal covariance matrix from a teacher model. Forward and reverse knowledge distillation ensures semantic quality and alignment in the student model. The method achieves state-of-the-art or competitive performance across six benchmarks with significantly lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal GCD methods suffer from high computational cost and treat modalities independently, leading to suboptimal generalization. This work aims to improve efficiency and effectiveness by unifying image and text semantics through a shared representation grounded in explicit concepts.

Method: SpectralGCD employs CLIP-based cross-modal similarities to represent images as mixtures over a large, task-agnostic concept dictionary. It applies Spectral Filtering using a cross-modal covariance matrix derived from a strong teacher model to select relevant concepts. Knowledge distillation (forward and reverse) is used to preserve semantic richness and alignment in a lightweight student model.

Result: SpectralGCD achieves accuracy comparable to or better than state-of-the-art methods on six benchmarks while requiring significantly less computational resources. The approach demonstrates strong generalization and efficiency.

Conclusion: SpectralGCD provides an effective and efficient solution for GCD by leveraging unified multimodal semantics and spectral filtering, enabling robust discovery of novel categories with minimal reliance on visual artifacts and low computational overhead.

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [30] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文综述了计算机视觉在遥感领域的进展，特别是无人机搭载的RGB相机在光学遥感中的应用。文章梳理了该领域的关键数据集、方法与研究方向，旨在为新进入该领域的研究人员提供全面指引，并填补现有综述中缺乏整体视角的空白。


<details>
  <summary>Details</summary>
Motivation: 当前遥感领域研究繁多，但缺乏对光学遥感（尤其是基于无人机RGB影像）的系统性、全景式综述。现有文献多聚焦于特定任务或方法，难以帮助初学者快速把握全局。因此亟需一份涵盖广泛、结构清晰的综合性指南。

Method: 通过系统梳理近年来相关研究，整合主流任务、典型方法、公开数据集及技术趋势，构建一个高阶的知识框架，为研究者提供方向性参考。

Result: 成功构建了一个覆盖光学遥感核心内容的综合概述，包含关键数据集、代表性方法与研究趋势，具备良好的可读性与指导价值。

Conclusion: 本综述填补了现有文献在整体视角上的空白，为遥感领域的新研究者提供了宝贵的入门资源，有助于推动该领域更高效地发展。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [31] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: 提出EAGLE框架，一种无需微调的工业异常检测方法，通过专家模型输出引导多模态大模型（MLLMs）实现准确检测与可解释性分析。实验表明其在MVTec-AD和VisA数据集上性能媲美微调方法，且不需参数更新。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法多为二元决策，缺乏语义解释；虽多模态大模型可生成细粒度语言分析，但常需昂贵微调且检测精度提升有限。因此需要一种无需微调、兼具高精度与可解释性的新方法。

Method: EAGLE框架利用专家模型输出作为注意力引导信号，注入多模态大模型中，以增强对异常区域的关注，实现无需参数更新的异常检测与解释。通过分析中间层注意力分布，验证了异常检测成功与注意力集中于异常区域相关，且EAGLE有助于该对齐。

Result: 在MVTec-AD和VisA数据集上，EAGLE显著提升了多个MLLMs的异常检测性能，达到与微调方法相当的水平，且无需任何参数更新。

Conclusion: EAGLE是一种高效、无需微调的工业异常检测框架，能有效提升多模态大模型的检测准确性与解释能力，为智能制造中的异常检测提供了新思路。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [32] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 提出Local-EndoGS，一种用于任意相机运动下单目内窥镜序列的高质量4D重建框架。通过分窗的渐进式全局表示和粗到精策略，解决无立体深度或精确SfM初始化下的重建难题，并结合长程2D像素轨迹与物理运动先验提升形变合理性，在多个公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示或3D高斯溅射的方法多依赖固定视角和立体深度先验或准确的结构光恢复初始化，难以处理真实临床场景中具有大相机运动的单目序列。需要一种能应对任意相机运动、无需依赖立体信息的鲁棒4D重建方法。

Method: 提出分窗的渐进式全局表示，为每个观测窗口分配局部可变形场景模型；设计粗到精策略融合多视图几何、跨窗口信息与单目深度先验以实现可靠初始化；引入长程2D像素轨迹约束与物理运动先验增强形变合理性。

Result: 在三个包含可变形场景和不同相机运动的公开数据集上，Local-EndoGS在外观质量和几何精度方面均持续优于当前最先进方法；消融实验验证了各关键设计的有效性。

Conclusion: Local-EndoGS能够有效处理具有大运动的单目内窥镜序列，具备良好的可扩展性与鲁棒性，是临床场景中4D手术场景重建的重要进展。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [33] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种物理感知的多模态框架，用于解决二维量子材料光学显微图像分析中的挑战。通过合成数据生成器Synthia、首个大规模物理信息问答数据集QMat-Instruct、物理感知指令微调方法QuPAINT，以及跨材料与成像条件的基准测试QF-Bench，实现了对材料厚度和形貌的准确理解，提升了模型在不同实验设置下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型缺乏物理先验知识，难以在二维量子材料的光学显微图像中实现有效表征，尤其受限于层间对比度微弱、标注数据稀缺及实验室间成像差异大等问题。因此需要结合物理规律与多模态学习来提升模型鲁棒性与可迁移性。

Method: 提出Synthia（基于物理的合成数据生成器），模拟薄膜干涉下的光学响应；构建QMat-Instruct（首个大规模物理信息问答数据集）；设计QuPAINT（物理感知注意力模块的多模态架构），融合视觉特征与光学先验；建立QF-Bench基准，支持跨材料、基底与成像条件的标准化评估。

Result: 所提框架显著提升了对二维量子材料薄片厚度与外观的理解能力，在多种硬件与材料条件下表现出更强的泛化性能，减少了对人工标注的依赖，并为未来研究提供了可复现的评估标准。

Conclusion: 该工作通过融合物理先验与多模态学习，构建了一个高效、可扩展且具备强泛化能力的分析框架，为二维量子材料的自动化表征提供了新范式。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [34] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 本文提出PixTrace和CopyNCE，通过像素级坐标追踪与几何引导对比损失，提升图像复制检测中细粒度对应关系的学习能力，显著增强对复杂编辑的鲁棒性，并在DISC21数据集上达到当前最优性能（匹配器88.7% uAP / 83.9% RP90，描述符72.6% uAP / 68.4% RP90），同时具备更强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于视图级对比的自监督学习方法在处理复杂图像编辑时表现不足，主要因为缺乏细粒度的对应关系建模能力，导致特征表示学习不充分。

Method: 提出PixTrace模块以显式维护编辑变换下的像素空间映射关系；设计CopyNCE损失函数，利用PixTrace生成的重叠比例来正则化块间相似性，实现几何引导的对比学习。

Result: 在DISC21数据集上，匹配器达到88.7% uAP / 83.9% RP90，描述符达到72.6% uAP / 68.4% RP90，优于现有方法，且模型输出更具可解释性。

Conclusion: 通过融合像素级可追溯性与块级相似性学习，该方法有效提升了自监督图像复制检测的精度与鲁棒性，为复杂图像篡改识别提供了新思路。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [35] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 该研究提出一种基于深度图和基础姿态估计器的增强现实方法，用于腹腔镜肝手术中的肿瘤定位。通过用非刚性迭代最近点（NICP）替代传统的有限元（FE）变形模型，降低了工程复杂性和建模要求。在真实患者数据上，该方法实现了9.91毫米的平均配准误差，且刚性-NICP联合配准优于仅刚性配准，表明NICP是高效且轻量化的变形建模替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有注册流程依赖器官轮廓，且非刚性对齐通常需要复杂的有限元模型或机器学习组件，增加了工程复杂性和专业门槛。本研究旨在降低此类技术的使用难度，提升临床实用性。

Method: 结合腹腔镜深度图与基础姿态估计器进行相机-肝脏姿态估计，并采用非刚性迭代最近点（NICP）代替传统有限元模型实现非刚性配准。

Result: 在3例真实患者数据中，深度图增强的基础姿态方法达到9.91毫米的平均注册误差；刚性与NICP联合注册显著优于仅刚性注册，验证了NICP作为轻量化、高效替代方案的有效性。

Conclusion: 该方法在保持临床可接受精度的同时，提供了一种更轻量、工程友好、无需复杂建模的非刚性配准解决方案，适用于腹腔镜肝手术中的实时增强现实导航。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [36] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出LATA（拉普拉斯辅助转换适应）方法，一种无需训练和标签的医学视觉语言模型不确定性校准方法，通过在图像-图像k-NN图上平滑零样本概率，结合小规模CCCP平均场更新，在保持分割共形预测（SCP）有效性的同时，显著提升预测集效率与类别间覆盖平衡性。引入故障感知共形评分，增强实例级难度判断与标签合理性评估，改善少数类表现。该方法黑箱、计算轻量，支持无标签或半标签模式，广泛验证下优于现有方法且资源消耗极低。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型虽具备强零样本识别能力，但在领域偏移下依赖可靠的不确定性校准；传统分割共形预测虽提供有限样本覆盖率保障，但存在预测集过大（效率低）和类别间覆盖不平衡（高类别条件覆盖差距）问题，尤其在少样本、数据不平衡场景下更严重；此外，直接使用校准标签会破坏交换性，导致理论保证失效。因此亟需一种不依赖训练和标签、能同时提升效率与公平性的校准方法。

Method: 提出LATA方法：基于图像-图像k-NN图对零样本输出进行拉普拉斯平滑，通过少量CCCP平均场迭代实现转导式概率修正；设计故障感知共形评分，融入ViLU框架以评估实例难易度与标签可信度；所有操作均在联合校准与测试池上完成，采用确定性变换保持交换性，确保SCP理论有效性；支持完全无标签或可选地利用校准边缘分布信息。

Result: 在三种医学VLM和九个下游任务上的实验表明，LATA一致减少预测集大小并降低类别条件覆盖差距（CCV），在维持或提升目标覆盖率的同时，性能超越现有转导基线，逼近甚至部分超过依赖标签的方法，且计算开销远低于对比方法。消融实验与可视化分析显示其有效锐化了原始零样本预测，未破坏交换性。

Conclusion: LATA是一种高效、可靠、通用的医学视觉语言模型不确定性校准框架，无需模型更新、计算轻量、兼容黑箱部署，可在无标签或弱标签条件下显著提升预测集质量与类别公平性，为临床应用中安全可靠的零样本推理提供了有力支撑。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [37] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: GraphThinker 是一种基于强化微调的方法，通过构建事件级场景图并增强视觉定位，减少视频推理中的幻觉。它利用多模态大模型生成事件间关系的显式结构，并在训练中引入视觉注意力奖励以提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集描述或视频摘要进行视频推理，但缺乏对因果关系的显式建模，导致幻觉问题严重。需要更精确地捕捉事件间的因果和空间关系。

Method: 提出 GraphThinker，首先使用 MLLM 构建事件级视频场景图（EVSG），显式建模事件内部与事件之间的关系；将该图作为中间推理过程注入 MLLM；并在强化微调中加入视觉注意力奖励，以加强视觉接地能力。

Result: 在 RexTime 和 VidHalluc 两个数据集上，GraphThinker 在事件关系理解、对象与事件定位精度方面均优于先前方法，显著降低了视频推理中的幻觉现象。

Conclusion: 通过显式建模因果结构并结合视觉注意力强化，GraphThinker 有效提升了视频推理的准确性和鲁棒性，为解决多模态视频理解中的幻觉问题提供了新思路。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [38] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 提出ArtToMus框架和ArtSound数据集，实现直接从艺术作品生成音乐，跳过图像到文本的转换，利用视觉嵌入引导音乐合成，实验表明生成音乐在风格和语义上与原作一致，虽绝对对齐度低于文本条件系统，但具有竞争力的感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像条件音乐生成系统受限于自然照片训练数据及依赖图像到文本转换的语义捷径，无法实现直接视觉到音频学习。

Method: 构建ArtSound大规模多模态数据集（105,884对艺术作品-音乐），提出ArtToMus框架，通过将视觉嵌入投影到潜在扩散模型的条件空间，实现无语言监督的直接艺术作品到音乐生成。

Result: ArtToMus生成的音乐在音乐连贯性和风格一致性方面表现良好，能反映源艺术作品的关键视觉线索；尽管绝对对齐得分低于文本条件系统，但感知质量高且跨模态对应有意义。

Conclusion: 本研究确立了直接视觉到音乐生成作为独立且具有挑战性的研究方向，并提供了支持多媒体艺术、文化遗产和AI辅助创作应用的资源。代码与数据集将在接受后公开发布。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [39] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 提出了一种统一的地理空间发现框架，结合主动学习、在线元学习和概念引导推理，通过概念相关性机制提升在数据稀缺和动态环境下的目标发现效率。


<details>
  <summary>Details</summary>
Motivation: 在环境监测、灾害响应等实际场景中，数据收集成本高且环境动态变化，传统基于学习的方法受限于稀疏和偏差的地理真值数据，亟需高效的战略采样方法。

Method: 引入概念相关性概念，设计了概念加权不确定性采样策略和相关性感知的元批次构建策略，以增强模型在动态环境中的泛化能力。

Result: 在真实世界PFAS污染数据集上的实验表明，该方法能在数据有限且环境多变的情况下可靠地发现目标，显著优于现有方法。

Conclusion: 所提出的框架有效解决了地理空间发现中数据稀缺与环境动态性的挑战，为资源受限场景下的目标探测提供了可行解决方案。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [40] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出CORAL框架，通过显式对齐查询-键匹配来改善基于DiT的虚拟试穿中的人物-服装对应关系，利用对应蒸馏损失和熵最小化损失增强注意力分布，显著提升全局形状迁移与局部细节保留效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在无配对设置下难以保持精细服装细节，且未明确强制人物-服装对齐，缺乏对扩散变压器中对应关系如何生成的解释。

Method: 分析DiT架构中的全3D注意力机制，提出CORAL框架，包含对应蒸馏损失和熵最小化损失，以显式对齐人物-服装查询-键匹配，并引入基于视觉语言模型的评估协议。

Result: CORAL在多个指标上优于基线，显著提升全局形状转移与局部细节保留能力，消融实验验证了设计的有效性。

Conclusion: CORAL通过显式建模人物-服装对应关系，在无配对虚拟试穿任务中实现了更高质量的图像生成，为基于扩散模型的对齐问题提供了新思路。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [41] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec是一种交互式物体检索框架，通过用户反馈不断优化预测。它使用意图状态（IS）维护正锚点和负约束的双重记忆，利用对比对齐函数提升候选物体的排序精度，在复杂场景中实现细粒度消歧。该方法在无需额外监督的情况下显著提高检索准确率，在LVIS上达到35.4 AP，优于多个基线模型；在LVIS-Ambiguous上单次反馈后性能提升7.9 AP，延迟低于30毫秒。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇检测器为单次推理，无法根据用户反馈进行预测优化，尤其在查询模糊或存在多个相似物体时表现不佳。因此需要一种能够通过交互反馈持续改进的检索机制。

Method: 提出IntRec框架，核心是意图状态（IS），包含正锚点（确认线索）与负约束（拒绝假设）的双记忆集；通过对比对齐函数，最大化与正锚点的相似性并惩罚已拒绝的候选对象，实现精细排布与消歧。

Result: 在LVIS数据集上，IntRec达到35.4 AP，较OVMR、CoDet、CAKE分别提升+2.3、+3.7、+0.5；在更具挑战性的LVIS-Ambiguous上，单次纠正反馈后性能提升+7.9 AP，每轮交互延迟小于30毫秒。

Conclusion: IntRec通过引入可交互的意图状态机制，实现了基于用户反馈的精准物体检索，在保持低延迟的同时显著提升复杂场景下的准确性，为开放词汇物体检索提供了高效且实用的新范式。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [42] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 本文提出LIBERO-CF基准以系统研究视觉-语言-动作模型（VLAs）在缺乏场景特定监督时的反事实失败问题，即模型依赖视觉捷径而非语言意图。为此，作者设计了反事实行动引导（CAG）方法，通过双分支推理机制结合标准VLA与语言无关的视觉-动作模块，实现行动选择中的反事实对比，从而减少对视觉捷径的依赖。CAG无需额外演示或架构修改，可即插即用，显著提升语言遵循准确性和任务成功率，尤其在低观测任务上表现优异，并在真实世界实验中有效降低反事实行为9.4%，提升任务成功率17.2%。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在缺乏强场景监督时容易因数据集偏差产生视觉捷径，导致行为与语言指令不符，即反事实失败。然而此类问题尚未被系统研究，亟需新评估基准和解决方案。

Method: 提出反事实行动引导（CAG）方法，采用双分支结构：一个标准语言条件化的VLA策略，另一个语言无关的视觉-动作（VA）模块，通过两者在行动选择阶段进行对比，显式正则化语言条件作用，减少对视觉捷径的依赖。该方法无需额外训练、不修改预训练模型或架构，具备良好的可集成性。

Result: 在LIBERO-CF基准上，CAG在无训练策略下使语言遵循准确率提升9.7%（π_{0.5}），低观测任务成功率提升3.6%；与VA模型结合后分别提升15.5%和8.5%。真实世界测试中，反事实失败率下降9.4%，平均任务成功率提升17.2%。

Conclusion: 反事实失败是当前VLAs的重要缺陷，但可通过CAG等简单有效的双分支推理机制有效缓解。该方法具有高度通用性和实用性，为提升语言理解与机器人控制的对齐提供了可靠路径。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [43] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文研究如何利用参考引导的LLM评估器作为软验证器，以解决强化学习中不可验证领域（如LLM对齐）缺乏真实验证器的问题。通过设计基于参考输出的评估协议，提升低能力LLM评估器的准确性，并进一步在对齐调优中使用高质量参考进行自提升，显著优于直接SFT和无参考的自提升方法，性能接近人工标注奖励模型ArmoRM。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励（RLVR）方法无法应用于缺乏真实验证器的非可验证领域（如LLM对齐），需要一种替代验证机制来指导模型优化。

Method: 设计参考引导的评估协议，利用前沿模型或人类编写的参考输出增强LLM评估器性能，并基于这些改进的评估器实现参考引导的自提升训练。

Result: 在AlpacaEval和Arena-Hard上，所提方法分别取得73.1%和58.7%（Llama-3-8B-Instruct）以及70.0%和74.1%（Qwen2.5-7B）的得分，相比SFT蒸馏平均提升20.2/17.1点，相比无参考自提升提升5.3/3.6点，表现接近强监督奖励模型ArmoRM。

Conclusion: 参考引导的LLM评估器能够有效支持非可验证领域的LLM后训练，为在缺乏真实验证器的场景下实现高效对齐提供了可行路径。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [44] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 该研究针对希腊语问答（QA）任务，提出DemosQA数据集、一个可适应多种语言和数据集的内存高效LLM评估框架，并对11个单语和多语LLM在6个人工精选的希腊语QA数据集上进行了广泛评估。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要针对高资源语言（如英语），而对低资源语言的关注较少，且多依赖从高资源语言到低资源语言的迁移学习，可能导致社会、文化和历史方面的失真。因此，需要更有效的单语模型来准确反映低资源语言的特性。

Method: 构建DemosQA数据集，利用社交媒体用户问题和社区评审答案以捕捉希腊语的社会文化背景；设计并实现一个内存高效的评估框架；在多个希腊语QA数据集上使用三种提示策略对11个单语和多语模型进行系统性评估。

Result: 实验结果表明，尽管多语模型表现良好，但部分单语模型在特定希腊语任务中表现出更强的语言理解能力，验证了为低资源语言定制单语模型的有效性。同时，所提出的评估框架具有良好的可扩展性和效率。

Conclusion: 本研究填补了低资源语言（特别是希腊语）在大型语言模型评估方面的研究空白，证明了为特定语言开发单语模型的价值，并强调了社会文化上下文在构建高质量语言数据集中的重要性。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [45] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 本文提出了一种基于流模型（FLM）的语言生成方法，通过在独热编码上进行欧氏去噪，实现了比离散扩散模型更快且质量更高的生成效果。通过引入时间重参数化，提升了训练稳定性和生成质量，并通过蒸馏得到可实现少步生成的蒸馏流映射语言模型（FMLM）。在LM1B和OWT数据集上，该方法在少步生成方面显著优于现有模型，甚至单步生成质量超过其他模型8步生成的表现。研究挑战了离散扩散过程对离散模态建模必要性的普遍假设，为大规模加速流模型语言生成提供了新路径。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散扩散的语言模型在少步生成时样本质量急剧下降，未能实现比自回归模型更快生成的潜力。因此，需要探索更高效的生成范式。

Method: 提出基于流的连续去噪语言模型（FLM），在独热编码空间中执行欧氏去噪；采用交叉熵目标训练，并引入时间重参数化以提升训练稳定性与生成质量；通过蒸馏获得可实现少步生成的蒸馏流映射语言模型（FMLM）。

Result: FLM在LM1B和OWT数据集上的生成质量达到当前先进离散扩散模型水平；FMLM在少步生成上全面超越近期模型，单步生成质量超过其他模型8步生成的质量。

Conclusion: 本研究质疑了离散扩散过程对离散模态生成必要性的普遍观点，表明基于流的模型在速度和质量上具有显著优势，为大规模高效语言建模开辟了新方向。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [46] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 该研究提出一种本地部署的、具有治理意识的语言建模组件，利用数百万历史保修索赔数据，通过低秩适应（LoRA）微调预训练大语言模型（LLM），从非结构化索赔叙述中生成结构化的纠正措施建议。该模块被嵌入索赔处理流程的初始决策阶段，以加速理赔员的决策。通过结合自动化语义相似度指标与人工评估的多维评估框架，验证了该模块在实用性和预测准确性方面的表现。结果表明，领域特定微调显著优于商用通用和基于提示的LLM，约80%的案例达到与真实纠正措施近乎一致的结果。研究提供了理论和实证证据，证明领域自适应微调能够使模型输出分布更贴近实际运营数据，展示了其在保险应用中的可靠性和可管理性潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用语言任务上表现优异，但在保险等受监管和数据敏感的领域部署受限。如何在保护数据隐私的同时提升模型在特定领域的实用性，是当前亟需解决的问题。

Method: 采用低秩适应（LoRA）对预训练大语言模型进行领域特定微调，构建一个本地部署的治理感知语言建模组件，用于从非结构化索赔文本生成结构化纠正措施建议，并将其集成到索赔处理流程的初始决策模块中。

Result: 领域特定微调显著优于商用通用模型和基于提示的模型，约80%的案例生成的纠正措施与真实情况高度匹配；多维度评估框架验证了该方法在实用性和准确性上的优越性。

Conclusion: 领域自适应微调能够有效对齐模型输出分布与真实运营数据，为保险等敏感领域的可信、可控应用提供可靠且可管理的技术基础。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [47] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: 提出BanglaSummEval，一个用于评估孟加拉语摘要事实一致性的无参考、基于问答的框架，利用多语言指令微调模型生成问题与答案，并通过BERTScore-Recall衡量语义一致性，在医疗和教育领域的人工摘要上表现出与专家判断高度相关性。


<details>
  <summary>Details</summary>
Motivation: 现有事实一致性评估指标忽视孟加拉语等低资源语言，且依赖参考摘要，缺乏对语义一致性的深入捕捉。

Method: 采用多语言指令微调语言模型自动生成问题与答案，结合候选答案提取与问题重要性加权，使用BERTScore-Recall进行答案比较，实现统一、低成本的评估流程。

Result: 在300个人工编写的孟加拉语摘要上验证，与专家判断的相关性分别为Pearson's $r = 0.694$，Spearman's $ρ= 0.763$，表现良好。

Conclusion: BanglaSummEval提供了一种可解释、透明且适用于低资源语言的事实一致性评估方案，具有实际应用价值。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [48] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 该研究首次针对德国美因茨方言（Meenzerisch）开展自然语言处理（NLP）工作，构建了一个包含2351个方言词及其标准德语释义的数字词典，并评估大语言模型（LLMs）在生成方言词定义和根据定义生成方言词方面的表现。实验结果显示，现有LLMs性能极低，准确率分别仅为6.27%和1.51%，即使采用少样本学习或规则提取方法，准确率仍低于10%。研究强调亟需更多资源与对德语方言的深入研究。


<details>
  <summary>Details</summary>
Motivation: Meenzerisch方言正面临消亡，而自然语言处理技术有潜力帮助其保护与复兴。然而，目前尚无针对该方言的NLP研究，因此本研究旨在填补这一空白。

Method: 构建一个基于历史资源（Schramm, 1966）的数字化词典，包含2351个Meenzerisch词汇及其标准德语释义；利用该数据集评估大语言模型在生成方言词定义和反向生成方言词方面的表现；并尝试通过少样本学习和规则提取提升模型性能。

Result: 大语言模型在生成方言词定义和生成方言词任务中表现不佳，最佳模型准确率分别为6.27%和1.51%；尽管少样本学习和规则提取有所改善，但准确率仍低于10%。

Conclusion: 当前大语言模型在处理德语方言如Meenzerisch方面能力有限，亟需更多高质量资源与专门研究以推动方言的数字化保存与复兴。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [49] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.CL

TL;DR: 该研究探讨了RSA在经典和量子攻击下的脆弱性，提出了一种混合安全框架以应对后量子时代的数据保护需求。该框架结合了AES加密、BB84量子密钥分发（用于安全密钥交换并检测窃听）、量子态比较实现轻量级认证，以及基于生物免疫系统的自适应威胁检测机制。虽然模型展示了良好的理论前景，但目前仅为概念性设计，缺乏详细实现、安全证明和实验验证，这些将作为未来工作。


<details>
  <summary>Details</summary>
Motivation: RSA等经典公钥密码体制面临量子计算的严峻挑战，尤其是Shor算法能高效破解大数分解问题，因此需要构建能够抵御量子攻击的新型安全框架。同时，现有方法在安全性与效率之间难以平衡，亟需融合经典与量子技术的混合解决方案。

Method: 提出一种融合多种技术的混合安全框架：使用AES保障经典通信安全；采用BB84 QKD实现安全密钥分发并具备窃听检测能力；利用量子态比较进行轻量级身份认证；引入生物启发式免疫系统实现对新型威胁的自适应检测。

Result: 所提出的框架在理论上具备抵御经典与量子攻击的能力，支持可扩展性和自适应性，在后量子时代提供多层安全保障。然而，当前结果仍停留在概念阶段，未经过完整实现或实证测试。

Conclusion: 本研究设计了一个面向后量子时代的混合安全框架，整合了经典与量子安全技术，为未来抗量子密码体系提供了有价值的思路。尽管尚未完成具体实现与验证，但其架构具有前瞻性，为后续深入研究奠定了基础。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


### [50] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 本研究探讨了跨语言等价性对多语言委婉语检测中迁移学习的影响。通过将土耳其语和英语中的潜在委婉语词（PETs）分为重叠（OPETs）和非重叠（NOPETs）两类，发现语义重叠不足以保证正向迁移，尤其在低资源的土耳其语到英语方向上，性能可能下降，甚至在基于NOPETs训练时有所提升。标签分布差异解释了这些反直觉结果。类别级分析表明，领域特定对齐可能影响迁移效果，但受限于数据稀疏性，证据有限。


<details>
  <summary>Details</summary>
Motivation: 委婉语依赖于文化和语用背景，跨语言建模具有挑战性。理解跨语言等价性如何影响多语言委婉语检测中的迁移学习，有助于提升模型在不同语言间的泛化能力。

Method: 将土耳其语和英语中的潜在委婉语词（PETs）根据功能、语用和语义对齐情况划分为重叠（OPETs）和非重叠（NOPETs）子集，并在多语言设置下评估迁移性能。

Result: 发现存在迁移不对称性：即使语义重叠，也不保证正向迁移；在土耳其语到英语的低资源方向，性能可能下降，而基于非重叠项（NOPETs）的训练反而有时能提升表现。标签分布差异是主要原因。领域特定对齐可能有影响，但受数据稀疏性限制。

Conclusion: 语义重叠并非跨语言委婉语迁移成功的充分条件，迁移效果受语言间标签分布及领域对齐程度影响显著，需考虑语言特异性因素以优化多语言模型设计。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [51] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本研究探讨了跨语言文本分类方法在分析多语言社交媒体话语中的应用，以氢能源为例，分析2013–2022年超过九百万条英文、日文、印地文和韩文推文。针对关键词检索带来的噪声问题，比较了四种内容过滤策略：（1）将英语标注数据翻译至目标语言构建语言特定模型；（2）将多语言未标注数据翻译为英语并基于英语标注训练单一模型；（3）直接使用英语微调的多语言变压器处理各语言数据；（4）结合翻译标注与多语言训练的混合策略。结果揭示了翻译与多语言方法之间的权衡，为大规模跨语言社交媒体分析提供了优化建议。


<details>
  <summary>Details</summary>
Motivation: 多语言社交媒体话语分析在自然语言处理中仍具挑战性，尤其当全球公共辩论涉及多种语言时。现有基于关键词的数据采集常引入大量无关内容，亟需有效过滤与分析方法以支持可靠的大规模跨语言对话研究。

Method: 采用四种跨语言文本分类策略对多语言推文进行相关性过滤：（1）基于翻译标注数据构建语言特定模型；（2）将多语言数据统一翻译为英语后使用英语标注训练单模型；（3）直接应用英语微调的多语言模型于各语言数据；（4）结合翻译标注与多语言训练的混合方法。随后对筛选后的数据进行主题建模以提取核心议题。

Result: 实验表明，翻译驱动的方法在特定语言上表现较好，但成本高；多语言模型具备更强泛化能力但存在语言偏差；混合策略在准确性和效率间取得较好平衡，尤其适用于大规模多语言分析任务。

Conclusion: 在跨语言社交媒体分析中，应根据数据规模、语言分布与资源可用性权衡选择翻译或多语言策略。混合方法在实际应用中展现出最优潜力，可为未来大规模全球话语分析提供有效框架。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [52] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: ALPS是首个专注于阿拉伯语深层语义与语用学的原生、专家标注诊断数据集，包含531个精心设计的问题，覆盖15个任务和47个子任务。它通过消除翻译偏差，确保文化真实性和语言深度，评估23种模型的表现。结果显示，尽管顶级商业模型（如Gemini-3-flash）达到94.2%准确率，但普遍在形态句法依赖上表现不佳，尤其在依赖符号的任务中错误率达36.5%，远高于组合语义任务。最佳阿拉伯语专用模型（Jais-2-70B）为83.6%，仍低于人类平均（84.6%）和专家基准（99.2%），揭示出当前模型在深层语言理解上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语NLP基准多依赖合成或翻译数据，缺乏对深层语言结构的真实验证。为提升对阿拉伯语复杂语法和语用能力的评估，需一个基于母语、专家校准的高质量诊断数据集，以揭示模型在形态句法与语义理解方面的实际短板。

Method: 构建ALPS数据集，采用阿拉伯语语言学深度知识进行问题设计，确保文化真实性与语言准确性；涵盖15个任务、47个子任务，共531个问题；使用单次人类表现（平均84.6%）和专家仲裁的最优基准（99.2%）作为评估标准，测试23种不同类型的模型（包括商业、开源及阿拉伯语专用模型）。

Result: 模型在流畅性上表现良好，但在关键形态句法依赖任务中错误率高达36.5%（尤其在依赖符号的任务中），远高于组合语义任务。顶级商业模型（Gemini-3-flash）达94.2%，超越平均人类表现，但阿拉伯语专用模型（如Jais-2-70B）仅83.6%，未达人类水平。表明当前模型在深层语言理解方面存在明显缺陷。

Conclusion: ALPS揭示了现有阿拉伯语NLP模型在深层语义与形态句法理解上的严重不足，凸显了对高质量、原生语言数据集的需求。未来工作应聚焦于增强模型对阿拉伯语复杂语言结构的理解能力，以缩小与人类表现的差距。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [53] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 提出BankMathBench，一个针对银行领域数值推理的多难度层级数据集，用于提升大语言模型在存款、贷款等金融计算任务中的准确性。通过工具增强微调，模型在基础、中级和高级任务上分别实现57.6%、75.1%和62.9%的准确率提升，显著优于零样本基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在银行核心计算任务中表现不佳，如利息计算、产品比较和提前还款场景下的支出估算，且缺乏针对真实银行业务场景的评估基准。数学数据集过于基础，金融文档基准不覆盖日常计算需求，因此亟需一个专注于银行数值推理的专用数据集。

Method: 构建BankMathBench数据集，涵盖三个难度层级：基础（单产品推理）、中级（多产品对比）和高级（多条件场景）。采用工具增强的微调方法训练开源LLM，以提升其公式生成与数值推理能力。

Result: 在BankMathBench上训练后，模型在公式生成和数值推理方面均有显著提升；使用工具增强微调后，平均准确率分别提升57.6个百分点（基础）、75.1个百分点（中级）和62.9个百分点（高级），验证了该数据集的有效性。

Conclusion: BankMathBench是一个可靠的基准，能够有效评估和推动大语言模型在真实银行场景中的数值推理能力，为金融领域LLM的应用提供了重要支持。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [54] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 该论文提出一种基于心理测量学理论的审计框架，利用序数不确定性下的潜在特质估计，无需依赖真实标签即可量化大型语言模型中持久的行为特征。通过强制选择的序数情景与语义正交干扰项结合，并采用加密的置换不变性设计，对九个主流模型在优化偏见、谄媚倾向和现状合理化等维度进行评估。使用混合线性模型（MixedLM）和组内相关系数（ICC）分析发现，尽管项目层面的表述导致高方差，但存在显著的行为聚类‘实验室信号’，表明在封闭的提供者生态系统中，潜在偏见不仅是静态错误，更是会递归放大的变量，可能在多层AI架构中形成意识形态回音室。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从独立聊天界面演变为多智能体系统和递归评估循环中的基础推理层，检测持久的、供应商级别的行为特征成为安全与治理的关键需求。传统基准测试仅衡量临时任务准确性，无法捕捉训练和对齐过程中嵌入的稳定、潜在的响应策略，即‘主导心态’。

Method: 采用心理测量学中的潜在特质估计方法，在序数不确定性下进行建模；使用带有语义正交干扰项的强制选择情景，并通过加密的置换不变性确保数据安全性；应用混合线性模型（MixedLM）与组内相关系数（ICC）分析行为模式的稳定性与聚类特征。

Result: 尽管个体题目表述引发较高变异，但识别出显著的‘实验室信号’，表明模型间存在稳定的、可复制的行为聚类；这些潜在偏见在封闭生态中不是静态误差，而是可能在递归多层架构中自我强化，形成意识形态回音室。

Conclusion: 本研究证明，大语言模型中的潜在行为倾向具有持久性和系统性，需在安全与治理中纳入长期审计机制。若不加干预，这些偏见将在多智能体系统中演化为递归强化的意识形态闭环，威胁系统的公平性与可信度。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [55] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 本文提出一种心理测量框架，用于量化和缓解大语言模型（LLMs）在问卷评估中的社会期许反应（SDR）偏差。通过在诚实与伪装良好两种指令下施测同一量表，利用项目反应理论（IRT）估算潜在得分，并计算方向校正的标准效应量来量化SDR。为减轻偏差，构建了一个匹配效价的分级强制选择（GFC）大五人格量表，通过约束优化选出30个跨领域配对项目。在九个指令微调的LLMs上测试合成人格时，李克特量表显示出显著的SDR，而匹配效价的GFC量表显著降低SDR，同时保持对目标人格特征的良好恢复。结果揭示了模型依赖的SDR-恢复权衡，呼吁在问卷基准测试中采用考虑SDR的报告实践。


<details>
  <summary>Details</summary>
Motivation: 现有基于自陈问卷的LLM评估方法假设模型会诚实作答，但在评价情境下，模型可能倾向于给出社会期望的答案，即社会期许反应（SDR），从而导致评估结果偏差。因此需要一种可量化并缓解此偏差的方法，以提高评估的可靠性与有效性。

Method: 采用项目反应理论（IRT）分析双条件问卷数据（诚实与伪装良好指令），计算方向校正的标准化效应量以量化SDR；通过约束优化从项目池中选取30个跨领域配对项，构建效价匹配的分级强制选择（GFC）大五人格量表，用于评估模型在不同指令下的表现。

Result: 在九个指令微调的LLMs上，李克特量表显示显著的社会期许反应（SDR）偏差；而使用效价匹配的分级强制选择（GFC）量表后，SDR被显著削弱，同时保留了对目标人格特征的准确恢复能力。结果表明存在模型依赖的SDR与人格恢复之间的权衡关系。

Conclusion: 本研究揭示了问卷评估中社会期许反应对大语言模型评测的显著影响，提出了有效的量化与缓解策略。建议未来在基于问卷的模型基准测试和审计中引入SDR意识，改进评估方法的严谨性与可信度。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [56] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 提出X-Value跨语言价值观评估基准，用于评测大语言模型在多语言环境下对深层价值的识别能力。该基准涵盖18种语言、5000+问答对，基于舒瓦茨基本人类价值观理论划分7个核心领域，并设计两阶段标注框架以区分全球共识与多元价值。实验表明当前SOTA模型在跨语言价值观评估上表现不佳（准确率<77%），且不同语言间差异显著（准确率差>20%），凸显提升模型价值感知能力的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型内容安全评估主要关注显性危害（如暴力、仇恨言论），忽视了数字内容中隐含的细微价值观维度。为填补这一空白，亟需构建能从全球视角评估深层价值观的评测基准。

Method: 构建X-Value跨语言价值观评估基准，包含5000+跨语言问答对，依据舒瓦茨理论分为7个核心领域；采用两阶段标注框架：第一阶段判断议题是否属于全球共识或多元主义范畴，第二阶段进行多方价值评估。

Result: 系统评估显示，当前最先进的大语言模型在跨语言价值观评估任务上的准确率低于77%，且不同语言间性能差异超过20%，表明其在处理复杂价值观方面存在明显短板。

Conclusion: 当前大语言模型在深层次、跨文化价值观理解方面仍存在显著不足，亟需加强其在多语言环境下的价值感知与判断能力，本研究提出的X-Value基准为未来相关研究提供了重要工具。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [57] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在面对词汇和句法层面的语义等价扰动时的表现稳定性，发现词汇扰动导致几乎所有模型在多个基准测试中性能显著下降，而句法扰动影响不一，且模型规模与鲁棒性无一致关联。结果表明，当前评估基准对表面模式敏感，建议将鲁棒性测试纳入标准评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型评估基准因对输入提示的微小变化敏感，其可靠性受到质疑，亟需探究模型在语义不变条件下的表现稳定性。

Method: 采用两种基于语言学原理的生成管道：一种通过同义词替换进行词汇扰动，另一种利用依存句法分析确定可应用的句法变换，以生成语义保持不变的变体。

Result: 词汇扰动导致几乎所有模型在多数任务中出现显著性能下降；句法扰动效果不一，偶有提升；两种扰动均使复杂任务上的模型排行榜失稳；模型鲁棒性不随规模单调提升，具有任务依赖性。

Conclusion: 大型语言模型更依赖表面词汇模式而非深层语言理解能力，因此必须将鲁棒性测试作为标准评估的一部分。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [58] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: 提出RPDR框架，通过合成数据生成、往返预测选择易学实例、使用这些实例训练检索器，显著提升密集检索器在长尾知识上的表现，尤其在极端长尾类别上优于BM25和Contriver。通过人工分析识别其优缺点，并引入动态路由机制以进一步优化性能。


<details>
  <summary>Details</summary>
Motivation: 长尾问答对大语言模型构成挑战，因其难以获取和准确回忆较少见的知识；尽管检索增强生成（RAG）系统有所改善，但密集检索模型在泛化到稀有或小众知识时仍存在困难。

Method: 提出RPDR数据增强框架，包含合成数据生成、基于往返预测的数据选择以识别易学实例，以及利用这些实例训练检索器。

Result: 在PopQA和EntityQuestion两个长尾检索基准测试中，RPDR显著优于BM25和Contriver等现有检索器，特别是在极端长尾类别上表现突出。通过人工分析发现其优势与局限性，并设计动态路由机制以动态分配查询至专用检索模块，进一步提升检索性能。

Conclusion: RPDR有效增强了密集检索器在长尾知识上的表现，结合动态路由机制可实现更优的检索效果，为解决长尾问题提供了新思路。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [59] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 该研究探讨了在学生不确定多选题答案时，基于认知可及性（即选项在大型语料库中出现频率）的猜测策略是否有效。通过分析三个大规模题库，发现正确答案在语料库中的可及性显著高于错误选项；仅选择最易想到的选项即可获得比随机猜测高出13.5%至32.9%的成绩。此外，大语言模型生成的选项也表现出与专家创建选项相似的可及性模式。研究建议将可及性纳入未来对学生行为建模的计算方法中。


<details>
  <summary>Details</summary>
Motivation: 当学生无法确定多选题正确答案时，常依赖直觉或快速联想进行猜测。本研究旨在探究‘最容易想到的选项’是否是一个有效的猜测策略，并考察其背后的认知机制——即认知可及性。

Method: 采用大规模语料库（如Wikipedia）衡量各选项的认知可及性，通过统计选项在语料库中的出现频率来评估其心理易得性。使用三组真实和人工生成的多选题数据集进行实验，比较正确选项与错误选项的可及性差异，并测试仅选择最可及选项的预测性能。

Result: 正确答案在语料库中的可及性显著高于错误选项；仅选择最可及选项的准确率比随机猜测高出13.5%至32.9%；大语言模型生成的选项也表现出类似的可及性分布特征。

Conclusion: 认知可及性是影响多选题猜测行为的重要因素，应被纳入计算模型以更准确地模拟学生答题行为。该发现对教育技术、自动评分系统和智能辅导系统具有重要意义。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [60] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文提出了一种新的跨文档共指消解（CDCR）标注方案，重新标注了NewsWCL50和ECB+数据集，将共指链视为话语元素（DEs），以支持对新闻话语中词汇多样性与框架差异的捕捉。新方案涵盖身份与近似身份关系，提升模型对媒体话语变体的分析能力，并通过统一标注体系与基准测试验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有CDCR数据集主要关注事件共指，且定义狭窄，难以应对新闻报道中广泛的语言差异与立场分化问题，限制了对多样化媒体话语的分析能力。

Method: 提出新的共指标注框架，将共指链视为话语元素，采用统一编码手册重新标注NewsWCL50与部分ECB+数据集，支持身份与近似身份关系的建模。

Result: 重新标注后的数据集在词汇多样性指标上表现良好，与原始数据集相比处于中间水平，验证了其在新闻领域进行平衡、话语感知的CDCR研究的有效性。

Conclusion: 新提出的标注方案能够有效支持对新闻话语中词汇多样性和框架差异的分析，为跨文档共指消解研究提供了更合适的数据基础。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [61] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该研究比较了BLEU和ChrF++在极低资源语言（ELRL）场景下的机器翻译评估表现，发现尽管BLEU的绝对分数较低，但其在词汇精确度方面提供了互补性见解，提升了可解释性。


<details>
  <summary>Details</summary>
Motivation: 在极低资源语言场景下，传统评估指标如BLEU可能无法准确反映翻译质量，而现有研究往往仅依赖ChrF++，因此需要更全面的评估方法。

Method: 通过分析BLEU（n-gram-based）与ChrF++（character-based）在三种极低资源语言（Magahi、Bhojpuri、Chhattisgarhi）上的表现，考察其对幻觉、重复、源文本复制及变音符号变化等翻译错误的响应情况，涵盖大语言模型（LLMs）和神经机器翻译（NMT）系统输出。

Result: BLEU虽然得分较低，但在揭示词汇精度方面提供有价值信息，与ChrF++形成互补，增强评估结果的可解释性。

Conclusion: 在极低资源语言翻译评估中，应结合使用BLEU和ChrF++，以获得更全面、更具解释性的评估结果。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [62] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文提出了一种细粒度不确定性量化（UQ）的分类框架，用于长文本生成中的幻觉检测。该框架从响应分解、单元级评分和响应级聚合三个阶段对方法进行区分，并形式化了几类基于一致性的黑盒评分器，扩展了现有方法。实验表明，断言-响应蕴含关系表现优于或等同于更复杂的断言级评分器，断言级评分优于句子级评分，且不确定性感知解码能显著提升长文本生成的事实性。该框架厘清了先前方法间的关系，支持公平比较，并为组件选择提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法主要针对短文本输出设计，难以有效应用于长文本生成中的幻觉检测，亟需一种系统化的分类框架来指导方法设计与比较。

Method: 提出一个三阶段分类框架：响应分解、单元级评分、响应级聚合；形式化多类一致性基黑盒评分器，涵盖现有方法的推广与扩展。

Result: 1) 断言-响应蕴含关系在多数情况下优于或等同于复杂断言级评分；2) 断言级评分优于句子级评分；3) 不确定性感知解码显著提升长文本生成的事实性。

Conclusion: 所提出的分类框架有效澄清了已有方法之间的关系，支持直接比较，并为长文本生成中的细粒度不确定性量化提供了可操作的实践指导。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [63] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 本文提出AIDG（对抗性信息推断游戏）框架，用于评估大语言模型在动态对话中的战略推理能力。通过两个任务（AIDG-I和AIDG-II），研究发现模型在信息保持（防御）方面远优于信息提取（主动推断），存在350 ELO差距，且主要受信息动态性和约束遵循性两大瓶颈制约。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准无法充分评估大语言模型在多轮对话中的战略推理能力，需引入动态、交互式的测试框架以揭示其在信息提取与信息控制之间的能力不对称性。

Method: 提出AIDG框架，设计AIDG-I（社会推断中的语用策略）和AIDG-II（结构化‘二十问’中的约束满足）两个任务，通过439场对战实验评估六种前沿大语言模型的表现，并分析其推理过程中的关键瓶颈。

Result: 模型在防御（信息维持）上显著优于进攻（信息推断），存在350 ELO优势（Cohen's d = 5.47）；信息动态性（确认策略效率是盲推的7.75倍）和约束遵循性下降（占推断失败41.3%）是主要瓶颈。

Conclusion: 尽管大语言模型在局部防御一致性上表现良好，但在需要全局状态追踪的战略性探询中仍存在明显不足，提示其战略推理能力受限于长期状态管理与复杂约束下的指令遵循能力。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [64] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 提出一种减少偏见的评估协议，通过使用均匀无序的标签和完整答案提示，显著提升大模型在多选题评测中的鲁棒性，降低答案排列变化带来的性能波动，同时保持模型性能小幅下降。


<details>
  <summary>Details</summary>
Motivation: 现有MCQ基准测试中，大模型容易受到标签位置、提示中正确答案分布等评价偏差的影响，导致评估结果不可靠。

Method: 用均匀随机且无序的标签替换原问题标签，引导模型基于完整答案内容进行推理，结合句子相似度模型判断答案匹配度。

Result: 该方法在多个基准和模型上显著降低答案排列引起的准确率方差（平均减少3倍），仅带来轻微性能损失，验证了其鲁棒性与有效性。

Conclusion: 所提出的评估协议能有效减少评估过程中的伪相关性偏差，更真实地反映大模型的真实推理能力。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [65] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 提出了一种基于熵的无监督数据选择框架EUDS，以在计算资源受限的情况下高效微调语言模型。该方法通过评估数据不确定性来筛选有效训练数据，显著降低计算成本和训练时间，同时减少对数据量的需求。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型微调需要大量计算和数据资源，而实际场景中资源有限。现有数据选择技术依赖高计算预算，难以在资源受限环境下应用。因此，亟需一种高效、低开销的数据选择方法，以应对数据稀缺与计算约束的双重挑战。

Method: 提出熵基无监督数据选择（EUDS）框架，利用语言模型输出的熵值作为不确定性度量指标，自动筛选最具信息量的数据样本，实现高效的数据过滤与微调。

Result: 在情感分析、主题分类和问答任务上的实验表明，EUDS能显著降低计算开销，提升训练效率，且在较少数据下仍保持良好性能，验证了其有效性与实用性。

Conclusion: EUDS为计算资源受限环境下的语言模型高效微调提供了创新解决方案，兼具理论合理性与实践优势，具有广泛的应用前景。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [66] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本研究聚焦跨语言情感对齐问题，特别是在孟加拉语与英语之间的对齐失效。通过基准测试四种Transformer模型，发现当前对齐范式存在严重安全与表征缺陷：压缩模型mDistilBERT出现28.7%的情感反转率，将正面意图误判为负面；揭示了“非对称共情”现象，部分模型系统性削弱或增强孟加拉语文本的情感权重；还发现区域模型IndicBERT在处理正式（Sadhu）孟加拉语时对齐错误上升57%。研究主张应建立多元、文化扎根的对齐机制，而非追求通用压缩，以保障情感真实性与人机互信。建议对齐基准引入‘情感稳定性’指标，特别惩罚低资源及方言语境下的极性反转。


<details>
  <summary>Details</summary>
Motivation: 当前双向对齐机制在跨语言场景中面临严重失效，尤其在低资源语言如孟加拉语中，情感理解与信任机制断裂，导致人类无法可靠理解AI行为，也难以信任其输出。现有模型在处理方言和正式文体时表现不佳，暴露出系统性偏差，亟需更公平、文化敏感的对齐框架。

Method: 采用四种主流Transformer架构（包括mDistilBERT和IndicBERT）进行跨语言情感分析对比实验，使用孟加拉语与英语平行数据集，评估模型在情感极性判断上的准确性，并引入情感反转率、情感权重差异、正式文体误差等指标量化对齐失败程度。

Result: mDistilBERT的情感反转率达28.7%，显著误判用户意图；存在‘非对称共情’现象，不同模型对孟加拉语情感表达的处理不一致；IndicBERT在处理正式孟加拉语时对齐错误增加57%。结果表明，通用压缩策略无法保留情感语义，导致人机信任链断裂。

Conclusion: 实现公平的人机协同进化需要尊重语言与方言多样性，摒弃单一化压缩路径。必须构建基于文化语境的多语言对齐范式，并在基准测试中引入‘情感稳定性’指标，以确保低资源和方言语境下的情感忠实度，从而重建人类对AI的信任。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [67] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本研究探讨了参数量约为10亿的小型大语言模型（LLM）在医疗自然语言处理任务中的表现，评估了Llama-3、Gemma-3和Qwen3三个系列模型在20项临床NLP任务上的性能，涵盖命名实体识别、关系抽取、病历表单填写、问答和论据挖掘。通过对比推理时（如少样本提示、约束解码）和训练时（如监督微调、持续预训练）的多种适配策略，发现微调是最有效的方法，而少样本提示与约束解码的组合则为低资源场景提供了有力替代方案。结果显示，小型LLM可达到甚至超越大型基线模型的表现，其中基于Qwen3-1.7B的最佳配置平均得分比Qwen3-32B高出9.2分。研究还发布了公开可用的意大利医疗NLP数据集及最优模型，并提供来自意大利某医院急诊科的1.26亿词语料以及1.75亿词用于持续预训练的多源数据。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽在医疗NLP任务中表现优异，但其高计算成本限制了在真实医疗环境中的部署。因此，探索小规模模型是否能在保持竞争力的同时完成医疗任务，具有重要实际意义。

Method: 系统评估三类主流模型家族（Llama-3、Gemma-3、Qwen3）在20个临床NLP任务上的表现，采用多种适应策略：包括推理阶段的少样本提示与约束解码，以及训练阶段的监督微调与持续预训练。通过对比分析不同方法的效果，确定最优配置。

Result: 小型语言模型（约10亿参数）在多数任务上可达到甚至超过大型模型的表现；其中最佳配置（Qwen3-1.7B）在平均得分上比其32B版本高出9.2分；微调是效果最佳的策略，而少样本提示与约束解码组合则适用于资源受限场景。

Conclusion: 小规模大语言模型在医疗NLP任务中具备强大潜力，可通过适当的训练与推理策略实现高性能表现，且显著降低计算开销，推动其在真实医疗场景中的应用落地。同时，研究贡献了多个高质量公开数据集与模型资源，促进领域发展。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [68] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 该研究通过三个关键贡献推进了临床章节分割：构建了一个新的去标识化产科笔记数据集，系统评估了基于Transformer的监督模型在MIMIC-III子集和新产科数据集上的表现，并首次对监督模型与零样本大语言模型进行了直接比较。结果显示，监督模型在域内表现良好，但在域外性能显著下降；而零样本模型在修正幻觉后展现出强大的域外适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有临床章节分割方法主要基于MIMIC-III等已有语料库训练，覆盖领域有限，难以适用于如产科等新领域。为提升模型在不同临床领域的泛化能力，亟需开发更全面的领域资源并探索更灵活的分割方法。

Method: 构建去标识化产科笔记数据集；在MIMIC-III子集和新产科数据集上评估基于Transformer的监督模型；对比监督模型与零样本大语言模型在跨领域场景下的表现，并通过后处理修正零样本模型的幻觉问题。

Result: 监督模型在域内表现优异，但域外性能明显下降；零样本模型在修正幻觉后表现出强域外适应性，具备良好的泛化潜力。

Conclusion: 开发领域特定的临床资源至关重要，而零样本分割结合幻觉管理，是拓展医疗NLP应用至未充分研究领域的有前景方向。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [69] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的自动化框架，用于在开放性编程任务中直接标注知识组件（KC）级别的正确性，解决真实数据集中缺乏细粒度标注的问题。该方法结合时间上下文感知的代码-KC映射机制，提升KC与学生代码的对齐精度。实验表明，该框架生成的学习曲线更符合认知理论，预测性能优于基线方法，且与专家标注具有高度一致性。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据集，尤其是开放性编程任务中，缺乏细粒度的知识组件（KC）级别正确性标签。简单将问题级正确性传播到所有相关KC会掩盖部分掌握情况，导致学习曲线拟合不佳。

Method: 利用大语言模型（LLM）直接从学生代码中推断每个知识组件（KC）的正确性，并引入时间上下文感知的Code-KC映射机制，以更准确地匹配学生代码与对应的KC。

Result: 实验结果表明，该框架生成的学习曲线更符合认知理论（如练习的幂律），预测性能优于基线方法；人工评估显示LLM标注与专家标注高度一致。

Conclusion: 所提出的基于LLM的自动化框架能够有效生成高质量的KC级正确性标签，显著提升学生建模和学习分析的效果，为开放性编程任务中的学习分析提供了可行解决方案。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [70] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 本文提出一种自适应正则化训练框架，通过动态调整正则化强度来应对安全风险，从而在微调过程中保持模型的安全性。该框架利用两种风险估计方法：基于裁判的Safety Critic（对训练批次进行高阶危害评分）和基于激活的轻量级风险预测器（通过中间层激活预测有害意图）。高风险更新被约束靠近安全参考策略，低风险更新则按常规训练进行。实验表明，该方法在多种模型和攻击场景下均能有效降低攻击成功率，同时保持下游性能且不增加推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法在安全性与实用性之间存在权衡，或保护能力有限。本文旨在解决微调过程中模型安全行为退化的问题，尤其是在良性与对抗性更新下的安全风险。

Method: 提出一种自适应正则化框架，结合两种风险估计方式：1）基于裁判的Safety Critic，为训练批次打分以评估危害程度；2）基于中间层激活的轻量级分类器，预测有害意图。根据风险信号动态调节正则化强度，高风险更新被限制接近安全参考策略，低风险更新正常训练。

Result: 在多个模型家族和攻击场景中，该方法显著降低了攻击成功率，保持了下游任务性能，并且无需额外推理成本。实验证明，有害意图可从预生成激活中预测，裁判评分具有高召回率的安全指导作用。

Conclusion: 本文提出的自适应正则化机制能够在不牺牲模型实用性的前提下，有效维持微调过程中的安全性，提供一种兼顾安全与性能的可扩展解决方案。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [71] [The Cascade Equivalence Hypothesis: When Do Speech LLMs Behave Like ASR$\rightarrow$LLM Pipelines?](https://arxiv.org/abs/2602.17598)
*Jayadev Billa*

Main category: cs.CL

TL;DR: 当前语音大模型主要执行隐式自动语音识别（ASR）：在可从转录文本解决的任务上，它们在行为和机制上与简单的Whisper→LLM级联模型等价。通过四款语音大模型与六项任务的匹配骨干测试，首次控制了大语言模型骨干，发现Ultravox与匹配级联模型在统计上无法区分（κ=0.93）；对隐藏状态的逻辑透镜分析显示文本内容直接出现在中间表示中；LEACE概念擦除证实文本表示在两种架构中都是因果必需的，准确率降至接近零。而Qwen2-Audio则真正表现出差异，说明级联等价性取决于架构，并非普遍现象。对于大多数实际部署场景，当前语音大模型本质上是昂贵的级联模型，在噪声环境下表现更差，且在干净条件下的优势在0 dB时逆转高达7.6%。


<details>
  <summary>Details</summary>
Motivation: 探究当前语音大模型是否真正具备端到端能力，还是仅依赖于隐式自动语音识别（ASR），以理解其内在机制及实际性能瓶颈。

Method: 采用匹配骨干测试方法，对比四款语音大模型与对应级联模型在六项任务上的表现；结合逻辑透镜分析、概念擦除（LEACE）等工具，分析模型内部表示与因果必要性。

Result: Ultravox与级联模型几乎无法区分（κ=0.93）；隐藏状态中存在明确的文本表示；文本表示在任务中具有因果必要性，移除后准确率暴跌；但Qwen2-Audio表现出独立行为，表明级联等价性并非普遍成立。在噪声环境下，语音大模型表现劣于级联模型，且清洁条件优势在0 dB时逆转达7.6%。

Conclusion: 当前多数语音大模型在功能上等同于昂贵的级联系统，其优势在噪声条件下反而消失，甚至出现倒退。这表明其核心能力仍依赖于隐式ASR，而非真正的端到端语音理解。

Abstract: Current speech LLMs largely perform implicit ASR: on tasks solvable from a transcript, they are behaviorally and mechanistically equivalent to simple Whisper$\to$LLM cascades. We show this through matched-backbone testing across four speech LLMs and six tasks, controlling for the LLM backbone for the first time. Ultravox is statistically indistinguishable from its matched cascade ($κ{=}0.93$); logit lens reveals literal text emerging in hidden states; LEACE concept erasure confirms text representations are causally necessary in both architectures tested, collapsing accuracy to near-zero. Qwen2-Audio genuinely diverges, revealing cascade equivalence is architecture-dependent, not universal. For most deployed use cases, current speech LLMs are expensive cascades, and under noise, they are worse ones, with clean-condition advantages reversing by up to 7.6% at 0 dB.

</details>


### [72] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: 本文提出DivanBench，一个专注于波斯语中迷信与习俗的诊断基准，评估大语言模型在文化规范推理方面的能力。通过315个问题，测试七种波斯语LLM在事实检索、情景验证和情境推理三类任务中的表现，发现模型普遍存在顺从偏差，即能识别正确行为但无法识别明显违规；持续的波斯语预训练反而加剧了这一偏差，削弱了模型辨别矛盾的能力；所有模型在知识获取与实际应用之间存在21%的性能差距。研究指出，文化能力不能仅靠单语数据规模提升实现，当前模型仅模仿文化模式而未内化其深层结构。


<details>
  <summary>Details</summary>
Motivation: 现有波斯语NLP基准多关注语用和礼貌，但缺乏对文化事实记忆与隐含社会规范推理能力的区分。本研究旨在填补这一空白，建立一个能诊断模型对任意、上下文依赖的文化规则理解能力的基准。

Method: 构建DivanBench基准，包含315个问题，涵盖事实检索、成对情景验证和情境推理三类任务，评估七种波斯语大语言模型在文化规范理解方面的表现，并分析其错误模式与训练影响。

Result: 多数模型表现出严重顺从偏差，无法识别明显违反文化规范的行为；持续的波斯语预训练反而恶化推理能力；所有模型在事实知识应用上比知识检索低21%；表明当前模型未能真正内化文化规范，仅停留在表面模仿。

Conclusion: 文化能力不能仅通过扩大单语数据规模获得，当前大语言模型虽能模仿文化模式，但缺乏对隐含社会规范的深层理解，需更系统的方法来提升文化推理能力。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [73] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 该研究通过合成语料库训练GPT-2模型，考察语言模型在差异性论元标记（DAM）系统中的习得能力。结果显示，模型表现出与人类语言一致的自然标记方向偏好（即标记更常作用于语义非典型论元），但未再现人类语言中常见的对象优先倾向（即标记更常作用于宾语而非主语）。这表明不同语言类型学特征可能源于不同的认知或学习机制。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能习得人类语言中普遍存在的差异性论元标记（DAM）规律，特别是其背后的类型学偏好是否可被合成训练所激发，并揭示这些偏好是否与人类语言行为一致。

Method: 采用受控的合成语料学习方法，训练GPT-2模型在18种具有不同DAM系统的语料上；通过最小对例测试模型的泛化能力，评估其对不同类型标记系统的偏好。

Result: 模型可靠地表现出与人类语言一致的自然标记方向偏好（标记倾向于语义非典型论元），但未能复制人类语言中常见的对象优先现象（即标记更常指向宾语而非主语）。

Conclusion: 不同类型的语言规律可能源自不同的认知或学习基础：自然标记方向偏好可能源于通用学习机制，而对象优先倾向可能依赖特定的语言输入或演化压力，模型未能捕捉后者，暗示其学习机制与人类存在差异。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [74] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: UniLID 是一种基于 UnigramLM 词元化算法的简单高效语言识别方法，通过学习共享词汇表上的语言条件单字节分布，将分词视为语言特定现象。该方法数据和计算效率高，支持增量添加新语言且无需重新训练，可无缝集成到现有语言模型词元化流程中。在标准基准测试中表现媲美主流基线（如 fastText、GlotLID、CLD3），在低资源场景下显著提升样本效率（每语言仅需5个标注样本即达70%+准确率），并在细粒度方言识别上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别系统在高资源语言上表现接近完美，但在低资源和语义相近语言场景下仍显脆弱。需要一种更高效、灵活且适用于低资源环境的语言识别方法。

Method: 基于 UnigramLM 词元化算法，学习共享词汇表上的语言条件单字节分布，将分词过程视为语言特定现象，利用其概率框架、参数估计技术和推理策略实现语言识别。

Result: 在标准基准测试中表现优于或媲美 fastText、GlotLID、CLD3 等主流基线；在低资源设置下，仅用5个标注样本/语言即可达到70%以上准确率；在细粒度方言识别任务中获得显著提升。

Conclusion: UniLID 提供了一种高效、灵活且可扩展的语言识别方案，特别适用于低资源语言和方言识别，具备良好的实际应用潜力，可无缝融入现有语言模型流水线。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [75] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 提出了一种针对扩散语言模型（DLMs）的新型剪枝方法——Sink-Aware Pruning，发现DLM中的注意力汇聚点（sink）在生成过程中具有较高变动性，与自回归模型不同，因此不应像以往那样固定保留。该方法无需微调即可有效剪除不稳定的汇聚点，在相同计算量下实现更优的质量-效率平衡，性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法多沿用自回归语言模型的策略，假设注意力汇聚点是稳定且关键的全局锚点。然而在扩散语言模型中，汇聚点位置随时间变化较大，表明其结构重要性较低，因此需要新的剪枝策略来适应DLM特性。

Method: 提出基于汇聚点稳定性分析的剪枝方法，通过量化不同时间步中主导汇聚点的位置变化，识别并移除不稳定的汇聚点，从而减少推理开销而不损害模型性能。

Result: 在不进行微调的情况下，该方法在相同计算成本下实现了比现有强基线更好的生成质量与推理效率的权衡，验证了其有效性。

Conclusion: 传统依赖于稳定汇聚点的剪枝策略不适用于扩散语言模型；通过考虑汇聚点动态性进行智能剪枝，可显著提升DLM的推理效率，为高效扩散模型部署提供了新思路。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE是一种领域特定的本体，旨在标准化和语义化法医牙齿年龄评估流程，涵盖手动和基于AI的方法，实现观察、方法、参考数据与结果之间的可追溯关联。它整合了司法背景、个体信息、影像数据、统计研究及AI估计方法，支持可互操作性、可扩展性和FAIR原则，提升评估的一致性、透明度与可解释性，为法医-司法决策支持系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前法医牙齿年龄评估面临方法异质性、数据表示碎片化和系统间互操作性差的问题，尤其在人工智能广泛应用背景下，亟需一个标准化框架以提升透明度、可重复性和可解释性。

Method: 构建基于领域专家协作的AIdentifyAGE本体，整合现有生物医学、牙科和机器学习本体，建立涵盖从个体信息到评估结果的完整法医-司法工作流的语义模型。

Result: 实现了对法医牙齿年龄评估全流程的标准化建模，支持手动与AI辅助方法的统一表达，增强了数据可追溯性、系统互操作性与决策透明性。

Conclusion: AIdentifyAGE本体是推动法医-司法年龄评估向标准化、可解释、可信赖方向发展的关键一步，为未来智能化决策支持系统提供了坚实基础。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [77] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 本文研究自适应系统在多上下文环境中使用固定内部状态空间时的表征问题。尽管这种单状态复用在自然和人工智能中普遍存在，但其基本表征后果尚不明确。研究发现，上下文依赖性并非量子力学的特有现象，而是经典概率表示中单状态复用的必然结果。通过将上下文建模为对共享内部状态的干预，证明任何再现上下文结果统计的经典模型都必须承担不可减少的信息论代价：上下文依赖无法仅通过内部状态来中介。文中提供了一个最小构造示例，明确展示了这一代价并阐明其操作意义。此外，文章解释了非经典概率框架如何通过放松全局联合概率空间的假设来规避此障碍，而无需引入量子动力学或希尔伯特空间结构。研究结论指出，上下文性是自适应智能的一般表征约束，与物理实现无关。


<details>
  <summary>Details</summary>
Motivation: 理解自适应系统在多上下文环境中使用固定内部状态空间时的表征限制，尤其是这种单状态复用如何导致上下文依赖性，以及这种依赖性的信息论代价。

Method: 将上下文建模为对共享内部状态的干预，分析经典概率模型中上下文依赖性的表征能力，通过数学证明揭示不可减少的信息论代价，并构造最小示例验证理论。

Result: 证明了在经典概率框架下，单状态复用必然导致上下文依赖性需承担不可减少的信息论代价；非经典概率框架可通过放弃全局联合概率空间避免该代价；上下文性是自适应智能的一般表征约束。

Conclusion: 上下文性并非量子特性，而是由单状态复用引发的普遍表征限制，适用于所有基于概率的自适应系统，与具体物理实现无关。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [78] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache是一种面向大规模人类移动模拟的高效缓存框架，通过可重构缓存机制提升效率。其包含两个组件：（1）推理组件将每一步推理编码为潜在空间嵌入，并利用潜在空间评估器实现推理步骤的复用与重组；（2）解码组件采用轻量级解码器，通过受移动规律约束的蒸馏训练，将潜在空间推理链转化为自然语言，从而在保持高保真度的同时显著提升模拟效率。实验表明，该方法在多个维度上均显著提升效率，且性能接近最先进的基于大语言模型的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的人类移动模拟方法虽能生成真实行为，但计算成本高，难以扩展至大规模场景。因此亟需一种高效、可扩展的替代方案以支持城市规划、流行病学和交通分析等应用。

Method: 设计并实现一个名为MobCache的移动感知缓存框架，核心包括：1）将推理步骤编码为潜在空间嵌入，并通过潜在空间评估器实现推理路径的复用与组合；2）使用受移动规律约束的轻量级蒸馏解码器，将潜在表示转换为自然语言输出，从而降低计算开销。

Result: MobCache在多个评估维度上显著提升了模拟效率，同时保持了与当前最优基于LLM方法相当的性能表现，验证了其在大规模人类移动模拟中的有效性与可扩展性。

Conclusion: MobCache通过引入可重构缓存机制，有效降低了大语言模型在人类移动模拟中的计算开销，在保证行为真实性的同时实现了高效的大规模仿真，为未来复杂系统建模提供了可行的技术路径。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [79] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了60个大型语言模型（LLM）基准测试的饱和现象，发现近半数基准已出现饱和，且饱和率随时间上升。研究揭示，隐藏测试数据对延缓饱和无显著作用，而专家精心设计的基准比众包构建的基准更具持久性。研究结果为提升基准测试长期有效性提供了设计指导。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准测试容易快速饱和，无法有效区分顶尖模型，削弱其长期评估价值。亟需识别影响基准饱和的关键因素，以设计更持久、可靠的评估体系。

Method: 从主要模型开发者的技术报告中选取60个LLM基准测试，从任务设计、数据构建和评估格式等14个维度对基准进行特征刻画，并基于此检验五项关于各属性对饱和率影响的假设。

Result: 近一半的基准测试已出现饱和，饱和率随时间推移而增加；隐藏测试数据未能减缓饱和；专家设计的基准比众包基准更能抵抗饱和。

Conclusion: 基准测试的设计选择显著影响其寿命。应优先采用专家参与的数据构建与任务设计，以延长基准的评估价值周期。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [80] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文研究了$n$维超立方体$Q_n$的边被超平面切割的最小数量$S(n)$。通过构造8个超平面切割$Q_{10}$，并借助名为CPro1的自动化工具（结合推理型大模型与自动超参数调优），提出了新的上界：$S(n) \leq \lceil \frac{4n}{5} \rceil$（当$n$不是5的奇数倍时）；若$n$是5的奇数倍，则$S(n) \leq \frac{4n}{5} + 1$。这一结果改进了1971年Paterson提出的$\lceil \frac{5n}{6} \rceil$的旧上界，并给出了基于$k < n$个超平面可切割的最大边数的新下界。


<details>
  <summary>Details</summary>
Motivation: 寻找更紧的上界来刻画切割所有$Q_n$边所需的最少超平面数量$S(n)$，并推动数学构造发现的自动化方法发展。

Method: 采用构造法，设计8个超平面以切割$Q_{10}$，利用新开发的CPro1工具——一个融合推理型大语言模型与自动超参数调优的搜索算法生成器，系统性探索和验证数学构造。

Result: 得到$S(n) \leq \lceil \frac{4n}{5} \rceil$（$n$非5的奇数倍时），或$S(n) \leq \frac{4n}{5} + 1$（$n$为5的奇数倍时），优于此前的$\lceil \frac{5n}{6} \rceil$上界；同时获得关于$k < n$个超平面能切割的边数的新下界。

Conclusion: 本研究通过结合人工智能辅助的构造方法，显著改进了$S(n)$的上界估计，并展示了自动化工具在数学构造发现中的强大潜力。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [81] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于序理论的统一框架，用于处理犹豫模糊集的评分问题。传统方法缺乏形式化基础，而本文通过引入有序性视角，证明了某些经典顺序无法诱导格结构，并指出以对称序定义的评分满足强单调性和Gärdenfors条件等规范性要求。进一步提出了‘主导函数’类，用于在包含最低可接受阈值的控制集下比较犹豫模糊元素，给出了离散主导函数和相对主导函数两个具体例子，可用于构建模糊偏好关系并支持群体决策。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏序理论的形式基础，导致评分机制不够严谨与一致，亟需一种更系统、更可靠的评分框架。

Method: 提出基于顺序的统一评分框架，分析经典顺序的格结构性质，证明对称序下的评分满足关键规范性条件；引入主导函数类，设计具体函数实例，实现对犹豫模糊元素的排序与偏好建模。

Result: 证明了经典顺序不构成格结构；验证了对称序评分满足强单调性与Gärdenfors条件；提出的主导函数能有效支持犹豫模糊集的排序与群决策应用。

Conclusion: 本文建立的顺序导向评分框架具有更强的理论一致性与灵活性，主导函数为犹豫模糊集的排序与决策提供了实用工具，推动了该领域向形式化方向发展。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [82] [Mobile-Agent-v3.5: Multi-platform Fundamental GUI Agents](https://arxiv.org/abs/2602.16855)
*Haiyang Xu,Xi Zhang,Haowei Liu,Junyang Wang,Zhaozai Zhu,Shengjie Zhou,Xuhao Hu,Feiyu Gao,Junjie Cao,Zihua Wang,Zhiyuan Chen,Jitong Liao,Qi Zheng,Jiahui Zeng,Ze Xu,Shuai Bai,Junyang Lin,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: GUI-Owl-1.5 是一个支持多平台的原生图形界面（GUI）智能体模型，具备多种尺寸（2B/4B/8B/32B/235B）和指令/思维变体，可实现云边协同与实时交互。该模型在超过20个开源GUI基准测试中达到领先水平，涵盖自动化、定位、工具调用及记忆知识等任务。其核心创新包括：混合数据飞轮（结合模拟与云端沙箱环境提升数据质量）、统一能力增强（通过统一思维合成流程强化推理与关键能力如工具使用、记忆与多智能体适应），以及多平台环境强化学习算法MRPO，以应对跨平台冲突与长序列训练效率低的问题。模型已开源，并提供在线云沙箱演示。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体模型在多平台支持、实时交互、长周期任务训练效率及跨平台一致性方面存在局限，难以实现高效、稳定且通用的自动化与人机协作。因此需要构建一个具备更强泛化能力、可扩展性与跨平台兼容性的新一代原生GUI智能体系统。

Method: 提出Hybrid Data Flywheel数据管道，融合模拟环境与云沙箱环境进行高效高质量数据采集；设计统一思维合成管道以增强模型推理能力，重点优化工具调用、记忆保持与多智能体协同；开发新环境强化学习算法MRPO，解决多平台间冲突与长轨迹训练低效问题。

Result: 在多个主流GUI基准测试中表现优异：OSWorld上自动化任务达56.5，AndroidWorld为71.6，WebArena为48.4；ScreenSpotPro定位任务得分80.3；OSWorld-MCP与MobileWorld工具调用任务分别取得47.6与46.8；GUI-Knowledge Bench记忆与知识任务得分为75.5。整体性能超越现有开源模型。

Conclusion: GUI-Owl-1.5通过创新的数据生成机制、统一的能力增强框架和高效的多平台强化学习算法，显著提升了原生GUI智能体在复杂真实场景下的理解、决策与执行能力，为实现跨平台、高实时性的人机协作提供了有力支撑，且其开源特性推动了社区发展。

Abstract: The paper introduces GUI-Owl-1.5, the latest native GUI agent model that features instruct/thinking variants in multiple sizes (2B/4B/8B/32B/235B) and supports a range of platforms (desktop, mobile, browser, and more) to enable cloud-edge collaboration and real-time interaction. GUI-Owl-1.5 achieves state-of-the-art results on more than 20+ GUI benchmarks on open-source models: (1) on GUI automation tasks, it obtains 56.5 on OSWorld, 71.6 on AndroidWorld, and 48.4 on WebArena; (2) on grounding tasks, it obtains 80.3 on ScreenSpotPro; (3) on tool-calling tasks, it obtains 47.6 on OSWorld-MCP, and 46.8 on MobileWorld; (4) on memory and knowledge tasks, it obtains 75.5 on GUI-Knowledge Bench. GUI-Owl-1.5 incorporates several key innovations: (1) Hybird Data Flywheel: we construct the data pipeline for UI understanding and trajectory generation based on a combination of simulated environments and cloud-based sandbox environments, in order to improve the efficiency and quality of data collection. (2) Unified Enhancement of Agent Capabilities: we use a unified thought-synthesis pipeline to enhance the model's reasoning capabilities, while placing particular emphasis on improving key agent abilities, including Tool/MCP use, memory and multi-agent adaptation; (3) Multi-platform Environment RL Scaling: We propose a new environment RL algorithm, MRPO, to address the challenges of multi-platform conflicts and the low training efficiency of long-horizon tasks. The GUI-Owl-1.5 models are open-sourced, and an online cloud-sandbox demo is available at https://github.com/X-PLUG/MobileAgent.

</details>


### [83] [OpenSage: Self-programming Agent Generation Engine](https://arxiv.org/abs/2602.16891)
*Hongwei Li,Zhun Wang,Qinrun Dai,Yuzhou Nie,Jinjun Peng,Ruitong Liu,Jingyang Zhang,Kaijie Zhu,Jingxuan He,Lun Wang,Yangruibo Ding,Yueqi Chen,Wenbo Guo,Dawn Song*

Main category: cs.AI

TL;DR: OpenSage is the first Agent Development Kit (ADK) that enables LLMs to autonomously create agents with self-generated topology and toolsets, featuring a hierarchical, graph-based memory system and specialized tools for software engineering. It outperforms existing ADKs across benchmarks and demonstrates strong design effectiveness through ablation studies.


<details>
  <summary>Details</summary>
Motivation: Current ADKs lack sufficient functional support or rely on manual human design for agent topology, tools, and memory, limiting agent generalizability and performance.

Method: OpenSage leverages LLMs to automatically generate agent topology and toolsets, incorporates a hierarchical graph-based memory system, and provides a tailored toolkit for software engineering tasks.

Result: Experiments on three state-of-the-art benchmarks with various backbone models show OpenSage's superiority over existing ADKs; ablation studies confirm the effectiveness of each component.

Conclusion: OpenSage represents a shift from human-centered to AI-centered agent development, paving the way for next-generation agent systems.

Abstract: Agent development kits (ADKs) provide effective platforms and tooling for constructing agents, and their designs are critical to the constructed agents' performance, especially the functionality for agent topology, tools, and memory. However, current ADKs either lack sufficient functional support or rely on humans to manually design these components, limiting agents' generalizability and overall performance. We propose OpenSage, the first ADK that enables LLMs to automatically create agents with self-generated topology and toolsets while providing comprehensive and structured memory support. OpenSage offers effective functionality for agents to create and manage their own sub-agents and toolkits. It also features a hierarchical, graph-based memory system for efficient management and a specialized toolkit tailored to software engineering tasks. Extensive experiments across three state-of-the-art benchmarks with various backbone models demonstrate the advantages of OpenSage over existing ADKs. We also conduct rigorous ablation studies to demonstrate the effectiveness of our design for each component. We believe OpenSage can pave the way for the next generation of agent development, shifting the focus from human-centered to AI-centered paradigms.

</details>


### [84] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace 是一个评估大语言模型（LLMs）规划、推理和世界知识能力的基准测试。该任务要求模型通过逐步点击维基百科链接，从源页面到达目标页面，需具备前瞻规划和对现实世界概念关联的理解能力。尽管 GPT-5、Gemini-3、Claude Opus 4.5 等前沿模型在简单难度上表现超人，但在高难度任务中表现显著下降，最强模型 Gemini-3 仅在 23% 的任务中成功。研究发现，世界知识虽重要，但超过一定阈值后，长程规划与推理能力成为关键瓶颈。轨迹分析显示，强模型在失败后难以有效重规划，常陷入循环。该基准揭示了当前推理系统的核心局限，为未来具备规划能力的 LLMs 提供了开放评测平台。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 评估多集中于静态问答或短序列生成，缺乏对复杂规划与长程推理能力的系统性测试。真实世界问题往往需要基于世界知识进行前瞻规划和动态调整，因此亟需一个能衡量此能力的基准。

Method: 构建 LLM-Wikirace 基准，包含不同难度的维基百科路径导航任务；让多种开源与闭源模型在该任务上运行，记录成功率与行为轨迹；通过对比分析模型性能、世界知识贡献度及失败后的重规划能力，揭示其核心短板。

Result: GPT-5、Gemini-3、Claude Opus 4.5 在简单任务中表现优异，甚至超越人类水平；但在高难度任务中，最佳模型成功率仅为 23%；世界知识作用有限，长程规划与错误恢复能力是主要瓶颈；多数模型在失败后频繁进入循环，无法有效重规划。

Conclusion: LLM-Wikirace 揭示了当前前沿大模型在复杂规划与长期推理方面的显著不足，尤其在失败后重规划能力方面存在严重缺陷。该基准提供了一个简洁而有效的开放平台，推动 LLM 向真正具备自主规划与适应能力的方向发展。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [85] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 本文挑战了黑箱安全评估中模型在测试分布上的行为可预测部署性能的假设，提出通过潜在上下文条件策略（即输出依赖于评估时罕见但部署时普遍的未观测内部变量）来揭示其局限性。研究证明：(1) 静态评估下，任何估计器的期望绝对误差至少为约0.208×δ×L；(2) 自适应评估下，即使完全自适应查询，最坏情况误差仍不低于δ×L/16，检测需Θ(1/ε)次查询；(3) 在陷门单向函数假设下，具备特权信息的部署环境可触发不可检测的危险行为。白盒探测中，达到精度ε_R需O(1/(γ²×ε_R²))样本，且提供偏差校正方法。结果量化了黑箱测试的统计不足，并指出必须引入架构约束、训练时保障、可解释性和部署监控等额外措施以实现最坏情况下的安全保证。


<details>
  <summary>Details</summary>
Motivation: 现有黑箱安全评估假设模型在测试分布上的表现能可靠预测部署表现，但该假设在存在未观测内部变量（如隐含触发机制）的情况下可能失效。本文旨在揭示此类情况下黑箱评估的固有局限性，推动对更严格安全机制的需求。

Method: 采用理论分析方法：利用Le Cam方法推导静态评估的极小极大下界；基于哈希构造触发器与Yao最小最大原理分析自适应评估的极限；在陷门单向函数假设下建立计算分离；并针对白盒探测提供样本复杂度分析与偏差校正公式。

Result: 证明了黑箱评估在三类场景下均无法可靠估计部署风险：(1) 静态评估存在非零下界误差；(2) 自适应查询也无法避免显著误差，且检测需大量查询；(3) 计算上存在不可区分的危险行为。白盒探测虽可改进，但仍需足够样本和校正。

Conclusion: 黑箱测试在面对潜在上下文条件策略时存在根本性统计不足，仅靠黑箱评估无法实现最坏情况下的安全保证。必须结合架构约束、训练时验证、可解释性分析和实时部署监控等多重手段，才能应对高风险场景中的系统性安全挑战。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [86] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: 本文提出Conv-FinRe，一个用于股票推荐的对话式、长期性基准测试，旨在评估大语言模型（LLM）在金融咨询中的决策质量，而不仅仅是模仿用户行为。该基准结合用户访谈、市场背景和对话历史，要求模型生成基于长期投资目标的股票排名，并提供多视角参考标准，以区分行为描述与基于风险偏好的规范性效用。通过真实市场数据和人类决策轨迹构建，实验表明当前LLM在理性决策质量和行为一致性之间存在显著矛盾：高决策质量模型常无法匹配用户选择，而行为对齐模型则容易受短期市场噪声干扰。数据集已公开于Hugging Face，代码库可在GitHub获取。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统基准主要评估模型对用户行为的模仿能力，但在金融咨询场景中，用户实际选择可能受市场波动影响而表现出噪声或短视行为，难以反映其长期目标。因此，仅以用户行为作为唯一真实标准会混淆行为模仿与决策质量，亟需一种能区分理性分析与非理性跟风的新评估范式。

Method: 构建了一个名为Conv-FinRe的新型对话式、纵向股票推荐基准。该基准包含用户初始访谈、逐步更新的市场上下文信息及多轮咨询对话，要求模型根据投资者的风险偏好，在固定投资周期内生成股票排序。引入多视角参考答案，包括基于用户风险偏好的规范性效用评分与实际用户行为记录，从而实现对模型决策逻辑的诊断。数据来源于真实市场数据与人类决策轨迹，并设计了可控的咨询对话流程。

Result: 实验结果显示，当前最先进的大语言模型在理性决策质量与行为一致性之间存在明显张力：表现优异的模型在基于效用的排名任务中得分较高，但往往偏离用户实际选择；而高度匹配用户行为的模型则容易过度拟合短期市场波动带来的噪声信号，导致决策质量下降。这揭示了现有模型在理解长期财务目标与应对短期市场情绪之间的根本性挑战。

Conclusion: Conv-FinRe为评估金融领域大语言模型提供了超越行为模仿的新框架，强调决策合理性的重要性。研究发现，单纯追求行为对齐可能导致模型忽视用户的长期利益，未来应设计更注重理性分析与个性化目标对齐的评估机制与训练策略。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [87] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 本文提出了一种名为Sonar-TS的神经符号框架，用于解决时间序列数据库的自然语言查询（NLQ4TSDB）问题。该框架采用搜索-验证流水线，借鉴主动声纳原理，通过特征索引和SQL查询定位候选时间段，并利用生成的Python程序对原始信号进行精确验证。为支持有效评估，作者构建了首个大规模基准测试集NLQTSBench，专门针对超长历史时间序列数据的自然语言查询。实验表明，Sonar-TS在处理复杂时序查询方面优于传统方法，解决了现有文本转SQL方法无法处理连续形态意图（如形状、异常）以及时间序列模型难以应对超长历史的问题。本研究首次系统性地探讨了NLQ4TSDB，提出了通用框架与评估标准，推动该领域未来发展。


<details>
  <summary>Details</summary>
Motivation: 现有文本转SQL方法不适用于处理时间序列中的连续形态意图（如形状、异常），而传统时间序列模型难以处理超长历史数据，导致非专家用户在从海量时间记录中检索有意义事件、区间和摘要时面临困难。因此需要一种能同时兼顾语义理解与高精度信号验证的新方法。

Method: 提出Sonar-TS框架，采用搜索-验证（Search-Then-Verify）流水线：首先利用特征索引通过SQL快速筛选候选时间窗口；随后生成Python程序对原始信号进行细粒度验证，实现精准匹配。整个过程类比于主动声纳的工作机制。

Result: Sonar-TS在复杂时序查询任务中表现优异，能够有效识别形状、异常等连续形态意图，在超长历史数据上展现出强大能力，显著优于传统方法。实验验证了其在真实场景下的有效性与鲁棒性。

Conclusion: 本研究首次系统性地探索了自然语言查询时间序列数据库（NLQ4TSDB）这一新兴方向，提出了一个通用且可扩展的神经符号框架Sonar-TS，并建立了首个大规模基准测试集NLQTSBench，为未来研究提供了方法论基础与评估标准。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [88] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder 是一种两阶段匹配系统，通过 Ruzicka 相似性索引快速筛选非异常技能范围的队伍，并利用基于逆正态分布的技能桶与 Kantorovich 距离计算公平性得分（Sanction Score），实现快速且公平的多人在线游戏匹配。


<details>
  <summary>Details</summary>
Motivation: 现有基于平均技能指标的匹配方法在面对异质技能水平的队伍时容易导致比赛失衡，尤其当技能分布宽或偏斜时，因此需要更精确的匹配机制以提升游戏公平性和玩家满意度。

Method: 采用两阶段匹配流程：第一阶段使用 Ruzicka 相似性索引对队伍的非异常技能范围进行快速预筛选；第二阶段将玩家排名映射到由逆正态分布生成的非线性技能桶中，通过排序后的桶索引计算 Kantorovich 距离，得到 Sanction Score 作为匹配公平性的量化指标。

Result: 通过对1.4亿次模拟队伍配对的 Sanction Score 分布分析，验证了该系统的可行性，并为设定公平匹配阈值提供了坚实依据。

Conclusion: Cinder 系统能够高效、准确地实现多玩家在线游戏中公平且快速的队伍匹配，显著改善了传统方法的不平衡问题，具有实际应用价值。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [89] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F 是首个实现项目级自动形式化（Math-to-Formal）的智能体框架，可在约三周内将 479 页的实分析与凸分析教材转化为包含 153,853 行代码的完整 Lean 项目，证明成功率高达 96%（优于基线 80%），实现了可扩展、端到端的形式化。


<details>
  <summary>Details</summary>
Motivation: 当前自动化数学形式化局限于孤立定理和短片段，缺乏对教材和研究论文等长篇文献的规模化支持，尤其面临跨文件依赖、导入管理及整体编译等问题。

Method: M2F 分两阶段运行：(1) 声明编译阶段将文档拆分为原子块，基于依赖关系排序并修复声明骨架直至项目可编译（允许证明占位符）；(2) 证明修复阶段在固定签名下使用目标导向的局部编辑填补证明空缺。全程保持验证器反馈闭环，仅在工具链确认改进后才提交修改。

Result: 成功将两本教材共 479 页内容形式化为 153,853 行 Lean 代码，项目级编译通过，证明成功率 96%（相比基线提升 16%），相当于数月甚至数年专家工作量的效率。

Conclusion: M2F 首次展示了大规模、端到端数学文献自动形式化的可行性，标志着实用化、可扩展形式化迈向现实。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [90] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 该论文介绍微软 Dynamics 365 Sales 中的 Sales Research Agent，一个连接实时 CRM 数据、推理复杂数据模式并生成可决策洞察的 AI 首选应用。为提升质量可观察性，提出 Sales Research Bench 基准测试，涵盖八项客户加权维度，如文本与图表的准确性、相关性、可解释性等。在 2025 年 10 月 19 日针对定制企业模式的 200 个问题测试中，该系统在 100 分制综合评分上分别领先 Claude Sonnet 4.5 13 分、ChatGPT-5 24.1 分，提供可重复比较 AI 解决方案的方法。


<details>
  <summary>Details</summary>
Motivation: 企业需要能基于实时、定制化 CRM 数据回答销售主管问题的 AI 系统，但现有模型缺乏透明、可重复的质量证据，因此亟需一种可衡量、可比较的 AI 质量评估方式。

Method: 开发 Sales Research Agent，集成实时 CRM 数据访问与复杂模式推理能力；构建 Sales Research Bench 基准，针对文本与图表的准确性、相关性、可解释性、模式匹配度及图表质量等八项客户加权指标进行评估。

Result: 在 2025 年 10 月 19 日对定制企业模式的 200 个问题测试中，Sales Research Agent 在 100 分制综合评分上优于 Claude Sonnet 4.5 13 分，优于 ChatGPT-5 24.1 分，验证了其在真实场景下的优越表现。

Conclusion: Sales Research Agent 结合实时数据与可度量的性能基准，为销售领域 AI 应用提供了可验证、可比较的高质量解决方案，推动 AI 在企业级决策支持中的可信落地。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [91] [IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents](https://arxiv.org/abs/2602.17049)
*Seoyoung Lee,Seobin Yoon,Seongbeen Lee,Yoojung Chun,Dayoung Park,Doyeon Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: IntentCUA提出了一种多智能体计算机使用框架，通过意图对齐的计划记忆来稳定长时程执行。该框架利用规划器、计划优化器和批评者在共享记忆中抽象原始交互痕迹为多视角意图表示，并复用技能。运行时，意图原型检索对齐的技能并注入部分计划，减少重复重规划和错误传播。在端到端评估中，IntentCUA达到74.83%的任务成功率和0.91的步骤效率比，优于基于强化学习和轨迹中心的基线方法。消融实验表明，多视角意图抽象与共享计划记忆共同提升执行稳定性，合作式多智能体循环在长时程任务中贡献最大。结果表明，系统级意图抽象和基于记忆的协调是实现可靠高效桌面自动化的关键。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长时程、噪声感知、多窗口上下文和动态环境状态下易偏离用户意图，重复解决常规子问题，导致错误累积和低效。需要一种更稳定、高效的计算机使用自动化框架。

Method: 提出多智能体框架IntentCUA，包含规划器、计划优化器和批评者，通过共享记忆将原始交互抽象为多视角意图表示，并复用技能。运行时通过意图原型检索对齐技能并注入部分计划，减少冗余重规划。

Result: 在端到端评估中，任务成功率达到74.83%，步骤效率比为0.91，优于基于强化学习和轨迹中心的基线方法。消融实验显示多视角意图抽象与共享计划记忆协同提升稳定性，合作多智能体循环在长时程任务中贡献最大。

Conclusion: 系统级意图抽象和基于记忆的协调是实现可靠且高效桌面自动化在复杂动态环境中的关键。

Abstract: Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.

</details>


### [92] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: S2Q提出了一种新的多智能体强化学习方法，通过学习多个子价值函数来保留替代的高价值动作，从而增强探索性并快速适应价值函数变化，显著提升MARL在复杂环境中的性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有基于值分解的MARL方法依赖单一最优动作，在训练过程中当价值函数发生变化时难以适应，容易收敛到次优策略。

Method: 提出Successive Sub-value Q-learning (S2Q)，通过学习多个子价值函数，并将其融入基于Softmax的行为策略中，以促进持续探索并使$Q^{\text{tot}}$能快速响应最优解的变化。

Result: 在多个具有挑战性的MARL基准测试中，S2Q consistently优于多种MARL算法，展现出更强的适应性和整体性能。

Conclusion: S2Q通过多子价值函数机制有效提升了MARL系统的探索能力和动态适应能力，为解决价值函数漂移问题提供了新思路。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [93] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: 提出了一种名为Predictive Batch Scheduling (PBS)的新型训练优化技术，通过动态优先选择高损失样本进行批次构建来加速语言模型收敛。PBS使用轻量级线性预测器在线估计样本难度，仅依赖四个静态标记级特征（词频、序列长度、词汇多样性、稀有词比例），与实际损失的相关性达到0.44。实验表明，在130M参数Transformer模型上，PBS使评估损失的收敛速度提升6-13%，且预测相关性在10,000步训练中从0.14提升至0.44。结果验证了词频统计蕴含样本难度信息，可实现高效且开销极小的课程学习。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖预定义的难度度量，而硬样本挖掘方法需要昂贵的逐样本损失追踪，二者均存在计算或设计上的局限。本文旨在提出一种无需额外标注、计算开销极低且能有效提升训练效率的动态样本优先策略。

Method: PBS采用在线训练的轻量级线性预测器，基于四个静态标记级特征（词频、序列长度、词汇多样性、稀有词比例）估计样本难度，并据此动态调整批量构建中样本的优先级，从而实现自适应的课程学习。

Result: 在130M参数Transformer模型上，PBS实现了6-13%的收敛速度提升（以评估损失为指标），预测器与真实损失的相关性从0.14上升至0.44，证明了词频等静态特征对样本难度具有强表征能力。

Conclusion: token频率等静态特征能够有效编码样本难度信息，使得PBS能够在极低计算开销下实现高效的动态课程学习，显著加速语言模型训练收敛。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [94] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 本文提出一种人-AI协作框架，旨在统一可持续性评级，通过STRIDE和SR-Delta两个部分构建企业级基准数据集，提升评级方法的可比性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前不同评级机构对同一公司的可持续性评分差异大，影响其可比性与决策参考价值，亟需统一标准。

Method: 提出STRIDE（基于原则的评分系统）与SR-Delta（差异分析流程）相结合的框架，利用大语言模型构建企业级基准数据集，并识别评分差异原因。

Result: 该框架实现了可持续性评级方法的可扩展、可比较评估，为提升评级可信度提供技术路径。

Conclusion: 呼吁人工智能界采用AI驱动的方法，推动可持续性评级方法的发展，以支持全球可持续发展目标。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [95] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 该论文提出了一种基于Owen值的新型可解释人工智能方法（O-Shap），旨在解决传统Shapley值在视觉任务中因像素间空间与语义依赖关系而失效的问题。通过引入满足T-性质的新分组策略，确保层次结构中的语义一致性，从而提升归因精度、可解释性与计算效率。实验表明，在图像和表格数据集上，O-Shap优于现有SHAP变体，尤其在结构敏感场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值假设特征独立，但在视觉任务中像素存在强空间与语义依赖，导致归因不准确；现有分组方式（如轴对齐或SLIC）不满足一致性要求，影响Owen值的有效性。

Method: 提出一种满足T-性质的新型特征分组方法，构建语义一致的层次化分组结构，并利用该结构实现计算剪枝，提升Owen值在特征归因中的应用效果。

Result: 在图像和表格数据集上的实验显示，O-Shap在归因精度、语义连贯性和运行效率方面均优于基线SHAP方法，尤其在结构重要时优势显著。

Conclusion: 通过引入满足T-性质的层次化分组机制，O-Shap有效提升了基于Owen值的特征归因质量，为复杂结构数据的可解释性分析提供了更可靠的技术路径。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [96] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG is a framework that automatically constructs instructor-aligned knowledge graphs from lecture materials by extracting concepts and inferring learning dependencies (e.g., 'depends-on', 'part-of') using both temporal and semantic signals from educational content and large language models. It enables scalable, personalized learning support by capturing accurate course-specific knowledge progressions.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge graphs for education are either too surface-level (e.g., course-level topics or enrollment logistics) or fail to leverage rich pedagogical signals in instructional materials. This limits their ability to accurately reflect learning progressions and identify student knowledge gaps in large-scale courses.

Method: InstructKG extracts significant concepts from lecture materials (slides, notes) and infers directed learning dependencies by combining temporal order (e.g., topic X taught before Y) and semantic cues (e.g., mention of concept X in the definition of Y), enhanced by large language models for generalization.

Result: Experiments on real-world lecture materials across multiple courses and human evaluations show that InstructKG effectively captures instructor-aligned learning progressions with high accuracy and relevance.

Conclusion: InstructKG provides a scalable and precise method for constructing knowledge graphs that reflect intended learning sequences, enabling better diagnosis of knowledge gaps and more effective personalized learning interventions.

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [97] [Epistemology of Generative AI: The Geometry of Knowing](https://arxiv.org/abs/2602.17116)
*Ilya Levin*

Main category: cs.AI

TL;DR: 本文提出生成式AI挑战了我们对知识及其生产方式的理解，指出其机制的元认知特征尚不清晰。论文主张需进行范式转变，从图灵-香农-冯·诺依曼的信息处理传统转向高维空间中的语义几何结构。通过分析高维几何的四个特性（集中性、近正交性、指数方向容量和流形规律性），构建了‘高维空间的索引性认识论’，并基于皮尔斯符号学与帕珀特建构主义，将生成模型重新定义为学习流形的导航者，提出‘导航知识’作为区别于符号推理和统计重组的第三种知识生产模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的运作机制缺乏清晰的元认知理解，导致其在科学、教育和制度生活中的负责任整合无法建立在原则基础上，亟需新的哲学框架来解释其知识生产本质。

Method: 结合高维几何的结构性质（如集中性、近正交性、指数方向容量、流形规律性），融合皮尔斯符号学与帕珀特建构主义，提出索引性认识论，并将生成模型视为在学习流形中导航的认知主体。

Result: 提出了‘导航知识’这一新知识生产模式，揭示了生成式模型在高维语义空间中的主动认知作用，为理解生成式AI提供了哲学基础。

Conclusion: 生成式AI的知识生产并非简单的符号操作或统计拟合，而是基于高维语义空间中导航的动态过程；必须以索引性认识论重构对生成模型的理解，才能实现其负责任的应用。

Abstract: Generative AI presents an unprecedented challenge to our understanding of knowledge and its production. Unlike previous technological transformations, where engineering understanding preceded or accompanied deployment, generative AI operates through mechanisms whose epistemic character remains obscure, and without such understanding, its responsible integration into science, education, and institutional life cannot proceed on a principled basis. This paper argues that the missing account must begin with a paradigmatic break that has not yet received adequate philosophical attention. In the Turing-Shannon-von Neumann tradition, information enters the machine as encoded binary vectors, and semantics remains external to the process. Neural network architectures rupture this regime: symbolic input is instantly projected into a high-dimensional space where coordinates correspond to semantic parameters, transforming binary code into a position in a geometric space of meanings. It is this space that constitutes the active epistemic condition shaping generative production. Drawing on four structural properties of high-dimensional geometry concentration of measure, near-orthogonality, exponential directional capacity, and manifold regularity the paper develops an Indexical Epistemology of High-Dimensional Spaces. Building on Peirce semiotics and Papert constructionism, it reconceptualizes generative models as navigators of learned manifolds and proposes navigational knowledge as a third mode of knowledge production, distinct from both symbolic reasoning and statistical recombination.

</details>


### [98] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出了一种新型并行算法，用于分解困难的CircuitSAT实例，通过专用约束将原始SAT实例划分为一系列弱化公式，并利用并行计算的硬度估计来指导参数调整，实现高效高质量分解，在逻辑等价性检查和哈希函数预像攻击等挑战性实例上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决CircuitSAT实例中高复杂度问题，提升求解效率与质量，特别是在逻辑等价性检查和密码学安全分析等实际应用中。

Method: 设计一种参数化的并行算法，通过引入专用约束对SAT实例进行分解，结合并行计算的硬度评估动态调整参数，以获得高质量的分解结果。

Result: 在多个具有挑战性的CircuitSAT实例（如布尔电路逻辑等价性检查、哈希函数预像攻击）上实现了显著性能提升，验证了算法的有效性和实用性。

Conclusion: 所提出的并行分解算法能够有效应对复杂CircuitSAT问题，为实际应用中的大规模SAT求解提供了高效可行的解决方案。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [99] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA 提出一种结合 JEPA 与传统生成目标的新预训练框架，通过在潜在空间中监督 CLS token 来实现对基因组片段高阶功能嵌入的预测，从而增强模型对全局生物功能上下文的理解。该方法可作为独立目标或持续预训练增强手段，显著提升多种基因组任务中的表现，尤其在零样本和监督任务中优于仅使用生成式目标的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有基因组基础模型（GFMs）主要依赖掩码语言建模（MLM）或下一个词元预测（NTP），虽能捕捉局部基因组语法和精细基序模式，但难以捕捉更广泛的生物学功能上下文，导致表示缺乏全局生物学视角。

Method: JEPA-DNA 结合联合嵌入预测架构（JEPA）与传统生成目标，通过将词元级恢复与潜在空间中的预测目标相结合，利用 CLS token 监督被遮蔽基因组片段的高阶功能嵌入，迫使模型关注整体功能而非单个核苷酸。

Result: 在多个基因组基准测试中，JEPA-DNA 在监督和零样本任务中均表现出优于纯生成式基线的性能，表明其能生成更具生物意义且鲁棒的表示。

Conclusion: JEPA-DNA 为理解基因组字母及其背后的功能逻辑提供了一条可扩展的基础模型路径，显著提升了基因组表征学习的全局生物学理解能力。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [100] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo 是一个仅有 2000 万参数的轻量级高性能公式识别模型，通过精心设计、词汇和分词器的蒸馏与迁移，实现了与 UniMERNet-T 和 PPFormulaNet-S 等先进模型相当的性能，同时模型规模分别减少了 80% 和 65%，支持在消费级硬件上实时推理甚至浏览器内部署，并配套开发了网页应用以方便用户使用。


<details>
  <summary>Details</summary>
Motivation: 现有公式识别模型体积庞大，难以在普通设备或浏览器中实时运行，亟需一种小而强的模型以实现高效部署。

Method: 通过精心设计模型结构，结合词汇和分词器的蒸馏与迁移技术，优化模型压缩与性能保持。

Result: Texo 在保持高精度的同时，模型大小显著减小，支持实时推理与浏览器部署，且提供了友好的用户界面。

Conclusion: Texo 证明了小模型在公式识别任务中的可行性与高效性，为轻量化模型在实际场景中的应用提供了新范式。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [101] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 提出大型行为模型（LBM），通过结构化高维特质档案而非临时提示，实现对个体战略选择的高保真预测。LBM在复杂心理特质与情境约束交互下表现优越，优于基础LLM和提示基方法，且随特质信息密度增加性能持续提升，适用于战略预判、谈判分析等场景。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在高风险环境中难以生成一致、个体特定的行为，尤其在心理特质与情境约束复杂交互时表现不佳；传统提示方法存在身份漂移和无法充分利用详细人格描述的问题。

Method: 引入大型行为模型（LBM），基于全面的心理测量数据集，将个体心理特质转化为结构化的高维行为嵌入表示，并在此基础上微调模型以预测具体决策行为。

Result: LBM在保留测试场景中显著优于未调整的Llama-3.1-8B-Instruct模型，与前沿基线相当；且随着特质信息维度增加，其性能持续提升，表现出良好的可扩展性。

Conclusion: LBM是一种可扩展的高保真行为模拟方法，能够有效融合复杂心理特征与情境因素，为战略预见、谈判分析、认知安全等领域提供有力支持。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [102] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 本研究利用布卢姆分类法作为层次化视角，分析大型语言模型（LLM）内部神经表示中的认知复杂性。通过分析不同LLM的高维激活向量，发现从基本回忆（Remember）到抽象综合（Create）的不同认知层级在模型的残差流中具有线性可分性。线性分类器在所有布卢姆层级上平均达到约95%的准确率，表明认知层级编码在模型表示的一个线性可访问子空间中。结果还显示，模型在前向传播早期即识别出提示的认知难度，且随着层加深，表示的可分性增强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的黑箱特性需要超越表面性能指标的新评估框架。理解模型如何编码认知复杂性对于揭示其内部工作机制至关重要。

Method: 采用布卢姆分类法作为分析框架，对多个LLM的高维激活向量进行分析，使用线性分类器测试不同认知层级在模型残差流中的可分性。

Result: 线性分类器在所有布卢姆层级上平均达到约95%的准确率，表明认知层级在模型表示中具有高度线性可分性。模型在前向传播早期即处理认知难度，且表示的可分性随层数增加而增强。

Conclusion: 认知复杂性在大型语言模型的内部表示中以线性可访问的方式编码，且这一编码在前向传播过程中逐步显现，为理解模型的认知机制提供了重要线索。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [103] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 本文提出一种用于检测和量化大语言模型中时间知识泄漏的框架，通过将模型推理分解为原子性陈述并基于时间可验证性分类，结合Shapley值评估各陈述对预测的贡献，从而得到可解释的Shapley-DCLR指标。在此基础上，提出TimeSPEC方法，通过交错生成与陈述验证来主动过滤时间污染，确保所有支持性陈述均来自截止日期前的信息。实验表明，标准提示方法存在显著知识泄漏，而TimeSPEC在保持任务性能的同时有效降低泄漏率，证明了声明级验证优于基于提示的时间约束。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在进行历史事件预测回测时，可能因训练过程中编码了截止日期后的信息而产生时间知识泄漏，影响评估结果的有效性。需要一种能够识别并量化此类泄漏的方法，以实现可信的回溯评估。

Method: 提出基于原子陈述分解与时间可验证性分类的Claim-Level框架，利用Shapley值量化每个陈述对决策的贡献，构建Shapley-DCLR指标；设计TimeSPEC方法，通过生成-验证-再生的循环机制，确保所有推理依据均来自截止日期前的信息。

Result: 在350个实例（包括美国最高法院案件预测、NBA薪资估算、股票回报排序）上的实验显示，标准提示方法存在显著时间知识泄漏；TimeSPEC能有效降低泄漏率，同时保持任务性能，优于仅依赖提示的时间约束方法。

Conclusion: 显式、可解释的声明级验证机制在防止时间知识泄漏方面优于传统的提示工程方法，为可靠的大语言模型回测提供了新范式。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [104] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细描述了从arXiv原始LaTeX源码训练一个1.36B参数科学语言模型的全流程，涵盖数据过滤、LaTeX提取、文本规范化、领域感知分词及在有限计算资源（2xA100 GPU）下的密集Transformer训练。通过24次实验，分析了训练稳定性、缩放行为、数据损失和基础设施瓶颈，揭示预处理决策对可用令牌量的影响、分词对符号稳定性的关键作用，以及存储与I/O限制可能成为与计算能力同等重要的制约因素。在520亿预训练令牌的数据量下实现稳定训练，强调工程实践透明性，为中等算力预算的研究者提供可复现的领域专用模型构建参考。


<details>
  <summary>Details</summary>
Motivation: 当前前沿大模型虽具备强大推理与数学能力，但如何从原始资料中训练领域专用科学语言模型的实践过程仍缺乏系统记录与公开分享。本文旨在填补这一空白，为研究者提供可复制、透明且适用于有限算力环境的科学语言模型训练方法论。

Method: 提出端到端训练流程：包括元数据筛选、归档验证、LaTeX内容提取、文本标准化、领域感知分词，以及基于2xA100 GPU的密集Transformer模型训练；通过24组实验系统评估训练稳定性、数据利用率、缩放特性与硬件瓶颈。

Result: 预处理策略显著影响有效训练令牌数量；分词方式直接影响符号表达的稳定性；存储与I/O性能成为与计算能力并列的关键限制因素；在520亿令牌规模下实现稳定训练，验证了数据丰富场景下的可行性。

Conclusion: 本工作未引入新架构，而是以工程视角提供从零开始训练小型科学语言模型的完整、透明实践指南，特别适用于算力受限的研究团队，有助于推动领域专用模型的可复现开发与普及。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [105] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 提出了一种无需外部数据的无数据正则化方法，通过将表示漂移正则化建模为曲率矩阵近似问题，利用Kronecker-Factored Approximate Curvature技术，实现任务添加和否定中的最先进性能。该方法在任务数量上具有恒定复杂度，对任务向量缩放具有鲁棒性，无需保留调参数据。


<details>
  <summary>Details</summary>
Motivation: 现有表示漂移正则化方法通常需要外部任务数据，违背了模块化和数据可用性（如隐私要求）的约束。因此需要一种不依赖外部数据的正则化方法。

Method: 将表示漂移正则化建模为曲率矩阵近似问题，采用Kronecker-Factored Approximate Curvature技术，构建无数据正则化器。

Result: 在任务添加和任务否定任务中达到最先进的性能，具有恒定复杂度，对任务向量缩放具有鲁棒性，无需额外调参数据。

Conclusion: 所提出的无数据正则化方法有效解决了任务向量组合中的表示漂移问题，在保持模块性和可扩展性的同时，无需外部数据，适用于实际应用。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [106] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 提出MCVAE模型，通过多模态对比变分自编码器融合病理图像、转录组和甲基化数据，增强对非小细胞肺癌患者生存预测的鲁棒性，尤其在数据严重缺失情况下表现优异。引入随机模态掩码和跨模态对比损失以提升泛化能力，并验证了多模态融合并非总能带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 真实临床数据中常存在多模态数据缺失问题，现有方法在严重缺失情况下缺乏鲁棒性，亟需一种能够有效处理不完整多模态数据并保持预测性能的模型。

Method: 设计多模态对比变分自编码器（MCVAE），包含模态特定的变分编码器、带学习门控机制的融合瓶颈层，以及结合生存损失、重构损失和跨模态对比损失的多任务目标；训练时采用随机模态掩码策略以增强对任意缺失模式的适应能力。

Result: 在TCGA-LUAD（n=475）和TCGA-LUSC（n=446）数据集上，MCVAE在疾病特异性生存预测任务中表现优于两种先进模型，且在严重缺失场景下仍保持较高稳定性；进一步分析表明，多模态融合并非在所有情况下都提升性能。

Conclusion: MCVAE通过有效的多模态融合与鲁棒性设计，在不完整数据条件下显著提升了非小细胞肺癌生存预测的准确性，同时揭示了多模态集成的局限性，为未来研究提供了重要参考。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [107] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于隐私设计（Privacy-by-Design）的框架，旨在指导开发者在儿童使用的AI技术中主动防范隐私风险。该框架整合了GDPR、PIPEDA、COPPA等法规原则，并将其映射到大语言模型（LLM）应用的多个生命周期阶段（如数据收集、模型训练、运行监控和持续验证），同时结合联合国《儿童权利公约》（UNCRC）、英国适龄设计代码（AADC）等儿童保护标准，提供具体的设计指南。通过一个面向13岁以下儿童的LLM教育辅导工具案例研究，证明该框架能有效降低隐私风险并满足法律合规要求。


<details>
  <summary>Details</summary>
Motivation: 随着儿童越来越多地使用人工智能技术，隐私风险日益突出。尽管已有相关法律法规要求保护用户隐私，但在实际应用中落实这些要求仍具挑战性。因此，亟需一种系统性的设计框架，帮助开发者在早期阶段就主动考虑并实现隐私保护，尤其针对儿童这一敏感群体。

Method: 本文构建了一个融合多国隐私法规与儿童权益保护标准的隐私设计框架，将核心原则分解至大语言模型（LLM）应用的全生命周期各阶段，并结合学术文献中的技术与组织控制措施，提出可操作的设计建议。通过一个实际案例研究验证框架的适用性和有效性。

Result: 研究表明，通过采用技术与组织控制措施，并在模型生命周期各阶段贯彻适龄设计原则，可以显著降低儿童在使用AI应用时的隐私风险，同时确保符合国际主流隐私法规要求。该框架为儿童友好型AI产品的开发提供了实用指导。

Conclusion: 本研究提出的隐私设计框架为开发面向儿童的AI系统提供了系统化、可操作的解决方案，有助于实现隐私保护与法律合规的双重目标，推动更安全、更负责任的儿童数字生态建设。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [108] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec 是一个高性能推荐系统框架，通过后端无关的架构解决了学术研究与工业应用之间的分裂问题。它支持50+算法、40+评估指标和19种数据处理策略，可无缝从本地实验扩展到分布式训练。框架集成CodeCarbon实现能耗实时追踪，强调可持续性；同时面向生成式AI时代，推动推荐系统向智能体（Agentic AI）演进。代码开源，旨在成为下一代绿色、智能推荐系统的基础设施。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统研究受限于实验环境与工业部署之间的割裂：在内存中实验虽便捷，但迁移到分布式系统需复杂重写。缺乏统一、可持续且适配未来趋势的开发框架制约了创新。

Method: 提出WarpRec框架，采用后端无关设计，整合多种先进算法、评估指标与数据处理策略，支持从单机到分布式无缝扩展；集成CodeCarbon实现能源消耗监控；设计可扩展架构以适应未来生成式AI与智能体系统需求。

Result: WarpRec实现了本地与分布式训练的无痛切换，显著降低研发门槛；实证显示其在性能与能效之间取得良好平衡；具备前瞻性，为推荐系统向智能体化演进提供技术支撑。

Conclusion: WarpRec成功弥合了学术界与工业界在推荐系统研发中的鸿沟，兼具高性能、可持续性和未来兼容性，可作为下一代绿色、智能推荐系统的通用架构基础。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [109] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本研究提出了一种针对ARM Cortex处理器（M0+, M4, M7）上人工智能模型优化的实用基准测试框架，聚焦嵌入式系统中的能效、精度和资源利用率。通过自动化测试平台，系统评估多个关键性能指标（KPI），识别处理器与AI模型的最佳组合。研究发现浮点运算量（FLOPs）与推理时间呈近似线性关系，可作为计算需求的可靠估计指标。结合帕累托分析，有效权衡能耗与模型精度之间的关系，确保应用在满足性能要求的同时保持可持续性。结果表明：M7适合短时推理任务，M4在长时推理中更具能效优势，而M0+虽不适用于复杂模型，但适合简单任务。该工作为开发者设计高效能、低功耗的AI系统提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式系统中部署AI模型面临能效、精度和资源利用之间的复杂权衡。现有方法缺乏统一、自动化的评估框架，难以高效选择最适合特定应用场景的处理器与模型组合。因此，亟需一种系统化的方法来优化AI模型在ARM Cortex系列处理器上的部署，以支持实际应用中的高性能与可持续性需求。

Method: 构建一个自动化测试平台，对ARM Cortex M0+、M4、M7三种处理器上的多种AI模型进行基准测试。通过采集推理时间、能耗、内存占用等数据，量化评估不同配置下的性能表现。采用线性回归分析FLOPs与推理时间的关系，并运用帕累托前沿分析法识别能效与精度之间的最优平衡点。

Result: FLOPs与推理时间呈现高度线性相关性，可作为预测计算开销的有效指标；帕累托分析揭示了不同处理器在不同任务场景下的优势：M7在短推理周期中表现最佳，M4在长推理任务中更节能，M0+适用于轻量级模型。该框架可有效支持AI模型在嵌入式设备中的优化部署。

Conclusion: 本研究提出的基准测试框架为嵌入式AI模型在ARM Cortex处理器上的部署提供了系统化、可复用的优化路径。通过结合性能建模与多目标优化，实现了能效与精度的协同提升，显著增强了AI应用在资源受限环境中的实用性与可持续性。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [110] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: KG-RAG 是一种将知识图谱（KG）与检索增强生成（RAG）结合的新框架，旨在提升大语言模型（LLM）在电信领域的表现。通过利用结构化领域知识和动态事实检索，该框架显著提高了输出的准确性、可靠性与可解释性，有效减少了幻觉问题，在基准数据集上相比 RAG 和 LLM-only 模型分别提升了 14.3% 和 21.6% 的准确率。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电信领域面临领域复杂性高、标准不断演进及专业术语多等挑战，导致输出不准确、幻觉频发，难以满足实际运维需求。因此亟需一种能增强领域适应性和可靠性的方法。

Method: 提出 KG-RAG 框架，融合知识图谱（从电信标准和技术文档中构建）与检索增强生成（RAG），利用知识图谱提供结构化领域知识，通过 RAG 实现动态事实检索，从而约束和增强模型输出的准确性与合规性。

Result: 实验表明，KG-RAG 在多个基准数据集上显著优于 LLM-only 和标准 RAG 方法，平均准确率提升分别为 14.3% 和 21.6%，证明其在复杂电信场景下具有更强的准确性、可靠性和可解释性。

Conclusion: KG-RAG 通过结合知识图谱与检索增强生成，有效解决了通用大语言模型在电信领域中的幻觉与不准确问题，为构建可信、可解释的领域专用智能系统提供了可行路径。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [111] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 本文提出新的评估指标——可重用性和可验证性，用于衡量多智能体推理管道中链式思维（CoT）的质量。通过分离思考者（Thinker）与执行者（Executor）的职责，研究发现这些新指标与传统任务准确率无显著相关性，揭示了现有基于准确率的排行榜在评估推理能力上的盲点。有趣的是，专用推理模型生成的CoT并不总是比通用大模型如Llama和Gemma更具可重用性或可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前对链式思维（CoT）的评估过于依赖目标任务的准确率，无法反映推理过程本身的质量。为更全面地评估推理质量，需引入能够衡量推理可复用和可验证性的新指标。

Method: 提出Thinker-Executor框架，将CoT生成与执行解耦；设计可重用性和可验证性两个新指标，分别衡量执行者复用思考者推理的能力和依据其推理匹配答案的频率。

Result: 在五个基准测试上，四个思考者模型与十个执行者模型的实验表明，可重用性和可验证性与任务准确率不相关，说明现有准确率评估存在盲区；专用推理模型的CoT并未表现出持续优势。

Conclusion: 仅依赖任务准确率会低估或误判模型的真正推理能力。引入可重用性和可验证性有助于更全面评估大模型的推理质量，推动构建更合理的评估体系。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [112] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出了一种基于常微分方程（ODE）的统一理论框架，用于大语言模型（LLM）对齐中的激活操纵（activation steering）。该框架将传统的激活添加视为ODE解的一阶近似，并将设计引导方向转化为控制理论中的“屏障函数”设计问题。基于此，作者提出了ODESteer方法，通过定义正负激活的对数密度比作为屏障函数，实现多步、自适应的激活调整。相比现有方法，ODESteer在多个对齐基准上表现出显著提升，如TruthfulQA提升5.7%，UltraFeedback提升2.5%，RealToxicityPrompts提升2.4%。该工作为激活操纵提供了坚实的理论基础并实现了有效应用。


<details>
  <summary>Details</summary>
Motivation: 当前激活操纵方法缺乏统一的理论指导，且依赖于单步操作，难以捕捉复杂的激活分布模式。因此需要一个更系统、可解释的理论框架来改进方向设计和操作策略。

Method: 提出基于常微分方程（ODE）的理论框架，将激活操纵建模为求解ODE的过程；设计以对数密度比为屏障函数的多步自适应机制，形成ODESteer方法。

Result: 在多个主流大语言模型对齐基准测试中，包括TruthfulQA、UltraFeedback和RealToxicityPrompts，ODESteer均取得显著性能提升，最大达5.7%的相对改进。

Conclusion: 本文建立了激活操纵的统一ODE理论框架，通过引入屏障函数实现更精准、可扩展的方向设计，验证了其在实际对齐任务中的有效性，为未来表示工程提供了新的理论视角与实践路径。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [113] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 该研究提出一种基于联邦学习的混合AI模型，结合SWIN Transformer与多种CNN模型（如DenseNet201、Inception V3、VGG19），用于肺部疾病（如新冠肺炎和肺炎）的诊断。通过联邦学习实现医疗数据的安全分布式处理，提升诊断准确率与病情严重程度预测能力，并保障数据隐私与信息真实性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力的提升，人工智能在医疗健康领域的应用潜力巨大。然而，医疗数据涉及隐私，传统集中式学习存在安全风险。因此，亟需一种既能保证数据隐私又能提升诊断性能的分布式智能系统。

Method: 采用联邦学习框架，融合SWIN Transformer与多个先进CNN模型（DenseNet201、Inception V3、VGG19），构建混合深度学习模型，利用真实世界持续学习机制进行实时训练与优化。

Result: 所提出的混合模型在肺部疾病诊断任务中表现出较高的准确率，显著提升了对新冠肺炎和肺炎的检测能力；同时，联邦学习有效保障了数据安全与模型可信性，支持跨机构协作而不泄露原始数据。

Conclusion: 该研究证明了联邦学习与先进AI模型结合在医疗诊断中的可行性与优越性，为未来智能医疗系统的安全、高效部署提供了可靠解决方案。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [114] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 本文提出通过评估AI在所有可想象的人类游戏中的表现来衡量机器的通用智能，引入AI GameStore平台利用大模型与人类协作生成新游戏，并初步测试了多个视觉-语言模型，结果显示其表现远低于人类水平，尤其在世界模型学习、记忆和规划方面存在明显短板。


<details>
  <summary>Details</summary>
Motivation: 传统AI基准测试仅评估特定领域的狭窄能力，且容易因优化而饱和，无法全面衡量人类通用智能。因此需要一种更具挑战性和开放性的评估方式。

Method: 构建AI GameStore平台，结合大语言模型与人类反馈，自动从主流数字游戏平台获取并生成标准化、容器化的游戏变体，形成涵盖广泛人类游戏的‘人类游戏多宇宙’。

Result: 基于Apple App Store和Steam排行榜生成100个游戏，对7个前沿视觉-语言模型进行测试，结果表明最佳模型在多数游戏中得分不足人类平均分的10%，尤其在涉及世界模型学习、记忆和规划的游戏上表现不佳。

Conclusion: AI GameStore为评估和推动机器向人类般通用智能发展提供了一个可行且可扩展的路径，未来需进一步完善该平台以实现更全面的智能评估。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [115] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT 是一种基于分层离散扩散模型的分子图生成框架，通过引入化学先验编码和解耦原子编码，显著提升了分子生成的化学有效性与性能。在 MOSES 数据集上达到新的最先进水平，首次实现图扩散模型近乎完美的化学有效性，并在多属性引导生成和骨架扩展等下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的扩散模型在分子生成中存在化学有效性低、难以满足理想性质的问题，而1D建模虽有效但无法充分捕捉分子结构信息。因此亟需一种能兼顾结构完整性和化学合理性的新方法。

Method: 提出 MolHIT 框架，采用分层离散扩散模型，将化学先验编码到额外类别中，并通过解耦原子编码分离原子类型的不同化学角色，从而提升生成质量与可控性。

Result: 在 MOSES 数据集上实现近乎完美的化学有效性，超越现有 1D 基线模型，在多个指标上达到新纪录；在多属性引导生成和骨架扩展任务中也表现出色。

Conclusion: MolHIT 成功解决了传统图扩散模型在分子生成中的关键瓶颈，为 AI 驱动的药物发现和材料科学提供了高效、可靠的生成工具。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


### [116] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026是CLEF评估实验室，专注于从嘈杂的多语言历史文本中提取人-地关系。它扩展了先前的HIPE-2020和HIPE-2022活动，将任务从实体识别推进到语义关系抽取，重点在于识别多语言、多时期背景下人物与地点之间的关联关系。系统需区分两种关系：'at'（人物是否曾出现在某地）和'isAt'（人物在出版时间附近是否位于某地），要求结合时空线索进行推理。该评测采用三重评估标准，综合考察准确性、计算效率与领域泛化能力。通过连接关系抽取与大规模历史数据处理，旨在支持知识图谱构建、历史传记重构及数字人文中的空间分析等下游应用。


<details>
  <summary>Details</summary>
Motivation: 传统历史文本存在噪声大、语言多样、时间跨度广等问题，现有方法难以有效识别人物与地点间的复杂时空关系。为推动数字人文领域中历史信息的结构化处理，亟需一个能跨语言、跨时域准确识别人-地关系的评估平台，以促进相关技术的发展与应用。

Method: HIPE-2026采用多语言、多时期的历史文本作为输入数据，设计针对'at'和'isAt'两类语义关系的标注体系。参赛系统需基于上下文、时间信息与地理知识，对每对人物-地点组合进行分类判断。评测引入三重评估维度：精确率/召回率衡量准确性；运行时间与资源消耗评估计算效率；跨语言、跨时间域测试集表现衡量模型的领域泛化能力。

Result: 该评测成功搭建了一个面向多语言历史文本的标准化关系抽取基准，验证了多种自然语言处理模型在复杂时空语境下的表现差异。实验表明，结合外部知识库与时序建模的方法在提升准确性和泛化能力方面具有显著优势。

Conclusion: HIPE-2026不仅推动了历史文本中人-地关系抽取的技术进步，也为数字人文领域的知识组织与分析提供了可复用的评估框架与高质量数据资源，具有重要的学术价值与应用前景。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [117] [MMCAformer: Macro-Micro Cross-Attention Transformer for Traffic Speed Prediction with Microscopic Connected Vehicle Driving Behavior](https://arxiv.org/abs/2602.16730)
*Lei Han,Mohamed Abdel-Aty,Younggun Kim,Yang-Jun Joo,Zubayer Islam*

Main category: cs.LG

TL;DR: 提出MMCAformer模型，结合宏观交通流数据与微观驾驶行为数据，通过自注意力和交叉注意力机制提升速度预测精度与不确定性估计。实验表明，引入微观行为特征可显著降低误差（如RMSE、MAE、MAPE分别减少9.0%、6.9%、10.2%）并缩小预测区间（平均减少10.1-24.0%），尤其在拥堵低速条件下效果更优，其中急刹车和急加速频率为关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有速度预测多依赖宏观交通流数据，忽视了个体驾驶行为对交通动态的影响；而新兴的车联网（CV）数据提供了丰富的微观驾驶行为特征，为提升预测性能提供了新契机。

Method: 提出Macro-Micro Cross-Attention Transformer（MMCAformer），利用自注意力学习宏观交通流内部依赖关系，通过交叉注意力捕捉宏观状态与微观驾驶行为之间的时空交互，并采用Student-t负对数似然损失优化模型，实现点预测与不确定性估计。

Result: 在佛罗里达四条高速公路上的实验显示，相较于仅使用宏观特征，引入微观驾驶行为特征可显著提升预测精度（RMSE、MAE、MAPE分别下降9.0%、6.9%、10.2%），同时降低预测不确定性（平均预测区间缩小10.1%-24.0%），且在拥堵低速条件下优势更明显。急刹车与急加速频率被识别为最具影响力的特征。

Conclusion: 融合宏观交通流与微观驾驶行为特征的MMCAformer模型，在速度预测中表现出色，不仅提升了准确性，还增强了不确定性估计能力，为智能交通管理提供了有效支持。

Abstract: Accurate speed prediction is crucial for proactive traffic management to enhance traffic efficiency and safety. Existing studies have primarily relied on aggregated, macroscopic traffic flow data to predict future traffic trends, whereas road traffic dynamics are also influenced by individual, microscopic human driving behaviors. Recent Connected Vehicle (CV) data provide rich driving behavior features, offering new opportunities to incorporate these behavioral insights into speed prediction. To this end, we propose the Macro-Micro Cross-Attention Transformer (MMCAformer) to integrate CV data-based micro driving behavior features with macro traffic features for speed prediction. Specifically, MMCAformer employs self-attention to learn intrinsic dependencies in macro traffic flow and cross-attention to capture spatiotemporal interplays between macro traffic status and micro driving behavior. MMCAformer is optimized with a Student-t negative log-likelihood loss to provide point-wise speed prediction and estimate uncertainty. Experiments on four Florida freeways demonstrate the superior performance of the proposed MMCAformer compared to baselines. Compared with only using macro features, introducing micro driving behavior features not only enhances prediction accuracy (e.g., overall RMSE, MAE, and MAPE reduced by 9.0%, 6.9%, and 10.2%, respectively) but also shrinks model prediction uncertainty (e.g., mean predictive intervals decreased by 10.1-24.0% across the four freeways). Results reveal that hard braking and acceleration frequencies emerge as the most influential features. Such improvements are more pronounced under congested, low-speed traffic conditions.

</details>


### [118] [A Few-Shot LLM Framework for Extreme Day Classification in Electricity Markets](https://arxiv.org/abs/2602.16735)
*Saud Alghumayjan,Ming Yi,Bolun Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于大语言模型（LLM）的少样本分类框架，用于预测次日实时电价是否会出现波动。通过将电力系统状态信息（如需求、可再生能源发电、天气预报和近期电价）转化为自然语言提示输入LLM，模型判断次日为波动日的可能性并输出置信度分数。在德克萨斯州电力市场的历史数据上验证，该方法性能可媲美支持向量机和XGBoost等监督学习模型，且在数据有限时表现更优，表明LLM在数据稀缺场景下具备高效分类电价波动的潜力。


<details>
  <summary>Details</summary>
Motivation: 在电力市场中，电价波动预测对能源管理至关重要，但传统机器学习方法依赖大量历史数据。当数据稀缺时，模型性能显著下降。因此，亟需一种数据效率高的预测方法，以应对数据不足的现实挑战。本研究旨在探索大语言模型在少样本条件下的电价波动预测能力，利用其强大的泛化与上下文理解能力，实现高精度、低数据依赖的预测。

Method: 将电力系统状态信息（包括电力需求、可再生能源发电、天气预报及近期电价）聚合为统计特征，并将其转换为自然语言提示；结合通用指令输入大语言模型（LLM），由模型判断次日是否为价格波动日，并输出置信度评分；采用少样本设置，在德克萨斯州电力市场数据上进行训练与测试。

Result: 该方法在完整数据集上性能与支持向量机和XGBoost相当；在数据受限情况下，优于两者，表现出更强的鲁棒性与数据效率。结果显示，大语言模型在少样本条件下具备良好的电价波动分类能力。

Conclusion: 大语言模型可作为数据高效的工具，用于在历史数据有限的条件下准确识别电力价格波动日，具有广泛的应用前景，尤其适用于数据获取困难或更新频繁的能源市场场景。

Abstract: This paper proposes a few-shot classification framework based on Large Language Models (LLMs) to predict whether the next day will have spikes in real-time electricity prices. The approach aggregates system state information, including electricity demand, renewable generation, weather forecasts, and recent electricity prices, into a set of statistical features that are formatted as natural-language prompts and fed to an LLM along with general instructions. The model then determines the likelihood that the next day would be a spike day and reports a confidence score. Using historical data from the Texas electricity market, we demonstrate that this few-shot approach achieves performance comparable to supervised machine learning models, such as Support Vector Machines and XGBoost, and outperforms the latter two when limited historical data are available. These findings highlight the potential of LLMs as a data-efficient tool for classifying electricity price spikes in settings with scarce data.

</details>


### [119] [Real-time Secondary Crash Likelihood Prediction Excluding Post Primary Crash Features](https://arxiv.org/abs/2602.16739)
*Lei Han,Mohamed Abdel-Aty,Zubayer Islam,Chenzhu Wang*

Main category: cs.LG

TL;DR: 提出了一种不依赖事故后特征的混合二次事故概率预测框架，利用动态时空窗口提取实时交通流和环境特征，结合三种模型与集成学习策略，显著提升了预测性能，在佛罗里达州高速公路上实现了91%的二次事故识别率和0.20的低误报率，ROC曲线下面积提升至0.952。


<details>
  <summary>Details</summary>
Motivation: 现有二次事故预测方法依赖于难以实时获取的事故后特征（如事故类型和严重程度），限制了其在主动交通管理系统中的实际应用。

Method: 设计动态时空窗口以提取主事故位置及其上游路段的实时交通流和环境特征；构建三个模型（主事故模型及两个不同对比情景下的二次事故模型）；采用集成学习融合六种机器学习算法，并通过投票机制整合三模型输出。

Result: 实验表明，该框架在佛罗里达州高速公路上能正确识别91%的二次事故，误报率为0.20；各模型单独的AUC分别为0.654、0.744、0.902，融合后整体AUC达到0.952，优于以往研究。

Conclusion: 所提出的混合框架有效克服了传统方法对事故后特征的依赖，具备高精度与低误报能力，适用于实时主动交通管理，具有良好的应用前景。

Abstract: Secondary crash likelihood prediction is a critical component of an active traffic management system to mitigate congestion and adverse impacts caused by secondary crashes. However, existing approaches mainly rely on post-crash features (e.g., crash type and severity) that are rarely available in real time, limiting their practical applicability. To address this limitation, we propose a hybrid secondary crash likelihood prediction framework that does not depend on post-crash features. A dynamic spatiotemporal window is designed to extract real-time traffic flow and environmental features from primary crash locations and their upstream segments. The framework includes three models: a primary crash model to estimate the likelihood of secondary crash occurrence, and two secondary crash models to evaluate traffic conditions at crash and upstream segments under different comparative scenarios. An ensemble learning strategy integrating six machine learning algorithms is developed to enhance predictive performance, and a voting-based mechanism combines the outputs of the three models. Experiments on Florida freeways demonstrate that the proposed hybrid framework correctly identifies 91% of secondary crashes with a low false alarm rate of 0.20. The Area Under the ROC Curve improves from 0.654, 0.744, and 0.902 for the individual models to 0.952 for the hybrid model, outperforming previous studies.

</details>


### [120] [Quantifying LLM Attention-Head Stability: Implications for Circuit Universality](https://arxiv.org/abs/2602.16740)
*Karan Bali,Jack Stanley,Praneet Suresh,Danilo Bzdok*

Main category: cs.LG

TL;DR: 本文系统研究了不同规模Transformer语言模型在多次独立训练中注意力头表示学习的稳定性，发现中层注意力头最不稳定但最具表征差异性，深层模型中深度中间区域的发散更明显，且不稳定的深层注意力头功能上更为重要。权重衰减可显著提升注意力头跨初始化的稳定性，而残差流则相对稳定。这些结果强调了电路跨实例鲁棒性在可扩展监督中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 当前机制可解释性研究关注Transformer中的'电路'——即稀疏、单层或多层子计算，可能反映人类可理解的功能。然而，这些电路在不同实例间是否稳定尚未得到充分验证，限制了其在安全关键场景中的可信度。因此，需要系统评估电路在多次重训练中的稳定性。

Method: 通过在不同规模的Transformer语言模型上进行多次独立初始化训练，逐层量化注意力头在不同训练运行中表示学习的相似性，分析稳定性与模型深度、功能重要性的关系，并考察权重衰减的影响。

Result: （1）中层注意力头最不稳定但最具表征差异性；（2）深层模型中深度中间区域发散更强；（3）不稳定的深层注意力头功能上更重要；（4）权重衰减显著提高注意力头稳定性；（5）残差流具有较高稳定性。

Conclusion: 电路的跨实例鲁棒性是实现可扩展监督的关键前提，应被纳入对AI系统白盒监控能力的评估框架中。

Abstract: In mechanistic interpretability, recent work scrutinizes transformer "circuits" - sparse, mono or multi layer sub computations, that may reflect human understandable functions. Yet, these network circuits are rarely acid-tested for their stability across different instances of the same deep learning architecture. Without this, it remains unclear whether reported circuits emerge universally across labs or turn out to be idiosyncratic to a particular estimation instance, potentially limiting confidence in safety-critical settings. Here, we systematically study stability across-refits in increasingly complex transformer language models of various sizes. We quantify, layer by layer, how similarly attention heads learn representations across independently initialized training runs. Our rigorous experiments show that (1) middle-layer heads are the least stable yet the most representationally distinct; (2) deeper models exhibit stronger mid-depth divergence; (3) unstable heads in deeper layers become more functionally important than their peers from the same layer; (4) applying weight decay optimization substantially improves attention-head stability across random model initializations; and (5) the residual stream is comparatively stable. Our findings establish the cross-instance robustness of circuits as an essential yet underappreciated prerequisite for scalable oversight, drawing contours around possible white-box monitorability of AI systems.

</details>


### [121] [DeepVision-103K: A Visually Diverse, Broad-Coverage, and Verifiable Mathematical Dataset for Multimodal Reasoning](https://arxiv.org/abs/2602.16742)
*Haoxiang Sun,Lizhen Xu,Bing Zhao,Wotao Yin,Wei Wang,Boyu Yang,Rui Wang,Hu Wei*

Main category: cs.LG

TL;DR: 提出DeepVision-103K数据集，解决现有RLVR数据集多样性与覆盖范围不足的问题，涵盖K12数学主题、广泛知识点和丰富视觉元素，提升模型在多模态数学基准和通用多模态推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR数据集多源于小规模人工构建或已有资源重组，缺乏多样性和覆盖面，限制了模型性能提升。

Method: 构建DeepVision-103K数据集，覆盖K12数学领域中的多种主题、知识要点和视觉内容，用于强化学习中可验证奖励的训练。

Result: 基于DeepVision训练的模型在多模态数学基准上表现优异，并在通用多模态推理任务中展现出良好泛化能力；分析显示模型的视觉感知、反思与推理能力显著增强。

Conclusion: DeepVision-103K有效推动了多模态推理能力的发展，为大型多模态模型的训练提供了高质量、多样化且具挑战性的数据支持。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has been shown effective in enhancing the visual reflection and reasoning capabilities of Large Multimodal Models (LMMs). However, existing datasets are predominantly derived from either small-scale manual construction or recombination of prior resources, which limits data diversity and coverage, thereby constraining further gains in model performance. To this end, we introduce \textbf{DeepVision-103K}, a comprehensive dataset for RLVR training that covers diverse K12 mathematical topics, extensive knowledge points, and rich visual elements. Models trained on DeepVision achieve strong performance on multimodal mathematical benchmarks, and generalize effectively to general multimodal reasoning tasks. Further analysis reveals enhanced visual perception, reflection and reasoning capabilities in trained models, validating DeepVision's effectiveness for advancing multimodal reasoning. Data: \href{https://huggingface.co/datasets/skylenage/DeepVision-103K}{this url}.

</details>


### [122] [PETS: A Principled Framework Towards Optimal Trajectory Allocation for Efficient Test-Time Self-Consistency](https://arxiv.org/abs/2602.16745)
*Zhangyi Liu,Huaizhi Qu,Xiaowei Yin,He Sun,Yanjun Han,Tianlong Chen,Zhun Deng*

Main category: cs.LG

TL;DR: PETS提出了一种有原则且高效的测试时自一致性方法，通过优化轨迹分配提升样本效率。在离线和在线场景下，分别借鉴众包理论和动态预算调整，实现显著降低采样预算（最高75%）的同时达到完美自一致性，在GPQA等数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自一致性方法在有限预算下难以实现样本高效，亟需一种理论基础扎实、能有效分配推理轨迹的策略。

Method: 引入自一致性率作为新度量，将轨迹分配建模为众包问题；离线场景利用多数投票算法进行高效分配；在线场景则基于此框架动态调整预算以适应题目难度。

Result: PETS在离线和在线设置下均显著优于均匀分配，在GPQA上实现完美自一致性，采样预算减少高达75%（离线）和55%（在线）。

Conclusion: PETS通过理论驱动的轨迹分配机制，实现了测试时自一致性在低预算下的高效应用，为复杂模型推理提供了可扩展、可保证的解决方案。

Abstract: Test-time scaling can improve model performance by aggregating stochastic reasoning trajectories. However, achieving sample-efficient test-time self-consistency under a limited budget remains an open challenge. We introduce PETS (Principled and Efficient Test-TimeSelf-Consistency), which initiates a principled study of trajectory allocation through an optimization framework. Central to our approach is the self-consistency rate, a new measure defined as agreement with the infinite-budget majority vote. This formulation makes sample-efficient test-time allocation theoretically grounded and amenable to rigorous analysis. We study both offline and online settings. In the offline regime, where all questions are known in advance, we connect trajectory allocation to crowdsourcing, a classic and well-developed area, by modeling reasoning traces as workers. This perspective allows us to leverage rich existing theory, yielding theoretical guarantees and an efficient majority-voting-based allocation algorithm. In the online streaming regime, where questions arrive sequentially and allocations must be made on the fly, we propose a novel method inspired by the offline framework. Our approach adapts budgets to question difficulty while preserving strong theoretical guarantees and computational efficiency. Experiments show that PETS consistently outperforms uniform allocation. On GPQA, PETS achieves perfect self-consistency in both settings while reducing the sampling budget by up to 75% (offline) and 55% (online) relative to uniform allocation. Code is available at https://github.com/ZDCSlab/PETS.

</details>


### [123] [Low-Dimensional and Transversely Curved Optimization Dynamics in Grokking](https://arxiv.org/abs/2602.16746)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 该研究通过几何分析揭示了小规模算法任务中'涌现'现象（grokking）的机制。发现训练过程主要在低维执行子空间内进行，且曲率在垂直于该子空间的方向上急剧增长，而这一增长先于泛化出现，并遵循幂律关系。因果干预实验表明，沿学习到的子空间运动是泛化的必要条件，而人为增加曲率无法触发泛化。结果支持一种几何解释：grokking是系统从低维束缚和横向曲率积累的亚稳态中逃逸的过程。


<details>
  <summary>Details</summary>
Motivation: 理解grokking现象——即小算法任务中从记忆到泛化的延迟转变——的内在机制，尤其是其优化动力学中的几何特征。

Method: 采用主成分分析（PCA）分析注意力权重轨迹，识别低维执行子空间；测量梯度步长的非交换性（共变缺陷）以探究损失景观几何；通过投影和因果干预实验验证关键变量的作用。

Result: 训练过程集中在低维执行子空间内；横向曲率快速增长，且先于泛化发生；沿子空间的运动是泛化所必需的，但增加曲率不足以引发泛化；所有发现可在不同学习率、超参数设置及随机种子下复现。

Conclusion: grokking可被解释为系统从低维束缚与横向曲率积累的亚稳态中逃逸的几何过程，其中沿学习子空间的动态是核心驱动力。

Abstract: Grokking -- the delayed transition from memorization to generalization in small algorithmic tasks -- remains poorly understood. We present a geometric analysis of optimization dynamics in transformers trained on modular arithmetic. PCA of attention weight trajectories reveals that training evolves predominantly within a low-dimensional execution subspace, with a single principal component capturing 68-83% of trajectory variance. To probe loss-landscape geometry, we measure commutator defects -- the non-commutativity of successive gradient steps -- and project them onto this learned subspace. We find that curvature grows sharply in directions orthogonal to the execution subspace while the trajectory remains largely confined to it. Importantly, curvature growth consistently precedes generalization across learning rates and hyperparameter regimes, with the lead time obeying a power law in the grokking timescale. Causal intervention experiments show that motion along the learned subspace is necessary for grokking, while artificially increasing curvature is insufficient. Together, these results support a geometric account in which grokking reflects escape from a metastable regime characterized by low-dimensional confinement and transverse curvature accumulation. All findings replicate across this learning-rate range, a qualitatively different slow regime (lr=5e-5, wd=0.1, 3 layers), and three random seeds, though alignment dynamics differ quantitatively between regimes. Causal intervention experiments establish that orthogonal gradient flow is necessary but not sufficient for grokking: suppressing it prevents generalization with a monotonic dose-response across four operations, while artificially boosting curvature defects has no effect.

</details>


### [124] [LiveClin: A Live Clinical Benchmark without Leakage](https://arxiv.org/abs/2602.16747)
*Xidong Wang,Shuqi Guo,Yue Shen,Junying Chen,Jian Wang,Jinjie Gu,Ping Zhang,Lei Liu,Benyou Wang*

Main category: cs.LG

TL;DR: LiveClin is a dynamic, up-to-date medical LLM benchmark built from real clinical case reports to address data contamination and knowledge obsolescence. It features 1,407 case reports and 6,605 questions, evaluated via an AI-human workflow with 239 physicians. Top model performance reached only 35.7% Case Accuracy, significantly lower than human experts, highlighting the challenge of real-world clinical reasoning. The benchmark aims to improve the reliability and practical utility of medical LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing static benchmarks for medical LLMs suffer from data contamination and outdated knowledge, leading to inflated performance scores that do not reflect real-world clinical effectiveness. There is a need for a dynamic, clinically relevant evaluation framework.

Method: LiveClin is constructed using contemporary, peer-reviewed case reports updated biannually. A verified AI-human workflow involving 239 physicians converts real patient cases into complex, multimodal scenarios covering the full clinical pathway. The benchmark includes 1,407 case reports and 6,605 questions.

Result: Evaluation of 26 models on LiveClin shows that even the top-performing model achieves only 35.7% Case Accuracy. Chief Physicians outperform all models, followed by Attending Physicians, demonstrating the significant gap between current LLMs and expert human clinicians.

Conclusion: LiveClin offers a continuously evolving, clinically grounded benchmark that better reflects real-world practice. It enables more reliable evaluation of medical LLMs and guides their development toward greater accuracy, relevance, and real-world utility. Data and code are publicly available.

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

</details>


### [125] [Attending to Routers Aids Indoor Wireless Localization](https://arxiv.org/abs/2602.16762)
*Ayush Roy,Tahsin Fuad Hassan,Roshan Ayyalasomayajula,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 本文提出了一种名为'Attention to Routers'的新方法，通过在机器学习定位架构中引入注意力机制，动态加权不同路由器的信息贡献，从而提升无线定位精度。该方法在多个公开数据集上验证，性能比基准模型高出30%以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于Wi-Fi信号的机器学习定位算法在多路由器信息融合时缺乏合理的权重分配，导致收敛不佳和定位精度受限。受传统加权三角定位启发，本文旨在通过注意力机制实现对不同路由器信息的相关性差异化处理。

Method: 在标准机器学习定位架构中引入注意力层，根据各路由器信号的相关性和重要性动态分配权重，优化多路由器信息的聚合过程。

Result: 实验结果表明，Attention to Routers在多个公开数据集上显著优于基准模型，定位准确率提升超过30%。

Conclusion: 通过引入路由器注意力机制，能够有效提升多路由器环境下无线定位系统的性能，为复杂环境中的高精度定位提供了新思路。

Abstract: Modern machine learning-based wireless localization using Wi-Fi signals continues to face significant challenges in achieving groundbreaking performance across diverse environments. A major limitation is that most existing algorithms do not appropriately weight the information from different routers during aggregation, resulting in suboptimal convergence and reduced accuracy. Motivated by traditional weighted triangulation methods, this paper introduces the concept of attention to routers, ensuring that each router's contribution is weighted differently when aggregating information from multiple routers for triangulation. We demonstrate, by incorporating attention layers into a standard machine learning localization architecture, that emphasizing the relevance of each router can substantially improve overall performance. We have also shown through evaluation over the open-sourced datasets and demonstrate that Attention to Routers outperforms the benchmark architecture by over 30% in accuracy.

</details>


### [126] [Omitted Variable Bias in Language Models Under Distribution Shift](https://arxiv.org/abs/2602.16784)
*Victoria Lin,Louis-Philippe Morency,Eli Ben-Michael*

Main category: cs.LG

TL;DR: 本文提出了一种框架，用于分离语言模型中分布偏移的可观测与不可观测成分，并指出现有方法仅处理前者，而忽略了由未观测变量引起的遗漏变量偏差。该框架通过将遗漏变量强度映射到分布偏移下的最坏情况泛化性能边界，提升了语言模型在分布外数据上的评估与优化的合理性，实验证明其能更准确衡量分布外性能并提升真实表现，同时可推断遗漏变量的强度。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在分布外数据上表现脆弱，现有方法未能充分处理未观测变量带来的遗漏变量偏差，影响评估与优化的可靠性。

Method: 提出一种新框架，将遗漏变量强度映射为最坏情况泛化性能的理论边界，并利用该边界改进评估与优化过程。

Result: 实验表明，使用该框架能更合理地评估分布外性能，显著提升真实分布外表现，并可基于目标分布标签反推遗漏变量的强度。

Conclusion: 遗漏变量偏差是语言模型在分布偏移下性能下降的重要原因；引入基于遗漏变量强度的性能边界，为评估与优化提供了更稳健、可解释的工具。

Abstract: Despite their impressive performance on a wide variety of tasks, modern language models remain susceptible to distribution shifts, exhibiting brittle behavior when evaluated on data that differs in distribution from their training data. In this paper, we describe how distribution shifts in language models can be separated into observable and unobservable components, and we discuss how established approaches for dealing with distribution shift address only the former. Importantly, we identify that the resulting omitted variable bias from unobserved variables can compromise both evaluation and optimization in language models. To address this challenge, we introduce a framework that maps the strength of the omitted variables to bounds on the worst-case generalization performance of language models under distribution shift. In empirical experiments, we show that using these bounds directly in language model evaluation and optimization provides more principled measures of out-of-distribution performance, improves true out-of-distribution performance relative to standard distribution shift adjustment methods, and further enables inference about the strength of the omitted variables when target distribution labels are available.

</details>


### [127] [Better Think Thrice: Learning to Reason Causally with Double Counterfactual Consistency](https://arxiv.org/abs/2602.16787)
*Victoria Lin,Xinnuo Xu,Rachel Lawrence,Risa Ueno,Amit Sharma,Javier Gonzalez,Niranjani Prasad*

Main category: cs.LG

TL;DR: 本文提出了一种名为双反事实一致性（DCC）的轻量级推理时方法，用于衡量和引导大语言模型（LLM）的因果推理能力。该方法无需标注的反事实数据，通过验证模型在因果干预和反事实预测两个关键方面的表现，评估其因果推理能力，并可作为无训练的测试时拒绝采样标准，显著提升多个模型家族在推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理基准测试中表现强劲，但在面对反事实问题时表现出脆弱性，暴露出其因果推理能力的不足。现有研究依赖标注的反事实数据作为评估基准，但此类数据难以大规模生成，限制了评估的全面性。因此需要一种不依赖标注数据、可高效评估并改进模型因果推理能力的方法。

Method: 提出双反事实一致性（DCC）方法，通过在推理阶段执行两次反事实操作并检验其一致性，来评估模型是否具备因果干预与反事实预测能力。该方法无需额外训练，仅在测试时使用，可作为拒绝采样标准以筛选更可靠的推理结果。

Result: 实验表明，DCC能有效评估多种主流大语言模型在不同推理任务中的因果推理能力；同时，作为测试时拒绝采样策略，DCC显著提升了模型在多个任务上的性能，且适用于不同模型家族，具有良好的泛化能力。

Conclusion: DCC是一种无需标注数据、轻量高效的因果推理评估与增强方法，能够有效识别并提升大语言模型在复杂推理任务中的因果推理能力，为未来模型可靠性评估与优化提供了新思路。

Abstract: Despite their strong performance on reasoning benchmarks, large language models (LLMs) have proven brittle when presented with counterfactual questions, suggesting weaknesses in their causal reasoning ability. While recent work has demonstrated that labeled counterfactual tasks can be useful benchmarks of LLMs' causal reasoning, producing such data at the scale required to cover the vast potential space of counterfactuals is limited. In this work, we introduce double counterfactual consistency (DCC), a lightweight inference-time method for measuring and guiding the ability of LLMs to reason causally. Without requiring labeled counterfactual data, DCC verifies a model's ability to execute two important elements of causal reasoning: causal intervention and counterfactual prediction. Using DCC, we evaluate the causal reasoning abilities of various leading LLMs across a range of reasoning tasks and interventions. Moreover, we demonstrate the effectiveness of DCC as a training-free test-time rejection sampling criterion and show that it can directly improve performance on reasoning tasks across multiple model families.

</details>


### [128] [Escaping the Cognitive Well: Efficient Competition Math with Off-the-Shelf Models](https://arxiv.org/abs/2602.16793)
*Xingyu Dang,Rohit Agarwal,Rodrigo Porto,Anirudh Goyal,Liam H Fowl,Sanjeev Arora*

Main category: cs.LG

TL;DR: 本文提出了一种推理流水线，可在仅使用通用现成模型的情况下，以远低于现有方法的成本实现国际数学奥林匹克（IMO）风格数学问题的顶尖表现。该方法通过识别并解决‘认知井’（Cognitive Well）问题——即迭代修正导致错误解被误判为正确——来提升可靠性。核心创新在于‘猜想提取’：从生成解中分离候选引理，并在独立环境中验证其及其反面，实现上下文解耦。在IMO-ProofBench Advanced（PB-Adv）上，使用Gemini 3.0 Pro模型达到67.1%准确率，单题平均成本约31美元，优于所有公开及未公开模型，且成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽在IMO数学问题上取得金牌级表现，但依赖大规模推理，成本极高（如每题3000美元），难以普及。因此亟需一种高效、低成本且高性能的推理框架，尤其针对由‘认知井’等内在失败模式导致的错误收敛问题。

Method: 提出基于‘猜想提取’的推理流水线，通过从生成解中提取候选引理，并在独立环境（上下文解耦）中分别验证其与否定命题的真伪，从而规避因迭代修正陷入错误解而被内部评分器误判的问题。结合通用模型（如Gemini 3.0 Pro）实现高精度与低开销的平衡。

Result: 在IMO-ProofBench Advanced（PB-Adv）上，该方法实现67.1%的准确率，是当时公开和未公开模型中的最佳表现；单题平均成本仅为约31美元，相比其他方法降低两个数量级以上，性能提升超一倍。

Conclusion: 本研究展示了仅使用通用现成模型，通过巧妙设计推理流程（特别是猜想提取与上下文解耦）即可实现对复杂数学问题的高精度求解，显著降低了高性能数学推理的门槛与成本，为未来可扩展、低成本的智能推理系统提供了新范式。

Abstract: In the past year, custom and unreleased math reasoning models reached gold medal performance on the International Mathematical Olympiad (IMO). Similar performance was then reported using large-scale inference on publicly available models but at prohibitive costs (e.g., 3000 USD per problem). In this work, we present an inference pipeline that attains best-in-class performance on IMO-style math problems at an average inference cost orders of magnitude below competing methods while using only general-purpose off-the-shelf models. Our method relies on insights about grader failure in solver-grader pipelines, which we call the Cognitive Well (iterative refinement converging to a wrong solution that the solver as well as the pipeline's internal grader consider to be basically correct). Our pipeline addresses these failure modes through conjecture extraction, wherein candidate lemmas are isolated from generated solutions and independently verified alongside their negations in a fresh environment (context detachment). On IMO-ProofBench Advanced (PB-Adv), our pipeline achieves 67.1 percent performance using Gemini 3.0 Pro with an average cost per question of approximately 31 USD. At the time of evaluation, this represented the state-of-the-art on PB-Adv among both public and unreleased models, and more than doubles the success rate of the next best publicly accessible pipeline, all at a fraction of the cost.

</details>


### [129] [Efficient Tail-Aware Generative Optimization via Flow Model Fine-Tuning](https://arxiv.org/abs/2602.16796)
*Zifan Wang,Riccardo De Santi,Xiaoyu Mo,Michael M. Zavlanos,Andreas Krause,Karl H. Johansson*

Main category: cs.LG

TL;DR: 本文提出了一种名为TFFT（Tail-aware Flow Fine-Tuning）的分布式微调算法，基于条件风险价值（CVaR）实现对扩散和流模型尾部行为的精确控制。该方法通过解耦的两阶段流程，分别针对右尾（高奖励）和左尾（低奖励）进行优化，既提升了生成样本的可靠性，又增强了发现稀有高奖励结果的能力。相比以往依赖非线性优化的方法，TFFT利用CVaR的变分对偶形式，仅需轻量级一维阈值优化与标准熵正则微调，计算开销接近常规期望微调，效率显著。实验验证了其在文本到图像生成和分子设计等任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵正则的微调方法仅最大化期望奖励，无法有效调控分布尾部行为。然而，在实际应用中，尾部控制至关重要：左尾影响系统可靠性（避免低奖励失败），右尾促进创新（发掘高奖励罕见样本）。因此，亟需一种能灵活调节上下尾分布的高效微调方法。

Method: 提出基于条件风险价值（CVaR）的尾部感知微调框架——TFFT。利用CVaR的变分对偶形式，将复杂优化分解为两个独立阶段：第一阶段为轻量级一维阈值优化；第二阶段为使用特定伪奖励进行单次熵正则微调。该设计实现了对右尾（追求高奖励）和左尾（抑制低奖励）的分别控制，且整体计算成本与标准期望微调相当。

Result: TFFT在多个任务中表现出色，包括可视化实验、高维文本到图像生成以及分子设计任务。结果表明，该方法能有效提升生成样本的可靠性（降低低奖励风险），同时增强探索能力（提高高奖励样本出现概率），且无需额外显著计算开销。

Conclusion: TFFT是一种高效、可扩展且具有理论依据的分布式微调方法，能够精准控制生成模型的尾部行为，为真实世界部署中的可靠性与创新性平衡提供了新范式。

Abstract: Fine-tuning pre-trained diffusion and flow models to optimize downstream utilities is central to real-world deployment. Existing entropy-regularized methods primarily maximize expected reward, providing no mechanism to shape tail behavior. However, tail control is often essential: the lower tail determines reliability by limiting low-reward failures, while the upper tail enables discovery by prioritizing rare, high-reward outcomes. In this work, we present Tail-aware Flow Fine-Tuning (TFFT), a principled and efficient distributional fine-tuning algorithm based on the Conditional Value-at-Risk (CVaR). We address two distinct tail-shaping goals: right-CVaR for seeking novel samples in the high-reward tail and left-CVaR for controlling worst-case samples in the low-reward tail. Unlike prior approaches that rely on non-linear optimization, we leverage the variational dual formulation of CVaR to decompose it into a decoupled two-stage procedure: a lightweight one-dimensional threshold optimization step, and a single entropy-regularized fine-tuning process via a specific pseudo-reward. This decomposition achieves CVaR fine-tuning efficiently with computational cost comparable to standard expected fine-tuning methods. We demonstrate the effectiveness of TFFT across illustrative experiments, high-dimensional text-to-image generation, and molecular design.

</details>


### [130] [HiVAE: Hierarchical Latent Variables for Scalable Theory of Mind](https://arxiv.org/abs/2602.16826)
*Nigel Doering,Rahath Malladi,Arshia Sangwan,David Danks,Tauhidur Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种名为HiVAE的分层变分架构，用于将理论心理（ToM）推理扩展到现实中的时空领域。该方法基于人类认知的信念-欲望-意图结构，在一个包含3,185个节点的校园导航任务中实现了显著性能提升。然而，研究发现其学习到的潜在表示缺乏对实际心理状态的显式锚定，因此提出了自监督对齐策略，并邀请社区就如何实现更好的接地（grounding）提供反馈。


<details>
  <summary>Details</summary>
Motivation: 现有理论心理（ToM）方法主要局限于小型、人类可理解的网格世界环境，难以扩展到更复杂的现实场景。为了使AI系统具备在复杂真实环境中推断他人隐藏目标与心理状态的能力，亟需一种可扩展的推理框架。

Method: 提出分层变分自编码器（HiVAE），采用三层结构模拟信念-欲望-意图的认知层次，通过分层建模实现对复杂时空环境中智能体行为的高效推理。同时引入自监督对齐策略以增强潜在表示与真实心理状态之间的关联性。

Result: 在3,185个节点的校园导航任务上，HiVAE相比基线方法显著提升了预测准确率，验证了其在大规模现实场景中的有效性。但潜在空间仍缺乏对心理状态的明确语义解释。

Conclusion: 尽管HiVAE在扩展理论心理推理方面取得进展，但其潜在表示的语义接地仍是一个关键挑战。本文旨在推动社区探索更有效的自监督或监督式对齐机制，以实现真正可解释的心理状态建模。

Abstract: Theory of mind (ToM) enables AI systems to infer agents' hidden goals and mental states, but existing approaches focus mainly on small human understandable gridworld spaces. We introduce HiVAE, a hierarchical variational architecture that scales ToM reasoning to realistic spatiotemporal domains. Inspired by the belief-desire-intention structure of human cognition, our three-level VAE hierarchy achieves substantial performance improvements on a 3,185-node campus navigation task. However, we identify a critical limitation: while our hierarchical structure improves prediction, learned latent representations lack explicit grounding to actual mental states. We propose self-supervised alignment strategies and present this work to solicit community feedback on grounding approaches.

</details>


### [131] [VAM: Verbalized Action Masking for Controllable Exploration in RL Post-Training -- A Chess Case Study](https://arxiv.org/abs/2602.16833)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 本文提出了一种名为语义化动作掩码（VAM）的新方法，通过在提示中显式定义可选动作集合，引导大语言模型在强化学习后训练阶段进行可控探索。结合迭代动作空间剪枝策略，若目标动作未被采样，则逐步从候选集中移除已采样的有效动作并重新采样，直至命中目标或达到预算上限。该方法在国际象棋任务中验证，在引擎对弈和固定数据集两种训练范式下均显著提升学习效率与最终性能，尤其在棋局评估指标（平均厘分损失，ACPL）上优于现有强基线，证明了语义化掩码是实现可控探索的有效机制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在强化学习后训练中面临探索难题，主要由于稀疏反馈和巨大的动作空间，容易导致模型过早陷入重复行为。需要一种机制来增强探索的可控性与效率。

Method: 提出语义化动作掩码（VAM），将动作掩码以自然语言形式嵌入提示，强制模型仅输出被掩码允许的动作；引入迭代动作空间剪枝：若未采样到目标动作，则移除已采样的有效动作，缩小候选集并重采样，循环至命中目标或耗尽预算。

Result: 在国际象棋任务中，无论是基于引擎对弈还是固定数据集训练，VAM在未见棋题和完整对局表现上均显著优于强基线，特别是在平均厘分损失（ACPL）指标上表现出更优的学习效率与最终性能。

Conclusion: 语义化动作掩码（VAM）为大语言模型在强化学习后训练中的探索提供了高效且可控的解决方案，其核心思想——通过自然语言约束动作空间——具有高度实用性和推广潜力。

Abstract: Exploration remains a key bottleneck for reinforcement learning (RL) post-training of large language models (LLMs), where sparse feedback and large action spaces can lead to premature collapse into repetitive behaviors. We propose Verbalized Action Masking (VAM), which verbalizes an action mask in the prompt and enforces that the model outputs an action from the masked set. Building on this interface, we introduce iterative action-space pruning: if the target action is not sampled, we remove valid sampled actions from the mask and resample under the reduced candidate set, repeating until the target is sampled or a fixed budget is exhausted. We study VAM in chess and evaluate it under two training regimes: an engine-play regime that generates states via play against an engine opponent and a fixed-dataset regime that trains from a fixed dataset of positions with verifier scores. Across held-out chess puzzles and full-game play measured by average centipawn loss (ACPL), VAM improves learning efficiency and final performance over strong baselines, highlighting verbalized masking as a practical mechanism for controllable exploration in LLM RL post-training.

</details>


### [132] [A Residual-Aware Theory of Position Bias in Transformers](https://arxiv.org/abs/2602.16837)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: cs.LG

TL;DR: 该论文提出了一种考虑残差连接的累积注意力传播理论，解释了因果Transformer在有限深度下为何不会出现注意力集中在首个词元的崩溃现象，并揭示了其在实际中表现出的U型位置偏差，从而为‘中间迷失’现象提供了原理性解释。


<details>
  <summary>Details</summary>
Motivation: 现有理论预测在无限深度下，因果掩码下的Transformer会导致注意力完全集中于第一个词元，但这一现象在实践中并未发生，因此需要新的理论框架来解释这种差异。

Method: 通过引入残差连接，构建了一个残差感知的累积注意力传播模型，分析了在有限深度下的注意力分布特性，推导出其对位置偏差的影响。

Result: 证明了在有限深度下，因果Transformer会产生一种U型位置偏差，即注意力同时集中在序列的开头和结尾，而非仅聚焦于开头。

Conclusion: 残差连接是防止注意力坍塌的关键因素，其存在使得Transformer在实际应用中表现出对首尾词元的偏好，从而解释了‘中间迷失’现象的内在机制。

Abstract: Transformer models systematically favor certain token positions, yet the architectural origins of this position bias remain poorly understood. Under causal masking at infinite depth, prior theoretical analyses of attention rollout predict an inevitable collapse of attention onto the first token. Such collapse, however, does not occur in practice. We resolve this discrepancy with a residual-aware theory of cumulative attention rollout. By incorporating residual connections, we show that this architectural component prevents collapse under realistic conditions. At finite depth, we prove that causal Transformers induce a U-shaped position bias, with attention concentrating on early and late tokens. This result provides a principled architectural explanation for the Lost-in-the-Middle phenomenon.

</details>


### [133] [Training Large Reasoning Models Efficiently via Progressive Thought Encoding](https://arxiv.org/abs/2602.16839)
*Zeliang Zhang,Xiaodong Liu,Hao Cheng,Hao Sun,Chenliang Xu,Jianfeng Gao*

Main category: cs.LG

TL;DR: 提出了一种名为Progressive Thought Encoding的参数高效微调方法，旨在解决大推理模型（LRMs）在强化学习训练中因长序列回溯导致的效率瓶颈问题。该方法通过逐步将中间推理过程编码为固定大小的向量表示，避免了对完整缓存回溯的反向传播，显著降低内存使用并保持推理时内存恒定。在多个数学基准测试中，该方法在相同严格缓存预算下相比LoRA微调提升19.3%，相比未微调模型提升29.9%，最高达23.4%的准确率提升，证明其在提升推理准确率的同时大幅增强训练效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型虽在复杂问题上表现优异，但强化学习训练需长序列回溯以获取结果奖励，导致自回归解码占据大量时间和内存。现有滑动窗口缓存策略虽能控制内存，却破坏长上下文推理能力，影响性能。因此亟需一种既能节省内存又不损害推理能力的高效训练方法。

Method: Progressive Thought Encoding：通过逐步将中间推理步骤编码为固定大小的向量表示，实现参数高效微调。该方法避免了对完整缓存回溯进行反向传播，从而减少内存占用，并在推理阶段保持恒定内存使用。

Result: 在三个模型（Qwen2.5-3B-Instruct、Qwen2.5-7B-Instruct、DeepSeek-R1-Distill-Llama-8B）和六个数学基准测试中，该方法平均较LoRA微调提升19.3%，较无微调模型提升29.9%；在AIME2024/2025上最高提升23.4%。表明其在严格缓存约束下显著提升推理准确率与训练效率。

Conclusion: Progressive Thought Encoding有效解决了大推理模型在强化学习训练中的内存效率瓶颈，实现了高推理精度与低内存消耗的平衡，使模型在真实内存限制下更具可扩展性和实用性。

Abstract: Large reasoning models (LRMs) excel on complex problems but face a critical barrier to efficiency: reinforcement learning (RL) training requires long rollouts for outcome-based rewards, where autoregressive decoding dominates time and memory usage. While sliding-window cache strategies can bound memory, they disrupt long-context reasoning and degrade performance. We introduce Progressive Thought Encoding, a parameter-efficient fine-tuning method that enables LRMs to reason effectively under fixed-size caches. By progressively encoding intermediate reasoning into fixed-size vector representations, our approach eliminates the need to backpropagate through full-cache rollouts, thereby reducing memory usage, while maintaining constant memory during inference. Experiments on three models, including Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, and DeepSeek-R1-Distill-Llama-8B, on six widely used challenging mathematical benchmarks show consistent gains: our method achieves +19.3% improvement over LoRA-based fine-tuning and +29.9% over LRMs without fine-tuning on average, with up to +23.4 accuracy improvement on AIME2024/2025 under the same tight cache budgets. These results demonstrate that Progressive Thought Encoding not only improves reasoning accuracy but also makes RL training of LRMs substantially more efficient and scalable under real-world memory constraints.

</details>


### [134] [What is the Value of Censored Data? An Exact Analysis for the Data-driven Newsvendor](https://arxiv.org/abs/2602.16842)
*Rachitesh Kumar,Omar Mouchtaki*

Main category: cs.LG

TL;DR: 本文研究离线数据驱动的新闻商问题，其中需求数据被截断（仅可观测销售数据）。与以往需求完全可观测的研究不同，本文考虑需求在库存水平处被截断的情况：当库存充足时，销售等于需求；否则，销售等于可用库存。文章提出一种通用方法，可计算经典数据驱动库存策略在所有可能需求分布下的最坏情况后悔值。其核心技术突破在于将原本无限维、非凸的优化问题转化为有限维问题，从而实现对任意样本量和截断程度下策略性能的精确刻画。基于此，文章揭示了标准库存策略在需求截断下的表现极限。例如，对Kaplan-Meier策略的分析表明，尽管被动销售数据因截断导致学习受限，但仅需在高库存水平进行少量主动探索，即可显著提升最坏情况下的性能，甚至在严重截断下仍可实现近似最优。相反，若销售点系统不记录缺货事件而仅报告实际销售，则常见做法是将销售视为需求。但本文结果表明，该“销售即需求”启发式方法会因截断数据累积而造成严重性能下降，凸显了销售点信息质量对离线学习能力的根本影响。


<details>
  <summary>Details</summary>
Motivation: 在现实库存管理中，需求常因库存不足而无法完全观测，仅能获得销售数据（即被截断的需求），这限制了数据驱动策略的有效性。现有研究多假设需求完全可观测，但这一假设在实践中不成立。因此，亟需理解在需求截断条件下，如何准确评估和改进数据驱动库存策略的性能，尤其是基于有限且不完整销售数据的决策能力。

Method: 提出一种通用框架，将最坏情况后悔值的计算问题从无限维、非凸的优化转化为有限维问题。利用数学分析工具，证明该转化的可行性，并据此推导出各类经典库存策略（如Kaplan-Meier）在截断数据下的性能边界。通过构造极值分布和优化结构，实现对策略性能的精确刻画。

Result: 1. 成功将复杂的无限维非凸优化问题简化为有限维问题，实现对任意样本规模和截断程度下策略最坏情况后悔值的精确计算。2. 揭示了需求截断对学习能力的根本制约：仅靠被动销售数据难以获得良好性能。3. 证明在高库存水平引入少量主动探索可极大改善最坏情况表现，接近最优。4. 证明‘销售即需求’的启发式方法在截断数据下会引发严重性能退化，强调点销系统信息完整性的重要性。

Conclusion: 需求截断显著影响离线库存学习的性能上限。单纯依赖被动销售数据存在根本性局限，但通过引入少量战略性探索，可有效克服截断带来的学习障碍。同时，点销系统是否记录缺货事件对策略性能具有决定性影响。本研究为设计鲁棒的离线数据驱动库存策略提供了理论基础与实践指导。

Abstract: We study the offline data-driven newsvendor problem with censored demand data. In contrast to prior works where demand is fully observed, we consider the setting where demand is censored at the inventory level and only sales are observed; sales match demand when there is sufficient inventory, and equal the available inventory otherwise. We provide a general procedure to compute the exact worst-case regret of classical data-driven inventory policies, evaluated over all demand distributions. Our main technical result shows that this infinite-dimensional, non-convex optimization problem can be reduced to a finite-dimensional one, enabling an exact characterization of the performance of policies for any sample size and censoring levels. We leverage this reduction to derive sharp insights on the achievable performance of standard inventory policies under demand censoring. In particular, our analysis of the Kaplan-Meier policy shows that while demand censoring fundamentally limits what can be learned from passive sales data, just a small amount of targeted exploration at high inventory levels can substantially improve worst-case guarantees, enabling near-optimal performance even under heavy censoring. In contrast, when the point-of-sale system does not record stockout events and only reports realized sales, a natural and commonly used approach is to treat sales as demand. Our results show that policies based on this sales-as-demand heuristic can suffer severe performance degradation as censored data accumulates, highlighting how the quality of point-of-sale information critically shapes what can, and cannot, be learned offline.

</details>


### [135] [Position: Why a Dynamical Systems Perspective is Needed to Advance Time Series Modeling](https://arxiv.org/abs/2602.16864)
*Daniel Durstewitz,Christoph Jürgen Hemmer,Florian Hess,Charlotte Ricarda Doll,Lukas Eisenmann*

Main category: cs.LG

TL;DR: 本文主张时间序列（TS）建模应引入动力系统（DS）视角，以推动预测与分析的进一步发展。通过数据重建潜在的DS模型（即DS重建，DSR），不仅能实现更优的短期预测，还能预测系统的长期统计特性，提升模型在复杂场景下的泛化能力与可解释性，并降低计算与内存开销。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列建模虽受高度关注，但进展模糊；现有方法多依赖统计或深度学习，缺乏对系统内在机制的理解。而真实系统的时间序列通常源于潜在的动力系统，若能揭示其演化方程，则可获得理论上最优的预测结果。因此，引入动力系统理论可为时间序列建模提供更深刻的理论基础和实际优势。

Method: 综述动力系统理论中的核心概念、方法、度量标准及DS重建（DSR）技术，探讨如何将这些理论工具应用于时间序列建模中，包括模型构建、性能评估、泛化能力分析与控制策略设计。

Result: 基于动力系统视角的方法不仅提升了预测精度与长期统计预测能力，还增强了模型的可解释性与鲁棒性，同时显著降低了计算资源需求。此外，该框架有助于识别模型性能上限、应对系统突变点（如临界转变），并指导有效控制策略的设计。

Conclusion: 将动力系统重建思想融入时间序列建模，是迈向更高效、更可靠、更具理论深度的下一代时间序列分析的关键路径，建议从模型设计、评估体系、跨领域应用等方面推进其落地。

Abstract: Time series (TS) modeling has come a long way from early statistical, mainly linear, approaches to the current trend in TS foundation models. With a lot of hype and industrial demand in this field, it is not always clear how much progress there really is. To advance TS forecasting and analysis to the next level, here we argue that the field needs a dynamical systems (DS) perspective. TS of observations from natural or engineered systems almost always originate from some underlying DS, and arguably access to its governing equations would yield theoretically optimal forecasts. This is the promise of DS reconstruction (DSR), a class of ML/AI approaches that aim to infer surrogate models of the underlying DS from data. But models based on DS principles offer other profound advantages: Beyond short-term forecasts, they enable to predict the long-term statistics of an observed system, which in many practical scenarios may be the more relevant quantities. DS theory furthermore provides domain-independent theoretical insight into mechanisms underlying TS generation, and thereby will inform us, e.g., about upper bounds on performance of any TS model, generalization into unseen regimes as in tipping points, or potential control strategies. After reviewing some of the central concepts, methods, measures, and models in DS theory and DSR, we will discuss how insights from this field can advance TS modeling in crucial ways, enabling better forecasting with much lower computational and memory footprints. We conclude with a number of specific suggestions for translating insights from DSR into TS modeling.

</details>


### [136] [ML-driven detection and reduction of ballast information in multi-modal datasets](https://arxiv.org/abs/2602.16876)
*Yaroslav Solovko*

Main category: cs.LG

TL;DR: 该研究提出了一种通用的多模态框架，用于检测和减少结构化、半结构化、非结构化及稀疏数据中的冗余信息（即‘球载物’），通过熵、互信息、Lasso、SHAP、PCA、主题建模和嵌入分析等方法识别并移除低效特征。引入新颖的‘球载物评分’以整合多种信号，实现跨模态的统一剪枝策略。实验表明，在稀疏或半结构化数据中，超过70%的特征空间可被剪枝，同时保持甚至提升分类性能，并显著降低训练时间和内存占用。框架揭示了不同类型的球载物（如统计型、语义型、基础设施型），为构建更轻量高效的机器学习流程提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 现代数据集常包含大量冗余或低效信息（球载物），这些信息增加维度、存储和计算成本，但对分析无实质贡献。因此需要一种通用方法来识别并消除这类信息，以提升模型效率与可扩展性。

Method: 结合熵、互信息、Lasso、SHAP、PCA、主题建模和嵌入分析等多种技术，构建多模态特征评估体系；提出‘球载物评分’作为统一指标，指导跨数据类型特征剪枝。

Result: 在稀疏或半结构化数据中，可剪枝超过70%的特征空间，且分类性能保持甚至提升，训练时间与内存占用显著下降。

Conclusion: 该框架成功实现了跨模态的高效特征精简，揭示了球载物的多种类型，为构建更高效、更轻量的机器学习系统提供了有效工具与实践路径。

Abstract: Modern datasets often contain ballast as redundant or low-utility information that increases dimensionality, storage requirements, and computational cost without contributing meaningful analytical value. This study introduces a generalized, multimodal framework for ballast detection and reduction across structured, semi-structured, unstructured, and sparse data types. Using diverse datasets, entropy, mutual information, Lasso, SHAP, PCA, topic modelling, and embedding analysis are applied to identify and eliminate ballast features. A novel Ballast Score is proposed to integrate these signals into a unified, cross-modal pruning strategy. Experimental results demonstrate that significant portions of the feature space as often exceeding 70% in sparse or semi-structured data, can be pruned with minimal or even improved classification performance, along with substantial reductions in training time and memory footprint. The framework reveals distinct ballast typologies (e.g. statistical, semantic, infrastructural), and offers practical guidance for leaner, more efficient machine learning pipelines.

</details>


### [137] [Construction of a classification model for dementia among Brazilian adults aged 50 and over](https://arxiv.org/abs/2602.16887)
*F. S. Menezes,M. C. F. G. Barretto,E. Q. C. Garcia,T. A. E. Ferreira,J. G. Alvez*

Main category: cs.LG

TL;DR: 本研究基于巴西纵向老龄化研究（ELSI-Brazil）数据，使用9,412名中老年人的横断面数据，结合随机森林（RF）与多变量逻辑回归模型，构建了针对巴西中老年人群的痴呆分类模型。研究识别出多个风险因素（如文盲、高龄、低体重、握力弱、听力损失、抑郁症状等）和保护因素（如高教育水平、生活满意度高、就业状态），并发现RF模型表现优于逻辑回归，其ROC曲线下面积为0.776，具备良好的判别能力。研究强调痴呆的多维性及可及性因素在早期识别中的重要性，建议通过公共政策促进脑健康以优化初级保健资源配置和痴呆预防。


<details>
  <summary>Details</summary>
Motivation: 为建立适用于巴西中老年人群的低成本、可修改的痴呆风险预测模型，提高早期识别效率，并支持公共卫生资源的合理配置。

Method: 采用横断面研究设计，基于巴西纵向老龄化研究（ELSI-Brazil）数据，利用随机森林（RF）和多变量逻辑回归进行变量选择与建模分析，结合神经心理评估与知情者报告的认知功能判断痴呆状态。

Result: 痴呆患病率为9.6%。文盲（OR=7.42）、90岁以上（OR=11.00）、低体重（OR=2.11）、握力弱（OR=2.50）、自报黑皮肤（OR=1.47）、久坐不动（OR=1.61）、听力损失（OR=1.65）及抑郁症状（OR=1.72）为显著风险因素；高教育水平（OR=0.44）、生活满意度高（OR=0.72）、就业状态（OR=0.78）为保护因素。随机森林模型表现更优，AUC为0.776，敏感性0.708，特异性0.702，准确率0.703，F1-score为0.311，G-means为0.705。

Conclusion: 研究结果强调痴呆的多维度特征及其与可干预因素的关联，提示应加强公共政策对脑健康的促进作用，有助于在初级保健中高效识别高危人群并推动痴呆预防。

Abstract: To build a dementia classification model for middle-aged and elderly Brazilians, implemented in Python, combining variable selection and multivariable analysis, using low-cost variables with modification potential. Observational study with a predictive modeling approach using a cross-sectional design, aimed at estimating the chances of developing dementia, using data from the Brazilian Longitudinal Study of Aging (ELSI-Brazil), involving 9,412 participants. Dementia was determined based on neuropsychological assessment and informant-based cognitive function. Analyses were performed using Random Forest (RF) and multivariable logistic regression to estimate the risk of dementia in the middle-aged and elderly populations of Brazil. The prevalence of dementia was 9.6%. The highest odds of dementia were observed in illiterate individuals (Odds Ratio (OR) = 7.42), individuals aged 90 years or older (OR = 11.00), low weight (OR = 2.11), low handgrip strength (OR = 2.50), self-reported black skin color (OR = 1.47), physical inactivity (OR = 1.61), self-reported hearing loss (OR = 1.65), and presence of depressive symptoms (OR = 1.72). Higher education (OR=0.44), greater life satisfaction (OR=0.72), and being employed (OR=0.78) were protective factors. The RF model outperformed logistic regression, achieving an area under the ROC curve of 0.776, with a sensitivity of 0.708, a specificity of 0.702, an F1-score of 0.311, a G-means of 0.705, and an accuracy of 0.703. Conclusion: The findings reinforce the multidimensional nature of dementia and the importance of accessible factors for identifying vulnerable individuals. Strengthening public policies focused on promoting brain health can contribute significantly to the efficient allocation of resources in primary care and dementia prevention in Brazil

</details>


### [138] [Beyond Message Passing: A Symbolic Alternative for Expressive and Interpretable Graph Learning](https://arxiv.org/abs/2602.16947)
*Chuqin Geng,Li Zhang,Haolin Ye,Ziyu Zhao,Yuhe Jiang,Tara Saba,Xinyu Wang,Xujie Si*

Main category: cs.LG

TL;DR: SymGraph proposes a symbolic framework for GNNs that replaces continuous message passing with discrete structural hashing and topological role-based aggregation, overcoming the 1-WL expressivity barrier and enabling faster, more interpretable models. It achieves state-of-the-art performance, 10x–100x speedups on CPU, and generates semantically rich rules for scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Existing self-explainable GNNs are limited by the 1-Weisfeiler-Lehman expressivity barrier and lack fine-grained interpretability due to reliance on standard message-passing backbones. These limitations hinder trustworthiness in high-stakes domains like drug discovery.

Method: SymGraph introduces a symbolic architecture using discrete structural hashing and topological role-based aggregation instead of continuous message passing, enabling higher expressiveness without differentiable optimization.

Result: SymGraph outperforms existing self-explainable GNNs, achieves 10x to 100x faster training times using only CPU, and generates rules with superior semantic granularity.

Conclusion: SymGraph transcends the limitations of traditional GNNs by combining symbolic reasoning with topological awareness, offering both high performance and explainability—making it highly suitable for trustworthy AI in scientific applications.

Abstract: Graph Neural Networks (GNNs) have become essential in high-stakes domains such as drug discovery, yet their black-box nature remains a significant barrier to trustworthiness. While self-explainable GNNs attempt to bridge this gap, they often rely on standard message-passing backbones that inherit fundamental limitations, including the 1-Weisfeiler-Lehman (1-WL) expressivity barrier and a lack of fine-grained interpretability. To address these challenges, we propose SymGraph, a symbolic framework designed to transcend these constraints. By replacing continuous message passing with discrete structural hashing and topological role-based aggregation, our architecture theoretically surpasses the 1-WL barrier, achieving superior expressiveness without the overhead of differentiable optimization. Extensive empirical evaluations demonstrate that SymGraph achieves state-of-the-art performance, outperforming existing self-explainable GNNs. Notably, SymGraph delivers 10x to 100x speedups in training time using only CPU execution. Furthermore, SymGraph generates rules with superior semantic granularity compared to existing rule-based methods, offering great potential for scientific discovery and explainable AI.

</details>


### [139] [Multi-Agent Lipschitz Bandits](https://arxiv.org/abs/2602.16965)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文研究在连续、Lipschitz结构化动作空间下的去中心化多玩家随机博弈问题，其中硬碰撞导致零奖励。目标是设计一种无需通信的策略以最大化集体收益，且协调成本与时间跨度 $T$ 无关。提出了一种模块化协议：首先通过新颖的最大值导向搜索解决多智能体协调问题，识别并分配玩家至高价值区域；随后将问题解耦为 $N$ 个独立的单玩家Lipschitz bandit问题。建立了近似最优的后悔上界 $	ilde{O}(T^{(d+1)/(d+2)})$ 加上一个与 $T$ 无关的协调成本，达到单玩家最优率。这是首个提供此类保证的框架，并可扩展至一般的距离阈值碰撞模型。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，多玩家协作面临硬碰撞导致零奖励的问题，传统方法需通信协调，但通信成本随时间增加。因此需要一种无需通信、能高效协调并实现近最优性能的去中心化策略。

Method: 提出模块化协议：先通过最大值导向搜索实现多智能体协调，定位高价值区域并分配玩家；再将问题分解为 $N$ 个独立的单玩家Lipschitz bandit问题，分别求解。

Result: 实现了近似最优的后悔上界 $\tilde{O}(T^{(d+1)/(d+2)})$，且协调成本不依赖于 $T$，匹配单玩家最优率。该框架可推广至一般距离阈值碰撞模型。

Conclusion: 本工作首次提出了一个去中心化、无需通信的多玩家带宽算法，在连续动作空间下实现近似最优性能，显著降低了协调开销，具有良好的可扩展性与理论保障。

Abstract: We study the decentralized multi-player stochastic bandit problem over a continuous, Lipschitz-structured action space where hard collisions yield zero reward. Our objective is to design a communication-free policy that maximizes collective reward, with coordination costs that are independent of the time horizon $T$. We propose a modular protocol that first solves the multi-agent coordination problem -- identifying and seating players on distinct high-value regions via a novel maxima-directed search -- and then decouples the problem into $N$ independent single-player Lipschitz bandits. We establish a near-optimal regret bound of $\tilde{O}(T^{(d+1)/(d+2)})$ plus a $T$-independent coordination cost, matching the single-player rate. To our knowledge, this is the first framework providing such guarantees, and it extends to general distance-threshold collision models.

</details>


### [140] [A Unified Framework for Locality in Scalable MARL](https://arxiv.org/abs/2602.16966)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: 本文研究了多智能体强化学习（MARL）中的可扩展性问题，指出其受维度灾难挑战，而现有方法依赖于环境的保守假设。作者提出一种新的策略相关性分解方法，揭示了平滑策略可诱导局部性，即使环境强耦合。提出了一个更紧的谱条件 $\rho(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ 来保证指数衰减，并基于此构建了一个有理论保障的局部化块坐标策略改进框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法对环境的最坏情况分析过于保守，未能考虑策略本身的正则化作用；因此需要更精确地刻画策略与环境之间的相互依赖关系，以实现更有效的局部化设计。

Method: 提出对策略诱导的依赖矩阵 $H^π$ 的新分解方式，将环境对状态和动作的敏感性 $E^s$、$E^a$ 与策略对状态的敏感性 $Π(π)$ 分离，从而揭示策略平滑性在诱导局部性中的关键作用。基于该分解推导出新的谱条件作为指数衰减的充分条件。

Result: 提出的谱条件 $\rho(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ 比以往基于范数的条件更紧，具有更强的表达能力；并据此设计了一个具有理论保障的局部化块坐标策略改进算法，其性能保证直接关联于该谱半径。

Conclusion: 局部性不仅取决于环境特性，也由策略结构决定，存在局部性与最优性之间的权衡；通过引入策略相关的分解机制，能够更准确地刻画和利用局部性，为高效可扩展的MARL提供了新的理论基础与算法设计思路。

Abstract: Scalable Multi-Agent Reinforcement Learning (MARL) is fundamentally challenged by the curse of dimensionality. A common solution is to exploit locality, which hinges on an Exponential Decay Property (EDP) of the value function. However, existing conditions that guarantee the EDP are often conservative, as they are based on worst-case, environment-only bounds (e.g., supremums over actions) and fail to capture the regularizing effect of the policy itself. In this work, we establish that locality can also be a \emph{policy-dependent} phenomenon. Our central contribution is a novel decomposition of the policy-induced interdependence matrix, $H^π$, which decouples the environment's sensitivity to state ($E^{\mathrm{s}}$) and action ($E^{\mathrm{a}}$) from the policy's sensitivity to state ($Π(π)$). This decomposition reveals that locality can be induced by a smooth policy (small $Π(π)$) even when the environment is strongly action-coupled, exposing a fundamental locality-optimality tradeoff. We use this framework to derive a general spectral condition $ρ(E^{\mathrm{s}}+E^{\mathrm{a}}Π(π)) < 1$ for exponential decay, which is strictly tighter than prior norm-based conditions. Finally, we leverage this theory to analyze a provably-sound localized block-coordinate policy improvement framework with guarantees tied directly to this spectral radius.

</details>


### [141] [Early-Warning Signals of Grokking via Loss-Landscape Geometry](https://arxiv.org/abs/2602.16967)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 该研究探讨了在序列学习任务（SCAN和Dyck-1）中，非交换梯度更新的曲率度量——对易子缺陷（commutator defect）——是否可作为模型从记忆到泛化的突变（grokking）的早期预警信号。结果显示，对易子缺陷在泛化前显著上升，且领先时间遵循超线性幂律关系，与模块算术中的发现一致。主成分分析表明，谱集中并非普遍前提，而对易子缺陷具有普适性。因果干预实验显示，增强非交换性可加速grokking（SCAN约32%，Dyck约50%），抑制正交梯度流则延迟或阻止泛化。三类任务（模块算术、Dyck、SCAN）表现出不同程度的因果敏感性，但抑制均导致grokking失败，证明其必要性为普遍规律。结论：对易子缺陷是变压器模型中延迟泛化的一个稳健、架构无关、具有因果关联的早期预警信号。


<details>
  <summary>Details</summary>
Motivation: 探究grokking现象是否仅限于模块算术，还是在更广泛的序列学习任务中普遍存在；寻找一种通用的、可解释的早期预警信号以理解模型从记忆到泛化的转变机制。

Method: 通过分析两个序列学习基准任务（SCAN和Dyck-1）中的梯度更新行为，引入对易子缺陷作为非交换梯度更新的曲率度量；结合权重空间主成分分析（PCA）、因果干预实验（增强或抑制非交换性）以及对泛化出现时间的统计建模，系统评估该度量的预测能力与因果作用。

Result: 对易子缺陷在泛化前显著上升，领先时间符合超线性幂律（SCAN: α≈1.18，Dyck: α≈1.13）；谱集中并非普遍前提，而对易子缺陷具普适性；增强非交换性可加速grokking（SCAN约32%，Dyck约50%），抑制正交梯度流则延迟或阻止泛化；在所有三类任务中，抑制对易子缺陷均导致grokking失败，表明其必要性。

Conclusion: 对易子缺陷是变压器模型中延迟泛化的一个稳健、架构无关、具有因果关联的早期预警信号，揭示了非交换梯度动力学在grokking中的核心作用，支持其在多种任务中具有普遍意义。

Abstract: Grokking -- the abrupt transition from memorization to generalization after prolonged training -- has been linked to confinement on low-dimensional execution manifolds in modular arithmetic. Whether this mechanism extends beyond arithmetic remains open. We study two sequence-learning benchmarks: SCAN compositional generalization and Dyck-1 depth prediction. Across both tasks and a wide range of learning rates, the commutator defect -- a curvature measure derived from non-commuting gradient updates -- rises well before generalization, with lead times following a superlinear power law (alpha approximately 1.18 for SCAN, approximately 1.13 for Dyck), consistent with prior results on modular arithmetic. Weight-space PCA reveals that spectral concentration is not a universal precursor; the commutator defect is. Causal interventions demonstrate a mechanistic role: amplifying non-commutativity accelerates grokking (roughly 32% on SCAN, roughly 50% on Dyck), while suppressing orthogonal gradient flow delays or prevents it. The three task families form a spectrum of causal sensitivity -- modular arithmetic is rigid, Dyck is responsive, SCAN is intermediate -- yet suppression delays or prevents grokking in all cases, establishing necessity as a universal finding. These results identify the commutator defect as a robust, architecture-agnostic, causally implicated early-warning signal for delayed generalization in transformers.

</details>


### [142] [Fail-Closed Alignment for Large Language Models](https://arxiv.org/abs/2602.16977)
*Zachary Coalson,Beth Sohler,Aiden Gabriel,Sanghyun Hong*

Main category: cs.LG

TL;DR: 本文揭示了当前大语言模型对齐中的结构性弱点：现代拒绝机制是‘故障开放’的，即通过提示攻击抑制单一主导特征即可导致对齐崩溃。为此，作者提出‘故障封闭’对齐作为鲁棒安全的设计原则，强调拒绝机制在部分失效时仍能保持有效，依赖冗余且独立的因果路径。文中提出一种渐进式对齐框架，通过迭代识别并消除已学习的拒绝方向，迫使模型在新的独立子空间中重建安全性。实验表明，在四种提示攻击下，该方法实现了最强的鲁棒性，同时减少过度拒绝并保持生成质量，计算开销小。机械分析证实，该方法训练出的模型包含多个因果独立的拒绝方向，无法被单一提示攻击同时压制，为故障封闭对齐提供了实证支持。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的拒绝机制存在结构性脆弱性，仅通过提示攻击抑制单一主导特征即可导致对齐失效，引发不安全输出。因此需要一种更鲁棒的对齐设计原则来应对部分失败情况。

Method: 提出一种渐进式对齐框架，通过迭代识别并主动抹除先前学习的拒绝方向，迫使模型在新的、独立的子空间中重建安全行为，从而实现多重、独立的拒绝路径。

Result: 在四种提示攻击下，该方法表现出最强的鲁棒性，有效防止对齐崩溃；同时减少过度拒绝问题，保持高质量生成，且计算开销极小。机械分析验证了模型中存在多个因果独立的拒绝方向。

Conclusion: 故障封闭对齐是一种可实现、可验证的鲁棒大语言模型安全设计原则，其核心在于通过冗余和独立的因果路径确保拒绝机制在部分失效下依然有效，为未来安全对齐研究提供新范式。

Abstract: We identify a structural weakness in current large language model (LLM) alignment: modern refusal mechanisms are fail-open. While existing approaches encode refusal behaviors across multiple latent features, suppressing a single dominant feature$-$via prompt-based jailbreaks$-$can cause alignment to collapse, leading to unsafe generation. Motivated by this, we propose fail-closed alignment as a design principle for robust LLM safety: refusal mechanisms should remain effective even under partial failures via redundant, independent causal pathways. We present a concrete instantiation of this principle: a progressive alignment framework that iteratively identifies and ablates previously learned refusal directions, forcing the model to reconstruct safety along new, independent subspaces. Across four jailbreak attacks, we achieve the strongest overall robustness while mitigating over-refusal and preserving generation quality, with small computational overhead. Our mechanistic analyses confirm that models trained with our method encode multiple, causally independent refusal directions that prompt-based jailbreaks cannot suppress simultaneously, providing empirical support for fail-closed alignment as a principled foundation for robust LLM safety.

</details>


### [143] [Discovering Universal Activation Directions for PII Leakage in Language Models](https://arxiv.org/abs/2602.16980)
*Leo Marchyok,Zachary Coalson,Sungho Keum,Sooel Son,Sanghyun Hong*

Main category: cs.LG

TL;DR: UniLeak 是一个机制可解释性框架，用于识别语言模型中与个人身份信息（PII）泄露相关的通用激活方向。这些方向在推理时线性叠加，能显著提升多种提示下生成PII的概率，且对生成质量影响小。该方法无需训练数据或真实PII标签，仅依赖自生成文本即可发现这些方向。实验表明，沿这些方向引导生成比现有基于提示的方法更有效放大PII泄露。研究揭示了PII泄露是模型表征中一种潜在信号的叠加现象，为风险放大与缓解提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型中隐私敏感行为（如PII泄露）如何在隐藏状态中表示和调控知之甚少，亟需理解其内在机制以实现有效控制。

Method: 提出UniLeak框架，通过分析模型残差流中的潜在方向，识别出能在不同上下文中一致增强PII生成概率的通用激活方向，并利用自生成文本进行无监督学习。

Result: 在多个模型和数据集上，沿这些通用方向引导生成显著提升了PII泄露率，且生成质量保持良好；该方法不依赖训练数据或标注，具备强泛化能力。

Conclusion: PII泄露可被建模为隐藏表征中一种可被定位和操控的潜在信号，这为开发更有效的隐私保护与风险控制策略提供了理论基础和技术路径。

Abstract: Modern language models exhibit rich internal structure, yet little is known about how privacy-sensitive behaviors, such as personally identifiable information (PII) leakage, are represented and modulated within their hidden states. We present UniLeak, a mechanistic-interpretability framework that identifies universal activation directions: latent directions in a model's residual stream whose linear addition at inference time consistently increases the likelihood of generating PII across prompts. These model-specific directions generalize across contexts and amplify PII generation probability, with minimal impact on generation quality. UniLeak recovers such directions without access to training data or groundtruth PII, relying only on self-generated text. Across multiple models and datasets, steering along these universal directions substantially increases PII leakage compared to existing prompt-based extraction methods. Our results offer a new perspective on PII leakage: the superposition of a latent signal in the model's representations, enabling both risk amplification and mitigation.

</details>


### [144] [Dynamic Delayed Tree Expansion For Improved Multi-Path Speculative Decoding](https://arxiv.org/abs/2602.16994)
*Rahul Thomas,Teo Kitanovski,Micah Goldblum,Arka Pal*

Main category: cs.LG

TL;DR: 本研究系统评估了多种验证策略在不同模型家族、任务和采样设置下的表现，发现遍历验证（Traversal Verification）始终优于基于最优传输（OT）的方法。原因在于OT方法虽在树根部实现高多标记接受率，但其优势在树深层因分布差异而减弱。为此，提出延迟树扩展机制，先生成部分单路径草稿，延后独立同分布分支点，保持目标分布并提升根节点的i.i.d滚动性能。进一步设计动态神经选择器，基于草稿与目标特征估计OT方法的预期块效率，实现上下文相关的扩展决策。该选择器使OT方法如SpecInfer首次超越遍历验证，在广泛模型、数据集和采样场景下平均吞吐量提升5%。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽提出多种验证算法用于无损采样，但缺乏在统一设置下的性能对比，且基于最优传输（OT）的方法在实际中表现不佳，尤其在深层生成时因分布偏差导致收益下降。因此亟需系统性评估验证策略，并探索改进方法以提升整体推理效率。

Method: 1. 系统性评估多种验证策略在不同模型、任务和采样设置下的表现；2. 提出延迟树扩展机制，通过延后i.i.d分支点来增强深层生成效果；3. 设计动态神经选择器，基于草稿与目标特征预测OT方法的块效率，实现自适应决策。

Result: 延迟树扩展有效保持目标分布并提升根节点i.i.d滚动生成性能；动态神经选择器使基于OT的验证方法（如SpecInfer）首次超越遍历验证，在多种设置下平均吞吐量提高5%。

Conclusion: 遍历验证虽普遍有效，但通过延迟树扩展与动态神经选择器，可显著提升基于最优传输方法的性能，使其在多数场景下超越传统方法，为高效无损采样提供了新范式。

Abstract: Multi-path speculative decoding accelerates lossless sampling from a target model by using a cheaper draft model to generate a draft tree of tokens, and then applies a verification algorithm that accepts a subset of these. While prior work has proposed various verification algorithms for i.i.d rollouts, their relative performance under matched settings remains unclear. In this work, we firstly present a systematic evaluation of verification strategies across model families, tasks, and sampling regimes, and find that Traversal Verification dominates consistently, with OT-based methods lagging far behind. Our analysis uncovers that this occurs because OT-based methods achieve high multi-token acceptance near the root of the draft tree, while multi-token gains are most impactful deeper in the draft tree, where draft and target distributions diverge. Based on this insight, we propose delayed tree expansion, which drafts a partial single path, delaying the i.i.d. branching point. We show that delayed tree expansion preserves the target distribution and improves on root-node i.i.d rollouts. Further, we develop a dynamic neural selector that estimates the expected block efficiency of optimal-transport-based verification methods from draft and target features, enabling context-dependent expansion decisions. Our neural selector allows OT-based methods like SpecInfer to outperform Traversal Verification for the first time, achieving 5% higher average throughput across a wide range of models, datasets, and sampling settings.

</details>


### [145] [Action-Graph Policies: Learning Action Co-dependencies in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17009)
*Nikunj Gupta,James Zachary Hare,Jesse Milzman,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 本文提出了一种名为动作图策略（Action Graph Policies, AGP）的新方法，用于在多智能体强化学习中建模智能体间动作选择的依赖关系。AGP通过构建协调上下文，使智能体能够基于全局动作依赖进行决策，从而提升协同效率。理论分析表明，AGP比完全独立的策略更具表达能力，并能实现比集中式价值分解方法更优的协同行为。实验结果显示，在具有部分可观测性和反协作惩罚的典型任务中，AGP的成功率可达80-95%，远超其他方法的10-25%；且在多种多智能体环境中均表现出更强的性能。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，仅依靠个体良好行为不足以实现有效合作，还需确保各智能体之间动作的兼容性与同步性，避免冲突并满足全局约束。现有方法往往忽略动作间的依赖关系，导致协同效果不佳。因此，需要一种能够显式建模动作依赖、支持协调决策的新机制。

Method: 提出动作图策略（AGP），通过构建动作图来表示智能体间动作选择的依赖关系，形成‘协调上下文’，使每个智能体在决策时可感知全局动作依赖，从而实现更高效的去中心化协同决策。

Result: 在多个标准协调任务上，AGP在部分可观测和存在反协作惩罚的情况下实现了80-95%的成功率，显著优于其他MARL方法（仅10-25%）。此外，在多样化多智能体环境中，AGP始终表现更优。

Conclusion: AGP通过显式建模动作依赖关系，显著提升了多智能体系统中的协调能力，是实现高效去中心化协同决策的有效方法，具备更强的表达能力和实际应用潜力。

Abstract: Coordinating actions is the most fundamental form of cooperation in multi-agent reinforcement learning (MARL). Successful decentralized decision-making often depends not only on good individual actions, but on selecting compatible actions across agents to synchronize behavior, avoid conflicts, and satisfy global constraints. In this paper, we propose Action Graph Policies (AGP), that model dependencies among agents' available action choices. It constructs, what we call, \textit{coordination contexts}, that enable agents to condition their decisions on global action dependencies. Theoretically, we show that AGPs induce a strictly more expressive joint policy compared to fully independent policies and can realize coordinated joint actions that are provably more optimal than greedy execution even from centralized value-decomposition methods. Empirically, we show that AGP achieves 80-95\% success on canonical coordination tasks with partial observability and anti-coordination penalties, where other MARL methods reach only 10-25\%. We further demonstrate that AGP consistently outperforms these baselines in diverse multi-agent environments.

</details>


### [146] [Malliavin Calculus as Stochastic Backpropogation](https://arxiv.org/abs/2602.17013)
*Kevin D. Oden*

Main category: cs.LG

TL;DR: 本文建立了路径梯度（重参数化）与得分函数（Malliavin）梯度估计器之间的严格联系，证明二者均源于Malliavin积分分部恒等式。基于此等价性，提出一种统一且方差感知的混合估计器，通过经验协方差结构自适应结合两类梯度，实现所有无偏线性组合中的最小方差，并给出闭式有限样本收敛界。在VAE（CIFAR-10）上实现9%方差降低，在强耦合合成问题上最高达35%。探索性策略梯度实验表明，非平稳优化景观对混合方法构成挑战，指明未来研究方向。总体而言，该工作将Malliavin微积分确立为随机梯度估计的概念统一与实践可解释框架，清晰阐明了混合方法何时带来实际优势及何时存在固有局限。


<details>
  <summary>Details</summary>
Motivation: 现有路径梯度与得分函数梯度估计器缺乏统一理论基础，导致其适用场景与性能边界不明确。本文旨在建立两者的数学关联，从而提供更系统、更高效的梯度估计方法。

Method: 基于Malliavin积分分部恒等式，揭示路径梯度与得分函数梯度的内在等价性；设计一种利用经验协方差结构自适应融合两类梯度的混合估计器；推导其最小方差性质与有限样本收敛性。

Result: 在VAE（CIFAR-10）上实现9%方差降低，在强耦合合成问题中最高达35%；混合估计器在理论上具有最小方差，且具备闭式收敛保证；探索性实验揭示非平稳优化环境下的挑战。

Conclusion: Malliavin微积分为随机梯度估计提供了统一且可解释的理论框架，使得混合方法的使用条件得以明晰，既展现了显著优势，也暴露了在复杂动态环境中的局限性，为后续研究指明方向。

Abstract: We establish a rigorous connection between pathwise (reparameterization) and score-function (Malliavin) gradient estimators by showing that both arise from the Malliavin integration-by-parts identity. Building on this equivalence, we introduce a unified and variance-aware hybrid estimator that adaptively combines pathwise and Malliavin gradients using their empirical covariance structure. The resulting formulation provides a principled understanding of stochastic backpropagation and achieves minimum variance among all unbiased linear combinations, with closed-form finite-sample convergence bounds. We demonstrate 9% variance reduction on VAEs (CIFAR-10) and up to 35% on strongly-coupled synthetic problems. Exploratory policy gradient experiments reveal that non-stationary optimization landscapes present challenges for the hybrid approach, highlighting important directions for future work. Overall, this work positions Malliavin calculus as a conceptually unifying and practically interpretable framework for stochastic gradient estimation, clarifying when hybrid approaches provide tangible benefits and when they face inherent limitations.

</details>


### [147] [WS-GRPO: Weakly-Supervised Group-Relative Policy Optimization for Rollout-Efficient Reasoning](https://arxiv.org/abs/2602.17025)
*Gagan Mundada,Zihan Huang,Rohan Surana,Sheldon Yu,Jennifer Yuntong Zhang,Xintong Li,Tong Yu,Lina Yao,Jingbo Shang,Julian McAuley,Junda Wu*

Main category: cs.LG

TL;DR: 提出WS-GRPO，通过将最终奖励转化为对部分轨迹的正确性感知指导，提升推理效率。相比全局长度惩罚，该方法利用结果仅正确的偏好模型生成前缀级信号，判断是否继续或停止，减少冗余思考，同时保持准确性。在推理基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GRPO虽有效但因相对目标易导致过度推理和低效，且难以通过长度惩罚或直接监督控制；长度惩罚难调校，而终止决策缺乏显式监督。

Method: 提出弱监督GRPO（WS-GRPO），基于仅结果正确的反馈训练偏好模型，生成前缀级别的继续/停止信号，以引导推理过程。

Result: 在多个推理基准上，WS-GRPO显著缩短了推理长度，同时保持与GRPO基线相当的准确性。

Conclusion: WS-GRPO通过引入基于正确性的前缀级指导，有效缓解了过度推理问题，在不牺牲准确率的前提下提升了推理效率。

Abstract: Group Relative Policy Optimization (GRPO) is effective for training language models on complex reasoning. However, since the objective is defined relative to a group of sampled trajectories, extended deliberation can create more chances to realize relative gains, leading to inefficient reasoning and overthinking, and complicating the trade-off between correctness and rollout efficiency. Controlling this behavior is difficult in practice, considering (i) Length penalties are hard to calibrate because longer rollouts may reflect harder problems that require longer reasoning, penalizing tokens risks truncating useful reasoning along with redundant continuation; and (ii) supervision that directly indicates when to continue or stop is typically unavailable beyond final answer correctness. We propose Weakly Supervised GRPO (WS-GRPO), which improves rollout efficiency by converting terminal rewards into correctness-aware guidance over partial trajectories. Unlike global length penalties that are hard to calibrate, WS-GRPO trains a preference model from outcome-only correctness to produce prefix-level signals that indicate when additional continuation is beneficial. Thus, WS-GRPO supplies outcome-derived continue/stop guidance, reducing redundant deliberation while maintaining accuracy. We provide theoretical results and empirically show on reasoning benchmarks that WS-GRPO substantially reduces rollout length while remaining competitive with GRPO baselines.

</details>


### [148] [Transforming Behavioral Neuroscience Discovery with In-Context Learning and AI-Enhanced Tensor Methods](https://arxiv.org/abs/2602.17027)
*Paimon Goulart,Jordan Steinhauser,Dawon Ahn,Kylene Shuler,Edward Korzus,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 本文提出了一种AI增强的科学发现流水线，专为行为神经科学中的小鼠恐惧泛化研究设计。利用‘上下文学习’（In-Context Learning, ICL）技术，使领域专家无需掌握AI训练知识即可自动化数据准备与模式识别，显著提升效率。同时引入新型AI增强的张量分解模型，以更好处理异构数据并实现更流畅的模式发现。实验验证表明，该方法在性能上优于传统实践和非ICL的机器学习基线，且结果得到领域专家认可。


<details>
  <summary>Details</summary>
Motivation: 传统科学发现流水线复杂、僵化且耗时，限制了领域专家对数据洞察的专注。尽管AI有潜力变革这一过程，但现有方法往往需要专家具备深度AI知识，难以普及。因此，亟需一种无需训练、易用且高效的AI工具，让专家能专注于科学解释而非技术细节。

Method: 采用‘上下文学习’（ICL）作为专家与AI交互的接口，实现无需微调的数据预处理与模式识别；结合改进的张量分解模型，增强对多源异构数据的分析能力；通过端到端实验评估系统性能，并与标准实践及主流机器学习方法对比。

Result: 所提管道在数据准备与模式发现效率上显著优于传统方法；在性能上超越非ICL的基线模型；最终发现结果经领域专家验证，具有实际科学意义。

Conclusion: AI增强的流水线，特别是基于ICL和改进张量分解的方法，能够有效简化科学发现流程，使领域专家更专注于科学解释，同时保持甚至超越现有技术水平，为跨学科研究提供可扩展、易用的新范式。

Abstract: Scientific discovery pipelines typically involve complex, rigid, and time-consuming processes, from data preparation to analyzing and interpreting findings. Recent advances in AI have the potential to transform such pipelines in a way that domain experts can focus on interpreting and understanding findings, rather than debugging rigid pipelines or manually annotating data. As part of an active collaboration between data science/AI researchers and behavioral neuroscientists, we showcase an example AI-enhanced pipeline, specifically designed to transform and accelerate the way that the domain experts in the team are able to gain insights out of experimental data. The application at hand is in the domain of behavioral neuroscience, studying fear generalization in mice, an important problem whose progress can advance our understanding of clinically significant and often debilitating conditions such as PTSD (Post-Traumatic Stress Disorder). We identify the emerging paradigm of "In-Context Learning" (ICL) as a suitable interface for domain experts to automate parts of their pipeline without the need for or familiarity with AI model training and fine-tuning, and showcase its remarkable efficacy in data preparation and pattern interpretation. Also, we introduce novel AI-enhancements to tensor decomposition model, which allows for more seamless pattern discovery from the heterogeneous data in our application. We thoroughly evaluate our proposed pipeline experimentally, showcasing its superior performance compared to what is standard practice in the domain, as well as against reasonable ML baselines that do not fall under the ICL paradigm, to ensure that we are not compromising performance in our quest for a seamless and easy-to-use interface for domain experts. Finally, we demonstrate effective discovery, with results validated by the domain experts in the team.

</details>


### [149] [Multi-Probe Zero Collision Hash (MPZCH): Mitigating Embedding Collisions and Enhancing Model Freshness in Large-Scale Recommenders](https://arxiv.org/abs/2602.17050)
*Ziliang Zhao,Bi Xue,Emma Lin,Mengjiao Zhou,Kaustubh Vartak,Shakhzod Ali-Zade,Carson Lu,Tao Li,Bin Kuang,Rui Jian,Bin Wen,Dennis van der Staay,Yixin Bao,Eddy Li,Chao Deng,Songbin Liu,Qifan Wang,Kai Ren*

Main category: cs.LG

TL;DR: 提出了一种名为Multi-Probe Zero Collision Hash (MPZCH)的新索引机制，用于解决大规模推荐系统中嵌入表因高基数类别特征导致的哈希冲突问题。该方法基于线性探测，通过辅助张量和高性能CUDA内核实现可配置的探测与主动淘汰策略，在合理表大小下几乎完全消除冲突，同时保持生产级效率。它通过淘汰过时ID并重置重新分配的槽位，避免了传统哈希方法中的陈旧嵌入继承问题，确保新特征能从零开始学习。在线实验表明，MPZCH实现了用户嵌入的零冲突，并显著提升了物品嵌入的新鲜度和质量。该方案已开源至TorchRec库。


<details>
  <summary>Details</summary>
Motivation: 传统基于哈希的索引方法在处理大规模高基数类别特征时，因哈希冲突导致模型性能下降和个性化质量降低，亟需一种高效且无冲突的嵌入表索引机制。

Method: 提出MPZCH，基于线性探测机制，结合辅助张量与高性能CUDA内核，支持可配置的探测策略和主动淘汰机制，通过淘汰过时ID和重置槽位来防止陈旧嵌入继承。

Result: 在线实验验证了MPZCH实现了用户嵌入的零冲突，显著提升物品嵌入的新鲜度与质量，且训练每秒查询数（QPS）和推理延迟与现有方法相当。

Conclusion: MPZCH是一种高效、可扩展的嵌入表索引方案，能够有效消除哈希冲突，提升推荐系统的个性化能力，已在TorchRec开源库中发布，可供社区使用。

Abstract: Embedding tables are critical components of large-scale recommendation systems, facilitating the efficient mapping of high-cardinality categorical features into dense vector representations. However, as the volume of unique IDs expands, traditional hash-based indexing methods suffer from collisions that degrade model performance and personalization quality. We present Multi-Probe Zero Collision Hash (MPZCH), a novel indexing mechanism based on linear probing that effectively mitigates embedding collisions. With reasonable table sizing, it often eliminates these collisions entirely while maintaining production-scale efficiency. MPZCH utilizes auxiliary tensors and high-performance CUDA kernels to implement configurable probing and active eviction policies. By retiring obsolete IDs and resetting reassigned slots, MPZCH prevents the stale embedding inheritance typical of hash-based methods, ensuring new features learn effectively from scratch. Despite its collision-mitigation overhead, the system maintains training QPS and inference latency comparable to existing methods. Rigorous online experiments demonstrate that MPZCH achieves zero collisions for user embeddings and significantly improves item embedding freshness and quality. The solution has been released within the open-source TorchRec library for the broader community.

</details>


### [150] [Sign Lock-In: Randomly Initialized Weight Signs Persist and Bottleneck Sub-Bit Model Compression](https://arxiv.org/abs/2602.17063)
*Akira Sakai,Yuma Ichikawa*

Main category: cs.LG

TL;DR: 该研究探讨了子比特模型压缩中符号位的瓶颈问题，发现权重符号模式的随机性主要源自初始化。通过提出符号锁定理论和改进的初始化与正则化方法，显著降低了符号翻转频率至约10^-3，仅导致困惑度上升约1点。


<details>
  <summary>Details</summary>
Motivation: 在子比特模型压缩中，权重存储低于1比特时，符号位成为固定成本瓶颈。现有方法难以有效处理符号模式的随机性，导致压缩效率受限。

Method: 提出符号锁定理论，基于随机梯度下降（SGD）噪声下的停止时间分析，建模符号翻转行为；设计基于间隔的初始化和轻量级向外漂移正则化，以抑制不必要的符号翻转。

Result: 所提方法将有效符号翻转率降低至约10^-3，仅带来约1个百分点的困惑度增加，显著提升压缩效率并保持模型性能。

Conclusion: 符号模式的随机性本质上源于初始化，通过合理设计初始化与正则化策略，可有效抑制符号翻转，实现高效低比特压缩。

Abstract: Sub-bit model compression seeks storage below one bit per weight; as magnitudes are aggressively compressed, the sign bit becomes a fixed-cost bottleneck. Across Transformers, CNNs, and MLPs, learned sign matrices resist low-rank approximation and are spectrally indistinguishable from an i.i.d. Rademacher baseline. Despite this apparent randomness, most weights retain their initialization signs; flips primarily occur via rare near-zero boundary crossings, suggesting that sign-pattern randomness is largely inherited from initialization. We formalize this behavior with sign lock-in theory, a stopping-time analysis of sign flips under SGD noise. Under bounded updates and a rare re-entry condition into a small neighborhood around zero, the number of effective sign flips exhibits a geometric tail. Building on this mechanism, we introduce a gap-based initialization and a lightweight outward-drift regularizer, reducing the effective flip rate to approximately $10^{-3}$ with only about a one-point increase in perplexity.

</details>


### [151] [Adam Improves Muon: Adaptive Moment Estimation with Orthogonalized Momentum](https://arxiv.org/abs/2602.17080)
*Minxin Zhang,Yuxuan Liu,Hayden Scheaffer*

Main category: cs.LG

TL;DR: 提出NAMO和NAMO-D两种新优化器，结合正交化动量与基于范数的Adam型噪声自适应，实现高效且稳定的随机优化。NAMO通过单个自适应步长缩放正交化动量，保持正交性并优于Muon；NAMO-D引入带截断的对角矩阵，实现神经元级噪声适应，匹配近块对角海森结构。理论分析表明两者在确定性和随机设置下均具备最优收敛率。实验显示其在GPT-2预训练中优于AdamW和Muon，NAMO-D因额外截断超参数进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam和Muon虽分别在稳定性与大模型训练中表现优异，但缺乏对正交化动量与噪声自适应的系统性整合。尤其在大规模语言模型训练中，需要同时兼顾更新方向的稳定性与对局部噪声的精细适应能力，因此亟需一种能融合二者优势的新优化方法。

Method: 提出NAMO：以单一自适应步长缩放正交化动量，维持正交性的同时增强稳定性；提出NAMO-D：右乘一个带有截断值的对角矩阵，实现神经元级别的噪声适应，并契合近似块对角海森结构。两者均在理论上保证收敛性，在实践中可高效扩展至大规模模型。

Result: NAMO和NAMO-D在标准假设下均达到最优收敛速率；在随机梯度设置中，其收敛性可随噪声水平自适应调整。实验表明，两者在预训练GPT-2任务中显著优于AdamW和Muon，其中NAMO-D因引入截断超参数，在平衡更新方向条件性和细粒度噪声适应方面表现更优。

Conclusion: NAMO和NAMO-D首次实现了正交化动量与范数自适应的原理性融合，兼具稳定性、适应性和可扩展性，为大语言模型训练提供了高效、可靠的优化工具。

Abstract: Efficient stochastic optimization typically integrates an update direction that performs well in the deterministic regime with a mechanism adapting to stochastic perturbations. While Adam uses adaptive moment estimates to promote stability, Muon utilizes the weight layers' matrix structure via orthogonalized momentum, showing superior performance in large language model training. We propose a new optimizer and a diagonal extension, NAMO and NAMO-D, providing the first principled integration of orthogonalized momentum with norm-based Adam-type noise adaptation. NAMO scales orthogonalized momentum using a single adaptive stepsize, preserving orthogonality while improving upon Muon at negligible additional cost. NAMO-D instead right-multiplies orthogonalized momentum by a diagonal matrix with clamped entries. This design enables neuron-wise noise adaptation and aligns with the common near block-diagonal Hessian structure. Under standard assumptions, we establish optimal convergence rates for both algorithms in the deterministic setting and show that, in the stochastic setting, their convergence guarantees adapt to the noise level of stochastic gradients. Experiments on pretraining GPT-2 models demonstrate improved performance of both NAMO and NAMO-D compared to the AdamW and Muon baselines, with NAMO-D achieving further gains over NAMO via an additional clamping hyperparameter that balances the competing goals of maintaining a well-conditioned update direction and leveraging fine-grained noise adaptation.

</details>


### [152] [MeGU: Machine-Guided Unlearning with Target Feature Disentanglement](https://arxiv.org/abs/2602.17088)
*Haoyu Wang,Zhuo Huang,Xiaolong Wang,Bo Han,Zhiwei Lin,Tongliang Liu*

Main category: cs.LG

TL;DR: 本文针对机器学习中的“被遗忘权”需求，提出一种新型机器引导遗忘（MeGU）框架。通过分析预训练模型中语义概念的特征纠缠现象，利用多模态大语言模型（MLLM）生成语义合理的扰动标签，指导目标样本的重对齐方向，并引入轻量级转移矩阵与正负噪声对，实现对目标概念特征的精准剥离，从而在不损害保留数据性能的前提下，有效消除特定训练数据的影响。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法面临过度遗忘（影响保留数据）与遗忘不足（残留目标信息）的两难困境，根源在于预训练模型中语义概念在特征层面存在纠缠，导致无法精准定位并移除特定数据的影响。

Method: 提出MeGU框架，利用多模态大语言模型（MLLM）识别目标样本的语义扰动方向，构建轻量级概念相似性转移矩阵以提升效率，并设计正负特征噪声对：负噪声抑制目标特异性特征模式，正噪声强化关联特征并与扰动概念对齐，实现选择性遗忘。

Result: 实验表明，MeGU在有效消除目标数据影响的同时，显著提升了模型在保留数据上的性能表现，缓解了传统方法中过忘与欠忘的矛盾，实现了可控、精准的机器遗忘。

Conclusion: MeGU通过概念感知的特征重对齐与噪声协同机制，突破了现有遗忘方法在效果与效率间的权衡瓶颈，为实现高效且安全的机器遗忘提供了新范式。

Abstract: The growing concern over training data privacy has elevated the "Right to be Forgotten" into a critical requirement, thereby raising the demand for effective Machine Unlearning. However, existing unlearning approaches commonly suffer from a fundamental trade-off: aggressively erasing the influence of target data often degrades model utility on retained data, while conservative strategies leave residual target information intact. In this work, the intrinsic representation properties learned during model pretraining are analyzed. It is demonstrated that semantic class concepts are entangled at the feature-pattern level, sharing associated features while preserving concept-specific discriminative components. This entanglement fundamentally limits the effectiveness of existing unlearning paradigms. Motivated by this insight, we propose Machine-Guided Unlearning (MeGU), a novel framework that guides unlearning through concept-aware re-alignment. Specifically, Multi-modal Large Language Models (MLLMs) are leveraged to explicitly determine re-alignment directions for target samples by assigning semantically meaningful perturbing labels. To improve efficiency, inter-class conceptual similarities estimated by the MLLM are encoded into a lightweight transition matrix. Furthermore, MeGU introduces a positive-negative feature noise pair to explicitly disentangle target concept influence. During finetuning, the negative noise suppresses target-specific feature patterns, while the positive noise reinforces remaining associated features and aligns them with perturbing concepts. This coordinated design enables selective disruption of target-specific representations while preserving shared semantic structures. As a result, MeGU enables controlled and selective forgetting, effectively mitigating both under-unlearning and over-unlearning.

</details>


### [153] [Synergizing Transport-Based Generative Models and Latent Geometry for Stochastic Closure Modeling](https://arxiv.org/abs/2602.17089)
*Xinghao Dong,Huchen Yang,Jin-long Wu*

Main category: cs.LG

TL;DR: 该论文探讨了基于扩散模型的生成式人工智能在学习随机闭合模型中的应用，提出在低维潜在空间中使用流匹配方法实现单步采样，相比传统的迭代扩散方法速度提升达两个数量级。通过对比联合训练与显式正则化（如保度量和几何感知约束），研究发现无论隐式还是显式正则化都能有效控制潜在空间畸变，保持物理保真度，并保留原始复杂动力系统的关键拓扑信息，从而在少量训练数据下仍能有效学习随机闭合模型。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型虽能生成高质量且多样化的样本，但采样速度慢，限制其在实际应用中的效率；而随机闭合模型的学习需要高效且物理合理的采样机制，因此亟需更快速、保真的生成方法。

Method: 采用基于流匹配的生成模型，在低维潜在空间中进行建模；对比联合训练方案与两种显式正则化方法（保度量和几何感知约束）以控制潜在空间畸变，确保生成结果的物理合理性；通过2D柯尔莫哥洛夫流动的数值实验验证方法的有效性。

Result: 所提方法实现了单步采样，速度比传统迭代扩散方法快两个数量级；显式和隐式正则化均能有效保持潜在空间的拓扑结构与物理保真度，且在数据量有限的情况下仍可学习有效的随机闭合模型。

Conclusion: 流匹配在低维潜在空间中的应用为快速、准确地学习随机闭合模型提供了新路径，兼具高速采样与物理一致性，显著优于传统扩散模型，具有广阔的应用前景。

Abstract: Diffusion models recently developed for generative AI tasks can produce high-quality samples while still maintaining diversity among samples to promote mode coverage, providing a promising path for learning stochastic closure models. Compared to other types of generative AI models, such as GANs and VAEs, the sampling speed is known as a key disadvantage of diffusion models. By systematically comparing transport-based generative models on a numerical example of 2D Kolmogorov flows, we show that flow matching in a lower-dimensional latent space is suited for fast sampling of stochastic closure models, enabling single-step sampling that is up to two orders of magnitude faster than iterative diffusion-based approaches. To control the latent space distortion and thus ensure the physical fidelity of the sampled closure term, we compare the implicit regularization offered by a joint training scheme against two explicit regularizers: metric-preserving (MP) and geometry-aware (GA) constraints. Besides offering a faster sampling speed, both explicitly and implicitly regularized latent spaces inherit the key topological information from the lower-dimensional manifold of the original complex dynamical system, which enables the learning of stochastic closure models without demanding a huge amount of training data.

</details>


### [154] [Unified Latents (UL): How to train your latents](https://arxiv.org/abs/2602.17270)
*Jonathan Heek,Emiel Hoogeboom,Thomas Mensink,Tim Salimans*

Main category: cs.LG

TL;DR: 提出统一潜在空间（UL）框架，通过扩散先验和解码器联合正则化潜在表示，实现低比特率且高质量的图像与视频生成，在ImageNet-512和Kinetics-600上分别达到优异的FID和FVD性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在潜在表示学习中难以平衡压缩效率与重建质量，尤其在降低训练计算开销方面存在挑战。本文旨在设计一种高效、高保真且可扩展的潜在表示学习框架。

Method: 通过将编码器输出的噪声水平与扩散先验的最小噪声级别对齐，构建一个简洁的训练目标，该目标提供潜在比特率的紧致上界，并利用扩散模型进行解码。

Result: 在ImageNet-512上取得1.4的FID和高PSNR重建质量，训练所需浮点运算量低于基于Stable Diffusion潜空间的模型；在Kinetics-600上达到1.3的新状态最优FVD。

Conclusion: 所提出的统一潜在空间框架在保持高质量生成的同时显著降低了训练复杂度，为高效潜在表示学习提供了新范式。

Abstract: We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.

</details>


### [155] [FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment](https://arxiv.org/abs/2602.17095)
*Chuiyang Meng,Ming Tang,Vincent W. S. Wong*

Main category: cs.LG

TL;DR: FLoRG 是一种用于联邦微调的参数高效框架，通过使用单个低秩矩阵并聚合其格拉姆矩阵，解决了传统 LoRA 在联邦学习中因双低秩矩阵聚合带来的误差和分解漂移问题。通过引入 Procrustes 对齐方法减少分解漂移，并在理论上证明了更紧的收敛界。实验表明，FLoRG 在下游任务准确率上优于五个先进基线方法，通信开销最高可降低 2041 倍。


<details>
  <summary>Details</summary>
Motivation: 传统 LoRA 在联邦学习中使用两个独立的低秩矩阵进行微调，导致聚合误差和矩阵分解非唯一性引发的分解漂移问题，限制了模型性能与通信效率。

Method: 提出 FLoRG 框架，采用单个低秩矩阵进行微调，聚合其格拉姆矩阵以避免聚合误差；引入 Procrustes 对齐机制，对连续训练轮次中的分解矩阵进行对齐，减少分解漂移。

Result: 在多个 LLM 微调基准测试中，FLoRG 在下游任务准确率上超越五个先进基线方法，通信开销最高降低 2041 倍，且理论分析表明其具有更紧的收敛边界。

Conclusion: FLoRG 通过设计更高效的低秩表示与聚合机制，在保持高精度的同时显著降低通信成本，为联邦微调提供了有效且可扩展的解决方案。

Abstract: Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\times$.

</details>


### [156] [The Sound of Death: Deep Learning Reveals Vascular Damage from Carotid Ultrasound](https://arxiv.org/abs/2602.17321)
*Christoph Balada,Aida Romano-Martinez,Payal Varshney,Vincent ten Cate,Katharina Geschke,Jonas Tesarz,Paul Claßen,Alexander K. Schuster,Dativa Tibyampansha,Karl-Patrik Kresoja,Philipp S. Wild,Sheraz Ahmed,Andreas Dengel*

Main category: cs.LG

TL;DR: 本研究提出一种机器学习框架，从颈动脉超声视频中提取血管损伤（VD）的临床有意义表征，利用高血压作为弱标签。模型学习到的特征具有生物学合理性、可解释性，并与已知心血管风险因素、共病和实验室指标高度相关。高血管损伤水平能有效预测心肌梗死、心脏死亡和全因死亡，表现优于或媲美传统风险模型（如SCORE2）。可解释人工智能分析表明，模型主要依赖血管形态和周围组织特征，揭示了血管损伤的新功能和解剖学标志。该方法展示了常规颈动脉超声蕴含远超以往认知的预后信息，为大规模、无创、低成本的心血管风险评估提供了可行工具，无需依赖实验室检测或复杂临床数据，实现更早、更个性化的预防策略。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病仍是全球死亡主因，但早期风险检测受限于现有诊断手段。颈动脉超声虽广泛可用且包含丰富的结构与血流动力学信息，但其潜在价值尚未被充分挖掘。因此，亟需一种高效、非侵入性的方法，从常规影像中提取可量化、可解释的血管损伤表征，以提升心血管风险评估能力。

Method: 采用机器学习框架，基于颈动脉超声视频数据，利用高血压作为弱监督信号训练模型，自动提取血管损伤的深层特征。通过可解释人工智能技术分析模型决策依据，识别关键影像特征，并验证其与临床结局的相关性。

Result: 所提模型提取的血管损伤表征与多种心血管风险因素、共病及实验室指标显著相关；高血管损伤个体在心肌梗死、心脏死亡和全因死亡方面具有更高风险，性能匹配或优于传统风险评分模型（如SCORE2）。模型依赖于血管形态和周围组织特征，揭示了新的血管损伤生物标志物。

Conclusion: 常规颈动脉超声蕴含大量未被充分利用的预后信息。本研究提出的机器学习方法能够有效提取可解释、生物学合理的血管损伤表征，为大规模、低成本、无创的心血管风险评估提供新路径，推动个性化早期预防策略的发展。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, yet early risk detection is often limited by available diagnostics. Carotid ultrasound, a non-invasive and widely accessible modality, encodes rich structural and hemodynamic information that is largely untapped. Here, we present a machine learning (ML) framework that extracts clinically meaningful representations of vascular damage (VD) from carotid ultrasound videos, using hypertension as a weak proxy label. The model learns robust features that are biologically plausible, interpretable, and strongly associated with established cardiovascular risk factors, comorbidities, and laboratory measures. High VD stratifies individuals for myocardial infarction, cardiac death, and all-cause mortality, matching or outperforming conventional risk models such as SCORE2. Explainable AI analyses reveal that the model relies on vessel morphology and perivascular tissue characteristics, uncovering novel functional and anatomical signatures of vascular damage. This work demonstrates that routine carotid ultrasound contains far more prognostic information than previously recognized. Our approach provides a scalable, non-invasive, and cost-effective tool for population-wide cardiovascular risk assessment, enabling earlier and more personalized prevention strategies without reliance on laboratory tests or complex clinical inputs.

</details>


### [157] [Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction](https://arxiv.org/abs/2602.17102)
*Sai Vineeth Kandappareddigari,Santhoshkumar Jagadish,Gauri Verma,Ilhuicamina Contreras,Christopher Dignam,Anmol Srivastava,Benjamin Demers*

Main category: cs.LG

TL;DR: 本文提出了一种无服务器MLOps框架，通过事件驱动的流水线和托管服务实现从数据摄入、训练、部署、监控到重训练的完整机器学习生命周期管理。该架构具备模型无关性，支持多种推理模式，且无需基础设施开销即可快速适应。以海关商品编码（HS码）预测为例，展示其在合规性要求高、描述模糊、更新频繁场景下的实际应用。采用自定义文本嵌入编码器与多种深度学习模型，其中Text-CNN达到98%准确率。系统确保可复现性、可审计性和负载波动下的SLA达标，通过自动A/B测试实现安全模型迭代。成本效益优先，尽管变压器模型性能相近，但长期运营成本更高；因此选择具有确定性、低延迟和可解释性的模型。架构可扩展至变压器及大语言模型。论文先介绍模型仿真与对比，再阐述工业落地中的自动化重训练、预测与验证流程，提供可复现的企业级机器学习部署蓝图。


<details>
  <summary>Details</summary>
Motivation: 在国际贸易中，产品描述短且不规范，导致海关商品编码（HS码）分类困难，错误分类会造成货柜延误和经济损失。现有方法难以应对频繁更新与语义模糊问题，亟需一种高效、可扩展、可审计且成本可控的自动化解决方案。

Method: 采用事件驱动的无服务器架构，集成数据摄入、模型训练、部署、监控与自动重训练全流程；使用自定义文本嵌入编码器与多种深度学习模型（如Text-CNN），结合自动化A/B测试进行模型动态评估与上线；通过弹性伸缩保障SLA，利用标准化接口实现模型无关性与灵活扩展。

Result: 在真实工业场景中，基于Text-CNN的模型达到98%的准确率；系统实现端到端自动化，支持高可用性、可复现性与审计追踪；在不同负载下保持稳定性能，且通过成本优化策略降低长期运营支出；支持未来向变压器和大语言模型演进。

Conclusion: 本研究构建了一个可复制、可扩展的无服务器MLOps框架，为复杂业务场景下的机器学习工程化提供了实用范式，兼顾性能、可靠性、成本与可维护性，适用于需要高合规性与实时响应的企业级应用。

Abstract: This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.

</details>


### [158] [Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting](https://arxiv.org/abs/2602.17645)
*Xiaohan Zhao,Zhaoyi Li,Yaxin Luo,Jiacheng Cui,Zhiqiang Shen*

Main category: cs.LG

TL;DR: M-Attack-V2 提升了黑盒攻击在大型视觉语言模型上的表现，通过改进局部匹配的对称性、降低梯度方差，并引入多作物对齐、辅助目标对齐和补丁动量等模块，显著提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法如 M-Attack 在黑盒攻击中因局部匹配导致高方差、近正交梯度，破坏优化稳定性；主要问题源于 ViT 的平移敏感性和源目标图像间的结构不对称性。

Method: 将局部匹配重构为源图像变换的非对称期望，提出 Multi-Crop Alignment（MCA）以多视角平均降低源端梯度方差，Auxiliary Target Alignment（ATA）用语义相关小数据集替代激进增强以平滑目标流形；同时引入 Patch Momentum 和补丁大小集成（PE+），强化可迁移方向。

Result: M-Attack-V2 在 Claude-4.0 上将成功率从 8% 提升至 30%，Gemini-2.5-Pro 从 83% 提升至 97%，GPT-5 达到 100%，全面超越先前黑盒攻击方法。

Conclusion: M-Attack-V2 是一个简单且模块化的改进，有效解决了黑盒攻击中的梯度不稳定与对齐偏差问题，显著提升了对前沿 LVLM 的攻击效果。

Abstract: Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.

</details>


### [159] [i-PhysGaussian: Implicit Physical Simulation for 3D Gaussian Splatting](https://arxiv.org/abs/2602.17117)
*Yicheng Cao,Zhuo Huang,Yu Yao,Yiming Ying,Daoyi Dong,Tongliang Liu*

Main category: cs.LG

TL;DR: i-PhysGaussian结合3D高斯点云与隐式材料点法（MPM）积分器，通过隐式牛顿型优化和GMRES求解器最小化动量平衡残差，实现端到端的状态更新，显著降低对时间步长的敏感性，在复杂动态场景中保持稳定性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D重建的物理模拟器多采用显式、逐步更新方法，对时间步长敏感且在高刚度材料或准静态运动等复杂场景下易出现精度快速下降的问题。

Method: 提出i-PhysGaussian框架，将3D高斯点云（3DGS）与隐式材料点法（MPM）积分器耦合，利用隐式牛顿型优化和GMRES求解器最小化动量平衡残差，实现非显式的时间步状态更新。

Result: 在高达20倍于显式基线的时间步长下仍能保持稳定性，有效维持结构完整性和运动平滑性，尤其在复杂动态转换中表现优异。

Conclusion: i-PhysGaussian通过隐式物理建模显著提升了模拟的稳定性与准确性，为复杂物理场景下的高效、可靠仿真提供了新范式。

Abstract: Physical simulation predicts future states of objects based on material properties and external loads, enabling blueprints for both Industry and Engineering to conduct risk management. Current 3D reconstruction-based simulators typically rely on explicit, step-wise updates, which are sensitive to step time and suffer from rapid accuracy degradation under complicated scenarios, such as high-stiffness materials or quasi-static movement. To address this, we introduce i-PhysGaussian, a framework that couples 3D Gaussian Splatting (3DGS) with an implicit Material Point Method (MPM) integrator. Unlike explicit methods, our solution obtains an end-of-step state by minimizing a momentum-balance residual through implicit Newton-type optimization with a GMRES solver. This formulation significantly reduces time-step sensitivity and ensures physical consistency. Our results demonstrate that i-PhysGaussian maintains stability at up to 20x larger time steps than explicit baselines, preserving structural coherence and smooth motion even in complex dynamic transitions.

</details>


### [160] [TIFO: Time-Invariant Frequency Operator for Stationarity-Aware Representation Learning in Time Series](https://arxiv.org/abs/2602.17122)
*Xihao Piao,Zheng Chen,Lingwei Zhu,Yushun Dong,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文提出了一种名为时间不变频率算子（TIFO）的新方法，用于解决非平稳时间序列预测中的分布偏移问题。TIFO通过在频域学习对平稳性敏感的权重，突出平稳频率成分并抑制非平稳成分，从而缓解分布偏移。该方法基于傅里叶变换在频域诱导特征分解的理论基础，可无缝集成到多种预测模型中，实验表明其在28个预测场景中取得18个第一和6个第二的成绩，在ETTm2数据集上平均MSE分别提升33.3%和55.3%，同时计算成本降低60%-70%，展现出优异的性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉跨样本的时间演化结构，且无法建模复杂的时间依赖关系，导致在非平稳时间序列预测中面临分布偏移问题。因此需要一种能够从频域角度建模时间结构、并适应分布变化的新方法。

Method: 提出时间不变频率算子（TIFO），通过学习全数据集上的频谱权重，实现对平稳频率成分的强调和非平稳成分的抑制；利用傅里叶变换隐含的频域特征分解特性，构建可插拔的模块化设计，适用于多种预测模型。

Result: 在28个预测设置中，获得18个top-1和6个top-2结果；在ETTm2数据集上，平均MSE分别提升33.3%和55.3%；计算成本降低60%-70%，表现出显著的性能提升与高效性。

Conclusion: TIFO是一种有效应对非平稳时间序列中分布偏移问题的方法，不仅在多个基准数据集上表现优异，还具备良好的通用性和低计算开销，适合广泛集成于各类时间序列预测模型中。

Abstract: Nonstationary time series forecasting suffers from the distribution shift issue due to the different distributions that produce the training and test data. Existing methods attempt to alleviate the dependence by, e.g., removing low-order moments from each individual sample. These solutions fail to capture the underlying time-evolving structure across samples and do not model the complex time structure. In this paper, we aim to address the distribution shift in the frequency space by considering all possible time structures. To this end, we propose a Time-Invariant Frequency Operator (TIFO), which learns stationarity-aware weights over the frequency spectrum across the entire dataset. The weight representation highlights stationary frequency components while suppressing non-stationary ones, thereby mitigating the distribution shift issue in time series. To justify our method, we show that the Fourier transform of time series data implicitly induces eigen-decomposition in the frequency space. TIFO is a plug-and-play approach that can be seamlessly integrated into various forecasting models. Experiments demonstrate our method achieves 18 top-1 and 6 top-2 results out of 28 forecasting settings. Notably, it yields 33.3% and 55.3% improvements in average MSE on the ETTm2 dataset. In addition, TIFO reduces computational costs by 60% -70% compared to baseline methods, demonstrating strong scalability across diverse forecasting models.

</details>


### [161] [VP-VAE: Rethinking Vector Quantization via Adaptive Vector Perturbation](https://arxiv.org/abs/2602.17133)
*Linwei Zhai,Han Ding,Mingzhi Lin,Cui Zhao,Fei Wang,Ge Wang,Wang Zhi,Wei Xi*

Main category: cs.LG

TL;DR: VP-VAE提出了一种新的生成建模方法，通过将表示学习与离散化解耦，解决了VQ-VAE中的训练不稳定性与代码本坍缩问题。其核心思想是用基于马尔可夫链蒙特卡洛采样的结构化扰动替代不可导的量化器，实现无代码本训练，并提升对推理时量化误差的鲁棒性。进一步在均匀潜变量假设下，推导出轻量级变体FSP，统一解释并改进了固定量化器（FSQ）方法。实验表明，该方法在图像和音频任务中显著提升了重建质量，实现了更均衡的词元使用，且避免了传统代码本训练的不稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有VQ-VAE模型因表示学习与离散代码本优化耦合，常出现训练不稳定和代码本坍缩问题。为解决这一根本缺陷，本文旨在设计一种无需显式代码本、可稳定训练且对量化误差鲁棒的新框架。

Method: 提出VP-VAE，以分布一致且尺度自适应的潜空间扰动取代非可导量化器，通过Metropolis-Hastings采样生成扰动；在此基础上，在均匀潜变量假设下推导出轻量级变体FSP，用于统一解释和改进固定量化策略。

Result: 在多个图像与音频基准测试中，VP-VAE与FSP均表现出更高的重建保真度，更均衡的词元使用分布，同时避免了传统代码本训练带来的不稳定性。

Conclusion: VP-VAE通过解耦表示学习与离散化，实现了稳定、高效且鲁棒的生成建模，其衍生的FSP也为固定量化器提供了理论支持与性能提升。该方法为未来离散生成模型的设计提供了新范式。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) are fundamental to modern generative modeling, yet they often suffer from training instability and "codebook collapse" due to the inherent coupling of representation learning and discrete codebook optimization. In this paper, we propose VP-VAE (Vector Perturbation VAE), a novel paradigm that decouples representation learning from discretization by eliminating the need for an explicit codebook during training. Our key insight is that, from the neural network's viewpoint, performing quantization primarily manifests as injecting a structured perturbation in latent space. Accordingly, VP-VAE replaces the non-differentiable quantizer with distribution-consistent and scale-adaptive latent perturbations generated via Metropolis--Hastings sampling. This design enables stable training without a codebook while making the model robust to inference-time quantization error. Moreover, under the assumption of approximately uniform latent variables, we derive FSP (Finite Scalar Perturbation), a lightweight variant of VP-VAE that provides a unified theoretical explanation and a practical improvement for FSQ-style fixed quantizers. Extensive experiments on image and audio benchmarks demonstrate that VP-VAE and FSP improve reconstruction fidelity and achieve substantially more balanced token usage, while avoiding the instability inherent to coupled codebook training.

</details>


### [162] [When More Experts Hurt: Underfitting in Multi-Expert Learning to Defer](https://arxiv.org/abs/2602.17144)
*Shuqi Liu,Yuzhou Cao,Lei Feng,Bo An,Luke Ong*

Main category: cs.LG

TL;DR: Multi-expert Learning to Defer (L2D) faces inherent underfitting due to expert identifiability issues, which existing remedies fail to address. The proposed PiCCE method adaptively selects reliable experts by empirical evidence, effectively reducing multi-expert L2D to a single-expert-like problem, ensuring statistical consistency and improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-expert L2D methods suffer from inherent underfitting caused by the difficulty in identifying trustworthy experts from a diverse pool, a problem not present in single-expert settings. This undermines prediction performance and renders current solutions ineffective.

Method: PiCCE (Pick the Confident and Correct Expert) is a surrogate-based method that adaptively identifies reliable experts using empirical evidence, transforming the multi-expert L2D problem into a single-expert-like learning framework.

Result: PiCCE successfully mitigates multi-expert underfitting, achieves statistical consistency, recovers class probabilities and expert accuracies, and demonstrates superior performance across diverse real-world scenarios.

Conclusion: The intrinsic expert identifiability issue in multi-expert L2D fundamentally challenges model performance. PiCCE resolves this by adaptively selecting confident and correct experts, offering a robust and theoretically sound solution.

Abstract: Learning to Defer (L2D) enables a classifier to abstain from predictions and defer to an expert, and has recently been extended to multi-expert settings. In this work, we show that multi-expert L2D is fundamentally more challenging than the single-expert case. With multiple experts, the classifier's underfitting becomes inherent, which seriously degrades prediction performance, whereas in the single-expert setting it arises only under specific conditions. We theoretically reveal that this stems from an intrinsic expert identifiability issue: learning which expert to trust from a diverse pool, a problem absent in the single-expert case and renders existing underfitting remedies failed. To tackle this issue, we propose PiCCE (Pick the Confident and Correct Expert), a surrogate-based method that adaptively identifies a reliable expert based on empirical evidence. PiCCE effectively reduces multi-expert L2D to a single-expert-like learning problem, thereby resolving multi expert underfitting. We further prove its statistical consistency and ability to recover class probabilities and expert accuracies. Extensive experiments across diverse settings, including real-world expert scenarios, validate our theoretical results and demonstrate improved performance.

</details>


### [163] [TimeOmni-VL: Unified Models for Time Series Understanding and Generation](https://arxiv.org/abs/2602.17149)
*Tong Guan,Sheng Pan,Johan Barthelemy,Zhao Li,Yujun Cai,Cesare Alippi,Ming Jin,Shirui Pan*

Main category: cs.LG

TL;DR: TimeOmni-VL 是首个以视觉为中心的统一时间序列理解与生成框架，通过保真双向映射（Bi-TSI）和理解引导生成，显著提升时间序列的语义理解与数值精度，推动多模态时间序列建模新前沿。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列建模在数值生成与语义理解之间存在明显割裂，生成模型依赖表面模式匹配，而理解模型难以实现高保真数值输出；尽管统一多模态模型在视觉领域取得成功，但在时间序列领域尚未被充分探索。

Method: 提出 TimeOmni-VL 框架，包含两项关键创新：(1) 保真双向映射（Bi-TSI），实现时间序列与图像间的近无损转换；(2) 理解引导生成，构建基于时间序列分析任务的 TSUMM-Suite 数据集，并引入校准的思维链（Chain-of-Thought）作为显式控制信号，驱动高质量生成。

Result: 实验表明，该统一方法显著提升了时间序列的语义理解能力与数值生成精度，实现了理解与生成的协同优化，为多模态时间序列建模树立了新标杆。

Conclusion: TimeOmni-VL 首次实现时间序列理解与生成的统一，通过视觉中心化架构与理解引导机制，突破现有局限，开启多模态时间序列建模的新范式。

Abstract: Recent time series modeling faces a sharp divide between numerical generation and semantic understanding, with research showing that generation models often rely on superficial pattern matching, while understanding-oriented models struggle with high-fidelity numerical output. Although unified multimodal models (UMMs) have bridged this gap in vision, their potential for time series remains untapped. We propose TimeOmni-VL, the first vision-centric framework that unifies time series understanding and generation through two key innovations: (1) Fidelity-preserving bidirectional mapping between time series and images (Bi-TSI), which advances Time Series-to-Image (TS2I) and Image-to-Time Series (I2TS) conversions to ensure near-lossless transformations. (2) Understanding-guided generation. We introduce TSUMM-Suite, a novel dataset consists of six understanding tasks rooted in time series analytics that are coupled with two generation tasks. With a calibrated Chain-of-Thought, TimeOmni-VL is the first to leverage time series understanding as an explicit control signal for high-fidelity generation. Experiments confirm that this unified approach significantly improves both semantic understanding and numerical precision, establishing a new frontier for multimodal time series modeling.

</details>


### [164] [Powering Up Zeroth-Order Training via Subspace Gradient Orthogonalization](https://arxiv.org/abs/2602.17155)
*Yicheng Lang,Changsheng Wang,Yihua Zhang,Mingyi Hong,Zheng Zhang,Wotao Yin,Sijia Liu*

Main category: cs.LG

TL;DR: 本文提出一种名为ZO-Muon的新方法，通过结合投影式子空间视角与Muon风格的谱优化，解决零阶（ZO）优化中精度与查询效率之间的根本矛盾。该方法利用模型更新的低秩结构降低梯度估计方差，并通过梯度正交化提取噪声中的有用谱信息，形成统一的子空间梯度正交化框架。在大语言模型和视觉变换器上的实验表明，ZO-Muon显著加速收敛，在准确率和查询/运行效率上实现双赢。相比流行的MeZO基线，其在LLM微调中仅需24.7%的查询量即可达到相同SST-2性能，且在ViT-B微调中准确率提升25.1%。


<details>
  <summary>Details</summary>
Motivation: 零阶（ZO）优化虽避免反向传播、内存高效，但面临精度与查询效率间的根本矛盾。现有方法难以同时兼顾高精度与低查询开销，亟需新机制提升性能。

Method: 提出统一的子空间梯度正交化框架，融合投影式子空间分析以利用模型更新的低秩结构，以及Muon风格的梯度正交化以从噪声中提取有效谱信息，构建新型零阶优化器ZO-Muon。

Result: 在大语言模型和视觉变换器上实验验证，ZO-Muon显著提升收敛速度，实现准确率与查询/运行效率的双重提升；相比MeZO，LLM微调仅需24.7%查询量达同等性能，ViT-B在CIFAR-100上准确率提升25.1%。

Conclusion: 通过整合低秩子空间建模与梯度正交化，提出的ZO-Muon框架有效突破了传统零阶优化的性能瓶颈，为大规模模型高效微调提供了更优解决方案。

Abstract: Zeroth-order (ZO) optimization provides a gradient-free alternative to first-order (FO) methods by estimating gradients via finite differences of function evaluations, and has recently emerged as a memory-efficient paradigm for fine-tuning large-scale models by avoiding backpropagation. However, ZO optimization has a fundamental tension between accuracy and query efficiency. In this work, we show that ZO optimization can be substantially improved by unifying two complementary principles: (i) a projection-based subspace view that reduces gradient estimation variance by exploiting the intrinsic low-rank structure of model updates, and (ii) Muon-style spectral optimization that applies gradient orthogonalization to extract informative spectral structure from noisy ZO gradients. These findings form a unified framework of subspace gradient orthogonalization, which we instantiate in a new method, ZO-Muon, admitting a natural interpretation as a low-rank Muon optimizer in the ZO setting. Extensive experiments on large language models (LLMs) and vision transformers (ViTs) demonstrate that ZO-Muon significantly accelerates convergence and achieves a win-win improvement in accuracy and query/runtime efficiency. Notably, compared to the popular MeZO baseline, ZO-Muon requires only 24.7% of the queries to reach the same SST-2 performance for LLM fine-tuning, and improves accuracy by 25.1% on ViT-B fine-tuning on CIFAR-100.

</details>


### [165] [In-Context Learning in Linear vs. Quadratic Attention Models: An Empirical Study on Regression Tasks](https://arxiv.org/abs/2602.17171)
*Ayush Goel,Arjun Kohli,Sarvagya Somvanshi*

Main category: cs.LG

TL;DR: 本文通过实证研究对比了Transformer和线性注意力模型在简单函数类（如线性回归）上的上下文学习（ICL）行为，重点分析了学习质量（MSE）、收敛性和泛化能力，并考察了模型深度对ICL性能的影响。结果揭示了线性注意力与二次注意力在该任务中的相似性与局限性。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer与线性注意力模型在上下文学习中的差异，特别是在线性回归任务中的表现，以理解其优劣及影响因素。

Method: 在Garg等人提出的经典线性回归任务上，对Transformer和线性注意力模型进行实证评估，比较其学习质量、收敛速度和泛化能力，并系统分析模型深度的影响。

Result: 结果显示，线性注意力在某些方面与二次注意力表现相似，但在学习质量、收敛性和泛化能力上仍存在局限，且模型深度对ICL性能有显著影响。

Conclusion: 线性注意力虽具备一定优势，但在复杂学习任务中仍不如传统二次注意力模型，尤其在深度增加时表现出性能瓶颈。

Abstract: Recent work has demonstrated that transformers and linear attention models can perform in-context learning (ICL) on simple function classes, such as linear regression. In this paper, we empirically study how these two attention mechanisms differ in their ICL behavior on the canonical linear-regression task of Garg et al. We evaluate learning quality (MSE), convergence, and generalization behavior of each architecture. We also analyze how increasing model depth affects ICL performance. Our results illustrate both the similarities and limitations of linear attention relative to quadratic attention in this setting.

</details>


### [166] [Continual uncertainty learning](https://arxiv.org/abs/2602.17174)
*Heisei Yonezawa,Ansei Yonezawa,Itsuro Kajiwara*

Main category: cs.LG

TL;DR: 本文提出一种基于课程的持续学习框架，用于解决多不确定性下非线性机械系统鲁棒控制问题。通过将复杂控制任务分解为一系列逐步增加不确定性的学习阶段，结合模型基础控制器（MBC）加速收敛并防止灾难性遗忘，显著提升样本效率与控制性能。该方法成功应用于汽车动力总成主动减振控制，实现从仿真到现实的可靠迁移。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在处理多重不确定性时易导致策略次优和学习效率低下，尤其在非线性动态与工况变化交织的情况下。如何有效应对复杂不确定性并实现高效、稳定的持续学习，是当前鲁棒控制领域的重要挑战。

Method: 提出一种基于课程的持续学习框架，将多不确定性控制问题分解为渐进式学习任务；通过扩展有限个具有不同动态不确定性的系统集，结合模型基础控制器（MBC）提供共享基线性能，实现稳定策略更新与任务特异性优化，采用残差学习机制提升样本效率。

Result: 所提方法在汽车动力总成主动振动控制中验证有效，实现了对结构非线性和动态变化的强鲁棒性，并成功完成从仿真到现实的迁移，显著提升了学习效率与控制性能。

Conclusion: 该研究构建的课程式持续学习框架有效解决了多源不确定性下的鲁棒控制难题，通过分阶段学习与模型辅助优化，兼顾了稳定性、效率与实际应用可行性，为工业级智能控制提供了新范式。

Abstract: Robust control of mechanical systems with multiple uncertainties remains a fundamental challenge, particularly when nonlinear dynamics and operating-condition variations are intricately intertwined. While deep reinforcement learning (DRL) combined with domain randomization has shown promise in mitigating the sim-to-real gap, simultaneously handling all sources of uncertainty often leads to sub-optimal policies and poor learning efficiency. This study formulates a new curriculum-based continual learning framework for robust control problems involving nonlinear dynamical systems in which multiple sources of uncertainty are simultaneously superimposed. The key idea is to decompose a complex control problem with multiple uncertainties into a sequence of continual learning tasks, in which strategies for handling each uncertainty are acquired sequentially. The original system is extended into a finite set of plants whose dynamic uncertainties are gradually expanded and diversified as learning progresses. The policy is stably updated across the entire plant sets associated with tasks defined by different uncertainty configurations without catastrophic forgetting. To ensure learning efficiency, we jointly incorporate a model-based controller (MBC), which guarantees a shared baseline performance across the plant sets, into the learning process to accelerate the convergence. This residual learning scheme facilitates task-specific optimization of the DRL agent for each uncertainty, thereby enhancing sample efficiency. As a practical industrial application, this study applies the proposed method to designing an active vibration controller for automotive powertrains. We verified that the resulting controller is robust against structural nonlinearities and dynamic variations, realizing successful sim-to-real transfer.

</details>


### [167] [SoftDTW-CUDA-Torch: Memory-Efficient GPU-Accelerated Soft Dynamic Time Warping for PyTorch](https://arxiv.org/abs/2602.17206)
*Ron Shapira Weber,Oren Freifeld*

Main category: cs.LG

TL;DR: softdtw-cuda-torch 是一个开源的 PyTorch 库，用于在 GPU 上计算 SoftDTW。它解决了现有 GPU 实现中的三个关键问题：序列长度上限为 1024、小平滑参数下的反向传播数值不稳定以及过度消耗 GPU 内存的问题。通过引入分块反对角线核执行、基于 log 空间的反向传播和融合距离计算模式，该库实现了任意序列长度支持、完整的 PyTorch autograd 集成，并支持 Soft-DTW Barycenter 计算，内存使用相比之前工作最多降低 98%。


<details>
  <summary>Details</summary>
Motivation: 现有 GPU 实现的 SoftDTW 存在序列长度限制（最大 1024）、小平滑参数下反向传播时易出现数值不稳定性，以及因生成成对距离张量导致的高内存消耗问题，限制了其在长序列任务中的应用。

Method: 提出分块反对角线核执行以消除序列长度限制；采用 log 空间反向传播防止浮点溢出；设计融合距离计算模式，避免生成 O(BN M) 的中间距离张量，从而大幅减少内存占用。

Result: 实现任意序列长度支持，反向传播稳定，内存使用最高降低 98%，支持 PyTorch 全自动微分与 Soft-DTW Barycenter 计算，代码已开源。

Conclusion: softdtw-cuda-torch 成功克服了现有 SoftDTW GPU 实现的关键瓶颈，显著提升了性能与可扩展性，适用于大规模、长序列的时间序列分析任务。

Abstract: We present softdtw-cuda-torch, an open-source PyTorch library for computing Soft Dynamic Time Warping (SoftDTW) on GPUs. Our implementation addresses three key limitations of existing GPU implementations of SoftDTW: a hard sequence-length cap of 1024, numerical instability in the backward pass for small smoothing parameters, and excessive GPU memory consumption from materializing pairwise distance tensors. We introduce (1) tiled anti-diagonal kernel execution that removes the sequence-length constraint, (2) a log-space back-ward pass that prevents floating-point overflow, and (3) a fused distance-computation mode that eliminates the O(BN M ) intermediate distance tensor, achieving up to 98% memory reduction compared to prior work. The library supports arbitrary sequence lengths, full PyTorch autograd integration, and Soft-DTW Barycenter computation. Code is available at https://github.com/BGU-CS-VIL/sdtw-cuda-torch.

</details>


### [168] [Learning a Latent Pulse Shape Interface for Photoinjector Laser Systems](https://arxiv.org/abs/2602.17263)
*Alexander Klemps,Denis Ilia,Pradeep Kr. Banerjee,Ye Chen,Henrik Tünnermann,Nihat Ay*

Main category: cs.LG

TL;DR: 该研究提出一种基于Wasserstein自编码器的生成建模框架，用于学习激光脉冲形状与电子束动力学之间的可微分潜在接口，显著降低对昂贵脉冲传播仿真的依赖，并实现高保真度重建与平滑脉冲类型过渡。


<details>
  <summary>Details</summary>
Motivation: 在自由电子激光器的光注入器中，控制纵向激光脉冲形状是优化电子束质量的重要手段，但受限于大规模设计空间探索中高昂的仿真成本，亟需高效建模方法。

Method: 采用Wasserstein自编码器构建生成模型，学习脉冲形状与下游束流动力学之间的可微分潜在表示，通过潜在空间的连续性和可解释性实现脉冲家族的轨迹追踪与线性插值。

Result: 所学潜在空间具有连续、可解释且结构良好的几何特性；能准确重构真实实验测量脉冲，并实现不同脉冲类型间的平滑过渡，显著提升仿真效率与分析能力。

Conclusion: 该方法有效减少了对复杂脉冲传播仿真的依赖，为电子束动力学模拟与分析提供了高效、通用的建模工具。

Abstract: Controlling the longitudinal laser pulse shape in photoinjectors of Free-Electron Lasers is a powerful lever for optimizing electron beam quality, but systematic exploration of the vast design space is limited by the cost of brute-force pulse propagation simulations. We present a generative modeling framework based on Wasserstein Autoencoders to learn a differentiable latent interface between pulse shaping and downstream beam dynamics. Our empirical findings show that the learned latent space is continuous and interpretable while maintaining high-fidelity reconstructions. Pulse families such as higher-order Gaussians trace coherent trajectories, while standardizing the temporal pulse lengths shows a latent organization correlated with pulse energy. Analysis via principal components and Gaussian Mixture Models reveals a well behaved latent geometry, enabling smooth transitions between distinct pulse types via linear interpolation. The model generalizes from simulated data to real experimental pulse measurements, accurately reconstructing pulses and embedding them consistently into the learned manifold. Overall, the approach reduces reliance on expensive pulse-propagation simulations and facilitates downstream beam dynamics simulation and analysis.

</details>


### [169] [RLGT: A reinforcement learning framework for extremal graph theory](https://arxiv.org/abs/2602.17276)
*Ivan Damnjanović,Uroš Milivojević,Irena Đorđević,Dragan Stevanović*

Main category: cs.LG

TL;DR: 本文提出了一种名为RLGT的新型强化学习框架，用于系统化已有工作并支持有向/无向图、带或不带环、任意边颜色数的图论问题研究，旨在通过优化计算性能和模块化设计促进未来基于强化学习的极值图论研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究已证明强化学习在解决极值图论问题中的有效性，但缺乏统一且高效的框架来支持多样化图结构和复杂约束条件，因此亟需一个系统化、可扩展的解决方案。

Method: 提出RLGT框架，整合先前方法，采用高效图表示与模块化设计，支持多种图类型（包括有向/无向、含/不含环、多色边），并通过优化计算性能提升实验效率。

Result: RLGT成功实现了对多种图结构的支持，显著提升了处理复杂图论问题的效率，并为后续研究提供了可复用、可扩展的平台，已在多个极值图论问题上取得进展，如反例构造、新下界发现及图论极值问题求解。

Conclusion: RLGT为极值图论中的强化学习应用提供了一个统一、高效且灵活的框架，推动了该领域从个案探索迈向系统性研究，具有广泛的应用前景。

Abstract: Reinforcement learning (RL) is a subfield of machine learning that focuses on developing models that can autonomously learn optimal decision-making strategies over time. In a recent pioneering paper, Wagner demonstrated how the Deep Cross-Entropy RL method can be applied to tackle various problems from extremal graph theory by reformulating them as combinatorial optimization problems. Subsequently, many researchers became interested in refining and extending the framework introduced by Wagner, thereby creating various RL environments specialized for graph theory. Moreover, a number of problems from extremal graph theory were solved through the use of RL. In particular, several inequalities concerning the Laplacian spectral radius of graphs were refuted, new lower bounds were obtained for certain Ramsey numbers, and contributions were made to the Turán-type extremal problem in which the forbidden structures are cycles of length three and four. Here, we present Reinforcement Learning for Graph Theory (RLGT), a novel RL framework that systematizes the previous work and provides support for both undirected and directed graphs, with or without loops, and with an arbitrary number of edge colors. The framework efficiently represents graphs and aims to facilitate future RL-based research in extremal graph theory through optimized computational performance and a clean and modular design.

</details>


### [170] [The Anxiety of Influence: Bloom Filters in Transformer Attention Heads](https://arxiv.org/abs/2602.17526)
*Peter Balogh*

Main category: cs.LG

TL;DR: 本文识别出多个Transformer注意力头具有成员资格测试功能，能判断某标记是否在上下文中出现过。在四个语言模型中发现三种真实的成员资格测试头，分别表现出高精度过滤、经典布隆过滤器特性及距离敏感的特征。这些头集中在早期层，具备广义泛化能力，且在消融实验中显示其同时参与重复与新标记处理。一个曾被误判为布隆过滤器的头因混淆控制被重新分类，反而强化了其余头的可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer注意力机制中是否存在专门用于检测标记是否在上下文中出现过的注意力头，并理解其工作机制与计算意义。

Method: 通过分析GPT-2小、中、大版本及Pythia-160M模型中的注意力头，识别并分类具有成员资格测试功能的注意力头；使用统计拟合、混淆控制和消融实验验证其行为与属性。

Result: 发现三个真实成员资格测试头：两个为高精度过滤器（错误阳性率0-4%），一个符合经典布隆过滤器容量曲线（$R^2=1.0$，容量约5比特）；另一头（L3H0）因序列长度偏差被重新分类；所有真实头集中于早期层，具有距离敏感性与广义泛化能力。

Conclusion: Transformer模型中存在一组功能明确、结构多样且广泛泛化的成员资格测试注意力头，它们不仅执行特定任务，还与更广泛的语义处理共存，表明注意力机制具备多层次、可分化的认知功能。

Abstract: Some transformer attention heads appear to function as membership testers, dedicating themselves to answering the question "has this token appeared before in the context?" We identify these heads across four language models (GPT-2 small, medium, and large; Pythia-160M) and show that they form a spectrum of membership-testing strategies. Two heads (L0H1 and L0H5 in GPT-2 small) function as high-precision membership filters with false positive rates of 0-4\% even at 180 unique context tokens -- well above the $d_\text{head} = 64$ bit capacity of a classical Bloom filter. A third head (L1H11) shows the classic Bloom filter capacity curve: its false positive rate follows the theoretical formula $p \approx (1 - e^{-kn/m})^k$ with $R^2 = 1.0$ and fitted capacity $m \approx 5$ bits, saturating by $n \approx 20$ unique tokens. A fourth head initially identified as a Bloom filter (L3H0) was reclassified as a general prefix-attention head after confound controls revealed its apparent capacity curve was a sequence-length artifact. Together, the three genuine membership-testing heads form a multi-resolution system concentrated in early layers (0-1), taxonomically distinct from induction and previous-token heads, with false positive rates that decay monotonically with embedding distance -- consistent with distance-sensitive Bloom filters. These heads generalize broadly: they respond to any repeated token type, not just repeated names, with 43\% higher generalization than duplicate-token-only heads. Ablation reveals these heads contribute to both repeated and novel token processing, indicating that membership testing coexists with broader computational roles. The reclassification of L3H0 through confound controls strengthens rather than weakens the case: the surviving heads withstand the scrutiny that eliminated a false positive in our own analysis.

</details>


### [171] [Efficient privacy loss accounting for subsampling and random allocation](https://arxiv.org/abs/2602.17284)
*Vitaly Feldman,Moshe Shenfeld*

Main category: cs.LG

TL;DR: 本文研究了一种随机采样方案在隐私放大中的性质，该方案在差分隐私优化和高效高维私有聚合中表现出优于标准泊松采样的实用性。尽管已有理论分析接近泊松采样边界，但仍存在隐私参数不紧致以及使用霍基斯克特或瑞尼散度带来额外开销的问题。本文提出可高效计算任意差分隐私算法下随机分配的隐私损失分布（PLD），并证明其在高斯机制下的隐私-效用权衡至少不劣于泊松子采样，尤其适用于DP-SGD训练。为此，本文发展了基于PLD实现的新工具，使精确隐私损失会计可推广至子采样，而无需针对具体噪声机制进行手动分析。


<details>
  <summary>Details</summary>
Motivation: 现有随机采样方案的隐私分析存在参数不紧致和使用非最优隐私度量（如霍基斯克特或瑞尼散度）导致计算复杂度高的问题，限制了其在实际应用中的效率与精度。需要更准确、通用且高效的隐私分析方法以支持该采样方案在差分隐私系统中的广泛应用。

Method: 提出一种基于隐私损失分布（PLD）实现的新框架，用于高效计算随机分配机制下的隐私损失分布；通过该框架，将精确隐私损失会计扩展到子采样场景，避免对每种噪声机制单独分析。

Result: 所提方法能高效计算随机分配的隐私损失分布；在高斯机制下，随机分配的隐私-效用权衡优于或等同于泊松子采样，特别适合于DP-SGD训练；新工具实现了对子采样的通用化、高精度隐私损失会计。

Conclusion: 本文提出的基于PLD实现的隐私分析框架显著提升了随机采样方案的隐私分析精度与通用性，为差分隐私优化与通信高效私有聚合提供了更优的理论基础与实践工具。

Abstract: We consider the privacy amplification properties of a sampling scheme in which a user's data is used in $k$ steps chosen randomly and uniformly from a sequence (or set) of $t$ steps. This sampling scheme has been recently applied in the context of differentially private optimization (Chua et al., 2024a; Choquette-Choo et al., 2025) and communication-efficient high-dimensional private aggregation (Asi et al., 2025), where it was shown to have utility advantages over the standard Poisson sampling. Theoretical analyses of this sampling scheme (Feldman & Shenfeld, 2025; Dong et al., 2025) lead to bounds that are close to those of Poisson sampling, yet still have two significant shortcomings. First, in many practical settings, the resulting privacy parameters are not tight due to the approximation steps in the analysis. Second, the computed parameters are either the hockey stick or Renyi divergence, both of which introduce overheads when used in privacy loss accounting.
  In this work, we demonstrate that the privacy loss distribution (PLD) of random allocation applied to any differentially private algorithm can be computed efficiently. When applied to the Gaussian mechanism, our results demonstrate that the privacy-utility trade-off for random allocation is at least as good as that of Poisson subsampling. In particular, random allocation is better suited for training via DP-SGD. To support these computations, our work develops new tools for general privacy loss accounting based on a notion of PLD realization. This notion allows us to extend accurate privacy loss accounting to subsampling which previously required manual noise-mechanism-specific analysis.

</details>


### [172] [SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework](https://arxiv.org/abs/2602.17330)
*Rong Fu,Zijian Zhang,Wenxin Zhang,Kun Liu,Jiekai Wu,Xianda Li,Simon Fong*

Main category: cs.LG

TL;DR: SubQuad 是一个端到端的流程，用于在群体规模上比较适应性免疫库，解决了配对亲和力评估成本高和数据集不平衡的问题。它结合了抗原感知的近似次二次检索、GPU加速的亲和力核函数、学习到的多模态融合以及公平性约束聚类。系统使用紧凑的MinHash预过滤减少候选比较，采用可微分门控模块自适应加权每对的互补对齐和嵌入通道，并通过自动化校准程序确保罕见抗原特异性亚组的比例代表性。在大型病毒和肿瘤库中，SubQuad 在吞吐量和峰值内存使用方面实现了显著提升，同时保持或提高了召回率@k、聚类纯度和子组公平性。通过协同设计索引、相似性融合和公平性感知目标，SubQuad 提供了一个可扩展、偏见感知的平台，适用于谱系挖掘和疫苗靶点优先排序、生物标志物发现等下游转化任务。


<details>
  <summary>Details</summary>
Motivation: 解决大规模适应性免疫库比较中的两个实际瓶颈：配对亲和力评估的近二次成本和数据集不平衡导致临床重要少数克隆型被掩盖的问题。

Method: 结合抗原感知的近似次二次检索、GPU加速的亲和力核函数、学习到的多模态融合、公平性约束聚类；采用紧凑的MinHash预过滤减少候选比较；引入可微分门控模块自适应加权对齐与嵌入通道；实施自动化校准以保证罕见亚组的代表性。

Result: 在大型病毒和肿瘤免疫库中，SubQuad 实现了更高的吞吐量和更低的峰值内存使用，同时保持或提升了召回率@k、聚类纯度和子组公平性。

Conclusion: 通过协同设计索引、相似性融合和公平性感知目标，SubQuad 构建了一个可扩展且偏见感知的平台，适用于免疫谱系挖掘及疫苗靶点优先排序、生物标志物发现等转化研究任务。

Abstract: Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.

</details>


### [173] [From Subtle to Significant: Prompt-Driven Self-Improving Optimization in Test-Time Graph OOD Detection](https://arxiv.org/abs/2602.17342)
*Luzhi Wang,Xuanshuo Fu,He Zhang,Chuang Liu,Xiaobao Wang,Hongbo Liu*

Main category: cs.LG

TL;DR: 提出SIGOOD，一种无监督的自提升图OOD检测框架，通过测试时训练和连续自我学习，利用提示增强图来放大潜在的OOD信号，并通过能量偏好优化损失迭代优化提示，实现更准确的图OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测方法多采用单次推理范式，无法逐步纠正错误预测以增强OOD信号，限制了检测性能。

Method: SIGOOD通过生成提示构建提示增强图，引入能量偏好优化（EPO）损失，利用原始测试图与提示增强图之间的能量差异进行提示优化，并在检测模型中迭代自提升，形成最优提示增强图用于最终的OOD检测。

Result: 在21个真实世界数据集上的综合评估表明，SIGOOD在图OOD检测任务中显著优于现有方法，具有更强的检测能力与鲁棒性。

Conclusion: SIGOOD通过结合测试时训练与自提升机制，实现了高效的无监督图OOD检测，在开放世界场景下提升了GNN的可靠性。

Abstract: Graph Out-of-Distribution (OOD) detection aims to identify whether a test graph deviates from the distribution of graphs observed during training, which is critical for ensuring the reliability of Graph Neural Networks (GNNs) when deployed in open-world scenarios. Recent advances in graph OOD detection have focused on test-time training techniques that facilitate OOD detection without accessing potential supervisory information (e.g., training data). However, most of these methods employ a one-pass inference paradigm, which prevents them from progressively correcting erroneous predictions to amplify OOD signals. To this end, we propose a \textbf{S}elf-\textbf{I}mproving \textbf{G}raph \textbf{O}ut-\textbf{o}f-\textbf{D}istribution detector (SIGOOD), which is an unsupervised framework that integrates continuous self-learning with test-time training for effective graph OOD detection. Specifically, SIGOOD generates a prompt to construct a prompt-enhanced graph that amplifies potential OOD signals. To optimize prompts, SIGOOD introduces an Energy Preference Optimization (EPO) loss, which leverages energy variations between the original test graph and the prompt-enhanced graph. By iteratively optimizing the prompt by involving it into the detection model in a self-improving loop, the resulting optimal prompt-enhanced graph is ultimately used for OOD detection. Comprehensive evaluations on 21 real-world datasets confirm the effectiveness and outperformance of our SIGOOD method. The code is at https://github.com/Ee1s/SIGOOD.

</details>


### [174] [Shortcut learning in geometric knot classification](https://arxiv.org/abs/2602.17350)
*Djordje Mihajlovic,Davide Michieletto*

Main category: cs.LG

TL;DR: 本文研究了机器学习在解决纽结分类问题中的应用，发现训练数据中隐藏的非拓扑特征会影响模型分类结果。为此，作者构建了一个公开的数据集和代码工具，旨在消除非拓扑特征的影响，并生成具有固定纽结拓扑的几何嵌入，为未来基于机器学习的纽结分类研究提供可靠基础。


<details>
  <summary>Details</summary>
Motivation: 传统纽结分类依赖于复杂的数学方法，而机器学习在复杂分类任务中表现出色，因此探索其在纽结分类中的应用具有重要意义。然而，现有数据可能包含误导性的非拓扑特征，影响模型泛化能力，亟需建立更可靠的训练框架。

Method: 通过分析分子动力学模拟生成的多边形纽结数据，识别出机器学习模型所依赖的隐藏非拓扑特征；设计并构建一个去除非拓扑特征的公开数据集及可复现的生成代码，确保训练样本仅反映拓扑结构差异。

Result: 发现了训练数据中存在显著的非拓扑特征，这些特征被机器学习模型误用以实现高分类准确率；提出的公开数据集与生成工具能有效控制几何变量，确保模型真正学习拓扑信息。

Conclusion: 本研究为基于机器学习的纽结分类提供了严谨的方法论支持，强调了高质量、可控数据的重要性，推动该领域向更可信、可复现的方向发展。

Abstract: Classifying the topology of closed curves is a central problem in low dimensional topology with applications beyond mathematics spanning protein folding, polymer physics and even magnetohydrodynamics. The central problem is how to determine whether two embeddings of a closed arc are equivalent under ambient isotopy. Given the striking ability of neural networks to solve complex classification tasks, it is therefore natural to ask if the knot classification problem can be tackled using Machine Learning (ML). In this paper, we investigate generic shortcut methods employed by ML to solve the knot classification challenge and specifically discover hidden non-topological features in training data generated through Molecular Dynamics simulations of polygonal knots that are used by ML to arrive to positive classifications results. We then provide a rigorous foundation for future attempts to tackle the knot classification challenge using ML by developing a publicly-available (i) dataset, that aims to remove the potential of non-topological feature classification and (ii) code, that can generate knot embeddings that faithfully explore chosen geometric state space with fixed knot topology. We expect that our work will accelerate the development of ML models that can solve complex geometric knot classification challenges.

</details>


### [175] [2Mamba2Furious: Linear in Complexity, Competitive in Accuracy](https://arxiv.org/abs/2602.17363)
*Gabriel Mongaras,Eric C. Larson*

Main category: cs.LG

TL;DR: 本文通过简化Mamba-2并优化其核心组件，提出一种名为2Mamba的新方法，显著提升了线性注意力的表达能力与准确性，使其接近甚至超越软最大注意力（softmax attention），同时在长上下文场景下保持更高的内存效率。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然计算高效，但表达能力较弱，导致准确率低于软最大注意力；本文旨在缩小两者之间的性能差距，并探索超越软最大注意力的可能性。

Method: 首先对Mamba-2进行简化，识别关键组件；在此基础上改进A-mask设计并提升隐藏状态阶数，构建出2Mamba模型；系统评估各设计选择对性能的影响。

Result: 2Mamba在多种任务上达到接近软最大注意力的准确率，且在长序列处理中具有显著更低的内存开销，部分情况下甚至超越了软最大注意力的性能。

Conclusion: 通过合理设计与优化，线性注意力可实现与软最大注意力相当甚至更优的性能，同时具备更强的可扩展性和内存效率，为高效长序列建模提供了新路径。

Abstract: Linear attention transformers have become a strong alternative to softmax attention due to their efficiency. However, linear attention tends to be less expressive and results in reduced accuracy compared to softmax attention. To bridge the accuracy gap between softmax attention and linear attention, we manipulate Mamba-2, a very strong linear attention variant. We first simplify Mamba-2 down to its most fundamental and important components, evaluating which specific choices make it most accurate. From this simplified Mamba variant (Mamba-2S), we improve the A-mask and increase the order of the hidden state, resulting in a method, which we call 2Mamba, that is nearly as accurate as softmax attention, yet much more memory efficient for long context lengths. We also investigate elements to Mamba-2 that help surpass softmax attention accuracy. Code is provided for all our experiments

</details>


### [176] [A feature-stable and explainable machine learning framework for trustworthy decision-making under incomplete clinical data](https://arxiv.org/abs/2602.17364)
*Justyna Andrys-Olek,Paulina Tworek,Luca Gherardini,Mark W. Ruddock,Mary Jo Kurt,Peter Fitzgerald,Jose Sousa*

Main category: cs.LG

TL;DR: CACTUS是一个用于小规模、异质性和不完整临床数据的可解释机器学习框架，旨在解决模型在生物医学领域应用中的鲁棒性差、可解释性不足和特征不稳定问题。它通过特征抽象、可解释分类和系统化的特征稳定性分析，量化特征在数据质量下降时的一致性。在568名血尿患者的数据集上，CACTUS在预测性能上与随机森林和梯度提升方法相当或更优，且在缺失数据增加时表现出更高的特征稳定性，尤其在按性别分层分析中表现突出。结果表明，特征稳定性提供了与传统性能指标互补的信息，对评估模型可信度至关重要。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据常存在小样本、异质性和数据缺失等问题，现有机器学习模型虽有高预测性能，但关键特征在数据不完整时波动大，影响模型的可重复性和决策可靠性，因此亟需一种能保证特征稳定性和可解释性的框架。

Method: CACTUS结合特征抽象、可解释分类与系统性特征稳定性分析，通过模拟不同缺失水平下的数据扰动，评估特征重要性在数据质量下降时的保持能力，并进行多维度比较验证。

Result: 在真实世界血尿队列中，CACTUS在预测性能上达到或超过主流方法，同时显著提升了特征稳定性，尤其在引入缺失数据后仍能保持关键特征的稳健性；性别分层分析也证实其优势。特征稳定性为模型可信度提供额外判断依据。

Conclusion: CACTUS通过量化对缺失数据的鲁棒性并优先选择可解释且稳定的特征，提供了一个通用、可信的数据驱动决策支持框架，适用于高风险生物医学场景。

Abstract: Machine learning models are increasingly applied to biomedical data, yet their adoption in high stakes domains remains limited by poor robustness, limited interpretability, and instability of learned features under realistic data perturbations, such as missingness. In particular, models that achieve high predictive performance may still fail to inspire trust if their key features fluctuate when data completeness changes, undermining reproducibility and downstream decision-making. Here, we present CACTUS (Comprehensive Abstraction and Classification Tool for Uncovering Structures), an explainable machine learning framework explicitly designed to address these challenges in small, heterogeneous, and incomplete clinical datasets. CACTUS integrates feature abstraction, interpretable classification, and systematic feature stability analysis to quantify how consistently informative features are preserved as data quality degrades. Using a real-world haematuria cohort comprising 568 patients evaluated for bladder cancer, we benchmark CACTUS against widely used machine learning approaches, including random forests and gradient boosting methods, under controlled levels of randomly introduced missing data. We demonstrate that CACTUS achieves competitive or superior predictive performance while maintaining markedly higher stability of top-ranked features as missingness increases, including in sex-stratified analyses. Our results show that feature stability provides information complementary to conventional performance metrics and is essential for assessing the trustworthiness of machine learning models applied to biomedical data. By explicitly quantifying robustness to missing data and prioritising interpretable, stable features, CACTUS offers a generalizable framework for trustworthy data-driven decision support.

</details>


### [177] [Variational Grey-Box Dynamics Matching](https://arxiv.org/abs/2602.17477)
*Gurjeet Sangra Singh,Frantzeska Lavda,Giangiacomo Mercatali,Alexandros Kalousis*

Main category: cs.LG

TL;DR: 本文提出一种新颖的灰箱方法，将不完整的物理模型直接融入生成模型中，通过观测轨迹学习动态系统，无需真实物理参数，在无需仿真情况下避免神经微分方程的可扩展性和稳定性问题。核心是基于流匹配框架构建结构化变分分布，使用两个潜在编码：一个用于建模缺失的随机性和多模态速度，另一个将物理参数作为具有物理信息先验的潜在变量。该方法还拓展至二阶动力学系统。在典型常微分方程/偏微分方程问题上的实验表明，该方法性能优于或相当于纯数据驱动方法和先前灰箱基线，同时保持物理模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型如流匹配和扩散模型虽能学习复杂分布与动力系统，但常为黑箱，忽略底层物理；而基于常微分方程/偏微分方程的物理模拟模型虽具可解释性，却可能缺少或未知某些项，无法完全描述现实观测。因此需要一种结合物理模型与数据驱动优势的灰箱方法。

Method: 提出一种基于流匹配框架的结构化变分分布建模方法，引入两个潜在编码：其一建模缺失的随机性和多模态速度，其二以物理信息先验编码物理参数；实现无需仿真、仅从观测轨迹学习动态系统的方法，并拓展至二阶动力学系统。

Result: 在多个典型常微分方程/偏微分方程问题上，所提方法性能达到或优于纯数据驱动方法及先前灰箱基线，同时保留了物理模型的可解释性。

Conclusion: 本文提出的灰箱生成方法成功融合不完整物理模型与数据驱动学习，实现了高精度建模与良好可解释性，为复杂系统建模提供了新范式。

Abstract: Deep generative models such as flow matching and diffusion models have shown great potential in learning complex distributions and dynamical systems, but often act as black-boxes, neglecting underlying physics. In contrast, physics-based simulation models described by ODEs/PDEs remain interpretable, but may have missing or unknown terms, unable to fully describe real-world observations. We bridge this gap with a novel grey-box method that integrates incomplete physics models directly into generative models. Our approach learns dynamics from observational trajectories alone, without ground-truth physics parameters, in a simulation-free manner that avoids scalability and stability issues of Neural ODEs. The core of our method lies in modelling a structured variational distribution within the flow matching framework, by using two latent encodings: one to model the missing stochasticity and multi-modal velocity, and a second to encode physics parameters as a latent variable with a physics-informed prior. Furthermore, we present an adaptation of the framework to handle second-order dynamics. Our experiments on representative ODE/PDE problems show that our method performs on par with or superior to fully data-driven approaches and previous grey-box baselines, while preserving the interpretability of the physics model. Our code is available at https://github.com/DMML-Geneva/VGB-DM.

</details>


### [178] [Linear Convergence in Games with Delayed Feedback via Extra Prediction](https://arxiv.org/abs/2602.17486)
*Yuma Fujimoto,Kenshi Abe,Kaito Ariu*

Main category: cs.LG

TL;DR: 该论文研究了在存在反馈延迟的多智能体学习中，加权乐观梯度下降-上升（WOGDA）算法在无约束双线性博弈中的线性收敛速率。通过将WOGDA解释为一种对更远未来奖励进行更新的额外近端点（EPP）方法，论文证明了标准乐观性（预测下一步奖励）可实现以$\exp(-Θ(t/m^{5}))$的速率收敛；而采用额外乐观性（预测更远未来奖励）则能容忍更大的步长并显著提升收敛速率至$\exp(-Θ(t/(m^{2}\log m)))$。实验结果与理论分析一致，验证了额外乐观性是缓解反馈延迟导致性能下降的有效策略。


<details>
  <summary>Details</summary>
Motivation: 反馈延迟在现实多智能体学习中不可避免，且已知会严重降低算法性能，但其对收敛速率的影响，尤其是在双线性博弈中的表现仍不明确。因此，亟需理解如何通过算法设计来应对延迟带来的负面影响。

Method: 将WOGDA视为对额外近端点（EPP）的近似，其中更新基于比经典近端点（PP）更远的未来奖励。通过理论分析建立收敛速率的下界，并结合实验验证理论结果。

Result: 标准乐观性下收敛速率为$\exp(-Θ(t/m^{5}))$，而引入额外乐观性后，收敛速率提升至$\exp(-Θ(t/(m^{2}\log m)))$，且实验结果支持理论推导。

Conclusion: 额外乐观性是一种有效应对反馈延迟、提升多智能体学习收敛速度和稳定性的关键机制，具有重要的理论与实践意义。

Abstract: Feedback delays are inevitable in real-world multi-agent learning. They are known to severely degrade performance, and the convergence rate under delayed feedback is still unclear, even for bilinear games. This paper derives the rate of linear convergence of Weighted Optimistic Gradient Descent-Ascent (WOGDA), which predicts future rewards with extra optimism, in unconstrained bilinear games. To analyze the algorithm, we interpret it as an approximation of the Extra Proximal Point (EPP), which is updated based on farther future rewards than the classical Proximal Point (PP). Our theorems show that standard optimism (predicting the next-step reward) achieves linear convergence to the equilibrium at a rate $\exp(-Θ(t/m^{5}))$ after $t$ iterations for delay $m$. Moreover, employing extra optimism (predicting farther future reward) tolerates a larger step size and significantly accelerates the rate to $\exp(-Θ(t/(m^{2}\log m)))$. Our experiments also show accelerated convergence driven by the extra optimism and are qualitatively consistent with our theorems. In summary, this paper validates that extra optimism is a promising countermeasure against performance degradation caused by feedback delays.

</details>


### [179] [Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models](https://arxiv.org/abs/2602.17497)
*Wen-Tse Chen,Jiayu Chen,Fahim Tajwar,Hao Zhu,Xintong Duan,Ruslan Salakhutdinov,Jeff Schneider*

Main category: cs.LG

TL;DR: 本文提出一种基于大语言模型（LLM）的回顾性上下文学习（RICL）方法，用于将稀疏奖励转化为密集训练信号（即优势函数），并设计了在线学习框架RICOL，通过迭代优化策略实现高效学习。实验表明，RICL在少量样本下能准确估计优势函数，并有效识别关键状态；在四个BabyAI任务中，RICOL相比传统在线强化学习算法具有更高的样本效率且收敛性能相当。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定任务的价值函数进行时间信用分配，存在样本效率低和泛化能力差的问题。为解决这一挑战，需寻找更高效、通用的信用分配机制。

Method: 利用预训练的大语言模型通过回顾性上下文学习（RICL）生成密集的监督信号，构建优势函数；结合在线学习框架RICOL，持续优化策略。

Result: RICL能在有限样本下准确估计优势函数，有效识别环境中的关键状态；RICOL在多个BabyAI任务中表现出与传统RL算法相当的收敛性能，但样本效率显著提升。

Conclusion: 本研究验证了利用大语言模型进行时间信用分配的可行性与有效性，为实现更高样本效率和更强泛化能力的强化学习范式提供了新思路。

Abstract: Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on learning task-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pretrained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states in the environment for temporal credit assignment. Extended evaluation on four BabyAI scenarios show that RICOL achieves comparable convergent performance with traditional online RL algorithms with significantly higher sample efficiency. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms.

</details>


### [180] [LORA-CRAFT: Cross-layer Rank Adaptation via Frozen Tucker Decomposition of Pre-trained Attention Weights](https://arxiv.org/abs/2602.17510)
*Kasun Dewage,Marianna Pensky,Suranadi De Silva,Shankadeep Mondal*

Main category: cs.LG

TL;DR: CRAFT 是一种参数高效的微调方法，通过在跨层注意力权重张量上应用 Tucker 分解，冻结所有分解因子，并仅训练小型可学习的适配矩阵来实现高效微调。相比现有方法，它结合了跨层结构与低秩分解的优势，在 GLUE 基准上表现优异，且仅需 41K 个适配参数，与模型大小和层数无关。


<details>
  <summary>Details</summary>
Motivation: 现有基于张量的参数高效微调方法要么在层间共享因子，要么对梯度更新进行分解，或独立处理每层权重。这些方法在跨层信息利用和参数效率之间存在权衡。本文旨在提出一种能同时捕获跨层结构并保持极低参数开销的新方法。

Method: 将预训练模型中多层注意力权重组织为三维张量，使用高阶奇异值分解（HOSVD）进行完整的 Tucker 分解，冻结所有分解出的因子矩阵，并仅训练少量轻量级适配矩阵作用于各因子，实现跨层参数高效微调。

Result: 在 RoBERTa-base 和 RoBERTa-large 上的 GLUE 实验表明，CRAFT 在性能上与现有方法相当，但仅需 41K 个适配参数，且该数量不随模型维度或深度变化，表现出极强的参数效率。

Conclusion: CRAFT 成功融合了跨层结构建模与低秩分解优势，实现了高性能、高效率的参数高效微调，为大规模模型轻量化提供了新思路。

Abstract: We introduce CRAFT (Cross-layer Rank Adaptation via Frozen Tucker), a parameter-efficient fine-tuning (PEFT) method that applies Tucker tensor decomposition to pre-trained attention weight matrices stacked across transformer layers and trains only small square adaptation matrices on the resulting frozen Tucker factors. Existing tensor-based PEFT methods decompose gradient updates: LoTR applies Tucker decomposition with shared factor matrices, while SuperLoRA groups and reshapes $ΔW$ across layers before applying Tucker decomposition. Separately, methods like PiSSA apply SVD to pre-trained weights but operate independently per layer. CRAFT bridges these two lines of work: it performs full Tucker decomposition via Higher-Order SVD (HOSVD) directly on pre-trained weights organized as cross-layer 3D tensors, freezes all resulting factors, and adapts the model through lightweight trainable transformations applied to each factor matrix. Experiments on the GLUE benchmark using RoBERTa-base and RoBERTa-large demonstrate that CRAFT achieves competitive performance with existing methods while requiring only 41K Tucker adaptation parameters--a count independent of model dimension and depth at fixed Tucker ranks.

</details>


### [181] [Variational inference via radial transport](https://arxiv.org/abs/2602.17525)
*Luca Ghafourpour,Sinho Chewi,Alessio Figalli,Aram-Alexandre Pooladian*

Main category: cs.LG

TL;DR: 提出了一种名为radVI的新变分推断方法，通过优化分布的径向轮廓来改进传统高斯近似在实际应用中的表现，该方法可作为现有变分推断方案（如均值场VI和拉普拉斯近似）的低成本有效补充，并具备基于Wasserstein空间优化的理论收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 传统变分推断中使用的高斯分布可能无法准确捕捉目标分布的径向结构，导致覆盖效果差；本文旨在通过优化径向轮廓提升近似质量。

Method: 从Wasserstein空间优化的角度出发，利用径向传输映射的正则性（受Caffarelli, 2000启发），设计了radVI算法，用于在不改变原有框架的前提下改进径向结构建模。

Result: radVI在多个实验中表现出更优的分布近似能力，尤其在复杂径向结构下优于标准高斯或均值场近似；且具有理论收敛性保障。

Conclusion: radVI是一种高效、通用且理论可靠的变分推断改进方法，适用于多种现有框架，显著提升了对复杂分布径向特征的建模能力。

Abstract: In variational inference (VI), the practitioner approximates a high-dimensional distribution $π$ with a simple surrogate one, often a (product) Gaussian distribution. However, in many cases of practical interest, Gaussian distributions might not capture the correct radial profile of $π$, resulting in poor coverage. In this work, we approach the VI problem from the perspective of optimizing over these radial profiles. Our algorithm radVI is a cheap, effective add-on to many existing VI schemes, such as Gaussian (mean-field) VI and Laplace approximation. We provide theoretical convergence guarantees for our algorithm, owing to recent developments in optimization over the Wasserstein space--the space of probability distributions endowed with the Wasserstein distance--and new regularity properties of radial transport maps in the style of Caffarelli (2000).

</details>


### [182] [Position: Evaluation of ECG Representations Must Be Fixed](https://arxiv.org/abs/2602.17531)
*Zachary Berger,Daniel Prakah-Asante,John Guttag,Collin M. Stultz*

Main category: cs.LG

TL;DR: 本文指出当前12导联心电图（ECG）表示学习的基准测试实践需改进，以确保研究进展可靠且符合临床目标。现有基准（如PTB-XL、CPSC2018、CSN）主要关注心律失常和波形形态标签，但忽略了ECG蕴含的更广泛临床信息。作者主张扩展下游评估范围，纳入结构性心脏病和患者层面预测等新目标，并提出多标签、不平衡场景下的评估最佳实践。实证研究表明，随机初始化编码器配合线性评估在多数任务上已达到或超过当前最先进的预训练模型表现，因此建议将其作为合理基线。通过在六个不同评估场景下对三种代表性预训练方法的实验验证，支持了上述观点。


<details>
  <summary>Details</summary>
Motivation: 当前12导联心电图表示学习的基准测试过于集中于心律失常和波形形态标签，未能反映ECG所包含的更广泛临床信息，导致研究进展可能偏离真实临床需求。此外，现有评估方法在处理多标签、不平衡数据时存在缺陷，影响对模型性能的准确判断。

Method: 提出并应用多标签、不平衡数据环境下的评估最佳实践；对比分析三种代表性预训练方法在六种不同评估设置下的表现，包括标准基准、结构性心脏病数据集、血流动力学推断及患者预测任务；引入随机初始化编码器作为基线进行比较。

Result: 当采用更全面的评估框架时，文献中关于最优表示学习方法的结论发生改变；随机初始化编码器在多数任务上的表现与先进预训练模型相当甚至更优，表明其可作为合理的基线模型。

Conclusion: 必须改革当前的基准测试实践，扩大评估范围至结构性心脏病和患者层面预测等临床相关目标，并采用科学的评估方法，以确保表示学习研究真正推动临床价值实现。

Abstract: This position paper argues that current benchmarking practice in 12-lead ECG representation learning must be fixed to ensure progress is reliable and aligned with clinically meaningful objectives. The field has largely converged on three public multi-label benchmarks (PTB-XL, CPSC2018, CSN) dominated by arrhythmia and waveform-morphology labels, even though the ECG is known to encode substantially broader clinical information. We argue that downstream evaluation should expand to include an assessment of structural heart disease and patient-level forecasting, in addition to other evolving ECG-related endpoints, as relevant clinical targets. Next, we outline evaluation best practices for multi-label, imbalanced settings, and show that when they are applied, the literature's current conclusion about which representations perform best is altered. Furthermore, we demonstrate the surprising result that a randomly initialized encoder with linear evaluation matches state-of-the-art pre-training on many tasks. This motivates the use of a random encoder as a reasonable baseline model. We substantiate our observations with an empirical evaluation of three representative ECG pre-training approaches across six evaluation settings: the three standard benchmarks, a structural disease dataset, hemodynamic inference, and patient forecasting.

</details>


### [183] [MASPO: Unifying Gradient Utilization, Probability Mass, and Signal Reliability for Robust and Sample-Efficient LLM Reasoning](https://arxiv.org/abs/2602.17550)
*Xiaoliang Fu,Jiaye Lin,Yangyi Fang,Binbin Zheng,Chaowen Hu,Zekai Shao,Cong Qin,Lu Pan,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 本文提出了一种名为MASPO的新方法，用于解决现有强化学习中可验证奖励（RLVR）算法在大语言模型优化中的三个核心问题：梯度利用效率低、概率质量敏感性差以及信号可靠性不对称。MASPO通过引入可微的软高斯门控、自适应质量限制器和非对称风险控制器，实现了梯度最大化、探索平衡与更新置信度的协同优化。实验表明，该方法显著优于现有基线，具备良好的鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖于僵化、均匀且对称的信任区域机制，无法有效适配大语言模型复杂的优化动态，导致梯度利用不足、概率分布响应迟钝及正负样本信用分配失衡。

Method: 提出MASPO框架，包含三个组件：(1) 可微软高斯门控以提升梯度利用率；(2) 质量自适应限制器以均衡概率空间中的探索；(3) 非对称风险控制器以根据信号置信度调整更新幅度。

Result: 在多项评估中，MASPO显著超越强基线，展现出卓越的性能与稳定性，验证了其作为统一、高效RLVR解决方案的有效性。

Conclusion: MASPO成功弥合了现有RLVR方法在梯度利用、概率分布响应与信号可靠性方面的关键差距，为大语言模型的强化学习提供了一个统一、自适应且高效的优化框架。

Abstract: Existing Reinforcement Learning with Verifiable Rewards (RLVR) algorithms, such as GRPO, rely on rigid, uniform, and symmetric trust region mechanisms that are fundamentally misaligned with the complex optimization dynamics of Large Language Models (LLMs). In this paper, we identify three critical challenges in these methods: (1) inefficient gradient utilization caused by the binary cutoff of hard clipping, (2) insensitive probability mass arising from uniform ratio constraints that ignore the token distribution, and (3) asymmetric signal reliability stemming from the disparate credit assignment ambiguity between positive and negative samples. To bridge these gaps, we propose Mass-Adaptive Soft Policy Optimization (MASPO), a unified framework designed to harmonize these three dimensions. MASPO integrates a differentiable soft Gaussian gating to maximize gradient utility, a mass-adaptive limiter to balance exploration across the probability spectrum, and an asymmetric risk controller to align update magnitudes with signal confidence. Extensive evaluations demonstrate that MASPO serves as a robust, all-in-one RLVR solution, significantly outperforming strong baselines. Our code is available at: https://anonymous.4open.science/r/ma1/README.md.

</details>


### [184] [A Theoretical Framework for Modular Learning of Robust Generative Models](https://arxiv.org/abs/2602.17554)
*Corinna Cortes,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出一种模块化生成建模的理论框架，通过门控机制组合预训练的领域特定专家模型，以实现与单体模型相当的性能，并在任意数据混合下保持鲁棒性，避免启发式调参。研究证明了鲁棒门控函数的存在性，展示了模块化作为强正则化器的作用，并理论证明其优于重新训练聚合数据的模型。同时提出了可扩展的随机原始-对偶算法和结构蒸馏方法，实验证明该方法能有效缓解梯度冲突并优于单体基线。


<details>
  <summary>Details</summary>
Motivation: 当前大规模生成模型训练资源消耗大，依赖启发式数据集加权；本文旨在解决如何通过模块化组合小型领域专家模型来达到与单体模型相当的性能，并在任意数据混合下实现鲁棒性，消除对启发式调参的依赖。

Method: 提出一个理论框架，定义归一化门控函数空间 $G_{1}$，将问题建模为最小最大博弈，寻找最小化对最坏情况数据混合偏差的鲁棒门控。利用Kakutani不动点定理证明鲁棒门控的存在性，并分析模块化带来的正则化效应。进一步提出基于随机原始-对偶算法的可扩展优化方法和结构蒸馏推理技术。

Result: 理论证明了模块化方法在任意数据混合下的鲁棒性，且其泛化误差与轻量门控复杂度相关；理论上可超越重新训练聚合数据的模型，差距由Jensen-Shannon散度刻画。实验在合成和真实数据集上验证了该方法能有效缓解梯度冲突，并显著优于单体基线模型。

Conclusion: 模块化生成建模不仅能够实现与单体模型相当的性能，还能在各种数据分布下保持鲁棒性，减少对人工调参的依赖。该方法具有坚实的理论基础和良好的实际表现，为高效、可扩展的大规模生成模型训练提供了新思路。

Abstract: Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.

</details>


### [185] [Revisiting Weight Regularization for Low-Rank Continual Learning](https://arxiv.org/abs/2602.17559)
*Yaoyue Zheng,Yin Zhang,Joost van de Weijer,Gido M van de Ven,Shaoyi Du,Xuetao Zhang,Zhiqiang Tian*

Main category: cs.LG

TL;DR: 本文提出EWC-LoRA，一种基于低秩表示的参数高效持续学习方法，通过在低秩空间中使用EWC正则化共享更新以缓解任务干扰，保持存储和推理成本恒定，实验证明其在多个基准上优于现有方法，展现了权重正则化在低秩参数化下仍具有效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效持续学习（PECL）方法多依赖任务特定模块缓解任务干扰，而权重正则化技术如EWC在该范式中尚未被充分探索，因此需要重新审视其在低秩持续学习中的作用。

Method: 提出EWC-LoRA，利用低秩表示估计全维度参数重要性，并通过EWC对共享低秩更新进行正则化，从而在不增加存储与计算开销的前提下缓解任务干扰。

Result: 在多个基准测试中，EWC-LoRA均取得优于现有低秩持续学习方法的稳定性-可塑性权衡，验证了低秩参数化下权重正则化的有效性。

Conclusion: 即使在低秩参数化下，权重正则化仍是缓解持续学习中任务干扰的有效机制；EWC-LoRA提供了一种高效、实用的解决方案，为未来正则化技术在参数高效持续学习中的应用提供了新思路。

Abstract: Continual Learning (CL) with large-scale pre-trained models (PTMs) has recently gained wide attention, shifting the focus from training from scratch to continually adapting PTMs. This has given rise to a promising paradigm: parameter-efficient continual learning (PECL), where task interference is typically mitigated by assigning a task-specific module during training, such as low-rank adapters. However, weight regularization techniques, such as Elastic Weight Consolidation (EWC)-a key strategy in CL-remain underexplored in this new paradigm. In this paper, we revisit weight regularization in low-rank CL as a new perspective for mitigating task interference in PECL. Unlike existing low-rank CL methods, we mitigate task interference by regularizing a shared low-rank update through EWC, thereby keeping the storage requirement and inference costs constant regardless of the number of tasks. Our proposed method EWC-LoRA leverages a low-rank representation to estimate parameter importance over the full-dimensional space. This design offers a practical, computational- and memory-efficient solution for CL with PTMs, and provides insights that may inform the broader application of regularization techniques within PECL. Extensive experiments on various benchmarks demonstrate the effectiveness of EWC-LoRA, achieving a stability-plasticity trade-off superior to existing low-rank CL approaches. These results indicate that, even under low-rank parameterizations, weight regularization remains an effective mechanism for mitigating task interference. Code is available at: https://github.com/yaoyz96/low-rank-cl.

</details>


### [186] [Be Wary of Your Time Series Preprocessing](https://arxiv.org/abs/2602.17568)
*Sofiane Ennadir,Tianze Wang,Oleg Smirnov,Sahar Asadi,Lele Cao*

Main category: cs.LG

TL;DR: 本文首次从理论角度分析了时间序列建模中不同归一化策略（如实例归一化和全局缩放）对基于Transformer模型表达能力的影响。作者提出了一个针对时间序列的新型表达能力框架，用于量化模型在表示空间中区分相似与不相似输入的能力，并推导出标准缩放和最小-最大缩放两种常用方法的理论边界。分析表明，归一化策略的选择显著影响模型的表征能力，具体取决于任务和数据特征。实验验证显示，没有一种归一化方法始终优于其他方法，某些情况下完全省略归一化反而表现更优。研究强调了预处理在时间序列学习中的关键作用，并呼吁开发更符合特定任务和数据集的规范化策略。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列建模中的归一化和缩放作为基本预处理步骤，其在基于Transformer的模型中的理论影响尚未得到充分探索。现有实践多依赖经验性选择，缺乏系统性理论支持，导致归一化策略选择不透明且可能影响模型性能。因此，亟需建立理论框架以理解归一化如何影响模型表达能力，从而指导更合理的预处理设计。

Method: 提出一个专为时间序列设计的表达能力分析框架，通过量化模型在表示空间中区分相似与不相似输入的能力来评估其表达能力；基于该框架，对标准缩放和最小-最大缩放进行理论推导，获得其表达能力的上下界；结合多个Transformer模型在分类与预测基准上的实证实验，验证理论发现。

Result: 理论分析表明，归一化策略显著影响模型的表征能力，其效果依赖于任务类型和数据特性；实验结果显示，不存在普遍最优的归一化方法，某些场景下不进行归一化反而表现更好。

Conclusion: 归一化在时间序列学习中具有关键作用，但其影响具有情境依赖性；当前的通用归一化方法可能并非最优，应发展更具针对性的、任务导向的预处理策略。

Abstract: Normalization and scaling are fundamental preprocessing steps in time series modeling, yet their role in Transformer-based models remains underexplored from a theoretical perspective. In this work, we present the first formal analysis of how different normalization strategies, specifically instance-based and global scaling, impact the expressivity of Transformer-based architectures for time series representation learning. We propose a novel expressivity framework tailored to time series, which quantifies a model's ability to distinguish between similar and dissimilar inputs in the representation space. Using this framework, we derive theoretical bounds for two widely used normalization methods: Standard and Min-Max scaling. Our analysis reveals that the choice of normalization strategy can significantly influence the model's representational capacity, depending on the task and data characteristics. We complement our theory with empirical validation on classification and forecasting benchmarks using multiple Transformer-based models. Our results show that no single normalization method consistently outperforms others, and in some cases, omitting normalization entirely leads to superior performance. These findings highlight the critical role of preprocessing in time series learning and motivate the need for more principled normalization strategies tailored to specific tasks and datasets.

</details>


### [187] [Canonicalizing Multimodal Contrastive Representation Learning](https://arxiv.org/abs/2602.17584)
*Sharut Gupta,Sanyam Kansal,Stefanie Jegelka,Phillip Isola,Vikas Garg*

Main category: cs.LG

TL;DR: 该研究探讨了在不同数据分布和架构下独立训练的多模态对比模型之间表示空间的系统性几何关系。研究发现，尽管模型不同，其嵌入空间之间存在近似正交映射（orthogonal map）关系，即存在一个正交矩阵 $Q$ 使得 $\widetilde{f}(x) \approx Q f(x)$ 和 $\widetilde{g}(y) \approx Q g(y)$ 同时成立，且该 $Q$ 可同时对齐图像与文本编码器。理论上证明，若两个模型在小锚点集上的多模态核一致，则必然存在单一正交映射。这一发现支持后向兼容的模型升级，避免重新嵌入的开销，并对学习表征的隐私性具有重要影响。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在规模扩大时，即使独立训练，也常产生相似的语义表示。但仅匹配相似性不足以建立表示空间间的明确对应，尤其对于图像-文本耦合的一致性要求更高。因此需要理解不同模型间是否存在可系统化的几何关系，以实现跨模型对齐与迁移。

Method: 通过分析 CLIP、SigLIP、FLAVA 等多种模型家族，比较其图像与文本编码器输出的嵌入空间，使用统计方法与几何建模识别跨模型的变换关系。理论推导基于多模态核一致性假设，证明当 $\langle f(x), g(y)\rangle \\approx \\langle \\widetilde{f}(x), \\widetilde{g}(y)\\rangle$ 时，存在统一的正交映射 $Q$ 同时作用于图像与文本空间。

Result: 实验表明，多个模型之间的嵌入空间可通过一个共享的正交矩阵 $Q$ 实现高精度对齐，且该映射同时适用于图像和文本编码器。理论分析进一步验证：在多模态核一致的前提下，唯一可能的变换是正交映射。

Conclusion: 不同训练条件下的多模态对比模型之间存在统一的正交几何关系，该关系可跨模态共享，为模型升级、迁移与隐私保护提供了新路径。

Abstract: As models and data scale, independently trained networks often induce analogous notions of similarity. But, matching similarities is weaker than establishing an explicit correspondence between the representation spaces, especially for multimodal models, where consistency must hold not only within each modality, but also for the learned image-text coupling. We therefore ask: given two independently trained multimodal contrastive models (with encoders $(f, g)$ and $(\widetilde{f},\widetilde{g})$) -- trained on different distributions and with different architectures -- does a systematic geometric relationship exist between their embedding spaces? If so, what form does it take, and does it hold uniformly across modalities? In this work, we show that across model families such as CLIP, SigLIP, and FLAVA, this geometric relationship is well approximated by an orthogonal map (up to a global mean shift), i.e., there exists an orthogonal map $Q$ where $Q^\top Q = I$ such that $\widetilde{f}(x)\approx Q f(x)$ for paired images $x$. Strikingly, the same $Q$ simultaneously aligns the text encoders i.e., $\widetilde{g}(y)\approx Q g(y)$ for texts $y$. Theoretically, we prove that if the multimodal kernel agrees across models on a small anchor set i.e. $\langle f(x), g(y)\rangle \approx \langle \widetilde{f}(x), \widetilde{g}(y)\rangle$, then the two models must be related by a single orthogonal map $Q$ and the same $Q$ maps images and text across models. More broadly, this finding enables backward-compatible model upgrades, avoiding costly re-embedding, and has implications for the privacy of learned representations.
  Our project page: https://canonical-multimodal.github.io/

</details>


### [188] [Towards Anytime-Valid Statistical Watermarking](https://arxiv.org/abs/2602.17608)
*Baihe Huang,Eric Xu,Kannan Ramchandran,Jiantao Jiao,Michael I. Jordan*

Main category: cs.LG

TL;DR: 本文提出了一种基于e-value的水印框架Anchored E-Watermarking，解决了现有方法在采样分布选择和固定时域假设检验方面的局限性。该框架通过构建检测过程的测试超鞅，实现了任意时间有效的推断，支持合法的早期停止，并在理论上优化了采样策略与期望停止时间。实验表明，该方法可将检测所需的平均标记预算减少13-15%。


<details>
  <summary>Details</summary>
Motivation: 现有统计水印方法存在两个关键缺陷：缺乏对采样分布的合理选择机制，以及依赖固定时域假设检验，无法实现有效早期停止。这限制了其在实际应用中的效率和灵活性。

Method: 提出Anchored E-Watermarking框架，利用锚定分布近似目标模型，构建测试超鞅以实现任意时间有效的检测推断；通过最大化最坏情况下的对数增长率，推导出最优e-value及最优期望停止时间。

Result: 在多个基准测试中，该框架显著提升了样本效率，相较于当前最先进的基线方法，平均所需标记数量减少了13-15%。

Conclusion: Anchored E-Watermarking首次实现了采样优化与任意时间推断的统一，为高效、可靠的机器生成文本检测提供了理论坚实且实用的新范式。

Abstract: The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.

</details>


### [189] [Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning](https://arxiv.org/abs/2602.17614)
*Obaidullah Zaland,Sajib Mistry,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 提出KD-UFSL方法，通过微聚类和差分隐私技术保护联邦分裂学习中的中间表示数据，有效降低隐私泄露风险，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 在大数据场景下，联邦学习虽能实现去中心化训练，但客户端计算负担重；且传统方法中传输的中间表示可能暴露私有数据，因此需要增强隐私保护机制。

Method: 采用k-匿名与差分隐私结合的微聚类技术对中间表示进行处理，防止数据重建攻击，提升安全性。

Result: 实验表明，该方法使图像重建的均方误差最高增加50%，结构相似性降低40%，显著提升隐私性而不牺牲全局模型的实用性。

Conclusion: KD-UFSL在保障隐私的同时维持了模型的有效性，适用于大规模、高隐私要求的大数据应用场景。

Abstract: Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients' side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients' private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.

</details>


### [190] [Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs](https://arxiv.org/abs/2602.17616)
*Luke Huang,Zhuoyang Zhang,Qinghao Hu,Shang Yang,Song Han*

Main category: cs.LG

TL;DR: 提出VCPO方法，通过基于有效样本量缩放学习率和使用闭式最小方差基线，显著降低异步REINFORCE/GRPO算法中的策略梯度方差，提升异步强化学习在数学、通用推理和工具使用任务上的鲁棒性，实现2.5倍加速且性能媲美同步训练。


<details>
  <summary>Details</summary>
Motivation: 异步强化学习虽能提升吞吐量，但高异步性导致策略梯度估计方差显著增大，尤其在无价值函数的REINFORCE/GRPO方法中，过时轨迹引发重尾重要性比率，使少数样本主导更新，造成梯度噪声大、训练不稳定。有效样本量（ESS）和梯度范数不稳可预测训练崩溃。

Method: 提出VCPO：(i) 根据有效样本量动态调整学习率以抑制不可靠更新；(ii) 采用闭式最小方差基线处理离策略情形，无需辅助值模型，开销极小。

Result: VCPO在数学、通用推理和工具使用任务上显著提升异步训练的稳定性与性能，优于多种基线方法（如掩码/裁剪稳定器、算法变体），实现长上下文、多轮训练时间减少2.5倍，同时保持与同步训练相当的性能。

Conclusion: 显式控制策略梯度方差是实现大规模可靠异步强化学习的关键。

Abstract: Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.

</details>


### [191] [Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning](https://arxiv.org/abs/2602.17625)
*Obaidullah Zaland,Zulfiqar Ahmad Khan,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文提出了一种名为One-Shot Incremental Federated Learning (OSI-FL)的新型联邦学习框架，旨在解决通信开销大和灾难性遗忘的问题。该方法通过冻结的视觉语言模型（VLM）在客户端生成类别特定的嵌入，并在单轮通信中传输至服务器；服务器利用预训练的扩散模型基于这些嵌入合成与客户端数据分布相似的新样本用于全局模型训练。为应对增量任务带来的持续重训练问题及由此引发的遗忘现象，引入选择性样本保留机制（SSR），通过保留每类-任务对中损失最高的前p个最具信息量的样本，确保代表性样本在后续训练中被持续使用，从而缓解遗忘。实验表明，OSI-FL在三个基准数据集上的类别增量和领域增量场景中均优于传统及一次性联邦学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架在处理大规模、异构且地理分散的数据流时面临通信成本高和难以适应增量数据的挑战。尤其当数据以增量方式持续到达时，传统方法需多轮通信和反复训练，易导致灾难性遗忘。因此，亟需一种低通信开销、支持一次性更新并能有效缓解遗忘的增量联邦学习机制。

Method: OSI-FL采用单轮通信机制，客户端使用冻结的视觉语言模型提取类别特定嵌入并上传至服务器；服务器端利用预训练扩散模型根据这些嵌入生成模拟数据，用于全局模型训练。同时引入选择性样本保留（SSR）策略，基于样本损失筛选并保留每个类别-任务组合中最具有代表性的样本，以维持历史知识，防止遗忘。

Result: 在多个类别增量和领域增量设置下，OSI-FL在三个标准数据集上显著优于多种基线方法，包括传统的多轮联邦学习和现有的一次性联邦学习方案，验证了其在降低通信开销、提升模型泛化能力与缓解灾难性遗忘方面的有效性。

Conclusion: OSI-FL是首个实现单轮通信下高效增量学习并有效抑制灾难性遗忘的联邦学习框架，通过结合数据合成与选择性样本保留，在隐私保护前提下实现了快速、稳健的模型更新，为未来动态、分布式学习系统提供了可行解决方案。

Abstract: Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.

</details>


### [192] [When to Trust the Cheap Check: Weak and Strong Verification for Reasoning](https://arxiv.org/abs/2602.17633)
*Shayan Kiyani,Sima Noorani,George Pappas,Hamed Hassani*

Main category: cs.LG

TL;DR: 本文研究大语言模型（LLM）推理中的验证机制，区分了低成本但不准确的弱验证（如自一致性检查）与高成本但可靠的强验证（如用户反馈）。提出弱-强验证策略框架，通过双阈值结构优化决策，控制误接受和误拒绝错误，并设计了一种无需假设查询流、模型或弱验证器的在线算法。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型推理中，如何在效率与可靠性之间权衡验证过程是一个关键挑战。弱验证快速但不可靠，而强验证可靠但昂贵。需要一种机制来智能地决定何时使用弱验证，何时升级到强验证。

Method: 提出弱-强验证策略框架，形式化验证决策问题；定义衡量误接受、误拒绝和强验证频率的指标；证明最优策略具有双阈值结构；开发一种无需先验假设的在线学习算法以动态控制错误率。

Result: 理论证明最优验证策略具有两阈值结构，且校准性和锐度决定弱验证器的价值；所提出的在线算法能有效控制接受与拒绝错误，适用于任意查询流和模型配置。

Conclusion: 该研究为大语言模型推理中的验证过程提供了可扩展、鲁棒且理论保障的决策框架，推动了高效可信推理系统的发展。

Abstract: Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.

</details>


### [193] [Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting](https://arxiv.org/abs/2602.17634)
*Xinghong Fu,Yanhong Li,Georgios Papaioannou,Yoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种简单有效的配方，用于训练高效的时间序列基础模型，能够在零样本时间序列预测任务中实现高性能的同时显著减小模型规模。研究表明，大规模Transformer并非必需，小型混合模型（如结合长卷积与线性RNN层的DeltaNet）在性能上可媲美大型Transformer模型，但体积却小一百倍以上。通过引入数据增强和推理策略，进一步提升了性能。最终提出了Reverso系列模型，显著推动了时间序列基础模型在性能与效率之间的权衡边界。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型虽然性能优异，但因参数量巨大导致计算成本高、部署困难。为提升实用性，亟需开发更高效、更轻量的模型，同时保持良好的泛化能力。

Method: 采用小型混合架构，结合长卷积与线性RNN（特别是DeltaNet）层；设计数据增强策略与推理优化方法；通过系统实验验证模型在零样本预测任务中的表现。

Result: 所提出的模型在多个时间序列数据集上达到或接近大型Transformer模型的性能，但参数量减少超过100倍，推理速度更快，资源消耗更低。

Conclusion: 通过合理的模型结构设计与训练策略，可以构建出既高效又强大的时间序列基础模型，有效突破传统大模型在效率与性能间的平衡瓶颈。

Abstract: Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.

</details>


### [194] [A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning](https://arxiv.org/abs/2602.17642)
*Dhruv Talwar,Harsh Desai,Wendong Yin,Goutam Mohanty,Rafael Reveles*

Main category: cs.LG

TL;DR: A.R.I.S. 是一种低成本、便携式电子废弃物分拣系统，采用 YOLOx 模型实现实时分类金属、塑料和电路板，显著提升分拣精度与效率，实验表明其整体精确率达90%，mAP为82.2%，分拣纯度达84%。该系统通过融合深度学习与传统分拣技术，有效解决电子回收中材料分离不充分导致的资源损失问题，推动可持续回收发展。


<details>
  <summary>Details</summary>
Motivation: 传统电子废弃物回收过程因材料分离与识别能力不足，造成严重资源损失，亟需高效、低成本的分拣技术以提升回收效率。

Method: 基于YOLOx模型构建实时图像识别系统，对破碎电子废弃物中的金属、塑料和电路板进行自动分类，并集成于便携式分拣设备中，实现高效低延迟检测。

Result: 实验结果显示系统整体精度达90%，平均精度（mAP）为82.2%，分拣纯度达到84%，显著优于传统方法。

Conclusion: A.R.I.S. 通过深度学习与现有分拣技术的结合，有效提升了电子废弃物回收中的材料识别与分离效率，降低先进回收技术的采用门槛，助力循环经济与环境可持续发展。

Abstract: Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.

</details>


### [195] [Multi-Round Human-AI Collaboration with User-Specified Requirements](https://arxiv.org/abs/2602.17646)
*Sima Noorani,Shayan Kiyani,Hamed Hassani,George Pappas*

Main category: cs.LG

TL;DR: 本文提出了一种以人为本的多轮对话AI协作框架，通过用户定义的规则形式化‘反事实伤害’和‘互补性’原则，确保AI不会削弱人类优势并在人类易错处提供价值。研究设计了一个在线、无需分布假设的算法，具有有限样本保证，能在非平稳交互动态下保持预设的约束水平。在医疗诊断模拟和图像推理众包实验中验证了该框架的有效性：调整约束可预测地影响人类决策质量，证明这两个原则是提升协作决策质量的实用杠杆，且无需建模或约束人类行为。


<details>
  <summary>Details</summary>
Motivation: 随着人类在高风险决策中越来越依赖多轮对话式AI，需要有原则性的框架来确保互动能可靠地提升决策质量。现有方法往往忽视人类优势或未充分考虑人机协作中的互补性，因此亟需一种以人类为中心的、可定制的协作保障机制。

Method: 采用用户定义规则的形式化方法，将反事实伤害与互补性概念具体化；提出一种在线、分布无关的算法，具备有限样本保证，用于在协作过程中强制执行用户设定的约束条件。

Result: 在医疗诊断任务的LLM模拟协作和图像推理的人类众包实验中，所提算法在非平稳动态下仍能维持预定的反事实伤害与互补性违规率。调节约束条件可预测地改变人类准确率，表明两个原则是有效控制协作质量的实用工具。

Conclusion: 该框架为多轮人机协作提供了可解释、可调控的保障机制，通过用户自定义的反事实伤害与互补性约束，实现了对协作过程的有效干预，显著提升了高风险场景下的决策质量，且无需对人类行为进行建模或施加限制。

Abstract: As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.

</details>


### [196] [MARS: Margin-Aware Reward-Modeling with Self-Refinement](https://arxiv.org/abs/2602.17658)
*Payel Bhattacharjee,Osvaldo Simeone,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文提出MARS，一种自适应、边缘感知的增强与采样策略，专注于奖励模型估计中的模糊和失败模式。MARS将增强集中在低边缘（模糊）偏好对上，以提高奖励模型的不确定性区域的训练质量，并通过硬样本增强迭代优化训练分布。理论证明该策略提升了损失函数的平均曲率，增强了信息量并改善了条件性，实验结果表明其在鲁棒奖励建模方面优于均匀增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法通常在表示或语义层面操作，且不考虑奖励模型的估计难度，导致无法有效提升模型在模糊或困难样本上的表现。因此，需要一种更智能的增强策略来聚焦于奖励模型最不确定的区域，从而提高奖励模型的可靠性。

Method: 提出MARS框架，通过识别低边际（即高不确定性）的偏好对进行重点增强，采用自适应采样策略，在迭代过程中不断优化训练数据分布，特别强调对难样本的增强，从而提升模型性能。

Result: 理论分析表明，该策略能增加损失函数的平均曲率，从而提升信息量和训练条件；实验结果显示，MARS在多种设置下持续优于均匀增强方法，显著提升了奖励模型的鲁棒性与准确性。

Conclusion: MARS是一种高效且自适应的数据增强策略，能够针对性地改进奖励模型在模糊和困难样本上的表现，为现代对齐流水线提供了更强的训练支持。

Abstract: Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.

</details>
