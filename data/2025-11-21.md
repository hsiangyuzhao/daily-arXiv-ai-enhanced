<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.CL](#cs.CL) [Total: 23]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UniFit: Towards Universal Virtual Try-on with MLLM-Guided Semantic Alignment](https://arxiv.org/abs/2511.15831)
*Wei Zhang,Yeying Jin,Xin Li,Yan Zhang,Xiaofeng Cong,Cong Wang,Fengcai Qiao,zhichao Lian*

Main category: cs.CV

TL;DR: UniFit提出一个由多模态大语言模型（MLLM）驱动的通用虚拟试穿框架，通过引入MLLM引导的语义对齐模块（MGSA）解决文本指令与参考图像之间的语义鸿沟问题，并采用两阶段渐进式训练策略结合自合成管道，在数据稀缺的复杂场景下仍能有效学习复杂任务。实验表明，该方法在多种虚拟试穿任务中表现优异，达到当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在处理多样化和复杂任务时存在语义鸿沟和数据稀缺问题，尤其在基于文本指令的多任务框架中表现受限，亟需一种更通用、鲁棒且高效的学习机制来提升跨模态理解与生成能力。

Method: 提出UniFit框架，核心包括：1）MLLM-Guided Semantic Alignment Module（MGSA），利用多模态大语言模型与可学习查询融合文本与图像输入，通过语义对齐损失建模跨模态关系；2）两阶段渐进式训练策略配合自合成数据生成管道，缓解复杂任务下的数据不足问题。

Result: UniFit在多种虚拟试穿任务上均取得领先性能，涵盖多衣物试穿、模特间换装等复杂场景，具备良好的泛化能力和灵活性，支持开放域任务执行。

Conclusion: UniFit通过融合多模态大语言模型与创新的语义对齐机制，有效解决了虚拟试穿中的语义鸿沟与数据稀缺问题，构建了一个真正通用、灵活且高性能的虚拟试穿框架，为未来个性化服装生成提供了新范式。

Abstract: Image-based virtual try-on (VTON) aims to synthesize photorealistic images of a person wearing specified garments. Despite significant progress, building a universal VTON framework that can flexibly handle diverse and complex tasks remains a major challenge. Recent methods explore multi-task VTON frameworks guided by textual instructions, yet they still face two key limitations: (1) semantic gap between text instructions and reference images, and (2) data scarcity in complex scenarios. To address these challenges, we propose UniFit, a universal VTON framework driven by a Multimodal Large Language Model (MLLM). Specifically, we introduce an MLLM-Guided Semantic Alignment Module (MGSA), which integrates multimodal inputs using an MLLM and a set of learnable queries. By imposing a semantic alignment loss, MGSA captures cross-modal semantic relationships and provides coherent and explicit semantic guidance for the generative process, thereby reducing the semantic gap. Moreover, by devising a two-stage progressive training strategy with a self-synthesis pipeline, UniFit is able to learn complex tasks from limited data. Extensive experiments show that UniFit not only supports a wide range of VTON tasks, including multi-garment and model-to-model try-on, but also achieves state-of-the-art performance. The source code and pretrained models are available at https://github.com/zwplus/UniFit.

</details>


### [2] [EfficientSAM3: Progressive Hierarchical Distillation for Video Concept Segmentation from SAM1, 2, and 3](https://arxiv.org/abs/2511.15833)
*Chengxi Zeng,Yuxuan Jiang,Aaron Zhang*

Main category: cs.CV

TL;DR: EfficientSAM3 introduces a family of lightweight models via Progressive Hierarchical Distillation (PHD) to enable on-device visual understanding with promptable concept segmentation and tracking, achieving strong performance-efficiency balance.


<details>
  <summary>Details</summary>
Motivation: The original SAM3 model, while powerful, is too resource-intensive for on-device deployment due to its unified architecture. There's a need for efficient yet accurate models that preserve the concept-level segmentation and tracking capabilities of SAM3 in constrained environments.

Method: Progressive Hierarchical Distillation (PHD) consists of three stages: (1) Encoder Distillation using prompt-in-the-loop training on SA-1B to align image features; (2) Temporal Memory Distillation replacing dense memory with a compact Perceiver-based module trained on SA-V for efficient spatiotemporal feature compression and retrieval; and (3) End-to-End Fine-Tuning on official SAM3 PCS data to refine the full pipeline and maintain high fidelity.

Result: EfficientSAM3 achieves strong performance-efficiency trade-offs across popular VOS datasets, enabling on-device concept segmentation and tracking with RepViT, TinyViT, and EfficientViT backbones while closely mimicking the behavior of the teacher model.

Conclusion: EfficientSAM3 successfully enables on-device visual understanding through progressive distillation, offering a scalable and efficient solution for real-time, concept-aware segmentation and tracking without sacrificing accuracy.

Abstract: The Segment Anything Model 3 (SAM3) advances visual understanding with Promptable Concept Segmentation (PCS) across images and videos, but its unified architecture (shared vision backbone, DETR-style detector, dense-memory tracker) remains prohibitive for on-device use. We present EfficientSAM3, a family of efficient models built on Progressive Hierarchical Distillation (PHD) that transfers capability from SAM3 to lightweight students in three stages: (1) Encoder Distillation aligns image features via prompt-in-the-loop training on SA-1B; (2) Temporal Memory Distillation replaces dense memory with a compact Perceiver-based module trained on SA-V to compress and retrieve spatiotemporal features efficiently; and (3) End-to-End Fine-Tuning refines the full pipeline on the official SAM3 PCS data to preserve concept-level performance. PHD yields a spectrum of student variants using RepViT, TinyViT, and EfficientViT backbones, enabling on-device concept segmentation and tracking while maintaining high fidelity to teacher behavior. We benchmark on popular VOS datasets, and compare with varies of releated work, achieing strong performance-efficiency trade-offs.

</details>


### [3] [Box6D : Zero-shot Category-level 6D Pose Estimation of Warehouse Boxes](https://arxiv.org/abs/2511.15884)
*Yintao Ma,Sajjad Pakdamansavoji,Amir Rasouli,Tongtong Cao*

Main category: cs.CV

TL;DR: Box6d是一种针对仓库环境中存储箱的类别级6D姿态估计方法，利用单个RGB-D观测通过快速二分搜索推断箱子尺寸，并使用类别级CAD模板进行姿态估计。结合基于深度的合理性过滤和提前停止策略，有效降低计算成本，在真实仓储场景和公开基准测试中表现出色，精度媲美或优于现有方法，推理时间减少约76%。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法在复杂环境（如遮挡、杂乱）下存在局限：基于模型的方法依赖高精度CAD模型且泛化能力差；无模型方法灵活性高但鲁棒性不足；类别级方法虽平衡了灵活性与精度，却常忽略具体环境和物体先验，实用性受限。因此需要一种兼顾精度、效率与实际应用能力的新方法。

Method: Box6d通过单个RGB-D图像进行输入，采用快速二分搜索估算盒子的三维尺寸，结合类别级CAD模板进行姿态估计。引入基于深度信息的合理性验证机制和早期停止策略，剔除不合理假设以提升效率。

Result: 在真实仓储场景和公共数据集上的实验表明，Box6d在保持竞争性甚至更优的6D姿态估计精度的同时，推理时间降低了约76%，显著提升了实时性与实用性。

Conclusion: Box6d为仓库自动化中的新型物体6D姿态估计提供了一种高效、准确且实用的解决方案，特别适用于存储箱这类具有结构规律性的工业对象，具备良好的部署前景。

Abstract: Accurate and efficient 6D pose estimation of novel objects under clutter and occlusion is critical for robotic manipulation across warehouse automation, bin picking, logistics, and e-commerce fulfillment. There are three main approaches in this domain; Model-based methods assume an exact CAD model at inference but require high-resolution meshes and transfer poorly to new environments; Model-free methods that rely on a few reference images or videos are more flexible, however often fail under challenging conditions; Category-level approaches aim to balance flexibility and accuracy but many are overly general and ignore environment and object priors, limiting their practicality in industrial settings.
  To this end, we propose Box6d, a category-level 6D pose estimation method tailored for storage boxes in the warehouse context. From a single RGB-D observation, Box6D infers the dimensions of the boxes via a fast binary search and estimates poses using a category CAD template rather than instance-specific models. Suing a depth-based plausibility filter and early-stopping strategy, Box6D then rejects implausible hypotheses, lowering computational cost. We conduct evaluations on real-world storage scenarios and public benchmarks, and show that our approach delivers competitive or superior 6D pose precision while reducing inference time by approximately 76%.

</details>


### [4] [RB-FT: Rationale-Bootstrapped Fine-Tuning for Video Classification](https://arxiv.org/abs/2511.15923)
*Meilong Xu,Di Fu,Jiaxing Zhang,Gong Yu,Jiayu Zheng,Xiaoling Hu,Dongdi Zhao,Feiyang Li,Chao Chen,Yong Cao*

Main category: cs.CV

TL;DR: 提出一种两阶段自提升范式，通过自生成的推理过程来弥补视觉语言模型在领域特定视频分类中的语义鸿沟，无需新标注数据。第一阶段引导模型生成详细文本推理以捕捉领域逻辑；第二阶段基于这些推理进行微调，再进行常规监督微调，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在领域特定视频分类任务中表现不佳，尤其在数据稀缺情况下，主要因复杂时空内容与抽象标签间存在语义鸿沟。

Method: 两阶段自提升：1）利用提示让模型生成视频的详细文本推理；2）基于自生成推理对模型进行微调，再进行标准监督微调。

Result: 在多个数据集上实验表明，该方法显著优于直接监督微调，证明自生成推理是高效、低成本的领域适应策略。

Conclusion: 自生成推理能有效缩小视觉语言模型在领域特定视频分析中的语义差距，是一种无需额外标注的高效适配方法。

Abstract: Vision Language Models (VLMs) are becoming increasingly integral to multimedia understanding; however, they often struggle with domain-specific video classification tasks, particularly in cases with limited data. This stems from a critical \textit{rationale gap}, where sparse domain data is insufficient to bridge the semantic distance between complex spatio-temporal content and abstract classification labels. We propose a two-stage self-improvement paradigm to bridge this gap without new annotations. First, we prompt the VLMs to generate detailed textual rationales for each video, compelling them to articulate the domain-specific logic. The VLM is then fine-tuned on these self-generated rationales, utilizing this intermediate supervision to align its representations with the nuances of the target domain. Second, conventional supervised fine-tuning (SFT) is performed on the task labels, achieving markedly higher effectiveness as a result of the model's pre-acquired domain reasoning. Extensive experiments on diverse datasets demonstrate that our method significantly outperforms direct SFT, validating self-generated rationale as an effective, annotation-efficient paradigm for adapting VLMs to domain-specific video analysis.

</details>


### [5] [Boosting Medical Visual Understanding From Multi-Granular Language Learning](https://arxiv.org/abs/2511.15943)
*Zihan Li,Yiqing Wang,Sina Farsiu,Paul Kinahan*

Main category: cs.CV

TL;DR: MGLL提出了一种多粒度语言学习框架，通过结构化多标签监督、跨粒度文本描述整合和软标签监督，提升视觉-语言模型在医学影像等复杂领域中的多标签与跨粒度对齐能力。该方法采用平滑KL散度保证粒度间一致性，并作为即插即用模块提升效率，在多个数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: CLIP等现有方法主要关注单标签、单粒度对齐，难以适应医学影像中多标签、多粒度的复杂标注需求，因此需要一种能同时处理多标签和跨粒度对齐的新方法。

Method: MGLL引入结构化多标签监督，整合不同粒度的文本描述，采用软标签与点对点约束增强对齐，并利用平滑Kullback-Leibler散度确保跨粒度一致性，以实现高效且灵活的多粒度对齐。

Result: MGLL在多个下游任务中表现优于现有最先进方法，尤其在多标签和跨粒度场景下显著提升性能，且可作为即插即用模块集成到现有视觉-语言模型中。

Conclusion: MGLL有效解决了多标签与跨粒度对齐问题，为复杂领域如医学影像的视觉-语言理解提供了更强大的预训练框架。

Abstract: Recent advances in image-text pretraining have significantly enhanced visual understanding by aligning visual and textual representations. Contrastive Language-Image Pretraining (CLIP) has played a pivotal role in multimodal learning. However, its focus on single-label, single-granularity alignment limits its effectiveness in complex domains such as medical imaging, where images often correspond to multiple high-level labels (e.g., disease categories) across different annotation granularities (e.g., diagnostic description, clinical explanation). To address this, we propose Multi-Granular Language Learning (MGLL), a contrastive learning framework designed to improve both multi-label and cross-granularity alignment. MGLL leverages structured multi-label supervision, integrates textual descriptions across granularities, and introduces soft-label supervision with point-wise constraints to enhance alignment. MGLL employs smooth Kullback-Leibler (KL) divergence to ensure cross-granularity consistency while maintaining computational efficiency as a plug-and-play module for vision-language models. Pretrained on our constructed large-scale multi-granular datasets and evaluated across multiple datasets, MGLL outperforms other state-of-the-art methods in downstream tasks. The code is available at \href{https://github.com/HUANGLIZI/MGLL}{https://github.com/HUANGLIZI/MGLL}.

</details>


### [6] [Automated Interpretable 2D Video Extraction from 3D Echocardiography](https://arxiv.org/abs/2511.15946)
*Milos Vukadinovic,Hirotaka Ieki,Yuki Sahasi,David Ouyang,Bryan He*

Main category: cs.CV

TL;DR: 本文提出一种自动化方法，从3D心脏超声体积数据中提取标准2D视图，结合深度学习分类器与基于解剖标志的启发式规则，实现快速、准确的视图重建。该方法在双医院1600个视频上经三位心脏病专家盲评，准确率达96%；所生成的2D视频在检测心脏异常和生成临床级测量值方面表现优异，保持了空间校准与诊断特征，支持临床实际应用。代码与数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 传统心脏超声依赖一系列2D视频，难以全面反映心脏复杂三维结构；尽管3D超声图像质量已满足临床需求，但医生仍习惯于2D解读方式。因此亟需一种将3D数据自动转换为标准2D视图的方法，以兼顾3D扫描的速度优势与医生熟悉的分析流程。

Method: 采用深度学习视图分类器识别3D超声中的关键视角，并结合解剖地标与心脏病专家提供的启发式规则，重构标准2D视图。通过多模态验证确保重建视图的准确性与诊断可用性。

Result: 在1,600个视频上实现96%的视图识别准确率；生成的2D视频可被AI模型（EchoPrime、PanEcho）有效用于异常检测，并能生成符合临床标准的心脏解剖测量值（EchoNet-Measurement），证明其保留了空间校准与诊断信息。

Conclusion: 本方法成功实现了从3D超声体积到标准2D视图的自动化转换，既保留了3D扫描的高效性，又兼容现有临床工作流程，具备良好的临床转化潜力。相关代码与数据集已公开，推动领域发展。

Abstract: Although the heart has complex three-dimensional (3D) anatomy, conventional medical imaging with cardiac ultrasound relies on a series of 2D videos showing individual cardiac structures. 3D echocardiography is a developing modality that now offers adequate image quality for clinical use, with potential to streamline acquisition and improve assessment of off-axis features. We propose an automated method to select standard 2D views from 3D cardiac ultrasound volumes, allowing physicians to interpret the data in their usual format while benefiting from the speed and usability of 3D scanning. Applying a deep learning view classifier and downstream heuristics based on anatomical landmarks together with heuristics provided by cardiologists, we reconstruct standard echocardiography views. This approach was validated by three cardiologists in blinded evaluation (96\% accuracy in 1,600 videos from 2 hospitals). The downstream 2D videos were also validated in their ability to detect cardiac abnormalities using AI echocardiography models (EchoPrime and PanEcho) as well as ability to generate clinical-grade measurements of cardiac anatomy (EchoNet-Measurement). We demonstrated that the extracted 2D videos preserve spatial calibration and diagnostic features, allowing clinicians to obtain accurate real-world interpretations from 3D volumes. We release the code and a dataset of 29 3D echocardiography videos https://github.com/echonet/3d-echo .

</details>


### [7] [Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948)
*Raphael Ruschel,Hardikkumar Prajapati,Awsafur Rahman,B. S. Manjunath*

Main category: cs.CV

TL;DR: Click2Graph是首个交互式全景视频场景图生成框架，结合用户提示与时空语义理解，仅需一次点击或框选即可实现跨时间的主体分割、跟踪、关系发现及场景图生成。其核心创新包括动态交互发现模块和联合实体-谓词分类头，在OpenPVSG基准上表现优异，推动了可控且可解释的视频场景理解发展。


<details>
  <summary>Details</summary>
Motivation: 现有视频场景图生成系统为封闭式前馈流程，无法融入人类指导；而提示性分割模型（如SAM2）缺乏语义与关系推理能力。因此需要一种能融合用户交互与复杂视觉理解的框架。

Method: 提出Click2Graph框架，包含动态交互发现模块（生成受主体条件影响的对象提示）与语义分类头（联合进行实体与谓词推理），通过单次用户输入（点击/框选）实现主体追踪、互动对象发现与三元组预测。

Result: 在OpenPVSG基准上，Click2Graph展现出优越性能，验证了人机交互与全景定位、关系推理融合的有效性，建立了用户引导式全景视频场景图生成的新基线。

Conclusion: Click2Graph首次实现了交互式全景视频场景图生成，通过用户提示驱动的时空语义理解，显著提升了视频场景理解的可控性与可解释性，为未来人机协同视觉理解提供了新范式。

Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts <subject, object, predicate> triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.

</details>


### [8] [InfoCLIP: Bridging Vision-Language Pretraining and Open-Vocabulary Semantic Segmentation via Information-Theoretic Alignment Transfer](https://arxiv.org/abs/2511.15967)
*Muyao Yuan,Yuanhong Zhang,Weizhan Zhang,Lan Ma,Yuan Gao,Jiangyong Ying,Yudeng Xin*

Main category: cs.CV

TL;DR: InfoCLIP提出一种基于信息论的方法，通过两个新颖的互信息目标来稳定微调过程中的模态对齐，有效缓解了现有方法在有限类别上微调CLIP时的过拟合问题，并提升了开放词汇语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有限类别上微调CLIP进行语义分割时容易导致过拟合并破坏预训练模型的视觉-语言对齐能力，因此需要一种更稳定的对齐迁移机制。

Method: InfoCLIP从信息论视角出发，设计了两个基于互信息的目标：一是压缩预训练CLIP中像素-文本的模态对齐以减少由粗粒度局部语义表示带来的噪声；二是最大化预训练模型与微调模型之间对齐知识的互信息，实现适合分割任务的紧凑局部语义关系迁移。

Result: 在多个基准测试上的大量实验验证了InfoCLIP的有效性，其在开放词汇语义分割任务中表现出更强的适应性和优越性，尤其在不对称迁移场景下表现突出。

Conclusion: InfoCLIP通过信息论指导的对齐知识迁移，显著提升了CLIP在开放词汇语义分割任务中微调的稳定性与性能，为视觉-语言模型在分割任务中的应用提供了新思路。

Abstract: Recently, the strong generalization ability of CLIP has facilitated open-vocabulary semantic segmentation, which labels pixels using arbitrary text. However, existing methods that fine-tune CLIP for segmentation on limited seen categories often lead to overfitting and degrade the pretrained vision-language alignment. To stabilize modality alignment during fine-tuning, we propose InfoCLIP, which leverages an information-theoretic perspective to transfer alignment knowledge from pretrained CLIP to the segmentation task. Specifically, this transfer is guided by two novel objectives grounded in mutual information. First, we compress the pixel-text modality alignment from pretrained CLIP to reduce noise arising from its coarse-grained local semantic representations learned under image-text supervision. Second, we maximize the mutual information between the alignment knowledge of pretrained CLIP and the fine-tuned model to transfer compact local semantic relations suited for the segmentation task. Extensive evaluations across various benchmarks validate the effectiveness of InfoCLIP in enhancing CLIP fine-tuning for open-vocabulary semantic segmentation, demonstrating its adaptability and superiority in asymmetric transfer.

</details>


### [9] [Externally Validated Multi-Task Learning via Consistency Regularization Using Differentiable BI-RADS Features for Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2511.15968)
*Jingru Zhang,Saed Moradi,Ashirbani Saha*

Main category: cs.CV

TL;DR: 提出一种一致性正则化方法，通过可微分的BI-RADS启发形态特征缓解乳腺超声肿瘤分割与分类任务间的破坏性干扰，显著提升多任务学习在外部数据集上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习常因任务间破坏性干扰导致性能下降，限制模型泛化能力，尤其在乳腺超声肿瘤分割任务中表现不佳。

Method: 引入可微分的BI-RADS启发形态特征作为一致性正则化项，以缓解分割与分类任务之间的干扰，提升多任务学习的稳定性与泛化性。

Result: 在BrEaST、UDIAT、BUSI、BUS-UCLM四个数据集上验证，新方法在分割任务上显著优于基线模型（p<0.001），Dice系数分别提升至0.81、0.66、0.69，且在UDIAT上达到当前最优水平。

Conclusion: 所提出的多任务学习框架通过一致性正则化有效缓解了任务间干扰，显著提升了乳腺超声图像分割的泛化性能，具有临床应用潜力。

Abstract: Multi-task learning can suffer from destructive task interference, where jointly trained models underperform single-task baselines and limit generalization. To improve generalization performance in breast ultrasound-based tumor segmentation via multi-task learning, we propose a novel consistency regularization approach that mitigates destructive interference between segmentation and classification. The consistency regularization approach is composed of differentiable BI-RADS-inspired morphological features. We validated this approach by training all models on the BrEaST dataset (Poland) and evaluating them on three external datasets: UDIAT (Spain), BUSI (Egypt), and BUS-UCLM (Spain). Our comprehensive analysis demonstrates statistically significant (p<0.001) improvements in generalization for segmentation task of the proposed multi-task approach vs. the baseline one: UDIAT, BUSI, BUS-UCLM (Dice coefficient=0.81 vs 0.59, 0.66 vs 0.56, 0.69 vs 0.49, resp.). The proposed approach also achieves state-of-the-art segmentation performance under rigorous external validation on the UDIAT dataset.

</details>


### [10] [UniDGF: A Unified Detection-to-Generation Framework for Hierarchical Object Visual Recognition](https://arxiv.org/abs/2511.15984)
*Xinyu Nan,Lingtao Mao,Huangyu Dai,Zexin Zheng,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li*

Main category: cs.CV

TL;DR: 提出一种检测引导的生成框架，通过细粒度特征和BART生成器实现类别与属性的层次化语义生成，在电商场景下显著提升细粒度识别能力与统一推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局相似性，难以捕捉细粒度类别差异和类别特异性属性多样性，尤其在大规模电商场景中表现不足。

Method: 引入检测引导的生成框架，对每个检测到的物体提取精细的ROI特征，并使用基于BART的生成器以粗到细的方式生成涵盖类别层级和属性值对的语义标记，支持属性条件下的属性识别。

Result: 在大规模专有电商数据集和开源数据集上的实验表明，该方法显著优于现有的基于相似性的流水线和多阶段分类系统，实现了更强的细粒度识别和更连贯的统一推理。

Conclusion: 所提出的检测引导生成框架有效解决了视觉语义理解中的细粒度识别难题，适用于复杂电商场景下的统一语义理解任务。

Abstract: Achieving visual semantic understanding requires a unified framework that simultaneously handles object detection, category prediction, and attribute recognition. However, current advanced approaches rely on global similarity and struggle to capture fine-grained category distinctions and category-specific attribute diversity, especially in large-scale e-commerce scenarios. To overcome these challenges, we introduce a detection-guided generative framework that predicts hierarchical category and attribute tokens. For each detected object, we extract refined ROI-level features and employ a BART-based generator to produce semantic tokens in a coarse-to-fine sequence covering category hierarchies and property-value pairs, with support for property-conditioned attribute recognition. Experiments on both large-scale proprietary e-commerce datasets and open-source datasets demonstrate that our approach significantly outperforms existing similarity-based pipelines and multi-stage classification systems, achieving stronger fine-grained recognition and more coherent unified inference.

</details>


### [11] [Fairness in Multi-modal Medical Diagnosis with Demonstration Selection](https://arxiv.org/abs/2511.15986)
*Dawei Li,Zijian Gu,Peng Wang,Chuhan Song,Zhen Tan,Mohan Zhang,Tianlong Chen,Yu Tian,Song Wang*

Main category: cs.CV

TL;DR: 本文探讨了在医疗图像推理中使用上下文学习（ICL）来提升多模态大模型的公平性，提出了一种新的公平性感知示范选择方法（FADS），通过聚类采样构建人口统计上平衡且语义相关的示范样本，有效减少了性别、种族和族裔相关的偏差，同时保持了高准确性，为实现公平的医疗图像推理提供了一种高效、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的去偏方法通常依赖于大规模标注数据集或微调，对于基础规模的模型不切实际。因此需要一种轻量级、无需调优的方法来提升多模态大语言模型在医疗图像推理中的公平性。

Method: 提出公平性感知示范选择（FADS），利用聚类采样策略构建人口统计上平衡且语义相关的示范样本，以解决传统示范选择因人口统计不平衡导致的不公平问题。

Result: 在多个医疗影像基准测试中，FADS consistently reduces disparities related to gender, race, and ethnicity while maintaining high accuracy, demonstrating its effectiveness in enhancing fairness without sacrificing performance.

Conclusion: FADS 为实现公平的医疗图像推理提供了一种高效、可扩展且数据高效的解决方案，凸显了公平性感知的上下文学习在医疗人工智能中的巨大潜力。

Abstract: Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.

</details>


### [12] [Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution](https://arxiv.org/abs/2511.16024)
*Xiao He,Zhijun Tu,Kun Cheng,Mingrui Zhu,Jie Hu,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出Mixture-of-Ranks（MoR）架构，将LoRA中的每个秩视为独立专家，实现细粒度的专家划分，提升图像超分辨率中对复杂退化样本的适应能力。通过引入退化估计模块与降级感知负载均衡损失，动态调节专家激活数量，优化计算资源分配，在保持高效的同时增强模型泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像超分辨率方法依赖于密集的LoRA微调，难以自适应处理多样化的现实退化情况，且在相同计算预算下知识共享能力有限。因此需要更灵活、高效的架构来提升模型表现。

Method: 提出MoR架构，将LoRA的每个秩作为独立专家；设计细粒度专家划分策略，固定位置秩作为共享专家以保留通用特征；引入基于CLIP嵌入和正负文本对的退化估计模块，动态引导专家激活；结合零专家槽位与退化感知负载均衡损失，按退化程度动态调整活跃专家数。

Result: 实验表明，所提方法在单步图像超分辨率任务中达到领先性能，有效提升了对复杂退化图像的建模能力，并实现了更优的计算资源利用效率。

Conclusion: MoR架构通过稀疏专家机制与动态路由策略，显著增强了真实世界图像超分辨率模型的适应性与效率，为未来多专家模型在视觉任务中的应用提供了新思路。

Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.

</details>


### [13] [CuriGS: Curriculum-Guided Gaussian Splatting for Sparse View Synthesis](https://arxiv.org/abs/2511.16030)
*Zijian Wu,Mingfeng Jiang,Zidian Lin,Ying Song,Hanjie Ma,Qun Wu,Dongping Zhang,Guiyang Pu*

Main category: cs.CV

TL;DR: CuriGS 是一种基于课程学习的 3D 高斯点阵（3DGS）框架，用于稀疏视图下的 3D 重建。通过引入围绕真实视角（教师）生成的伪视图（学生视图），并采用渐进式扰动水平的训练策略，结合深度相关性与协同正则化，以及多信号质量评估，实现高质量稀疏视图重建。实验表明其在渲染保真度和几何一致性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下的 3DGS 面临监督信号不足和过拟合问题，尤其在视点覆盖有限时难以保证重建质量。因此需要一种有效机制增强训练数据多样性并提升泛化能力。

Method: 提出学生视图概念，即在真实视角周围生成不同扰动程度的伪视图；采用课程学习策略逐步开放更高扰动级别；通过深度相关性、协同正则化及多信号指标（SSIM、LPIPS、图像质量）对学生视图进行筛选与优化，并定期将优质视图加入训练集以扩充稀疏视图数据。

Result: 在多种合成与真实稀疏视图场景中，CuriGS 在渲染质量与几何一致性方面均超越当前最优基线方法，表现出更强的鲁棒性和重建性能。

Conclusion: CuriGS 通过设计合理的伪视图生成与筛选机制，有效缓解了稀疏视图下 3DGS 的过拟合与监督不足问题，显著提升了重建质量，为稀疏视图 3D 重建提供了新的有效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as an efficient, high-fidelity representation for real-time scene reconstruction and rendering. However, extending 3DGS to sparse-view settings remains challenging because of supervision scarcity and overfitting caused by limited viewpoint coverage. In this paper, we present CuriGS, a curriculum-guided framework for sparse-view 3D reconstruction using 3DGS. CuriGS addresses the core challenge of sparse-view synthesis by introducing student views: pseudo-views sampled around ground-truth poses (teacher). For each teacher, we generate multiple groups of student views with different perturbation levels. During training, we follow a curriculum schedule that gradually unlocks higher perturbation level, randomly sampling candidate students from the active level to assist training. Each sampled student is regularized via depth-correlation and co-regularization, and evaluated using a multi-signal metric that combines SSIM, LPIPS, and an image-quality measure. For every teacher and perturbation level, we periodically retain the best-performing students and promote those that satisfy a predefined quality threshold to the training set, resulting in a stable augmentation of sparse training views. Experimental results show that CuriGS outperforms state-of-the-art baselines in both rendering fidelity and geometric consistency across various synthetic and real sparse-view scenes. Project page: https://zijian1026.github.io/CuriGS/

</details>


### [14] [LLMs-based Augmentation for Domain Adaptation in Long-tailed Food Datasets](https://arxiv.org/abs/2511.16037)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim,Qianru Sun*

Main category: cs.CV

TL;DR: 本文提出一种基于大语言模型（LLM）的框架，用于解决食物识别中的域偏移、长尾分布和细微视觉差异等挑战。通过LLM解析图像生成食物名称和配料，将文本与跨域图像映射到共享嵌入空间以增强对齐，并利用多模态特征进行识别。该方法在两个食物数据集上优于针对长尾分布、域适应和细粒度分类设计的现有方法。


<details>
  <summary>Details</summary>
Motivation: 食物识别面临训练样本与真实场景图像之间的域偏移问题，且真实世界数据集常呈长尾分布，不同类别间存在细微视觉差异，导致识别困难。

Method: 利用大语言模型（LLM）从食物图像中提取食物标题和成分信息；将生成的文本与来自不同域的食物图像投影至共享嵌入空间，最大化模态间配对相似性；最终结合对齐后的多模态特征进行识别。

Result: 所提方法在两个食物识别数据集上表现优于专门针对长尾分布、域适应和细粒度分类优化的现有方法。

Conclusion: 基于大语言模型的多模态对齐框架能有效缓解食物识别中的域偏移、长尾分布和细粒度区分难题，具有较强的泛化能力。

Abstract: Training a model for food recognition is challenging because the training samples, which are typically crawled from the Internet, are visually different from the pictures captured by users in the free-living environment. In addition to this domain-shift problem, the real-world food datasets tend to be long-tailed distributed and some dishes of different categories exhibit subtle variations that are difficult to distinguish visually. In this paper, we present a framework empowered with large language models (LLMs) to address these challenges in food recognition. We first leverage LLMs to parse food images to generate food titles and ingredients. Then, we project the generated texts and food images from different domains to a shared embedding space to maximize the pair similarities. Finally, we take the aligned features of both modalities for recognition. With this simple framework, we show that our proposed approach can outperform the existing approaches tailored for long-tailed data distribution, domain adaptation, and fine-grained classification, respectively, on two food datasets.

</details>


### [15] [AMS-KV: Adaptive KV Caching in Multi-Scale Visual Autoregressive Transformers](https://arxiv.org/abs/2511.16047)
*Boxun Xu,Yu Wang,Zihu Wang,Peng Li*

Main category: cs.CV

TL;DR: 提出AMS-KV，一种针对基于下一尺度预测的视觉自回归模型（VAR）的自适应KV缓存策略，通过优化缓存使用，显著降低内存占用和计算延迟，提升大规模图像生成的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存技术在视觉自回归模型中尚未充分适配下一尺度预测任务，导致缓存内存随尺度增加而急剧膨胀，严重限制了模型的可扩展性。

Method: 基于对多尺度注意力机制的系统分析，提出AMS-KV策略：优先缓存粗粒度（浓缩尺度）和局部尺度的键值对，利用尺度间相似性识别高需求层并优化缓存分配。

Result: 相比基线模型，AMS-KV将KV缓存使用减少84.83%，自注意力延迟降低60.48%；在相同硬件条件下，使模型稳定支持双倍批量大小（从128增至256），提升吞吐量。

Conclusion: AMS-KV通过尺度自适应缓存设计，有效解决了下一尺度预测中KV缓存的可扩展性瓶颈，为大规模视觉生成提供了高效、稳定的解决方案。

Abstract: Visual autoregressive modeling (VAR) via next-scale prediction has emerged as a scalable image generation paradigm. While Key and Value (KV) caching in large language models (LLMs) has been extensively studied, next-scale prediction presents unique challenges, and KV caching design for next-scale based VAR transformers remains largely unexplored. A major bottleneck is the excessive KV memory growth with the increasing number of scales-severely limiting scalability. Our systematic investigation reveals that: (1) Attending to tokens from local scales significantly contributes to generation quality (2) Allocating a small amount of memory for the coarsest scales, termed as condensed scales, stabilizes multi-scale image generation (3) Strong KV similarity across finer scales is predominantly observed in cache-efficient layers, whereas cache-demanding layers exhibit weaker inter-scale similarity. Based on the observations, we introduce AMS-KV, a scale-adaptive KV caching policy for next-scale prediction in VAR models. AMS-KV prioritizes storing KVs from condensed and local scales, preserving the most relevant tokens to maintain generation quality. It further optimizes KV cache utilization and computational efficiency identifying cache-demanding layers through inter-scale similarity analysis. Compared to the vanilla next-scale prediction-based VAR models, AMS-KV reduces KV cache usage by up to 84.83% and self-attention latency by 60.48%. Moreover, when the baseline VAR-d30 model encounters out-of-memory failures at a batch size of 128, AMS-KV enables stable scaling to a batch size of 256 with improved throughput.

</details>


### [16] [LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving](https://arxiv.org/abs/2511.16049)
*Pei Liu,Songtao Wang,Lang Zhang,Xingyue Peng,Yuandong Lyu,Jiaxin Deng,Songxin Lu,Weiliang Ma,Xueyang Zhang,Yifei Zhan,XianPeng Lang,Jun Ma*

Main category: cs.CV

TL;DR: LiSTAR 是一种针对 4D LiDAR 数据生成的新型生成世界模型，通过混合圆柱-球面（HCS）表示减少量化误差，利用射线中心的时空注意力（START）建模点云随时间的演化以增强时序一致性，并提出基于 4D 点云对齐体素布局与离散掩码生成的 START 框架（MaskSTART），实现高效、高分辨率且受布局引导的可控生成。实验表明其在重建、预测和条件生成任务上均达到领先性能，显著降低生成差异（MMD 降低 76%）、提升重建交并比（IoU 提升 32%）、降低预测误差（L1 Med 降低 50%）。


<details>
  <summary>Details</summary>
Motivation: 4D LiDAR 数据合成在自动驾驶仿真中至关重要，但面临传感器球面几何、点云时间稀疏性以及动态场景复杂性的挑战，现有方法难以兼顾保真度、时序一致性和可控性。

Method: 提出 HCS 表示以保留原始几何结构；设计 START 架构，通过射线中心注意力捕捉时空特征演变；引入 4D 点云对齐体素布局与 MaskSTART 框架，实现基于场景布局的高效、可控制生成。

Result: 在 4D LiDAR 重建、预测与条件生成任务中均取得领先性能：生成 MMD 降低 76%，重建 IoU 提升 32%，预测 L1 Med 降低 50%。

Conclusion: LiSTAR 为构建真实、可控的自动驾驶仿真环境提供了强大新范式，具备高保真、强时序一致性与灵活可控性，具有广泛应用前景。

Abstract: Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.

</details>


### [17] [VideoSeg-R1:Reasoning Video Object Segmentation via Reinforcement Learning](https://arxiv.org/abs/2511.16077)
*Zishan Xu,Yifu Guo,Yuquan Lu,Fengyu Yang,Junxin Li*

Main category: cs.CV

TL;DR: VideoSeg-R1 是首个将强化学习引入视频推理分割的框架，采用解耦架构，通过分阶段处理实现更优的推理与分割性能。其包含三个阶段：文本引导的帧采样、显式推理链生成和基于SAM2与XMem的分割传播。通过任务难度感知机制自适应控制推理长度，提升效率与准确率。在多个基准上表现优于现有方法，代码将公开。


<details>
  <summary>Details</summary>
Motivation: 传统视频推理分割方法依赖监督微调，泛化能力差且缺乏显式推理过程，难以应对分布外场景。需要一种能模拟人类注意力并生成可解释推理链的新方法。

Method: 提出 VideoSeg-R1 框架，采用解耦架构，分为三阶段：(1) 层次化文本引导帧采样以模拟人类注意力；(2) 推理模型生成空间线索与显式推理链；(3) 利用 SAM2 和 XMem 实现分割与掩码传播。引入任务难度感知机制，动态调整推理长度以平衡效率与精度。

Result: 在多个复杂视频推理与分割基准上达到当前最优性能，验证了方法的有效性与泛化能力。

Conclusion: VideoSeg-R1 通过引入强化学习与显式推理机制，显著提升了视频推理分割的准确性与可解释性，为未来研究提供了新范式。

Abstract: Traditional video reasoning segmentation methods rely on supervised fine-tuning, which limits generalization to out-of-distribution scenarios and lacks explicit reasoning. To address this, we propose \textbf{VideoSeg-R1}, the first framework to introduce reinforcement learning into video reasoning segmentation. It adopts a decoupled architecture that formulates the task as joint referring image segmentation and video mask propagation. It comprises three stages: (1) A hierarchical text-guided frame sampler to emulate human attention; (2) A reasoning model that produces spatial cues along with explicit reasoning chains; and (3) A segmentation-propagation stage using SAM2 and XMem. A task difficulty-aware mechanism adaptively controls reasoning length for better efficiency and accuracy. Extensive evaluations on multiple benchmarks demonstrate that VideoSeg-R1 achieves state-of-the-art performance in complex video reasoning and segmentation tasks. The code will be publicly available at https://github.com/euyis1019/VideoSeg-R1.

</details>


### [18] [Rad-GS: Radar-Vision Integration for 3D Gaussian Splatting SLAM in Outdoor Environments](https://arxiv.org/abs/2511.16091)
*Renxiang Xiao,Wei Liu,Yuanfan Zhang,Yushuai Chen,Jinming Chen,Zilu Wang,Liang Hu*

Main category: cs.CV

TL;DR: Rad-GS 是一种用于千米级室外环境的4D雷达-相机SLAM系统，利用3D高斯作为可微分的空间表示。它结合原始雷达点云与多普勒信息，以及几何增强的点云来指导同步图像中的动态物体遮蔽，从而减轻渲染伪影并提高定位精度；同时利用非同步图像帧全局优化3D高斯表示，提升纹理一致性和新视角合成质量。通过全局八叉树结构与针对性的高斯原型管理策略，有效抑制噪声并显著降低大规模环境下的内存消耗。大量实验和消融研究证明，Rad-GS性能可媲美基于相机或激光雷达的传统3D高斯方法，验证了使用4D毫米波雷达实现稳健室外建图的可行性，并在真实世界中实现了千米级重建。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM系统在千米级室外环境中面临挑战，尤其在恶劣天气或低光照条件下，传统视觉或激光雷达方法受限。4D毫米波雷达具备全天候、远距离探测能力，但其数据稀疏性与动态干扰影响建图精度。因此，亟需一种高效融合雷达与视觉信息的新型方法，以实现大尺度、鲁棒、高保真度的场景重建。

Method: Rad-GS采用3D高斯作为可微分的空间表示，融合原始雷达点云与多普勒信息生成几何增强点云，用于动态物体掩码；通过同步图像与雷达数据对齐，引导动态区域剔除；利用非同步图像帧进行全局优化，提升3D高斯表示的纹理一致性与视图合成质量；引入全局八叉树结构与高斯管理策略，实现高效存储与噪声抑制。

Result: Rad-GS在多种室外场景下实现了接近传统相机/激光雷达3D高斯方法的定位精度与重建质量；在千米级真实场景中成功完成大尺度重建，验证了其在复杂户外环境中的可行性与鲁棒性；相比基准方法，在内存占用上显著降低，且对动态物体具有更强的抗干扰能力。

Conclusion: Rad-GS证明了4D毫米波雷达在大尺度室外环境建图中的潜力，通过融合雷达与视觉信息，实现了高精度、低内存消耗、鲁棒性强的4D SLAM系统，为未来智能交通、自动驾驶等应用提供了可靠感知基础。

Abstract: We present Rad-GS, a 4D radar-camera SLAM system designed for kilometer-scale outdoor environments, utilizing 3D Gaussian as a differentiable spatial representation. Rad-GS combines the advantages of raw radar point cloud with Doppler information and geometrically enhanced point cloud to guide dynamic object masking in synchronized images, thereby alleviating rendering artifacts and improving localization accuracy. Additionally, unsynchronized image frames are leveraged to globally refine the 3D Gaussian representation, enhancing texture consistency and novel view synthesis fidelity. Furthermore, the global octree structure coupled with a targeted Gaussian primitive management strategy further suppresses noise and significantly reduces memory consumption in large-scale environments. Extensive experiments and ablation studies demonstrate that Rad-GS achieves performance comparable to traditional 3D Gaussian methods based on camera or LiDAR inputs, highlighting the feasibility of robust outdoor mapping using 4D mmWave radar. Real-world reconstruction at kilometer scale validates the potential of Rad-GS for large-scale scene reconstruction.

</details>


### [19] [T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs](https://arxiv.org/abs/2511.16107)
*Shao-Jun Xia,Huixin Zhang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出T2T-VICL框架，探索视觉语言模型在跨任务视觉-上下文学习中的潜力。通过生成和选择最佳文本提示以隐式描述不同低层视觉任务间的差异，并构建首个跨任务视觉-上下文学习数据集。进一步设计结合感知评分与传统评估指标的推理框架，在九个跨任务场景中达到顶尖表现，十个额外场景中取得次优结果，显著拓展了视觉语言模型在跨任务学习中的边界。


<details>
  <summary>Details</summary>
Motivation: 探究当视觉提示与目标图像来自不同视觉任务时，视觉语言模型是否仍能实现有效的跨任务视觉-上下文学习，推动该领域的发展。

Method: 设计一种机制生成并选择最能描述两个不同低层视觉任务差异的文本提示；构建首个跨任务视觉-上下文学习数据集；提出结合感知评分与传统评估指标的新型推理框架。

Result: 在九个跨任务场景中达到顶尖性能，在十个额外场景中取得第二梯队表现，验证了跨任务视觉-上下文学习在视觉语言模型中的可行性与优越性。

Conclusion: T2T-VICL框架成功实现了跨任务视觉-上下文学习，显著提升了视觉语言模型在多样化任务中的泛化能力，为未来多任务统一学习提供了新范式。

Abstract: In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across nine cross-task scenarios and second-tier performance in ten additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.

</details>


### [20] [Clustered Error Correction with Grouped 4D Gaussian Splatting](https://arxiv.org/abs/2511.16112)
*Taeho Kang,Jaeyeon Park,Kyungjin Lee,Youngki Lee*

Main category: cs.CV

TL;DR: 本文提出一种新型4D高斯点阵方法，通过椭圆误差聚类与误差校正点阵添加，精准定位动态区域并优化点阵初始化；结合分组4D高斯点阵提升点阵与动态物体间的映射一致性。针对渲染误差分类为缺失颜色与遮挡类型，分别采用反投影或基于多视角颜色一致性的前景分割进行修正。在Neural 3D Video和Technicolor数据集上的实验表明，该方法显著提升时间一致性与感知渲染质量，在Technicolor Light Field数据集上PSNR提升0.39dB。可视化结果展示点阵与动态物体对齐更优，且误差校正方法能有效识别错误并正确初始化新点阵。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯点阵方法在动态场景重建中存在像素对应关系模糊、动态区域稀疏化不足等问题，导致重建精度和一致性差，亟需改进点阵初始化与动态区域建模能力。

Method: 提出椭圆误差聚类与误差校正点阵添加机制，用于识别和修复动态区域的渲染误差；引入分组4D高斯点阵策略，增强点阵与动态对象之间的映射一致性；通过跨视角颜色一致性判断，对缺失颜色和遮挡类误差分别采用反投影或前景分割进行针对性修正。

Result: 在Neural 3D Video和Technicolor数据集上验证了方法的有效性，显著提升时间一致性与渲染质量，在Technicolor Light Field数据集上实现0.39dB的PSNR提升；可视化结果表明点阵与动态物体对齐更准确，误差识别与新点阵初始化能力良好。

Conclusion: 所提方法有效解决了4D高斯点阵在动态场景中的重建难题，通过精准误差定位与分组建模策略，实现了高质量、高一致性的动态3D视频重建，具备实际应用潜力。

Abstract: Existing 4D Gaussian Splatting (4DGS) methods struggle to accurately reconstruct dynamic scenes, often failing to resolve ambiguous pixel correspondences and inadequate densification in dynamic regions. We address these issues by introducing a novel method composed of two key components: (1) Elliptical Error Clustering and Error Correcting Splat Addition that pinpoints dynamic areas to improve and initialize fitting splats, and (2) Grouped 4D Gaussian Splatting that improves consistency of mapping between splats and represented dynamic objects. Specifically, we classify rendering errors into missing-color and occlusion types, then apply targeted corrections via backprojection or foreground splitting guided by cross-view color consistency. Evaluations on Neural 3D Video and Technicolor datasets demonstrate that our approach significantly improves temporal consistency and achieves state-of-the-art perceptual rendering quality, improving 0.39dB of PSNR on the Technicolor Light Field dataset. Our visualization shows improved alignment between splats and dynamic objects, and the error correction method's capability to identify errors and properly initialize new splats. Our implementation details and source code are available at https://github.com/tho-kn/cem-4dgs.

</details>


### [21] [Decoupling Complexity from Scale in Latent Diffusion Model](https://arxiv.org/abs/2511.16117)
*Tianxiong Zhong,Xingye Tian,Xuebo Wang,Boyuan Jiang,Xin Tao,Pengfei Wan*

Main category: cs.CV

TL;DR: DCS-LDM 提出了一种新的视觉生成范式，将信息复杂度与尺度解耦，构建了层级化的、与尺度无关的潜在空间，通过多级令牌建模样本复杂度，并支持在固定潜在表示下任意分辨率和帧率的解码，实现灵活的计算-质量权衡。同时，通过跨层级分解结构与细节信息，支持从粗到细的渐进生成。实验表明，DCS-LDM 在性能上可媲美最先进方法，且具备跨多种尺度和视觉质量的灵活生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型通常将尺度与内容复杂度耦合，使用更多潜在标记表示更高分辨率图像或更高帧率视频，但潜在容量主要取决于内容复杂度，尺度仅作为上限。因此，提出解耦复杂度与尺度的需求。

Method: 提出 DCS-LDM，构建层次化、尺度无关的潜在空间，利用多级令牌建模内容复杂度，支持任意分辨率与帧率的解码，并实现从粗到细的渐进生成。

Result: 实验结果表明，DCS-LDM 在性能上达到或接近当前最优水平，同时在不同尺度和视觉质量间实现灵活生成，支持高效计算-质量权衡。

Conclusion: DCS-LDM 成功实现了信息复杂度与尺度的解耦，为视觉生成提供了更灵活、高效的潜在空间设计范式。

Abstract: Existing latent diffusion models typically couple scale with content complexity, using more latent tokens to represent higher-resolution images or higher-frame rate videos. However, the latent capacity required to represent visual data primarily depends on content complexity, with scale serving only as an upper bound. Motivated by this observation, we propose DCS-LDM, a novel paradigm for visual generation that decouples information complexity from scale. DCS-LDM constructs a hierarchical, scale-independent latent space that models sample complexity through multi-level tokens and supports decoding to arbitrary resolutions and frame rates within a fixed latent representation. This latent space enables DCS-LDM to achieve a flexible computation-quality tradeoff. Furthermore, by decomposing structural and detailed information across levels, DCS-LDM supports a progressive coarse-to-fine generation paradigm. Experimental results show that DCS-LDM delivers performance comparable to state-of-the-art methods while offering flexible generation across diverse scales and visual qualities.

</details>


### [22] [Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions](https://arxiv.org/abs/2511.16221)
*Caixin Kang,Yifei Huang,Liangyang Ouyang,Mingfang Zhang,Ruicong Liu,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了一项名为MIDA的新任务和多模态数据集，用于评估多模态大模型在社交欺骗识别中的表现。实验发现，即使最先进的模型如GPT-4o也难以可靠区分真话与谎言，主要原因是其无法有效结合多模态社交线索，且缺乏对他人认知、信念和意图的建模能力。为此，作者设计了社会思维链（SoCoT）推理流程和动态社会认知记忆（DSEM）模块，在该任务上实现了性能提升，为构建更具感知力和可信度的人类级社交推理AI提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型虽具备强大推理能力，但在理解复杂社交互动中的欺骗行为方面存在明显缺陷，缺乏‘读空气’的能力，亟需一种可量化评估其社交智能的方法，并推动更接近人类的社会推理能力发展。

Method: 提出MIDA任务与配套多模态数据集；构建Social Chain-of-Thought（SoCoT）推理框架与Dynamic Social Epistemic Memory（DSEM）模块，以增强模型对社会认知状态的理解和推理能力。

Result: 12个主流开源与闭源多模态大模型在MIDA任务上表现不佳，即使是GPT-4o也难以稳定区分真伪；所提出的SoCoT与DSEM框架显著提升了模型性能，验证了其有效性。

Conclusion: 现有MLLMs在社会推理方面仍严重不足，尤其在欺骗识别中表现薄弱。通过引入社会认知机制的新型架构，可有效提升模型对复杂社交情境的理解能力，是迈向真正人类级社交智能的重要一步。

Abstract: Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.

</details>


### [23] [Degradation-Aware Hierarchical Termination for Blind Quality Enhancement of Compressed Video](https://arxiv.org/abs/2511.16137)
*Li Yu,Yingbo Zhao,Shiyu Wu,Siyue Yu,Moncef Gabbouj,Qingshan Liu*

Main category: cs.CV

TL;DR: 本文提出一种基于预训练退化表征学习（DRL）模块的盲质量增强方法，用于压缩视频质量提升（QECV）。针对现有盲方法仅依赖全局退化信息、缺乏空间细节的问题，该方法通过多尺度高维退化表征实现更精细的伪影去除。同时引入分层终止机制，根据压缩等级动态调整处理阶段数量，以优化计算效率。实验表明，该方法在QP=22时相比当前最优盲方法提升PSNR 110%（从0.31 dB到0.65 dB），且推理时间减少50%。


<details>
  <summary>Details</summary>
Motivation: 现有非盲方法依赖已知量化参数（QP），在真实场景中因QP未知而受限；现有盲方法使用分类模型生成全局退化向量，缺乏空间细节，难以适应不同位置的伪影模式；同时，统一架构忽视不同压缩级别下的计算需求差异，导致效率低下。因此亟需发展更鲁棒、自适应的盲质量增强技术。

Method: 提出预训练的退化表征学习（DRL）模块，用于解耦并提取视频内容中的多尺度、高维退化特征，以提供细粒度的空间引导信息；设计分层终止机制，依据压缩等级动态决定处理阶段数量，实现计算资源按需分配。

Result: 在QP=22下，相比当前最优盲方法，PSNR提升110%（0.31 dB → 0.65 dB）；在相同条件下，平均推理时间减少50%，显著提升效率与性能。

Conclusion: 所提方法有效解决了盲QECV中退化信息表达不足与计算不均衡问题，通过多尺度退化表征和动态终止机制，实现了更高精度与更低延迟的质量增强，具备良好的实际应用潜力。

Abstract: Existing studies on Quality Enhancement for Compressed Video (QECV) predominantly rely on known Quantization Parameters (QPs), employing distinct enhancement models per QP setting, termed non-blind methods. However, in real-world scenarios involving transcoding or transmission, QPs may be partially or entirely unknown, limiting the applicability of such approaches and motivating the development of blind QECV techniques. Current blind methods generate degradation vectors via classification models with cross-entropy loss, using them as channel attention to guide artifact removal. However, these vectors capture only global degradation information and lack spatial details, hindering adaptation to varying artifact patterns at different spatial positions. To address these limitations, we propose a pretrained Degradation Representation Learning (DRL) module that decouples and extracts high-dimensional, multiscale degradation representations from video content to guide the artifact removal. Additionally, both blind and non-blind methods typically employ uniform architectures across QPs, hence, overlooking the varying computational demands inherent to different compression levels. We thus introduce a hierarchical termination mechanism that dynamically adjusts the number of artifact reduction stages based on the compression level. Experimental results demonstrate that the proposed approach significantly enhances performance, achieving a PSNR improvement of 110% (from 0.31 dB to 0.65 dB) over a competing state-of-the-art blind method at QP = 22. Furthermore, the proposed hierarchical termination mechanism reduces the average inference time at QP = 22 by half compared to QP = 42.

</details>


### [24] [Real-Time 3D Object Detection with Inference-Aligned Learning](https://arxiv.org/abs/2511.16140)
*Chenyu Zhao,Xianwei Zheng,Zimin Xia,Linwei Yue,Nan Xue*

Main category: cs.CV

TL;DR: SR3D是一种针对室内点云的新型3D目标检测框架，通过引入空间优先级最优传输分配和排名感知自蒸馏机制，有效弥合了训练与推理之间的差距，显著提升检测精度并保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D目标检测方法在训练过程中缺乏空间可靠性和排名感知，导致与推理时基于排名的选择策略不一致，影响模型学习与推理行为对齐的能力。

Method: 提出空间优先级最优传输分配，动态强调定位准确、空间可靠的样本；设计排名感知自蒸馏方案，通过自蒸馏机制自适应注入排名感知能力。

Result: 在ScanNet V2和SUN RGB-D数据集上，SR3D显著优于现有方法，在保持实时速度的同时提升了检测精度。

Conclusion: SR3D成功弥合了训练-推理间隙，为点云场景下的实时3D目标检测提供了高效且准确的新范式。

Abstract: Real-time 3D object detection from point clouds is essential for dynamic scene understanding in applications such as augmented reality, robotics and navigation. We introduce a novel Spatial-prioritized and Rank-aware 3D object detection (SR3D) framework for indoor point clouds, to bridge the gap between how detectors are trained and how they are evaluated. This gap stems from the lack of spatial reliability and ranking awareness during training, which conflicts with the ranking-based prediction selection used as inference. Such a training-inference gap hampers the model's ability to learn representations aligned with inference-time behavior. To address the limitation, SR3D consists of two components tailored to the spatial nature of point clouds during training: a novel spatial-prioritized optimal transport assignment that dynamically emphasizes well-located and spatially reliable samples, and a rank-aware adaptive self-distillation scheme that adaptively injects ranking perception via a self-distillation paradigm. Extensive experiments on ScanNet V2 and SUN RGB-D show that SR3D effectively bridges the training-inference gap and significantly outperforms prior methods in accuracy while maintaining real-time speed.

</details>


### [25] [TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding](https://arxiv.org/abs/2511.16595)
*Boshen Xu,Zihan Xiao,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Qin Jin*

Main category: cs.CV

TL;DR: TimeViper is a hybrid Mamba-Transformer vision-language model designed for long video understanding, enabling processing of hour-long videos (>10,000 frames) through efficient architecture and a novel token transfer module (TransV) that reduces vision token redundancy. It reveals vision-to-text information flow and enhances multimodal comprehension.


<details>
  <summary>Details</summary>
Motivation: Long video understanding requires efficient handling of extended temporal contexts; existing models struggle with scalability and redundancy in vision tokens across deep layers.

Method: TimeViper employs a hybrid Mamba-Transformer backbone to balance efficiency and expressivity. It introduces TransV, a token information transfer module that compresses vision tokens into instruction tokens, reducing redundancy while preserving multimodal understanding.

Result: TimeViper achieves competitive performance on multiple benchmarks, effectively processing videos exceeding 10,000 frames. Analysis shows clear vision-to-text information aggregation and improved interpretability of hybrid architectures.

Conclusion: This work advances the design, interpretation, and compression of hybrid Mamba-Transformer models, paving the way for scalable long-video understanding systems.

Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.

</details>


### [26] [LEGO-SLAM: Language-Embedded Gaussian Optimization SLAM](https://arxiv.org/abs/2511.16144)
*Sibaek Lee,Seongbo Ha,Kyeongsu Kang,Joonyeol Choi,Seungjun Tak,Hyeonwoo Yu*

Main category: cs.CV

TL;DR: LEGO-SLAM 是首个在 3DGS 基础上实现实时、开放词汇语义映射的 SLAM 框架。通过场景自适应编码器-解码器将高维语言嵌入压缩至 16 维，降低内存与渲染开销，并支持在线适应新场景。该设计还实现了语言引导的稀疏化与闭环检测，使高斯点数量减少超 60%，保持渲染质量的同时达到 15 FPS 实时性能，兼具先进语义理解与高效性。


<details>
  <summary>Details</summary>
Motivation: 现有 3DGS-based SLAM 系统虽能构建逼真地图，但缺乏开放词汇语义理解能力，难以支持高级机器人交互。传统方法因高维语言特征存储成本高且模型静态，难以适应新环境，亟需一种高效、可自适应的语义集成方案。

Method: 提出 LEGO-SLAM 框架，核心为场景自适应的编码器-解码器，将语言嵌入压缩至 16 维；利用压缩后的特征实现语言引导的高斯点剪枝与闭环检测，无需额外模型。

Result: 实验表明，LEGO-SLAM 在保持竞争性映射质量与跟踪精度的同时，实现 15 FPS 实时性能，高斯点数量减少超过 60%，具备开放词汇语义能力。

Conclusion: LEGO-SLAM 成功实现了 3DGS-based SLAM 中实时、开放词汇的语义映射，通过紧凑语言特征与自适应机制，在不牺牲性能的前提下显著提升语义表达能力与系统效率。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled Simultaneous Localization and Mapping (SLAM) systems to build photorealistic maps. However, these maps lack the open-vocabulary semantic understanding required for advanced robotic interaction. Integrating language features into SLAM remains a significant challenge, as storing high-dimensional features demands excessive memory and rendering overhead, while existing methods with static models lack adaptability for novel environments. To address these limitations, we propose LEGO-SLAM (Language-Embedded Gaussian Optimization SLAM), the first framework to achieve real-time, open-vocabulary mapping within a 3DGS-based SLAM system. At the core of our method is a scene-adaptive encoder-decoder that distills high-dimensional language embeddings into a compact 16-dimensional feature space. This design reduces the memory per Gaussian and accelerates rendering, enabling real-time performance. Unlike static approaches, our encoder adapts online to unseen scenes. These compact features also enable a language-guided pruning strategy that identifies semantic redundancy, reducing the map's Gaussian count by over 60\% while maintaining rendering quality. Furthermore, we introduce a language-based loop detection approach that reuses these mapping features, eliminating the need for a separate detection model. Extensive experiments demonstrate that LEGO-SLAM achieves competitive mapping quality and tracking accuracy, all while providing open-vocabulary capabilities at 15 FPS.

</details>


### [27] [Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation](https://arxiv.org/abs/2511.16671)
*Ziyu Guo,Renrui Zhang,Hongyu Li,Manyuan Zhang,Xinyan Chen,Sifan Wang,Yan Feng,Peng Pei,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 本文提出Thinking-while-Generating (TwiG)，一个首次实现生成过程中动态交织文本推理的框架，使推理与视觉生成协同演化。通过在生成过程中实时引导局部区域并反思已生成内容，提升了输出的上下文感知性和语义丰富性。研究对比了零样本提示、基于自建TwiG-50K数据集的监督微调及定制化的TwiG-GRPO强化学习三种策略，验证了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成方法虽引入文本推理，但多为事前规划或事后修正，缺乏生成过程中的实时多模态交互。为弥补这一不足，本文旨在探索生成过程中动态交织推理的可能性，以提升生成质量与语义一致性。

Method: 提出TwiG框架，实现文本推理与视觉生成的交替进行：在生成图像的每个阶段，结合当前进展进行推理，用于指导后续生成区域，并对已完成部分进行反思。采用三种策略评估：零样本提示、监督微调（SFT）和强化学习（RL）中的TwiG-GRPO方法。

Result: 实验表明，TwiG框架显著提升了视觉生成的上下文感知能力与语义丰富度。不同策略各有优势：零样本提示展现泛化潜力，SFT增强任务适配性，而强化学习进一步优化生成质量。整体证明了动态推理在生成过程中的有效性。

Conclusion: TwiG是首个在视觉生成中实现动态交织推理的框架，展示了在生成过程中持续推理对提升输出质量的巨大潜力。未来工作可进一步探索更复杂的推理机制与跨模态反馈机制。

Abstract: Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.

</details>


### [28] [Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval](https://arxiv.org/abs/2511.16150)
*Chunxu Liu,Jiyuan Yang,Ruopeng Gao,Yuhan Zhu,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为推理引导嵌入（RGE）的新方法，通过显式引入多模态大模型（MLLM）的推理能力来增强嵌入质量。与传统直接编码不同，RGE 允许模型在生成结构化推理过程的基础上提取表示，结合对比学习提升上下文条件下的推理信号，从而改善多模态检索性能。在 MMEB 基准测试中，相比非推理基线，性能提升 4.9%。


<details>
  <summary>Details</summary>
Motivation: 现有方法将嵌入提取视为直接编码步骤，忽略了多模态大模型（MLLM）所具备的推理生成能力，而该能力可显著提升表示质量。因此，需要探索如何将推理过程融入嵌入生成中以增强上下文感知和语义理解。

Method: 提出推理引导嵌入（RGE）框架：首先让模型根据指令生成结构化的推理过程，随后基于推理结果提取嵌入表示，并结合对比学习进行训练，使嵌入更充分反映推理过程中的上下文信息。

Result: 在 MMEB 基准上，RGE 方法相较于非推理基线，多模态检索性能提升 4.9%，验证了显式推理对嵌入质量的有效提升作用。

Conclusion: 显式引入推理过程能够有效增强多模态嵌入的质量，证明了利用 MLLMs 的生成与推理能力改进嵌入表示的可行性与有效性。

Abstract: Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.

</details>


### [29] [Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers](https://arxiv.org/abs/2511.16156)
*Jian Ma,Qirong Peng,Xujie Zhu,Peixing Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: PPCL是一种针对扩散变压器（DiT）架构的灵活结构化剪枝框架，通过线性探测与一阶微分趋势分析识别冗余层区间，并采用即插即用的师生交替蒸馏方案，在单一训练阶段实现深度和宽度剪枝，支持多种剪枝率的知识迁移，无需为每种配置重新训练。实验表明，该方法在多模型上实现50%参数量减少，关键指标下降小于3%，显著提升压缩比并保持高质量图像生成能力，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 扩散变压器（DiT）虽在图像生成中表现优异，但其庞大的参数量导致计算成本高昂，限制了在资源受限场景下的部署。因此，亟需一种高效且灵活的剪枝方法以降低计算开销，同时保持生成质量。

Method: 提出一种名为Pluggable Pruning with Contiguous Layer Distillation (PPCL)的结构化剪枝框架：首先利用线性探测与相似性度量的一阶微分趋势分析识别冗余层区间；随后设计即插即用的师生交替蒸馏机制，整合深度与宽度剪枝于同一训练过程，实现跨不同剪枝比例的灵活知识迁移，避免重复训练。

Result: 在多个多模态扩散变压器模型上，PPCL实现了50%的参数量减少，关键评价指标下降不足3%，在保持高质量图像生成的同时达到更高的压缩比，验证了其在资源受限环境中的适用性。

Conclusion: PPCL是一种高效、灵活且可扩展的剪枝框架，能够显著降低DiT模型的参数量与计算开销，同时维持优秀的图像生成性能，为轻量化扩散模型部署提供了可行解决方案。

Abstract: Diffusion Transformers (DiTs) have shown exceptional performance in image generation, yet their large parameter counts incur high computational costs, impeding deployment in resource-constrained settings. To address this, we propose Pluggable Pruning with Contiguous Layer Distillation (PPCL), a flexible structured pruning framework specifically designed for DiT architectures. First, we identify redundant layer intervals through a linear probing mechanism combined with the first-order differential trend analysis of similarity metrics. Subsequently, we propose a plug-and-play teacher-student alternating distillation scheme tailored to integrate depth-wise and width-wise pruning within a single training phase. This distillation framework enables flexible knowledge transfer across diverse pruning ratios, eliminating the need for per-configuration retraining. Extensive experiments on multiple Multi-Modal Diffusion Transformer architecture models demonstrate that PPCL achieves a 50\% reduction in parameter count compared to the full model, with less than 3\% degradation in key objective metrics. Notably, our method maintains high-quality image generation capabilities while achieving higher compression ratios, rendering it well-suited for resource-constrained environments. The open-source code, checkpoints for PPCL can be found at the following link: https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning.

</details>


### [30] [Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning](https://arxiv.org/abs/2511.16160)
*Yibin Huang,Wang Xu,Wanyue Zhang,Helu Zhi,Jingjing Huang,Yangbin Xu,Yangang Sun,Conghui Zhu,Tiejun Zhao*

Main category: cs.CV

TL;DR: 提出Video2Layout框架，通过连续物体边界坐标实现视频到度量空间布局的重建，提升多模态大模型在细粒度空间推理上的能力。利用AI2THOR构建高质量数据集进行监督微调，并通过强化学习微调增强现实泛化能力。引入QVS-Bench评估图像数量对认知地图与空间推理准确性的影响，实验表明V2LO-7B相比基于网格的地图模型平均提升4.92%。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的认知地图方法依赖离散栅格表示，限制了细粒度空间推理能力；需更精确的度量空间表示以支持量化空间计算和减少自然语言描述中的模糊性。

Method: 构建Video2Layout框架，包含两个阶段：1）在AI2THOR模拟器上构建高质量数据集，进行监督微调以学习从视觉输入到精确边界坐标的映射；2）通过强化学习微调提升模型在真实场景中的泛化能力。使用连续对象边界坐标量化物体间距离与大小，实现度量空间布局重建。

Result: 在QVS-Bench及主流空间推理基准测试中，V2LO-7B模型相比基于网格地图的模型平均提升4.92%，验证了该方法在空间理解与推理上的优越性。

Conclusion: Video2Layout通过连续坐标表示实现了更精确的空间布局重建，显著提升了多模态大模型在空间智能方面的表现，为未来物理世界理解提供了有效路径。

Abstract: Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.

</details>


### [31] [Simba: Towards High-Fidelity and Geometrically-Consistent Point Cloud Completion via Transformation Diffusion](https://arxiv.org/abs/2511.16161)
*Lirui Zhang,Zhengkai Zhao,Zhi Zuo,Pan Gao,Jie Qin*

Main category: cs.CV

TL;DR: 本文提出Simba框架，通过将点云补全中的点级变换回归转化为分布学习问题，结合对称性先验与扩散模型的生成能力，有效避免了实例特定记忆，提升了几何结构的鲁棒性和泛化性。同时采用分层Mamba架构实现高保真上采样，在PCN、ShapeNet和KITTI等多个基准上达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于直接回归的方法在保留细粒度几何细节的同时，难以保证全局结构完整性，且易过拟合于特定实例，对输入噪声敏感，导致鲁棒性差。

Method: 将点级变换回归重构为分布学习问题，引入对称性先验与扩散模型生成能力，并设计分层Mamba架构以实现高精度上采样。

Result: 在PCN、ShapeNet和KITTI等多个基准上取得SOTA性能，显著提升补全质量与鲁棒性。

Conclusion: Simba通过分布学习与生成建模，有效克服了传统回归方法的过拟合与噪声敏感问题，实现了更高质量、更稳健的点云补全。

Abstract: Point cloud completion is a fundamental task in 3D vision. A persistent challenge in this field is simultaneously preserving fine-grained details present in the input while ensuring the global structural integrity of the completed shape. While recent works leveraging local symmetry transformations via direct regression have significantly improved the preservation of geometric structure details, these methods suffer from two major limitations: (1) These regression-based methods are prone to overfitting which tend to memorize instant-specific transformations instead of learning a generalizable geometric prior. (2) Their reliance on point-wise transformation regression lead to high sensitivity to input noise, severely degrading their robustness and generalization. To address these challenges, we introduce Simba, a novel framework that reformulates point-wise transformation regression as a distribution learning problem. Our approach integrates symmetry priors with the powerful generative capabilities of diffusion models, avoiding instance-specific memorization while capturing robust geometric structures. Additionally, we introduce a hierarchical Mamba-based architecture to achieve high-fidelity upsampling. Extensive experiments across the PCN, ShapeNet, and KITTI benchmarks validate our method's state-of-the-art (SOTA) performance.

</details>


### [32] [An Image Is Worth Ten Thousand Words: Verbose-Text Induction Attacks on VLMs](https://arxiv.org/abs/2511.16163)
*Zhi Luo,Zenghui Yuan,Wenqi Wei,Daizong Liu,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出一种新型的冗长文本诱导攻击（VTIA），通过两阶段框架在良性图像中注入难以察觉的对抗扰动，以最大化视觉语言模型（VLM）生成的输出令牌数量。该方法首先利用强化学习搜索对抗性提示，诱导VLM中的语言模型产生冗长输出；随后通过视觉对齐的扰动优化，使扰动图像的视觉嵌入与对抗性提示的嵌入高度相似，从而构造出能触发冗长文本生成的恶意图像。实验表明，该方法在有效性、效率和泛化能力上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在多模态任务中表现优异，但其部署效率问题日益突出，尤其是生成过程中的令牌消耗成为关键评估指标。已有研究发现，特定输入可导致模型生成低信息密度的长输出，增加能耗、延迟和令牌成本。然而，现有方法仅通过延迟结束标记（EOS）来隐式延长输出，缺乏对输出长度的显式优化，且控制性和稳定性不足。因此亟需一种更有效、可控的机制来最大化输出长度。

Method: 提出两阶段框架：1）对抗性提示搜索，采用强化学习自动寻找能够诱导大语言模型（LLM）生成冗长输出的对抗性提示；2）视觉对齐的扰动优化，通过最大化扰动图像的视觉嵌入与对抗性提示嵌入之间的相似性，生成能触发冗长文本的恶意图像。

Result: 在四个主流VLM上进行的全面实验表明，所提方法在输出长度、生成效率和跨模型泛化能力方面均取得显著提升，验证了其有效性与实用性。

Conclusion: 本文提出的VTIA方法成功实现了对VLM生成输出长度的显式最大化，具备更强的可控性与稳定性，为评估和防御高消耗生成行为提供了新的视角和工具。

Abstract: With the remarkable success of Vision-Language Models (VLMs) on multimodal tasks, concerns regarding their deployment efficiency have become increasingly prominent. In particular, the number of tokens consumed during the generation process has emerged as a key evaluation metric.Prior studies have shown that specific inputs can induce VLMs to generate lengthy outputs with low information density, which significantly increases energy consumption, latency, and token costs. However, existing methods simply delay the occurrence of the EOS token to implicitly prolong output, and fail to directly maximize the output token length as an explicit optimization objective, lacking stability and controllability.To address these limitations, this paper proposes a novel verbose-text induction attack (VTIA) to inject imperceptible adversarial perturbations into benign images via a two-stage framework, which identifies the most malicious prompt embeddings for optimizing and maximizing the output token of the perturbed images.Specifically, we first perform adversarial prompt search, employing reinforcement learning strategies to automatically identify adversarial prompts capable of inducing the LLM component within VLMs to produce verbose outputs. We then conduct vision-aligned perturbation optimization to craft adversarial examples on input images, maximizing the similarity between the perturbed image's visual embeddings and those of the adversarial prompt, thereby constructing malicious images that trigger verbose text generation. Comprehensive experiments on four popular VLMs demonstrate that our method achieves significant advantages in terms of effectiveness, efficiency, and generalization capability.

</details>


### [33] [Target Refocusing via Attention Redistribution for Open-Vocabulary Semantic Segmentation: An Explainability Perspective](https://arxiv.org/abs/2511.16170)
*Jiahao Li,Yang Lu,Yachao Zhang,Yong Xie,Fangyong Wang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文系统研究了CLIP在密集预测任务中的内部机制，发现其存在类似人类分心的现象：注意力资源被无关标记过度占用。通过分析，作者识别出这些干扰标记源于特定维度的过激活，并提出无需训练的ReFocusing CLIP（RF-CLIP）方法，模拟人类的注意力重聚焦行为，将注意力重新引导至目标区域，从而提升多模态对齐的精细度。该方法在八个基准上达到当前最优性能，且推理效率高。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法虽利用CLIP的视觉-语言对齐能力取得良好效果，但缺乏对CLIP在密集预测任务中性能边界及其可解释机制的深入研究。尤其未关注其注意力机制中存在的分心现象，限制了多模态对齐的精度。因此亟需从机制层面理解并优化CLIP在像素级对齐中的表现。

Method: 提出ReFocusing CLIP（RF-CLIP），一种无需训练的方法。通过分析CLIP内部表示，识别出导致注意力分散的维度特异性过激活标记，并设计策略过滤这些干扰项，引导模型注意力更聚焦于目标区域，从而增强像素级视觉-语言对齐的精确性。

Result: RF-CLIP在8个主流语义分割基准上均达到当前最优性能，显著优于现有方法；同时保持高推理效率，无额外训练开销。

Conclusion: 本研究揭示了CLIP在密集预测中因维度过激活导致注意力分散的内在机制，并提出有效的注意力重聚焦策略。所提方法无需训练即可显著提升多模态对齐精度，为开放词汇语义分割提供了新的可解释性优化路径。

Abstract: Open-vocabulary semantic segmentation (OVSS) employs pixel-level vision-language alignment to associate category-related prompts with corresponding pixels. A key challenge is enhancing the multimodal dense prediction capability, specifically this pixel-level multimodal alignment. Although existing methods achieve promising results by leveraging CLIP's vision-language alignment, they rarely investigate the performance boundaries of CLIP for dense prediction from an interpretability mechanisms perspective. In this work, we systematically investigate CLIP's internal mechanisms and identify a critical phenomenon: analogous to human distraction, CLIP diverts significant attention resources from target regions to irrelevant tokens. Our analysis reveals that these tokens arise from dimension-specific over-activation; filtering them enhances CLIP's dense prediction performance. Consequently, we propose ReFocusing CLIP (RF-CLIP), a training-free approach that emulates human distraction-refocusing behavior to redirect attention from distraction tokens back to target regions, thereby refining CLIP's multimodal alignment granularity. Our method achieves SOTA performance on eight benchmarks while maintaining high inference efficiency.

</details>


### [34] [Domain-Shared Learning and Gradual Alignment for Unsupervised Domain Adaptation Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.16184)
*Nianchang Huang,Yi Xu,Ruida Xi,Ruida Xi,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文针对可见光-红外行人重识别（VI-ReID）在真实场景中因数据分布差异导致性能下降的问题，提出无监督域自适应方法UDA-VI-ReID。针对域间模态差异和域内模态差异两大挑战，设计了两阶段模型DSLGA：第一阶段通过域共享学习策略（DSLS）缓解跨域模态差异对预训练的负面影响；第二阶段采用渐进对齐策略（GAS），以聚类到整体的方式逐步实现可见光与红外模态间的对齐。同时构建新的测试方法CMDA-XD，实验表明该方法显著优于现有无监督域适应方法，甚至超越部分有监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有VI-ReID方法在公开数据集上表现良好，但在真实场景中因数据分布差异导致性能下降，缺乏有效的无监督域自适应方法来迁移知识且无需新样本标注。

Method: 提出两阶段模型DSLGA，包含域共享学习策略（DSLS）和渐进对齐策略（GAS），分别解决跨域和域内模态差异问题，并设计新的测试方法CMDA-XD。

Result: 在多种设置下，所提方法显著优于现有无监督域适应方法，甚至超过部分有监督方法，在真实场景应用中展现出更强的泛化能力。

Conclusion: DSLGA通过有效处理域间与域内模态差异，实现了高效的知识迁移，为真实世界中的VI-ReID提供了强有力的解决方案。

Abstract: Recently, Visible-Infrared person Re-Identification (VI-ReID) has achieved remarkable performance on public datasets. However, due to the discrepancies between public datasets and real-world data, most existing VI-ReID algorithms struggle in real-life applications. To address this, we take the initiative to investigate Unsupervised Domain Adaptation Visible-Infrared person Re-Identification (UDA-VI-ReID), aiming to transfer the knowledge learned from the public data to real-world data without compromising accuracy and requiring the annotation of new samples. Specifically, we first analyze two basic challenges in UDA-VI-ReID, i.e., inter-domain modality discrepancies and intra-domain modality discrepancies. Then, we design a novel two-stage model, i.e., Domain-Shared Learning and Gradual Alignment (DSLGA), to handle these discrepancies. In the first pre-training stage, DSLGA introduces a Domain-Shared Learning Strategy (DSLS) to mitigate ineffective pre-training caused by inter-domain modality discrepancies via exploiting shared information between the source and target domains. While, in the second fine-tuning stage, DSLGA designs a Gradual Alignment Strategy (GAS) to handle the cross-modality alignment challenges between visible and infrared data caused by the large intra-domain modality discrepancies through a cluster-to-holistic alignment way. Finally, a new UDA-VI-ReID testing method i.e., CMDA-XD, is constructed for training and testing different UDA-VI-ReID models. A large amount of experiments demonstrate that our method significantly outperforms existing domain adaptation methods for VI-ReID and even some supervised methods under various settings.

</details>


### [35] [PrIntMesh: Precise Intersection Surfaces for 3D Organ Mesh Reconstruction](https://arxiv.org/abs/2511.16186)
*Deniz Sayin Mercadier,Hieu Le,Yihong Chen,Jiancheng Yang,Udaranga Wickramasinghe,Pascal Fua*

Main category: cs.CV

TL;DR: PrIntMesh 是一种基于模板、保持拓扑结构的框架，通过联合变形所有子结构来重建器官，同时保持内部边界和光滑无伪影的表面，适用于心脏、海马体和肺部，具有高几何精度、正确拓扑和对有限或噪声数据的鲁棒性，优于体素和表面基方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常独立处理器官的各个部分，导致解剖上不合理的重建结果；需要一种能够保持器官内部结构关系和拓扑一致性的方法。

Method: PrIntMesh 采用连接模板，联合变形所有子结构以匹配患者特异性解剖，显式保留内部边界并强制实现平滑、无伪影的表面。

Result: 在心脏、海马体和肺部上验证，实现了高几何精度、正确拓扑，且在少量或噪声数据下仍表现稳健；相比体素和表面基方法，更优地重建共享界面，保持结构一致性，并具备数据高效性。

Conclusion: PrIntMesh 为器官重建提供了一种数据高效、解剖合理且临床适用的解决方案，显著提升了重建质量与结构一致性。

Abstract: Human organs are composed of interconnected substructures whose geometry and spatial relationships constrain one another. Yet, most deep-learning approaches treat these parts independently, producing anatomically implausible reconstructions. We introduce PrIntMesh, a template-based, topology-preserving framework that reconstructs organs as unified systems. Starting from a connected template, PrIntMesh jointly deforms all substructures to match patient-specific anatomy, while explicitly preserving internal boundaries and enforcing smooth, artifact-free surfaces. We demonstrate its effectiveness on the heart, hippocampus, and lungs, achieving high geometric accuracy, correct topology, and robust performance even with limited or noisy training data. Compared to voxel- and surface-based methods, PrIntMesh better reconstructs shared interfaces, maintains structural consistency, and provides a data-efficient solution suitable for clinical use.

</details>


### [36] [When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.16203)
*Yuping Yan,Yuhan Xie,Yinxin Zhang,Lingjuan Lyu,Yaochu Jin*

Main category: cs.CV

TL;DR: 本文提出VLA-Fool，系统研究了具身视觉-语言-动作（VLA）模型在白盒和黑盒条件下的多模态对抗鲁棒性。该框架涵盖文本、视觉及跨模态错位三类攻击，并引入语义感知提示生成机制，揭示了微小多模态扰动即可导致行为显著偏差，凸显具身多模态对齐的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单模态扰动，忽视跨模态对齐对具身推理与决策的根本性影响，且缺乏对真实场景下多模态对抗攻击的系统评估，亟需建立全面的多模态对抗鲁棒性评测体系。

Method: 提出VLA-Fool框架，集成三类攻击：基于梯度和提示的文本扰动、基于补丁和噪声的视觉扰动，以及破坏感知与指令语义对应关系的跨模态错位攻击；构建首个自动化的语义引导提示生成方法，增强攻击针对性与可解释性。

Result: 在LIBERO基准上使用微调后的OpenVLA模型实验表明，即使极小的多模态扰动也会引发显著的行为偏离，证明当前具身VLA模型在多模态对齐方面极为脆弱。

Conclusion: 具身视觉-语言-动作模型在多模态环境下存在严重的对抗脆弱性，尤其受跨模态错位影响显著，亟需在模型设计与训练中强化多模态对齐的鲁棒性。

Abstract: Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.

</details>


### [37] [Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM](https://arxiv.org/abs/2511.16282)
*Gergely Dinya,Péter Halász,András Lőrincz,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: 提出一种基于视觉门控生成变压器（VGGT）的快速时空场景理解框架，通过滑动窗口处理图像流并对齐子图，降低内存消耗，实现近实时性能；利用VGGT跟踪头将2D语义实例掩码聚合为3D对象，并通过存储时间戳和实例身份实现时序一致性与上下文推理，适用于辅助导航等实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理动态环境中的3D场景表示时存在计算开销大、难以实现实时更新的问题，尤其在辅助导航等应用中需要高效且持续更新的场景理解能力。因此，亟需一种兼顾性能与准确性的轻量化、可扩展的时空建模框架。

Method: 采用滑动窗口机制处理图像流，对子图进行对齐以缓解VGGT的高内存需求；利用VGGT的跟踪头将2D语义实例掩码融合为3D物体；通过保存时间戳与实例身份信息，增强时序一致性和上下文推理能力。

Result: 在公开基准和专为辅助导航设计的自定义数据集上评估表现优异，验证了该框架在真实场景下的适用性与实时性能。

Conclusion: 所提出的VGGT框架能够高效实现连续的3D场景理解，在保证精度的同时具备近实时处理能力，适用于如辅助导航等对响应速度和上下文感知要求较高的应用。

Abstract: We present a fast, spatio-temporal scene understanding framework based on Vision Gated Generative Transformers (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.

</details>


### [38] [Optimizing 3D Gaussian Splattering for Mobile GPUs](https://arxiv.org/abs/2511.16298)
*Md Musfiqur Rahman Sanim,Zhihao Shu,Bahram Afsharmanesh,AmirAli Mirian,Jiexiong Guan,Wei Niu,Bin Ren,Gagan Agrawal*

Main category: cs.CV

TL;DR: 本文提出Texture3dgs，一种针对移动GPU优化的3D高斯点阵（3DGS）映射方法，重点解决移动端2D纹理缓存优化问题。通过设计新型排序算法，结合变量布局优化等技术，显著提升3D场景重建效率，实测在排序阶段最高提速4.1倍，整体重建速度提升1.7倍，同时内存占用降低1.6倍，验证了其在移动设备上高效实现3D场景重建的有效性。


<details>
  <summary>Details</summary>
Motivation: 推动3D场景重建在移动设备上的部署，以保障数据隐私、支持离线运行并提升响应速度；现有3DGS方法在移动端受限于2D纹理缓存性能瓶颈，亟需针对性优化。

Method: 提出新型排序算法，优化处理流程、数据移动与内存放置策略以适配2D纹理缓存；改进变量布局设计，对3DGS其他步骤进行系统性加速。

Result: 端到端评估显示，Texture3dgs在排序阶段实现最高4.1倍加速，整体3D场景重建速度提升1.7倍，内存使用减少最多1.6倍。

Conclusion: Texture3dgs通过针对移动端2D纹理缓存的深度优化，有效提升了3DGS在移动平台上的执行效率和资源利用率，为移动端高效3D场景重建提供了可行方案。

Abstract: Image-based 3D scene reconstruction, which transforms multi-view images into a structured 3D representation of the surrounding environment, is a common task across many modern applications. 3D Gaussian Splatting (3DGS) is a new paradigm to address this problem and offers considerable efficiency as compared to the previous methods. Motivated by this, and considering various benefits of mobile device deployment (data privacy, operating without internet connectivity, and potentially faster responses), this paper develops Texture3dgs, an optimized mapping of 3DGS for a mobile GPU. A critical challenge in this area turns out to be optimizing for the two-dimensional (2D) texture cache, which needs to be exploited for faster executions on mobile GPUs. As a sorting method dominates the computations in 3DGS on mobile platforms, the core of Texture3dgs is a novel sorting algorithm where the processing, data movement, and placement are highly optimized for 2D memory. The properties of this algorithm are analyzed in view of a cost model for the texture cache. In addition, we accelerate other steps of the 3DGS algorithm through improved variable layout design and other optimizations. End-to-end evaluation shows that Texture3dgs delivers up to 4.1$\times$ and 1.7$\times$ speedup for the sorting and overall 3D scene reconstruction, respectively -- while also reducing memory usage by up to 1.6$\times$ -- demonstrating the effectiveness of our design for efficient mobile 3D scene reconstruction.

</details>


### [39] [Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling](https://arxiv.org/abs/2511.16301)
*Minseok Seo,Mark Hamilton,Changick Kim*

Main category: cs.CV

TL;DR: Upsample Anything 是一种轻量级的测试时优化（TTO）框架，无需训练即可将低分辨率特征恢复为高分辨率像素级输出。它通过每张图像的简单优化学习一个各向异性高斯核，结合空间和范围线索，有效融合高斯点阵与联合双边上采样，作为通用、边缘感知的算子，可跨架构和模态无缝迁移，实现特征、深度或概率图的精确高分辨率重建，处理速度约为每224x224图像0.419秒，在语义分割、深度估计及上采样任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型的表示通常被下采样14x/16x，限制了其在像素级应用中的直接使用；而现有的特征上采样方法依赖数据集特定的重训练或重型隐式优化，难以扩展和泛化。

Method: 提出一种基于每图像优化的轻量级TTO框架，学习一个结合空间与范围线索的各向异性高斯核，作为通用边缘感知算子，实现高分辨率重建。

Result: 在语义分割、深度估计以及深度和概率图上采样任务中均达到最先进性能，单图处理时间仅约0.419秒，且具备良好的跨架构与跨模态泛化能力。

Conclusion: Upsample Anything 提供了一种无需训练、高效且通用的高分辨率特征重建方法，解决了现有方法在可扩展性和泛化性上的瓶颈，适用于多种像素级视觉任务。

Abstract: We present \textbf{Upsample Anything}, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only $\approx0.419 \text{s}$ per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling.

</details>


### [40] [Sparse Autoencoders are Topic Models](https://arxiv.org/abs/2511.16309)
*Leander Girrbach,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文提出将稀疏自编码器（SAE）理解为一种主题模型，通过将潜在狄利克雷分配（LDA）扩展到嵌入空间，推导出SAE目标函数作为该模型的最大后验估计。基于此观点，作者提出SAE-TM框架：(1) 训练SAE以学习可复用的主题原子；(2) 将其解释为下游数据中的词分布；(3) 无需重新训练即可合并成任意数量的主题。SAE-TM在文本和图像数据集上生成更连贯且多样化的主题，同时分析了图像数据的主题结构及日本版画中主题随时间的变化。


<details>
  <summary>Details</summary>
Motivation: 当前对稀疏自编码器（SAE）在嵌入分析中的作用和实际价值存在争议。为澄清其本质并提升实用性，本文从主题建模角度重新审视SAE，旨在提供一个更具解释性和通用性的分析框架。

Method: 将传统的潜在狄利克雷分配（LDA）模型推广至嵌入空间，证明SAE的目标函数可视为该模型下的最大后验估计。在此基础上构建SAE-TM框架，利用SAE学习主题原子，并将其转化为可解释的词分布，在不重新训练的情况下灵活组合生成多主题。

Result: SAE-TM在文本与图像数据集上均优于现有强基线方法，生成的主题具有更高的连贯性与多样性。此外，通过对图像数据的主题结构分析以及对日本木刻版画中主题演化的时间追踪，验证了该方法在跨模态主题分析中的有效性。

Conclusion: 本研究将SAE定位为跨模态大规模主题分析的有效工具，揭示了其作为主题模型的本质属性，并提出了一个可解释、可扩展的框架——SAE-TM，为嵌入空间的主题发现提供了新范式。

Abstract: Sparse autoencoders (SAEs) are used to analyze embeddings, but their role and practical value are debated. We propose a new perspective on SAEs by demonstrating that they can be naturally understood as topic models. We extend Latent Dirichlet Allocation to embedding spaces and derive the SAE objective as a maximum a posteriori estimator under this model. This view implies SAE features are thematic components rather than steerable directions. Based on this, we introduce SAE-TM, a topic modeling framework that: (1) trains an SAE to learn reusable topic atoms, (2) interprets them as word distributions on downstream data, and (3) merges them into any number of topics without retraining. SAE-TM yields more coherent topics than strong baselines on text and image datasets while maintaining diversity. Finally, we analyze thematic structure in image datasets and trace topic changes over time in Japanese woodblock prints. Our work positions SAEs as effective tools for large-scale thematic analysis across modalities. Code and data will be released upon publication.

</details>


### [41] [NaTex: Seamless Texture Generation as Latent Color Diffusion](https://arxiv.org/abs/2511.16317)
*Zeqiang Lai,Yunfei Zhao,Zibo Zhao,Xin Yang,Xin Huang,Jingwei Huang,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: NaTex 是一种原生纹理生成框架，直接在 3D 空间中预测纹理颜色，避免了传统多视图扩散模型（MVD）在处理遮挡、纹理-网格对齐和跨视图一致性方面的局限。该方法将纹理视为密集的颜色点云，提出潜空间颜色扩散机制，包括几何感知的颜色点云 VAE 和多控制扩散变压器（DiT），完全基于 3D 数据从零训练。通过引入原生几何控制，利用位置嵌入和几何潜在表示来引导 DiT，实现精确对齐。同时，VAE-DiT 架构通过紧密耦合的几何分支提供细粒度表面指导，保持与纹理强对应关系。实验表明，NaTex 在纹理一致性和对齐方面显著优于现有方法，并具备强大的泛化能力，适用于材质生成、纹理优化、部件分割与贴图等下游任务。


<details>
  <summary>Details</summary>
Motivation: 传统纹理生成方法依赖多视图扩散模型（MVD）生成2D图像再烘焙到3D网格，存在遮挡区域需修复、网格-纹理边界对齐困难、跨视图内容与颜色不一致等问题。为克服这些瓶颈，需要一种直接在3D空间中生成纹理的新范式。

Method: 提出将纹理建模为3D空间中的密集颜色点云；设计潜空间颜色扩散模型，包含几何感知的色点云变分自编码器（VAE）和多控制扩散变压器（DiT）；引入原生几何控制，通过位置嵌入和几何潜在表示直接在3D空间中引导纹理生成；构建协同设计的VAE-DiT架构，利用专门的几何分支提取几何潜在表示以提供细粒度表面指导。

Result: NaTex在纹理生成的连贯性、对齐精度和跨视图一致性方面显著优于现有方法；在无需训练或仅需简单微调的情况下，可有效应用于材料生成、纹理精炼、部件分割与贴图等多种下游任务，展现出强大泛化能力。

Conclusion: NaTex 提出了一种全新的原生3D纹理生成范式，通过直接在3D空间中建模纹理并结合几何感知的潜空间扩散机制，有效解决了传统方法的关键缺陷，实现了高质量、高对齐、强一致性的纹理生成，并具备良好的应用扩展性。

Abstract: We present NaTex, a native texture generation framework that predicts texture color directly in 3D space. In contrast to previous approaches that rely on baking 2D multi-view images synthesized by geometry-conditioned Multi-View Diffusion models (MVDs), NaTex avoids several inherent limitations of the MVD pipeline. These include difficulties in handling occluded regions that require inpainting, achieving precise mesh-texture alignment along boundaries, and maintaining cross-view consistency and coherence in both content and color intensity. NaTex features a novel paradigm that addresses the aforementioned issues by viewing texture as a dense color point cloud. Driven by this idea, we propose latent color diffusion, which comprises a geometry-awared color point cloud VAE and a multi-control diffusion transformer (DiT), entirely trained from scratch using 3D data, for texture reconstruction and generation. To enable precise alignment, we introduce native geometry control that conditions the DiT on direct 3D spatial information via positional embeddings and geometry latents. We co-design the VAE-DiT architecture, where the geometry latents are extracted via a dedicated geometry branch tightly coupled with the color VAE, providing fine-grained surface guidance that maintains strong correspondence with the texture. With these designs, NaTex demonstrates strong performance, significantly outperforming previous methods in texture coherence and alignment. Moreover, NaTex also exhibits strong generalization capabilities, either training-free or with simple tuning, for various downstream applications, e.g., material generation, texture refinement, and part segmentation and texturing.

</details>


### [42] [Aerial View River Landform Video segmentation: A Weakly Supervised Context-aware Temporal Consistency Distillation Approach](https://arxiv.org/abs/2511.16343)
*Chi-Han Chen,Chieh-Ming Chen,Wen-Huang Cheng,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文针对无人机遥感中的地形地貌分类任务，提出一种基于教师-学生架构的弱监督学习方法，结合关键帧选择与更新算法，有效提升mIoU和时间一致性（TC），仅用30%标注数据即可实现稳定地形定位。


<details>
  <summary>Details</summary>
Motivation: 传统方法在无人机遥感地形分类中面临数据标注复杂、数据稀缺及技术有效范围限制等问题，且完全依赖标注数据并非最优，关键帧缺失会降低时间一致性，影响定位稳定性。

Method: 提出教师-学生架构，结合关键帧选择与更新算法，实现弱监督学习与时间一致性知识蒸馏，克服传统方法在空中任务中对时间一致性的训练缺陷。

Result: 实验表明，该方法仅使用30%标注数据，即能同时提升mIoU与时间一致性，实现稳定的地形对象定位。

Conclusion: 所提方法在数据稀缺条件下显著提升了无人机遥感中地形分类的精度与时间一致性，为实际应用提供了高效可行的解决方案。

Abstract: The study of terrain and landform classification through UAV remote sensing diverges significantly from ground vehicle patrol tasks. Besides grappling with the complexity of data annotation and ensuring temporal consistency, it also confronts the scarcity of relevant data and the limitations imposed by the effective range of many technologies. This research substantiates that, in aerial positioning tasks, both the mean Intersection over Union (mIoU) and temporal consistency (TC) metrics are of paramount importance. It is demonstrated that fully labeled data is not the optimal choice, as selecting only key data lacks the enhancement in TC, leading to failures. Hence, a teacher-student architecture, coupled with key frame selection and key frame updating algorithms, is proposed. This framework successfully performs weakly supervised learning and TC knowledge distillation, overcoming the deficiencies of traditional TC training in aerial tasks. The experimental results reveal that our method utilizing merely 30\% of labeled data, concurrently elevates mIoU and temporal consistency ensuring stable localization of terrain objects. Result demo : https://gitlab.com/prophet.ai.inc/drone-based-riverbed-inspection

</details>


### [43] [CRISTAL: Real-time Camera Registration in Static LiDAR Scans using Neural Rendering](https://arxiv.org/abs/2511.16349)
*Joni Vanherck,Steven Moonen,Brent Zoomers,Kobe Werner,Jeroen Put,Lode Jorissen,Nick Michiels*

Main category: cs.CV

TL;DR: 本文提出一种实时相机定位方法，利用高精度彩色激光雷达点云实现无漂移、带度量尺度的相机跟踪。通过合成视图渲染建立真实图像与点云间的2D-3D对应关系，结合神经渲染技术减少合成与真实图像之间的域差距，提升特征匹配效果。提出了两种实时变体：Online Render and Match 和 Prebuild and Localize。在ScanNet++数据集上表现优于现有SLAM系统。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位方法存在漂移、尺度模糊问题，且依赖标记物或回环检测。为实现精确、实时、无需依赖外部标识的相机定位，需一种基于高精度环境模型的方法。

Method: 利用预捕获的彩色激光雷达点云生成合成视图，通过神经渲染缩小合成与真实图像的差异；在实时帧中提取特征并匹配到合成视图，建立2D-3D对应关系，从而估计相机位姿。

Result: 实现了无漂移、具有正确度量尺度的实时相机定位；在ScanNet++数据集上性能优于现有SLAM系统，对遮挡和背景干扰具有更强鲁棒性。

Conclusion: 该方法通过融合高精度点云与神经渲染技术，有效解决了传统视觉定位中的漂移与尺度问题，适用于机器人导航与扩展现实应用。

Abstract: Accurate camera localization is crucial for robotics and Extended Reality (XR), enabling reliable navigation and alignment of virtual and real content. Existing visual methods often suffer from drift, scale ambiguity, and depend on fiducials or loop closure. This work introduces a real-time method for localizing a camera within a pre-captured, highly accurate colored LiDAR point cloud. By rendering synthetic views from this cloud, 2D-3D correspondences are established between live frames and the point cloud. A neural rendering technique narrows the domain gap between synthetic and real images, reducing occlusion and background artifacts to improve feature matching. The result is drift-free camera tracking with correct metric scale in the global LiDAR coordinate system. Two real-time variants are presented: Online Render and Match, and Prebuild and Localize. We demonstrate improved results on the ScanNet++ dataset and outperform existing SLAM pipelines.

</details>


### [44] [CAMS: Towards Compositional Zero-Shot Learning via Gated Cross-Attention and Multi-Space Disentanglement](https://arxiv.org/abs/2511.16378)
*Pan Yang,Cheng Deng,Jing Yang,Han Zhao,Yun Liu,Yuling Chen,Xiaoli Ruan,Yanping Chen*

Main category: cs.CV

TL;DR: 本文提出CAMs方法，通过在多维空间中提取视觉特征的语义特征并实现属性与对象语义的解耦，以提升对未见属性-对象组合的泛化能力。该方法利用门控交叉注意力机制从CLIP的高层编码块中捕获细粒度语义特征，并自适应抑制背景等无关信息，随后进行多空间解耦。在MIT-States、UT-Zappos和C-GQA三个基准上实验表明，CAMs在闭世界和开世界设置下均达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的组合零样本学习方法依赖图像编码器获取的全局语义表示，但该表示容量有限，难以完全解耦属性与对象，导致对未见组合的泛化能力不足。

Method: 提出CAMs方法，包括门控交叉注意力模块用于从CLIP高层编码块中提取细粒度语义特征并抑制无关信息，以及多空间解耦模块实现属性与对象语义的分离。

Result: 在MIT-States、UT-Zappos和C-GQA三个基准数据集上，CAMs在闭世界和开世界设置下均取得当前最优性能，验证了其有效性与优越性。

Conclusion: CAMs通过在多维空间中实现语义解耦，有效提升了组合零样本学习中对未见组合的泛化能力，是当前最先进的方法之一。

Abstract: Compositional zero-shot learning (CZSL) aims to learn the concepts of attributes and objects in seen compositions and to recognize their unseen compositions. Most Contrastive Language-Image Pre-training (CLIP)-based CZSL methods focus on disentangling attributes and objects by leveraging the global semantic representation obtained from the image encoder. However, this representation has limited representational capacity and do not allow for complete disentanglement of the two. To this end, we propose CAMS, which aims to extract semantic features from visual features and perform semantic disentanglement in multidimensional spaces, thereby improving generalization over unseen attribute-object compositions. Specifically, CAMS designs a Gated Cross-Attention that captures fine-grained semantic features from the high-level image encoding blocks of CLIP through a set of latent units, while adaptively suppressing background and other irrelevant information. Subsequently, it conducts Multi-Space Disentanglement to achieve disentanglement of attribute and object semantics. Experiments on three popular benchmarks (MIT-States, UT-Zappos, and C-GQA) demonstrate that CAMS achieves state-of-the-art performance in both closed-world and open-world settings. The code is available at https://github.com/ybyangjing/CAMS.

</details>


### [45] [CylinderDepth: Cylindrical Spatial Attention for Multi-View Consistent Self-Supervised Surround Depth Estimation](https://arxiv.org/abs/2511.16428)
*Samer Abualhanud,Christian Grannemann,Max Mehltretter*

Main category: cs.CV

TL;DR: 本文提出一种几何引导的自监督多相机环视深度估计方法，通过将各图像的3D点投影到共享单位圆柱面，建立跨图像的邻域关系，并利用显式非学习的空间注意力机制，根据圆柱面上的距离聚合跨图像特征，实现高一致性、密集且度量一致的深度估计。在DDAD和nuScenes数据集上验证，该方法在跨图像深度一致性与整体精度上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督环视深度估计方法在重叠图像之间存在深度估计不一致的问题，影响了360°三维感知的准确性与鲁棒性。为解决这一问题，需要一种能够保证跨视角深度一致性的方法。

Method: 首先基于相机内参和相对姿态参数，为每张图像预测初始深度图；将各图像的3D点投影至共享单位圆柱面，生成每个像素在圆柱上的二维位置映射；基于这些位置映射，设计一种显式、非学习的空间注意力机制，按圆柱面上像素间的距离聚合跨图像特征，最终输出一致的深度图。

Result: 在DDAD和nuScenes数据集上，所提方法显著提升了跨图像深度估计的一致性，并在整体深度精度上优于当前最先进的自监督方法。

Conclusion: 本方法通过几何约束与显式空间注意力机制，有效解决了多视角环视深度估计中的不一致性问题，实现了高精度、跨视角一致的360°深度感知，适用于低成本、密集三维感知场景。

Abstract: Self-supervised surround-view depth estimation enables dense, low-cost 3D perception with a 360° field of view from multiple minimally overlapping images. Yet, most existing methods suffer from depth estimates that are inconsistent between overlapping images. Addressing this limitation, we propose a novel geometry-guided method for calibrated, time-synchronized multi-camera rigs that predicts dense, metric, and cross-view-consistent depth. Given the intrinsic and relative orientation parameters, a first depth map is predicted per image and the so-derived 3D points from all images are projected onto a shared unit cylinder, establishing neighborhood relations across different images. This produces a 2D position map for every image, where each pixel is assigned its projected position on the cylinder. Based on these position maps, we apply an explicit, non-learned spatial attention that aggregates features among pixels across images according to their distances on the cylinder, to predict a final depth map per image. Evaluated on the DDAD and nuScenes datasets, our approach improves the consistency of depth estimates across images and the overall depth compared to state-of-the-art methods.

</details>


### [46] [Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation](https://arxiv.org/abs/2511.16435)
*Jin Wang,Bingfeng Zhang,Jian Pang,Mengyu Liu,Honglong Chen,Weifeng Liu*

Main category: cs.CV

TL;DR: 本文提出一种基于语言驱动属性泛化的少样本分割方法（LDAG），通过利用大语言模型生成目标类别的多属性描述，构建更鲁棒的元引导机制。针对支持图像中的内在类内差异问题，该方法不依赖传统支持图像作为参考，而是通过多属性增强（MaE）模块生成详细语言描述，并结合多模态匹配建立视觉-文本先验；同时设计多模态属性对齐（MaA）模块缓解文本与视觉特征间的模态偏移，提升跨模态交互能力。实验表明，该方法在少样本分割任务上显著优于现有方法，达到新的性能上限。


<details>
  <summary>Details</summary>
Motivation: 现有少样本分割方法依赖支持图像提取元信息作为引导，但受限于类内视觉变化，难以提供准确且通用的指导。本文认为支持图像并非必要，关键在于提供对训练和未训练类别都无偏的元引导，因此提出以语言描述为核心的新范式。

Method: 提出语言驱动属性泛化（LDAG）架构，包含两个核心模块：1）多属性增强（MaE）模块，利用大语言模型生成目标类别的多种细粒度属性描述，并通过多模态匹配构建精炼的视觉-文本先验；2）多模态属性对齐（MaA）模块，实现文本属性与视觉特征之间的跨模态交互与对齐，缓解模态偏移问题。

Result: 在多个标准少样本分割数据集上，所提方法显著超越现有先进方法，取得新的最佳性能，验证了其有效性与泛化能力。

Conclusion: 本文证明了语言描述可替代传统支持图像作为元引导的核心来源。通过结合大语言模型生成的多属性描述与多模态对齐机制，能够构建更鲁棒、无偏的少样本分割策略，为未来少样本学习提供了新思路。

Abstract: Few-shot segmentation (FSS) aims to segment novel classes under the guidance of limited support samples by a meta-learning paradigm. Existing methods mainly mine references from support images as meta guidance. However, due to intra-class variations among visual representations, the meta information extracted from support images cannot produce accurate guidance to segment untrained classes. In this paper, we argue that the references from support images may not be essential, the key to the support role is to provide unbiased meta guidance for both trained and untrained classes. We then introduce a Language-Driven Attribute Generalization (LDAG) architecture to utilize inherent target property language descriptions to build robust support strategy. Specifically, to obtain an unbiased support representation, we design a Multi-attribute Enhancement (MaE) module, which produces multiple detailed attribute descriptions of the target class through Large Language Models (LLMs), and then builds refined visual-text prior guidance utilizing multi-modal matching. Meanwhile, due to text-vision modal shift, attribute text struggles to promote visual feature representation, we design a Multi-modal Attribute Alignment (MaA) to achieve cross-modal interaction between attribute texts and visual feature. Experiments show that our proposed method outperforms existing approaches by a clear margin and achieves the new state-of-the art performance. The code will be released.

</details>


### [47] [StreetView-Waste: A Multi-Task Dataset for Urban Waste Management](https://arxiv.org/abs/2511.16440)
*Diogo J. Paulo,João Martins,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出StreetView-Waste数据集，用于解决城市垃圾容器溢出监测问题，支持容器检测、跟踪和溢出分割三任务。通过引入启发式追踪方法和基于几何先验的分割框架，显著提升性能，尤其在计数误差减少79.6%、轻量模型分割mAP@0.5提升27%方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾检测数据集多为静态环境且缺乏容器跟踪标注，难以支持真实物流场景下的垃圾管理需求，亟需面向垃圾车拍摄图像的动态、复杂城市环境数据集与评估基准。

Method: 构建StreetView-Waste数据集，涵盖真实城市街景中的垃圾与容器；针对检测、跟踪、分割三任务设计基准模型，并提出两种改进策略：基于启发式的容器跟踪优化方法，以及利用几何先验的模型无关分割增强框架。

Result: 细调的目标检测器在容器检测上表现良好，但基础追踪方法计数误差大；所提启发式方法使平均绝对计数误差降低79.6%；几何感知策略使轻量级模型的分割mAP@0.5提升27%，证明多模态输入对非规则垃圾分割的有效性。

Conclusion: StreetView-Waste为城市垃圾管理中的实际感知系统研究提供了具有挑战性的基准，推动了从静态检测向动态、连续感知系统的演进。

Abstract: Urban waste management remains a critical challenge for the development of smart cities. Despite the growing number of litter detection datasets, the problem of monitoring overflowing waste containers, particularly from images captured by garbage trucks, has received little attention. While existing datasets are valuable, they often lack annotations for specific container tracking or are captured in static, decontextualized environments, limiting their utility for real-world logistics. To address this gap, we present StreetView-Waste, a comprehensive dataset of urban scenes featuring litter and waste containers. The dataset supports three key evaluation tasks: (1) waste container detection, (2) waste container tracking, and (3) waste overflow segmentation. Alongside the dataset, we provide baselines for each task by benchmarking state-of-the-art models in object detection, tracking, and segmentation. Additionally, we enhance baseline performance by proposing two complementary strategies: a heuristic-based method for improved waste container tracking and a model-agnostic framework that leverages geometric priors to refine litter segmentation. Our experimental results show that while fine-tuned object detectors achieve reasonable performance in detecting waste containers, baseline tracking methods struggle to accurately estimate their number; however, our proposed heuristics reduce the mean absolute counting error by 79.6%. Similarly, while segmenting amorphous litter is challenging, our geometry-aware strategy improves segmentation mAP@0.5 by 27% on lightweight models, demonstrating the value of multimodal inputs for this task. Ultimately, StreetView-Waste provides a challenging benchmark to encourage research into real-world perception systems for urban waste management.

</details>


### [48] [VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference](https://arxiv.org/abs/2511.16449)
*Ziyan Liu,Yeqiu Chen,Hongyi Cai,Tao Lin,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: VLA-Pruner 是一种针对视觉-语言-动作（VLA）模型的新型轻量化剪枝方法，旨在解决现有方法因仅关注语义重要性而忽略动作执行所需信息的问题。该方法基于双层次重要性标准——语义层面的预填充注意力与动作层面的时间平滑注意力，提出自适应的双层剪枝策略，在有限计算预算下保留对任务至关重要的视觉标记，显著提升VLA模型在机器人操作任务中的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型的标记剪枝方法仅依赖语义显著性（如预填充注意力），忽视了VLA模型中高层语义理解与底层动作执行的双重特性，导致关键动作相关信息被丢弃，影响模型在实际机器人任务中的表现。

Method: VLA-Pruner 提出双层次重要性评估机制：一是使用视觉-语言预填充注意力衡量语义相关性；二是通过时间平滑估计动作解码注意力以捕捉动作层面的重要性。基于此，设计一种自适应的双层标记选择策略，动态保留高效、紧凑且信息丰富的视觉标记集合。

Result: 实验表明，VLA-Pruner 在多种VLA架构和多样化的机器人任务中均达到当前最优性能，有效提升了推理速度并保持高精度，证明其在真实部署场景下的优越性。

Conclusion: VLA-Pruner 通过融合语义与动作双重重要性评估，成功实现了对VLA模型的高效剪枝，在不牺牲性能的前提下大幅降低计算开销，为实时嵌入式智能体应用提供了可行方案。

Abstract: Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.

</details>


### [49] [LLaVA$^3$: Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs](https://arxiv.org/abs/2511.16454)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: 提出LLaVA$^3$，一种仅使用多视角2D图像、无需微调即可提升视觉语言模型3D场景理解能力的新方法。受立体派绘画启发，通过全景视觉表示融合多视角信息以模拟3D场景。在3D VQA和3D语言定位任务上表现优于现有基于2D的方法。


<details>
  <summary>Details</summary>
Motivation: 由于3D训练数据稀缺，而2D数据丰富，现有视觉语言模型难以有效理解3D场景。因此需要一种利用现有2D数据增强模型3D理解能力的方法。

Method: 受立体派绘画启发，构建基于多视角2D图像的全景视觉表示，通过中间多视角3D重建生成每个物体的全方位表征，输入给预训练的视觉语言模型进行推理。

Result: 在3D视觉问答和3D语言定位任务中，该方法显著优于现有的基于2D图像的视觉语言模型，证明了其在无3D标注情况下有效建模3D场景的能力。

Conclusion: LLaVA$^3$成功利用多视角2D图像实现了对3D场景的高质量理解，为缺乏3D数据的场景提供了一种高效且无需微调的解决方案。

Abstract: Developing a multi-modal language model capable of understanding 3D scenes remains challenging due to the limited availability of 3D training data, in contrast to the abundance of 2D datasets used for vision-language models (VLM). As an alternative, we introduce LLaVA$^3$ (pronounced LLaVA-Cube), a novel method that improves the 3D scene understanding capabilities of VLM using only multi-view 2D images and without any fine-tuning. Inspired by Cubist painters, who represented multiple viewpoints of a 3D object within a single picture, we propose to describe the 3D scene for the VLM through omnidirectional visual representations of each object. These representations are derived from an intermediate multi-view 3D reconstruction of the scene. Extensive experiments on 3D VQA and 3D language grounding show that our approach outperforms previous 2D-based VLM solutions.

</details>


### [50] [PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting](https://arxiv.org/abs/2406.10219)
*Alex Hanson,Allen Tu,Vasu Singla,Mayuka Jayawardhana,Matthias Zwicker,Tom Goldstein*

Main category: cs.CV

TL;DR: 本文提出了一种基于灵敏度的剪枝方法（PUP 3D-GS），用于压缩3D高斯点云表示，以在保持视觉保真度和前景细节的同时实现更高压缩率。该方法通过二阶近似重建误差来计算每个高斯分布的空间参数敏感性，并结合多轮剪枝-精炼流程，在不改变训练流程的前提下显著减少模型大小。在多个基准数据集上，该方法在剪除90%高斯点后仍能提升3.56倍渲染速度，且图像质量优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点云（3D-GS）虽然实现了高精度实时渲染，但其在复杂场景中需数百万个高斯点，导致存储与内存开销大，难以在资源受限设备上部署。现有剪枝方法依赖启发式规则，高压缩比下易造成视觉质量下降和前景细节丢失，因此亟需一种更精确、可扩展的剪枝策略。

Method: 提出一种基于二阶近似重建误差的灵敏度评分，衡量每个高斯点对空间参数变化的敏感程度；设计多轮剪枝-精炼管道，可在不修改原训练流程的情况下对预训练3D-GS模型进行高效压缩。

Result: 在Mip-NeRF 360、Tanks & Temples和Deep Blending数据集上，剪除90%高斯点后，平均渲染速度提升3.56倍，同时保留更多关键前景信息，图像质量指标（如PSNR、SSIM）优于现有方法。

Conclusion: 所提出的PUP 3D-GS方法在保证高质量渲染的同时实现了极高的压缩率，为3D-GS在移动或边缘设备上的部署提供了有效解决方案。

Abstract: Recent advances in novel view synthesis have enabled real-time rendering speeds with high reconstruction accuracy. 3D Gaussian Splatting (3D-GS), a foundational point-based parametric 3D scene representation, models scenes as large sets of 3D Gaussians. However, complex scenes can consist of millions of Gaussians, resulting in high storage and memory requirements that limit the viability of 3D-GS on devices with limited resources. Current techniques for compressing these pretrained models by pruning Gaussians rely on combining heuristics to determine which Gaussians to remove. At high compression ratios, these pruned scenes suffer from heavy degradation of visual fidelity and loss of foreground details. In this paper, we propose a principled sensitivity pruning score that preserves visual fidelity and foreground details at significantly higher compression ratios than existing approaches. It is computed as a second-order approximation of the reconstruction error on the training views with respect to the spatial parameters of each Gaussian. Additionally, we propose a multi-round prune-refine pipeline that can be applied to any pretrained 3D-GS model without changing its training pipeline. After pruning 90% of Gaussians, a substantially higher percentage than previous methods, our PUP 3D-GS pipeline increases average rendering speed by 3.56$\times$ while retaining more salient foreground information and achieving higher image quality metrics than existing techniques on scenes from the Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets.

</details>


### [51] [FastSurfer-CC: A robust, accurate, and comprehensive framework for corpus callosum morphometry](https://arxiv.org/abs/2511.16471)
*Clemens Pollak,Kersten Diers,Santiago Estrada,David Kügler,Martin Reuter*

Main category: cs.CV

TL;DR: FastSurfer-CC 是一个高效且全自动的胼胝体形态测量框架，可自动识别中矢状切片、分割胼胝体和穹窿、定位前/后连合以标准化头部位置，并生成厚度图谱和细分结构，提取8个形状指标用于统计分析。该方法在各项任务上均优于现有工具，并揭示了亨廷顿病患者与健康对照组之间先前未被发现的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏公开可用的全面自动化胼胝体分析工具，限制了其在衰老和神经疾病研究中的应用，尤其在深部脑刺激干预和脱髓鞘治疗临床试验中作为生物标志物的潜力。

Method: FastSurfer-CC 采用自动化流程，包括中矢状切片检测、胼胝体与穹窿分割、前/后连合定位、厚度剖面生成、区域细分及八种形状特征提取，实现全流程无干预分析。

Result: FastSurfer-CC 在各项任务中表现优于现有专用工具；在亨廷顿病研究中发现了当前最先进方法未能检测到的显著群体差异。

Conclusion: FastSurfer-CC 提供了一套强大、准确且高效的胼胝体分析解决方案，为神经科学研究和临床应用提供了重要支持。

Abstract: The corpus callosum, the largest commissural structure in the human brain, is a central focus in research on aging and neurological diseases. It is also a critical target for interventions such as deep brain stimulation and serves as an important biomarker in clinical trials, including those investigating remyelination therapies. Despite extensive research on corpus callosum segmentation, few publicly available tools provide a comprehensive and automated analysis pipeline. To address this gap, we present FastSurfer-CC, an efficient and fully automated framework for corpus callosum morphometry. FastSurfer-CC automatically identifies mid-sagittal slices, segments the corpus callosum and fornix, localizes the anterior and posterior commissures to standardize head positioning, generates thickness profiles and subdivisions, and extracts eight shape metrics for statistical analysis. We demonstrate that FastSurfer-CC outperforms existing specialized tools across the individual tasks. Moreover, our method reveals statistically significant differences between Huntington's disease patients and healthy controls that are not detected by the current state-of-the-art.

</details>


### [52] [BoxingVI: A Multi-Modal Benchmark for Boxing Action Recognition and Localization](https://arxiv.org/abs/2511.16524)
*Rahul Kumar,Vipul Baghel,Sudhanshu Singh,Bikash Kumar Badatya,Shivam Yadav,Babji Srinivasan,Ravi Hegde*

Main category: cs.CV

TL;DR: 本文提出一个针对拳击中出拳检测与分类的高质量视频数据集，包含6,915个手动标注的出拳片段，涵盖6种不同类型的出拳，来源于20段公开的YouTube训练视频，涉及18名运动员。数据集具有多样化的动作风格、摄像角度和体型特征，旨在支持实时视觉动作识别研究，尤其在资源受限和非受控环境中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉在格斗运动分析中虽取得进展，但缺乏稳健的数据集，主要受限于动作动态性、结构不规则性及录制环境差异。因此需要一个高质量、全面标注的数据集来推动该领域发展。

Method: 从20段公开的YouTube拳击训练视频中提取并筛选出6,915个高质视频片段，每段视频均经人工分割与标注，确保时间边界精确和类别一致性，涵盖多种动作风格、视角和运动员体型。

Result: 构建了一个丰富且多样化的拳击出拳数据集，可作为基准用于真实场景下的动作识别研究，促进自动化教练系统与表现评估的发展。

Conclusion: 本研究提供的数据集为拳击及其他相关领域的动作分析、自动训练辅助与性能评估提供了重要支持，有助于加速低资源、非受控环境下视觉识别技术的发展。

Abstract: Accurate analysis of combat sports using computer vision has gained traction in recent years, yet the development of robust datasets remains a major bottleneck due to the dynamic, unstructured nature of actions and variations in recording environments. In this work, we present a comprehensive, well-annotated video dataset tailored for punch detection and classification in boxing. The dataset comprises 6,915 high-quality punch clips categorized into six distinct punch types, extracted from 20 publicly available YouTube sparring sessions and involving 18 different athletes. Each clip is manually segmented and labeled to ensure precise temporal boundaries and class consistency, capturing a wide range of motion styles, camera angles, and athlete physiques. This dataset is specifically curated to support research in real-time vision-based action recognition, especially in low-resource and unconstrained environments. By providing a rich benchmark with diverse punch examples, this contribution aims to accelerate progress in movement analysis, automated coaching, and performance assessment within boxing and related domains.

</details>


### [53] [Contrastive vision-language learning with paraphrasing and negation](https://arxiv.org/abs/2511.16527)
*Kwun Ho Ngan,Saman Sadeghi Afgeh,Joe Townsend,Artur d'Avila Garcez*

Main category: cs.CV

TL;DR: 本文提出SemCLIP，一种基于大语言模型生成的原始、同义改写和否定文本三元组，改进CLIP的对比损失函数，以增强视觉-语言模型对语义变换（如同义改写与否定）的鲁棒性。实验表明，SemCLIP在保持原有性能的同时显著提升了对否定文本的区分能力，在CC-Neg数据集上图像检索准确率从68.1%提升至78.1%，且在零样本分类任务中表现优于CLIP。


<details>
  <summary>Details</summary>
Motivation: 现有对比视觉-语言模型（如CLIP）在面对否定或同义改写文本时表现不稳定，因否定改变语义但词汇变化小，而同义改写则语义相同但表达不同，这对模型对齐能力构成挑战。

Method: 提出一种新对比损失函数，结合大语言模型生成的原始、同义改写和否定文本三元组进行训练，使模型在嵌入空间中将同义改写向原图靠近，同时将否定文本推远。

Result: 在CC-Neg基准上，图像检索准确率从68.1%提升至78.1%；在Sugarcrepe++上虽结果混合，但整体优于仅用否定样本训练的模型；零样本分类任务中，SemCLIP表现全面优于CLIP。

Conclusion: SemCLIP通过联合建模同义改写与否定，显著增强了视觉-语言模型对语义变换的鲁棒性，同时保持甚至提升原有性能。

Abstract: Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.

</details>


### [54] [Enhancing Multi-Camera Gymnast Tracking Through Domain Knowledge Integration](https://arxiv.org/abs/2511.16532)
*Fan Yang,Shigeyuki Odashima,Shoichi Masui,Ikuo Kusajima,Sosuke Yamao,Shan Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种鲁棒的多相机体操运动员追踪方法，应用于国际体操锦标赛的裁判系统。针对体操场馆中摄像头数量有限、光照变化、背景干扰、服装相似及遮挡等问题导致多视角检测失败的情况，作者引入体操领域知识，利用运动员3D中心在表演过程中通常位于预定义垂直平面内的特性，通过射线-平面相交生成共面3D轨迹候选。提出一种新型级联数据关联（DA）范式：当跨视角检测充足时使用三角测量生成3D轨迹候选；当检测不足时则采用射线-平面相交进行补偿，从而减少轨迹不确定性与跟踪失败。实验验证了该方法在复杂场景下的优越性，并已在世界体操锦标赛中成功应用，获得国际体操联合会高度认可。


<details>
  <summary>Details</summary>
Motivation: 多相机体操运动员追踪面临摄像头数量受限、光照与背景变化、服装相似、遮挡等挑战，导致部分视角检测失败，传统多视角三角测量难以准确恢复3D轨迹。因此需要引入领域知识以提升追踪鲁棒性。

Method: 提出一种基于领域知识的级联数据关联范式：结合三角测量与射线-平面相交技术，在跨视角检测充足时使用三角测量生成3D轨迹候选，在检测不足时利用运动员3D中心通常位于预定义垂直平面的特性，通过射线-平面相交生成共面轨迹候选，以补偿不确定轨迹。

Result: 所提方法在复杂场景下显著降低跟踪失败率，优于现有方法；已成功部署于世界体操锦标赛裁判系统，获得国际体操联合会高度评价。

Conclusion: 通过融合体操领域知识与智能数据关联策略，本方法有效应对多相机体操追踪中的关键挑战，实现了高鲁棒性与实用性，具备实际应用价值。

Abstract: We present a robust multi-camera gymnast tracking, which has been applied at international gymnastics championships for gymnastics judging. Despite considerable progress in multi-camera tracking algorithms, tracking gymnasts presents unique challenges: (i) due to space restrictions, only a limited number of cameras can be installed in the gymnastics stadium; and (ii) due to variations in lighting, background, uniforms, and occlusions, multi-camera gymnast detection may fail in certain views and only provide valid detections from two opposing views. These factors complicate the accurate determination of a gymnast's 3D trajectory using conventional multi-camera triangulation. To alleviate this issue, we incorporate gymnastics domain knowledge into our tracking solution. Given that a gymnast's 3D center typically lies within a predefined vertical plane during \revised{much of their} performance, we can apply a ray-plane intersection to generate coplanar 3D trajectory candidates for opposing-view detections. More specifically, we propose a novel cascaded data association (DA) paradigm that employs triangulation to generate 3D trajectory candidates when cross-view detections are sufficient, and resort to the ray-plane intersection when they are insufficient. Consequently, coplanar candidates are used to compensate for uncertain trajectories, thereby minimizing tracking failures. The robustness of our method is validated through extensive experimentation, demonstrating its superiority over existing methods in challenging scenarios. Furthermore, our gymnastics judging system, equipped with this tracking method, has been successfully applied to recent Gymnastics World Championships, earning significant recognition from the International Gymnastics Federation.

</details>


### [55] [Investigating Optical Flow Computation: From Local Methods to a Multiresolution Horn-Schunck Implementation with Bilinear Interpolation](https://arxiv.org/abs/2511.16535)
*Haytham Ziani*

Main category: cs.CV

TL;DR: 本文分析了局部与全局方法在光流计算中的应用，重点研究了Horn-Schunck算法。通过结合多分辨率策略、双线性插值和延拓技术，改进了算法的精度与收敛性，并评估了其在不同图像条件下的运动估计性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高光流估计的准确性与鲁棒性，特别是在复杂图像条件下，需要结合局部与全局方法的优势，优化现有算法的性能。

Method: 采用局部方法（如Lucas-Kanade）与全局方法（如Horn-Schunck）进行对比分析，实现多分辨率版本的Horn-Schunck算法，利用双线性插值和延拓技术提升计算效率与精度。

Result: 所提出的多分辨率Horn-Schunck算法在多种图像条件下表现出更好的运动估计精度与更快的收敛速度，验证了该策略的有效性。

Conclusion: 结合多分辨率策略与全局优化方法能够显著提升光流计算的性能，尤其在低纹理或噪声较大的图像中具有更强的适应性。

Abstract: This paper presents an applied analysis of local and global methods, with a focus on the Horn-Schunck algorithm for optical flow computation. We explore the theoretical and practical aspects of local approaches, such as the Lucas-Kanade method, and global techniques such as Horn-Schunck. Additionally, we implement a multiresolution version of the Horn-Schunck algorithm, using bilinear interpolation and prolongation to improve accuracy and convergence. The study investigates the effectiveness of these combined strategies in estimating motion between frames, particularly under varying image conditions.

</details>


### [56] [Supervised Contrastive Learning for Few-Shot AI-Generated Image Detection and Attribution](https://arxiv.org/abs/2511.16541)
*Jaime Álvarez Urueña,David Camacho,Javier Huertas Tato*

Main category: cs.CV

TL;DR: 本文提出一种两阶段合成图像检测框架，通过监督对比学习提取判别性嵌入，并结合少样本k-NN分类器实现对新生成模型的高效泛化检测。仅需每类150张图像，检测准确率达91.3%，优于现有方法5.2个百分点；在源归属任务中，AUC和OSCR分别提升14.70%和4.27%，显著增强对不断演进的生成式AI的适应能力，无需频繁重训练。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展导致合成图像与真实内容难以区分，且新模型发布迅速，传统依赖周期性重训练的检测方法在计算和操作上已不可行，亟需具备强泛化能力、可快速适应新生成模型的检测机制。

Method: 第一阶段采用监督对比学习训练视觉深度模型，基于部分生成器数据训练并刻意排除特定架构以测试跨生成器泛化能力；第二阶段在学习到的嵌入空间中使用少样本k-NN分类器，利用少量来自未见生成器的样本进行快速适应。

Result: 在少样本设置下（每类150张图像），平均检测准确率达到91.3%，较现有方法提升5.2个百分点；在开放集源归属任务中，AUC提升14.70%，OSCR提升4.27%，展现出显著的泛化与可扩展性优势。

Conclusion: 所提出的两阶段框架有效应对了合成图像检测中的泛化难题，能够在不进行大规模重训练的前提下，快速适应新型生成模型，为构建可持续、鲁棒的数字媒体真实性保障系统提供了可行路径。

Abstract: The rapid advancement of generative artificial intelligence has enabled the creation of synthetic images that are increasingly indistinguishable from authentic content, posing significant challenges for digital media integrity. This problem is compounded by the accelerated release cycle of novel generative models, which renders traditional detection approaches (reliant on periodic retraining) computationally infeasible and operationally impractical.
  This work proposes a novel two-stage detection framework designed to address the generalization challenge inherent in synthetic image detection. The first stage employs a vision deep learning model trained via supervised contrastive learning to extract discriminative embeddings from input imagery. Critically, this model was trained on a strategically partitioned subset of available generators, with specific architectures withheld from training to rigorously ablate cross-generator generalization capabilities. The second stage utilizes a k-nearest neighbors (k-NN) classifier operating on the learned embedding space, trained in a few-shot learning paradigm incorporating limited samples from previously unseen test generators.
  With merely 150 images per class in the few-shot learning regime, which are easily obtainable from current generation models, the proposed framework achieves an average detection accuracy of 91.3\%, representing a 5.2 percentage point improvement over existing approaches . For the source attribution task, the proposed approach obtains improvements of of 14.70\% and 4.27\% in AUC and OSCR respectively on an open set classification context, marking a significant advancement toward robust, scalable forensic attribution systems capable of adapting to the evolving generative AI landscape without requiring exhaustive retraining protocols.

</details>


### [57] [EOGS++: Earth Observation Gaussian Splatting with Internal Camera Refinement and Direct Panchromatic Rendering](https://arxiv.org/abs/2511.16542)
*Pierrick Bournez,Luca Savant Aira,Thibaud Ehret,Gabriele Facciolo*

Main category: cs.CV

TL;DR: EOGS++ 是一种针对卫星影像的改进版 3D Gaussian Splatting 方法，直接处理高分辨率全色数据，无需预处理；通过嵌入光流与捆绑调整提升相机位姿估计，结合早停和 TSDF 后处理，显著提升重建质量和几何精度。在 IARPA 2016 和 DFC2019 数据集上表现优于原 EOGS 及其他 NeRF 方法，平均 MAE 建筑误差从 1.33 降至 1.19。


<details>
  <summary>Details</summary>
Motivation: 现有方法在地球观测中依赖复杂预处理或外部优化工具，限制了效率与精度。为实现更高效、更准确的卫星影像三维重建，需发展直接处理原始数据并内嵌优化机制的新方法。

Method: 提出 EOGS++，直接处理原始高分辨率全色图像；利用光流嵌入捆绑调整至训练过程，避免外部优化；引入早停策略与 TSDF 后处理以增强几何细节与重建清晰度。

Result: 在 IARPA 2016 与 DFC2019 数据集上，EOGS++ 实现了最先进的重建质量与效率，建筑区域平均 MAE 误差由 1.33 降低至 1.19，显著优于原始 EOGS 与基于 NeRF 的方法。

Conclusion: EOGS++ 证明了直接处理原始卫星数据并集成内嵌优化的有效性，兼具高质量重建与计算高效性，是地球观测领域 3D 重建的有力新范式。

Abstract: Recently, 3D Gaussian Splatting has been introduced as a compelling alternative to NeRF for Earth observation, offering com- petitive reconstruction quality with significantly reduced training times. In this work, we extend the Earth Observation Gaussian Splatting (EOGS) framework to propose EOGS++, a novel method tailored for satellite imagery that directly operates on raw high-resolution panchromatic data without requiring external preprocessing. Furthermore, leveraging optical flow techniques we embed bundle adjustment directly within the training process, avoiding reliance on external optimization tools while improving camera pose estimation. We also introduce several improvements to the original implementation, including early stopping and TSDF post-processing, all contributing to sharper reconstructions and better geometric accuracy. Experiments on the IARPA 2016 and DFC2019 datasets demonstrate that EOGS++ achieves state-of-the-art performance in terms of reconstruction quality and effi- ciency, outperforming the original EOGS method and other NeRF-based methods while maintaining the computational advantages of Gaussian Splatting. Our model demonstrates an improvement from 1.33 to 1.19 mean MAE errors on buildings compared to the original EOGS models

</details>


### [58] [NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening](https://arxiv.org/abs/2511.16566)
*Misaal Khan,Mayank Vatsa,Kuldeep Singh,Richa Singh*

Main category: cs.CV

TL;DR: NutriScreener is a novel, retrieval-augmented multi-pose graph attention network that leverages CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable accurate malnutrition detection and anthropometric prediction from children's images. It achieves high recall (0.79), AUC (0.82), and reduced RMSE in unconstrained settings, with up to 25% recall improvement and 3.5 cm RMSE reduction when using demographically matched knowledge bases. Clinically validated by doctors with high ratings for accuracy and efficiency, it is well-suited for low-resource environments.


<details>
  <summary>Details</summary>
Motivation: Existing malnutrition screening methods are labor-intensive and not scalable, limiting early intervention. There is a critical need for accurate, robust, and deployable tools that can work across diverse populations and real-world conditions, especially in low-resource settings.

Method: NutriScreener integrates CLIP-based visual embeddings for image understanding, class-boosted knowledge retrieval to address class imbalance, and a multi-pose graph attention network that captures contextual relationships across body poses. It uses demographically matched knowledge bases for enhanced generalization and is trained on 2,141 children from AnthroVision, with cross-dataset evaluation on ARAN and CampusPose.

Result: The model achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs compared to baseline methods. Cross-dataset evaluations show up to 25% higher recall and up to 3.5 cm reduction in RMSE when using matched knowledge bases. Doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency.

Conclusion: NutriScreener provides a scalable, accurate, and clinically viable solution for early malnutrition detection in pediatric populations, particularly effective in low-resource and diverse real-world settings.

Abstract: Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.

</details>


### [59] [POMA-3D: The Point Map Way to 3D Scene Understanding](https://arxiv.org/abs/2511.16567)
*Ye Mao,Weixun Luo,Ranran Huang,Junpeng Jing,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: POMA-3D 是首个从点图（point maps）中学习的自监督 3D 表示模型，利用结构化 2D 网格编码显式 3D 坐标，保留全局 3D 几何信息并兼容 2D 基础模型输入格式。通过视图到场景对齐策略，将丰富的 2D 先验迁移到 POMA-3D。针对点图在规范空间中具有视角依赖性的问题，提出 POMA-JEPA 架构，实现多视角下几何一致的特征表示。构建了 ScenePoint 数据集，包含 6.5K 房间级 RGB-D 场景和 100 万张 2D 图像场景，支持大规模预训练。实验表明，POMA-3D 在 3D 问答、具身导航、场景检索和具身定位等任务中表现优异，仅使用几何输入（3D 坐标）即可达成良好效果，为 3D 场景理解提供了一种新的点图范式，缓解了 3D 表示学习中预训练先验稀缺与数据有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 表示学习面临预训练先验匮乏和数据不足的挑战，而 2D 基础模型拥有丰富的视觉先验。如何有效将 2D 先验迁移至 3D 领域，并构建适用于 3D 的高效表征学习框架，是关键问题。此外，当前 3D 表示常依赖密集点云或网格，难以与成熟 2D 模型兼容。因此，亟需一种能融合 2D 优势、保持 3D 几何完整性且适配大规模预训练的新型 3D 表示方法。

Method: 提出点图（point map）作为 3D 场景的结构化表示形式，将 3D 坐标映射到 2D 网格上；设计视图到场景对齐策略，使模型学习跨视角的一致语义；引入 POMA-JEPA 架构，通过联合嵌入与预测机制，强制多视角点图特征在几何上保持一致性；构建 ScenePoint 大规模数据集，涵盖真实 3D 场景与 2D 图像，用于预训练。

Result: POMA-3D 在多个 3D 理解任务中表现出色，包括 3D 问答、具身导航、场景检索和具身定位，所有任务均仅使用 3D 坐标作为输入。其性能优于多种基线模型，在通用性和专用性任务上均展现出强大潜力，验证了点图表示与自监督学习的有效性。

Conclusion: POMA-3D 提出了一种基于点图的自监督 3D 表示学习新范式，成功将 2D 基础模型的丰富先验迁移到 3D 领域，克服了 3D 数据稀疏与预训练资源不足的问题。该工作为 3D 场景理解提供了可扩展、高效且几何一致的骨干模型，具有广泛的应用前景。

Abstract: In this paper, we introduce POMA-3D, the first self-supervised 3D representation model learned from point maps. Point maps encode explicit 3D coordinates on a structured 2D grid, preserving global 3D geometry while remaining compatible with the input format of 2D foundation models. To transfer rich 2D priors into POMA-3D, a view-to-scene alignment strategy is designed. Moreover, as point maps are view-dependent with respect to a canonical space, we introduce POMA-JEPA, a joint embedding-predictive architecture that enforces geometrically consistent point map features across multiple views. Additionally, we introduce ScenePoint, a point map dataset constructed from 6.5K room-level RGB-D scenes and 1M 2D image scenes to facilitate large-scale POMA-3D pretraining. Experiments show that POMA-3D serves as a strong backbone for both specialist and generalist 3D understanding. It benefits diverse tasks, including 3D question answering, embodied navigation, scene retrieval, and embodied localization, all achieved using only geometric inputs (i.e., 3D coordinates). Overall, our POMA-3D explores a point map way to 3D scene understanding, addressing the scarcity of pretrained priors and limited data in 3D representation learning. Project Page: https://matchlab-imperial.github.io/poma3d/

</details>


### [60] [Adaptive Guided Upsampling for Low-light Image Enhancement](https://arxiv.org/abs/2511.16623)
*Angela Vivian Dcosta,Chunbo Song,Rafael Radkowski*

Main category: cs.CV

TL;DR: AGU是一种高效的低光图像上采样方法，通过多参数优化学习低光与明亮图像特征之间的关联，能够在实时处理中生成高质量图像，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于引导图像的方法在处理低光图像时因噪声高、亮度低而表现不佳，难以有效传递图像特征，因此需要一种能够同时优化降噪和锐化等多重质量特性的方法。

Method: 提出自适应引导上采样（AGU），利用少量样本图像对学习低光与明亮图像间的多特征关联，通过多参数优化实现高效且实时的图像质量提升。

Result: 实验表明，AGU能以低质量、低分辨率输入实时生成高质量图像，在低光场景下显著优于当前最先进的方法。

Conclusion: AGU通过学习多特征关联并结合多参数优化，有效解决了低光图像上采样中的噪声与模糊问题，实现了实时、高质量的图像增强效果。

Abstract: We introduce Adaptive Guided Upsampling (AGU), an efficient method for upscaling low-light images capable of optimizing multiple image quality characteristics at the same time, such as reducing noise and increasing sharpness. It is based on a guided image method, which transfers image characteristics from a guidance image to the target image. Using state-of-the-art guided methods, low-light images lack sufficient characteristics for this purpose due to their high noise level and low brightness, rendering suboptimal/not significantly improved images in the process. We solve this problem with multi-parameter optimization, learning the association between multiple low-light and bright image characteristics. Our proposed machine learning method learns these characteristics from a few sample images-pairs. AGU can render high-quality images in real time using low-quality, low-resolution input; our experiments demonstrate that it is superior to state-of-the-art methods in the addressed low-light use case.

</details>


### [61] [Solving Spatial Supersensing Without Spatial Supersensing](https://arxiv.org/abs/2511.16655)
*Vishaal Udandarao,Shyamgopal Karthik,Surabhi S. Nath,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.CV

TL;DR: 本文对Cambrian-S提出的视频世界模型空间超感知基准（VSR和VSC）进行了批判性分析，发现其基准存在可被简单基线绕过的漏洞。通过引入无感知基线（NoSense）和视频重复扰动实验（VSC-Repeat），揭示了现有方法依赖数据集中的快捷启发式而非真正的空间认知。研究指出当前基准无法可靠衡量空间超感知能力，且原论文的推理策略实为利用数据偏差而非鲁棒的空间理解。


<details>
  <summary>Details</summary>
Motivation: 评估当前视频空间超感知基准的有效性，检验其是否真正反映模型对空间结构的理解能力，而非依赖数据中的偶然模式或捷径。

Method: 提出简单基线NoSense，用于测试在不依赖时空结构的情况下能否解决任务；设计VSC-Repeat扰动实验，通过重复视频片段改变时间结构但保持对象数量不变，观察模型性能变化；对比原方法与新实验结果，揭示其对数据捷径的依赖。

Result: NoSense在VSR上达到95%准确率，表明该任务可被忽略空间信息的方法近乎完美解决；在VSC中，原方法在视频重复后平均相对准确率从42%暴跌至0%，说明其推理严重依赖‘房间不重复访问’这一数据捷径。

Conclusion: 当前的VSI-Super基准未能有效测量空间超感知能力，而Cambrian-S所采用的预测感知推理策略主要依赖于数据集中的快捷启发式，而非真正的空间认知与跨经验整合能力。建议未来工作需构建更鲁棒、抗捷径的基准以推动真实空间智能发展。

Abstract: Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity

</details>


### [62] [Dataset Distillation for Pre-Trained Self-Supervised Vision Models](https://arxiv.org/abs/2511.16674)
*George Cazenavette,Antonio Torralba,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文研究了在使用大型预训练自监督模型时，如何通过数据蒸馏生成少量合成图像，以优化在线性分类器上的训练效果。提出了一种名为线性梯度匹配（Linear Gradient Matching）的方法，使合成图像通过预训练特征提取器后，产生的梯度与真实数据相似。该方法生成的合成数据不仅优于所有真实图像基线，还能跨不同预训练模型泛化，例如用DINO模型蒸馏的数据可训练出性能媲美CLIP的线性探测器。此外，该方法在细粒度分类和模型可解释性方面表现优异，可用于预测模型嵌入空间的相似性或对对抗数据中伪相关性的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有数据蒸馏方法主要针对从零开始训练的随机初始化模型，而现代视觉模型越来越多依赖于大型预训练自监督模型。因此，需要一种新的数据蒸馏方法，能够有效支持在预训练模型基础上训练线性探测器的任务。

Method: 提出线性梯度匹配方法，通过优化合成图像，使其经过预训练特征提取器后，在线性分类器上产生的梯度与真实数据一致。该方法不依赖特定模型结构，具有良好的跨模型泛化能力。

Result: 所生成的合成数据在多个任务上超越真实图像基线，且能跨不同预训练模型通用；在细粒度分类、模型可解释性等方面表现出色，可有效评估模型间的嵌入空间相似性及对伪相关性的敏感性。

Conclusion: 线性梯度匹配是一种高效且通用的数据蒸馏方法，适用于基于预训练模型的线性探测训练，为模型训练、评估与解释提供了有力工具。

Abstract: The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.

</details>


### [63] [SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking](https://arxiv.org/abs/2511.16618)
*Haofeng Liu,Ziyue Wang,Sudhanshu Mishra,Mingqi Gao,Guanyi Qin,Chang Han Low,Alex Y. W. Kong,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出SA-SV，一个包含实例级时空标注（masklets）的大型外科交互式视频对象分割基准，涵盖八类手术程序，共61,000帧和1,600个masklets，用于长时跟踪与零样本泛化评估。基于此，作者提出SAM2S，通过可训练的多样化记忆机制（DiveMem）、时间语义学习和模糊性鲁棒学习，增强SAM2在手术场景中的表现。实验表明，微调后SAM2性能提升12.99点平均$\mathcal{J}$\&$\mathcal{F}$，而SAM2S达到80.42点，优于原始和微调版SAM2，并保持68 FPS实时推理与强零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有交互式视频对象分割模型如SAM2在手术场景中受限于领域差距和有限的长时跟踪能力，缺乏大规模、精细标注的外科视频数据集支持，亟需构建专用基准并改进模型以适应复杂手术环境。

Method: 提出SA-SV基准数据集，包含多源外科视频的实例级时空标注；设计SAM2S模型，引入DiveMem实现鲁棒长时跟踪，结合时间语义学习提升器械理解，采用模糊性鲁棒学习缓解标注不一致问题。

Result: 在SA-SV上微调使SAM2性能提升12.99点平均$\mathcal{J}$\&$\mathcal{F}$；SAM2S进一步达到80.42点，优于原始及微调版SAM2，且保持68 FPS实时推理速度和良好的零样本泛化能力。

Conclusion: SA-SV是首个大规模外科交互式视频对象分割基准，为该领域研究提供了重要基础；SAM2S显著提升了手术视频分割性能，在精度、效率和泛化性方面均取得突破，具备实际应用潜力。

Abstract: Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \textbf{SAM2} for \textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\mathcal{J}$\&$\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\mathcal{J}$\&$\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.

</details>


### [64] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文针对长尾分布下的2D目标检测问题，基于LVISv1数据集，提出改进的Balanced Group Softmax（BAGS）框架，并引入度量学习与k-NN分类策略，以提升稀有类别检测性能。实验表明，该方法在mAP上达到24.5%，优于此前24.0%的基准，实现了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 真实世界中的目标检测任务常面临类别分布极不均衡的问题，即多数类别样本丰富，而少数类别样本极少，导致模型偏向常见类别，影响稀有类别的检测效果。现有方法在处理长尾分布时仍存在局限性，亟需更有效的解决方案。

Method: 采用两阶段Faster R-CNN架构，在BAGS基础上进行优化，并引入度量学习以增强特征表示能力，使同类特征紧密聚集、异类特征充分分离；推理阶段使用k-Nearest Neighbors（k-NN）进行分类，尤其提升稀有类别的识别精度。

Result: 在LVISv1数据集上，所提方法取得24.5%的mAP，超越先前24.0%的基准，显著提升了长尾场景下对稀有类别的检测性能。

Conclusion: 通过结合改进的BAGS损失、度量学习和k-NN分类策略，有效缓解了长尾分布带来的类别不平衡问题，为复杂现实场景中的目标检测提供了更具鲁棒性的解决方案。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [65] [SAM 3D: 3Dfy Anything in Images](https://arxiv.org/abs/2511.16624)
*SAM 3D Team,Xingyu Chen,Fu-Jen Chu,Pierre Gleize,Kevin J Liang,Alexander Sax,Hao Tang,Weiyao Wang,Michelle Guo,Thibaut Hardin,Xiang Li,Aohan Lin,Jiawei Liu,Ziqi Ma,Anushka Sagar,Bowen Song,Xiaodong Wang,Jianing Yang,Bowen Zhang,Piotr Dollár,Georgia Gkioxari,Matt Feiszli,Jitendra Malik*

Main category: cs.CV

TL;DR: SAM 3D 是一个生成式模型，能够从单张图像中预测三维物体的几何、纹理和布局，特别适用于存在遮挡和场景杂乱的自然图像。通过人机协同标注流程，构建了大规模视觉对齐的3D重建数据集，并采用合成预训练与真实世界对齐相结合的多阶段训练框架，突破了3D数据瓶颈。在真实物体和场景上的用户偏好测试中，其表现至少优于现有方法5:1，将开源代码、模型权重、在线演示及新基准测试数据集。


<details>
  <summary>Details</summary>
Motivation: 解决自然图像中因遮挡和场景杂乱导致的3D物体重建难题，提升视觉上下文线索的利用效率，突破3D数据稀缺的瓶颈。

Method: 采用人-模型协同标注流程构建大规模视觉对齐的3D数据集，结合合成数据预训练与真实数据微调的多阶段训练框架，实现高精度的3D物体重建。

Result: 在真实世界物体和场景的重建任务中，相比现有方法有显著提升，在人类偏好测试中胜率至少达到5:1；同时发布代码、模型、在线演示和新基准数据集。

Conclusion: SAM 3D 成功实现了在复杂自然场景下高保真、视觉对齐的3D物体重建，为未来通用3D理解系统提供了重要基础。

Abstract: We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D "data barrier". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.

</details>


### [66] [TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming](https://arxiv.org/abs/2511.16642)
*Zeyuan Yin,Xiaoming Liu*

Main category: cs.CV

TL;DR: TRIM提出一种后训练方法，结合时空剪枝策略，通过轻量级选择器模型评估潜在高质高斯原型，并利用实例掩码去噪过滤冗余背景区域，显著提升3D扩散模型的生成效率与质量，同时支持推理时缩放。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯扩散模型因大量高斯原型导致的去噪和后处理耗时问题，实现高效且可扩展的生成。

Method: 采用轻量级选择器模型评估多采样噪声下的潜在高斯原型，实现早期轨迹剪枝；引入实例掩码去噪机制，通过过滤冗余背景区域减少每步去噪计算量。

Result: 实验表明，TRIM在不降低输出质量的前提下，显著提升了3D生成的效率与可扩展性。

Conclusion: TRIM是一种高效的后训练优化方法，能够有效加速3D高斯扩散模型的推理过程，同时保持高质量生成能力，并支持推理阶段的动态缩放。

Abstract: Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\textbf{TRIM}$ ($\textbf{T}$rajectory $\textbf{R}$eduction and $\textbf{I}$nstance $\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\href{https://github.com/zeyuanyin/TRIM}{link}$.

</details>


### [67] [Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision](https://arxiv.org/abs/2511.16650)
*Shuyu Cao,Chongshou Li,Jie Xu,Tianrui Li,Na Zhao*

Main category: cs.CV

TL;DR: This paper proposes a late-decoupled 3DHS framework with a bi-branch supervision mechanism to tackle multi-hierarchy conflicts and class imbalance. By using hierarchical consistency and semantic prototypes, it improves segmentation accuracy across multiple levels and outperforms existing methods on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Previous 3D hierarchical semantic segmentation (3DHS) methods have overlooked two key challenges: multi-hierarchy conflicts in cross-hierarchy optimization due to parameter-sharing models, and class imbalance across multiple hierarchies, which causes model performance to be dominated by major classes.

Method: The proposed framework includes a primary 3DHS branch and an auxiliary discrimination branch. It introduces a late-decoupled 3DHS architecture with multiple decoders guided by coarse-to-fine hierarchical consistency to mitigate underfitting and overfitting conflicts. Additionally, a 3DHS-oriented semantic prototype-based bi-branch supervision mechanism is designed to learn discriminative point cloud features and enable mutual supervision between branches, improving class-imbalance handling.

Result: Extensive experiments on multiple datasets and backbones show that the proposed method achieves state-of-the-art performance in 3DHS. Core components are also effective as plug-and-play enhancements for existing methods.

Conclusion: The proposed late-decoupled framework with bi-branch supervision effectively addresses multi-hierarchy conflicts and class imbalance in 3DHS, leading to significant performance improvements and strong generalization across different models and datasets.

Abstract: 3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.

</details>


### [68] [PartUV: Part-Based UV Unwrapping of 3D Meshes](https://arxiv.org/abs/2511.16659)
*Zhaoning Wang,Xinyue Wei,Ruoxi Shi,Xiaoshuai Zhang,Hao Su,Minghua Liu*

Main category: cs.CV

TL;DR: PartUV 是一种基于部件的 UV 展开方法，针对 AI 生成网格的噪声、不规则和条件差等问题，通过结合语义部件分解与新型几何启发式策略，在自顶向下的递归框架中生成更少且对齐部件的图表，同时保持低失真。该方法在多个数据集上优于现有工具和近期神经方法，在图表数量和接缝长度方面表现优异，且具备高成功率与新应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有 UV 展开方法在处理 AI 生成的噪声、不规则网格时容易产生大量碎片化图表和次优边界，导致伪影并影响下游任务。因此需要一种能减少图表数量、提升对齐性并控制失真的新方法。

Method: 基于 PartField 的学习型部件分解，结合高阶语义分解与几何启发式策略，采用自顶向下递归框架，确保每张图表失真低于用户设定阈值，并最小化总图表数。集成并扩展了参数化与打包算法，支持非流形与退化网格处理，实现高度并行化以提高效率。

Result: 在四个多样化数据集（人工、CAD、AI生成、通用形状）上评估，PartUV 在图表数量和接缝长度上优于现有方法，失真水平相当，对挑战性网格的成功率高，支持如部件特异性多图块打包等新应用。

Conclusion: PartUV 有效解决了 AI 生成网格在 UV 展开中的关键挑战，实现了更少、更合理、更高效的图表生成，为复杂网格的高质量展开提供了新范式。

Abstract: UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.

</details>


### [69] [TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing](https://arxiv.org/abs/2511.16662)
*Eddie Pokming Sheung,Qihao Liu,Wufei Ma,Prakhar Kaushik,Jianwen Xie,Alan Yuille*

Main category: cs.CV

TL;DR: TriDiff-4D提出了一种基于扩散模型的三平面重定位方法，用于生成高质量、时间连贯的4D角色。该方法通过自回归策略，以单次扩散过程合成每个3D帧，实现任意长度的4D序列生成，并支持骨骼驱动动画。相比现有方法，TriDiff-4D在时间一致性、运动准确性、计算效率和视觉保真度方面均有显著提升，生成时间从小时级缩短至秒级，且无需优化过程。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法存在时间与几何不一致、感知伪影、运动不规则、计算成本高以及动态控制能力有限等问题，难以满足高保真、可控3D动画的需求。

Method: 采用自回归策略，先从文本提示生成规范化的3D角色和对应的动作序列，再利用第二个扩散模型根据动作序列对角色进行动画化，通过学习大规模3D与动作数据中的结构和运动先验，实现骨骼驱动的4D生成。

Result: 实验表明，TriDiff-4D显著优于现有方法，在复杂动作生成、高保真外观和准确3D几何方面表现优异，生成时间从小时级降至秒级，且无需优化过程。

Conclusion: TriDiff-4D为文本驱动的4D角色生成提供了一种高效、高保真、可控制的新范式，具备广泛的应用前景。

Abstract: With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.

</details>


### [70] [V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models](https://arxiv.org/abs/2511.16668)
*Yang Luo,Xuanlei Zhao,Baijiong Lin,Lingting Zhu,Liyao Tang,Yuqi Liu,Ying-Cong Chen,Shengju Qian,Xin Wang,Yang You*

Main category: cs.CV

TL;DR: V-ReasonBench 是一个用于评估视频推理能力的基准，涵盖结构化问题解决、空间认知、模式推理和物理动态四个维度，基于合成与真实图像序列构建，提供可验证、可复现、无歧义的任务。对六种先进视频模型的评估揭示了各维度表现差异，同时对比了图像模型，分析了幻觉行为及视频时长对帧链推理的影响，为视频推理能力的统一评估提供了可靠框架。


<details>
  <summary>Details</summary>
Motivation: 当前生成式视频模型（如 Veo-3）展现出惊人的零样本推理能力，亟需系统化、可靠的评估方法来衡量其视频推理性能。

Method: 构建 V-ReasonBench 基准，整合合成与真实图像序列，设计四类可验证任务（结构化问题解决、空间认知、模式推理、物理动态），支持可复现、可扩展、无歧义的评估。

Result: 六种主流视频模型在不同推理维度上表现差异显著；视频模型与强图像模型相比存在特定幻觉模式；视频时长影响帧链推理效果。

Conclusion: V-ReasonBench 提供了一个统一、可复现的视频推理评估框架，有助于推动具备更可靠、人类对齐推理能力的模型发展。

Abstract: Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.

</details>


### [71] [Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO](https://arxiv.org/abs/2511.16669)
*Junhao Cheng,Liang Hou,Xin Tao,Jing Liao*

Main category: cs.CV

TL;DR: 本文提出VNEP（Video-Next-Event Prediction）任务，将视频作为回答模态用于预测下一个事件，突破传统文本回答的局限。针对现有模型在多模态理解、指令推理和视频生成一致性方面的挑战，提出VANS模型，结合视觉-语言模型（VLM）与视频扩散模型（VDM），通过联合强化学习优化（Joint-GRPO），使两者协同生成语义一致且可视化的视频输出。构建了包含10万样本的VANS-Data-100K数据集，并在程序性与预测性任务上达到当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 视频能直观展示物理世界信息，如教学系领带等过程，而文本难以表达；现有下一事件预测（NEP）仅输出文本，无法充分展现动态过程。因此，提出以视频为答案的新模态，提升对复杂动作或流程的理解与教学效果。

Method: 提出VANS模型，采用联合强化学习优化策略（Joint-GRPO），协调视觉-语言模型（VLM）生成可视觉化描述，同时引导视频扩散模型（VDM）生成符合描述与输入上下文的高质量视频，实现跨模态一致性。

Result: 在程序性与预测性基准测试中，VANS在视频事件预测与可视化生成方面均达到领先性能，证明其在动态内容生成与理解上的有效性。

Conclusion: VANS成功将视频引入下一事件预测任务，实现了从“说”到“演”的转变，为基于视频的智能交互和过程性知识传递提供了新范式。

Abstract: While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.

</details>


### [72] [Learning to Think Fast and Slow for Visual Language Models](https://arxiv.org/abs/2511.16670)
*Chenyu Lin,Cheng Chi,Jinlin Wu,Sharon Li,Kaiyang Zhou*

Main category: cs.CV

TL;DR: 本文提出一种简单的强化学习方法，使视觉语言模型（VLM）能够根据任务难度自动在快速思考和慢速思考模式之间切换。该方法分为两个阶段：第一阶段基于模型输出长度对数据进行标注，区分需要快速或慢速思考的任务；第二阶段使用GRPO与思维模式标签训练模型，实现双模式思考。所提出的DualMindVLM模型在保持极高令牌效率的同时，性能显著优于基础模型，并达到与当前最先进视觉推理模型相当的水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在推理时倾向于生成冗长、详细的思维链，导致计算成本过高，而人类在面对简单问题时会快速反应，复杂问题则深入分析。因此，亟需一种能根据任务难度动态切换思考模式的机制，以提高效率并合理分配认知资源。

Method: 首先基于预训练模型输出长度对数据进行分类，判断其是否需要快速或慢速思考；然后采用GRPO算法结合思维模式标签对模型进行训练，使其具备在不同任务下自主选择思考模式的能力。

Result: DualMindVLM在多项视觉推理任务上表现优异，显著超越基线模型，且在推理效率方面具有明显优势，实现了高性能与高令牌效率的平衡。

Conclusion: 通过引入双模式思考机制，DualMindVLM成功模拟了人类的高效认知策略，在保证推理质量的同时大幅降低计算开销，为视觉语言模型的高效推理提供了新思路。

Abstract: When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.

</details>


### [73] [NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses](https://arxiv.org/abs/2511.16673)
*Jing Wen,Alexander G. Schwing,Shenlong Wang*

Main category: cs.CV

TL;DR: NoPo-Avatar 从单张或稀疏图像中恢复可动画的 3D 人体化身，无需依赖人类姿态输入，从而避免因姿态估计噪声导致的性能下降，在实际场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测试时依赖准确的相机位姿和人体姿态来引导重建，但当姿态估计存在噪声时，性能会显著下降。因此需要一种不依赖姿态输入的方法以提高鲁棒性和适用性。

Method: 提出 NoPo-Avatar，仅通过图像进行人体化身重建，完全去除对测试时人体姿态的依赖，利用图像内容自适应地学习姿态无关的几何与外观表示。

Result: 在 THuman2.0、XHuman 和 HuGe100K 等挑战性数据集上，NoPo-Avatar 在无真实姿态的实际设置下优于现有基线，在有真实姿态的实验室设置下也保持相当的性能。

Conclusion: NoPo-Avatar 成功实现了无需姿态输入的高质量 3D 人体重建，提升了方法在真实场景中的实用性与鲁棒性。

Abstract: We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language](https://arxiv.org/abs/2511.15887)
*Seungbeen Lee,Jinhong Jeong,Donghyun Kim,Yejin Son,Youngjae Yu*

Main category: cs.CL

TL;DR: 本文提出Motion2Mind框架，用于评估机器在解读人类非语言行为（NVC）方面的心理理论（ToM）能力。该框架基于专家整理的肢体语言参考知识库，构建了一个精细标注的视频数据集，包含222种非语言线索和397种心理状态。结果显示，当前AI系统在识别和解释非语言信号方面表现不佳，尤其在解释环节存在过度解读问题，与人类标注者差距明显。


<details>
  <summary>Details</summary>
Motivation: 现有心理理论（ToM）评估基准主要关注错信念任务和信息不对称推理，忽视了除信念之外的其他心理状态以及丰富的人类非语言交流形式。因此需要一个能全面评估机器对非语言行为理解能力的新框架。

Method: 构建Motion2Mind框架，利用专家整理的肢体语言参考知识库，创建一个包含细粒度非语言线索标注和人工验证心理解释的视频数据集，涵盖222种非语言行为和397种心理状态，并用于评估当前AI系统的表现。

Result: 当前人工智能系统在非语言行为的检测任务中表现较差，在解释任务中普遍存在过度解读现象，其性能与人类标注者相比存在显著差距。

Conclusion: Motion2Mind为评估机器在复杂社会情境下理解非语言沟通的心理理论能力提供了新的基准，揭示了现有AI系统在处理人类非语言行为时的局限性，强调未来研究需更关注多维度心理状态与非语言信号的融合理解。

Abstract: Our ability to interpret others' mental states through nonverbal cues (NVCs) is fundamental to our survival and social cohesion. While existing Theory of Mind (ToM) benchmarks have primarily focused on false-belief tasks and reasoning with asymmetric information, they overlook other mental states beyond belief and the rich tapestry of human nonverbal communication. We present Motion2Mind, a framework for evaluating the ToM capabilities of machines in interpreting NVCs. Leveraging an expert-curated body-language reference as a proxy knowledge base, we build Motion2Mind, a carefully curated video dataset with fine-grained nonverbal cue annotations paired with manually verified psychological interpretations. It encompasses 222 types of nonverbal cues and 397 mind states. Our evaluation reveals that current AI systems struggle significantly with NVC interpretation, exhibiting not only a substantial performance gap in Detection, as well as patterns of over-interpretation in Explanation compared to human annotators.

</details>


### [75] [TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues](https://arxiv.org/abs/2511.15976)
*Sarik Ghazarian,Abhinav Gullapalli,Swair Shah,Anurag Beniwal,Nanyun Peng,Narayanan Sadagopan,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出TOD-ProcBench，一个用于评估大语言模型在多轮任务导向对话中遵循复杂流程指令能力的挑战性基准。该基准基于高质量的ABCD数据集，包含精细约束和多层次条件-动作指令，涵盖三项任务：指令相关性检索与动作预测、违规响应识别、基于条件的合规生成。同时研究了多语言环境与指令文本格式对性能的影响，并以Llama 3.3社区许可协议发布。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向对话基准过于简化指令内容，仅用意图、槽位和API配置等简单模式表示，无法真实反映现实场景中复杂自然语言指令的约束性和流程性，因此亟需一个能系统评估大模型复杂指令遵循能力的新基准。

Method: 从ABCD数据集中提取高质量指令文档，构建包含多级条件-动作语句的精细约束体系；设计三类任务：1）从复杂指令中检索最相关语句并预测下一步操作；2）合成违反指令的响应，测试模型识别能力；3）根据原始复杂指令进行条件化合规生成；同时考察多语言和不同指令格式的影响。

Result: TOD-ProcBench有效揭示了当前主流大模型在处理复杂流程指令时的局限性，尤其在条件推理与违规检测方面表现不佳；实验表明指令格式和多语言设置显著影响模型的合规性能。

Conclusion: TOD-ProcBench为评估大模型在真实复杂任务导向对话中的指令遵循能力提供了全面且严谨的基准工具，有助于推动更鲁棒、更可控的对话系统发展。

Abstract: In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.

</details>


### [76] [Liars' Bench: Evaluating Lie Detectors for Language Models](https://arxiv.org/abs/2511.16035)
*Kieron Kretschmar,Walter Laurito,Sharan Maiya,Samuel Marks*

Main category: cs.CL

TL;DR: 本文介绍了LIARS' BENCH，一个包含72,863个由四个开源模型生成的谎言与诚实回答的测试基准，涵盖七种数据集，覆盖多种类型的谎言，并在两个维度上进行区分：模型说谎的原因和谎言针对的信念对象。评估三种黑盒与白盒谎言检测技术发现，现有方法在无法仅从文本推断是否说谎的场景中系统性失效，揭示了现有技术的局限性，并为谎言检测研究提供了实用测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有谎言检测技术多在狭窄场景下验证，无法全面反映大语言模型可能产生的多样化谎言，因此需要更广泛、更具代表性的测试基准来评估和改进这些技术。

Method: 构建LIARS' BENCH测试基准，收集四个开源模型在七个数据集上生成的大量谎言与诚实回应，设计涵盖不同说谎动机和目标信念的多样化场景，用于评估现有谎言检测技术的性能。

Result: 评估显示，现有谎言检测技术在无法仅通过文本判断说谎行为的场景中表现不佳，尤其对某些类型的谎言存在系统性识别失败，表明现有方法存在明显局限。

Conclusion: LIARS' BENCH有效揭示了当前谎言检测技术的不足，提供了一个全面且实用的测试平台，有助于推动该领域的发展。

Abstract: Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.

</details>


### [77] [Learning Tractable Distributions Of Language Model Continuations](https://arxiv.org/abs/2511.16054)
*Gwen Yidou-Weng,Ian Li,Anji Liu,Oliver Broadrick,Guy Van den Broeck,Benjie Wang*

Main category: cs.CL

TL;DR: LTLA 是一种混合方法，结合了强大的语言模型（LM）的上下文编码能力与固定可处理的隐马尔可夫模型（HMM）的精确延续概率计算。它通过批量更新 HMM 来避免对每个候选词重新计算，同时仅将语言模型的隐藏表示用于调整 HMM 的初始状态，从而实现计算复用，显著提升条件生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用弱上下文感知的近似模型（如 HMM）来处理序列级约束，但其性能受限于上下文建模能力不足。为提高条件生成的准确性和质量，需要更有效的机制来捕捉上下文信息并高效计算未来文本的概率分布。

Method: 提出 Learning to Look Ahead (LTLA)，利用同一个基础语言模型进行丰富的前缀编码，并搭配一个固定的、可处理的 HMM 来精确计算延续概率。通过一次批量化的 HMM 更新处理所有候选下一个词，且仅将语言模型的隐藏状态作为 HMM 潜在状态的先验进行条件化，保持解码器不变以实现计算复用。

Result: 实验表明，LTLA 在条件似然上优于无条件 HMM，能够有效建模视觉-语言模型中的视觉上下文，提升受控生成任务中约束满足率的同时保持良好流畅性，且推理开销极小。

Conclusion: LTLA 通过融合神经上下文编码与可处理的精确概率模型，实现了高效、高精度的受控语言生成，在多种场景下表现优异，具有广泛的应用潜力。

Abstract: Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.

</details>


### [78] [Early science acceleration experiments with GPT-5](https://arxiv.org/abs/2511.16072)
*Sébastien Bubeck,Christian Coester,Ronen Eldan,Timothy Gowers,Yin Tat Lee,Alexandru Lupsasca,Mehtaab Sawhney,Robert Scherrer,Mark Sellke,Brian K. Spears,Derya Unutmaz,Kevin Weil,Steven Yin,Nikita Zhivotovskiy*

Main category: cs.CL

TL;DR: 本文通过多个案例研究展示了GPT-5在数学、物理、天文学、计算机科学、生物学和材料科学等领域的实际应用，证明其能加速科研进程并提出可验证的新成果，尤其在数学领域有四项经验证的新发现。虽然AI显著节省了专家时间，但人类的判断与输入仍不可或缺。


<details>
  <summary>Details</summary>
Motivation: 提升科研人员对前沿AI工具（如GPT-5）能力的认知，展示其在真实科研场景中的潜力与局限，推动人机协作模式的发展。

Method: 通过收集并分析人类研究人员与GPT-5协作的实际互动案例，记录其在不同学科中生成的科研步骤、创新点及失败环节，重点验证数学领域的新结果。

Result: GPT-5在多个学科中成功生成了可操作的研究步骤，其中四例数学成果经人工验证为正确，显著提升了研究效率，同时揭示了人类在关键判断上的不可替代性。

Conclusion: GPT-5等前沿AI已成为科学家强有力的助手，虽不能完全替代人类，但其在加速科研、解决复杂问题方面展现出巨大潜力，未来人机协同将成科研主流模式。

Abstract: AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.

</details>


### [79] [ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models](https://arxiv.org/abs/2511.16122)
*Qing Zhang,Bing Xu,Xudong Zhang,Yifan Shi,Yang Li,Chen Zhang,Yik Chung Wu,Ngai Wong,Yijie Chen,Hong Dai,Xiansen Chen,Mian Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于集成学习的自动提示优化框架ELPO，通过投票机制和共享生成策略结合不同搜索方法，提升提示优化的准确性和鲁棒性。相比现有方法，ELPO在多个任务上表现更优，如在ArSarcasm数据集上F1分数提升7.6%。


<details>
  <summary>Details</summary>
Motivation: 当前自动提示优化（APO）方法多依赖单一模型或算法，难以应对复杂任务，存在性能瓶颈。为突破此限制，需引入更高效、鲁棒的优化框架。

Method: 提出ELPO框架，采用集成学习思想，结合多种搜索方法与共享生成策略，通过投票机制筛选最优提示，并设计更高效的生成与搜索算法。

Result: 实验表明，ELPO在多个任务上优于现有先进方法，显著提升性能，例如在ArSarcasm数据集上F1分数提高7.6%。

Conclusion: ELPO通过集成学习机制实现了更高效、准确且鲁棒的提示优化，为大语言模型的实际应用提供了有效支持。

Abstract: The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.

</details>


### [80] [TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating](https://arxiv.org/abs/2511.16147)
*Dabiao Ma,Ziming Dai,Zhimin Xin,Shu Wang,Ye Wang,Haojun Fei*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数高效微调方法TS-PEFT，通过选择性地对部分位置索引应用微调修改，挑战了传统方法中对所有位置索引无差别修改的做法。实验表明，这种无差别修改不仅冗余，甚至可能降低性能。研究倡导更精准的微调策略，为大模型优化提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 传统参数高效微调（PEFT）方法对所有位置索引进行修改，但其必要性存疑，可能存在资源浪费或性能下降的问题。

Method: 提出Token-Selective PEFT（TS-PEFT），引入一个选择函数S，仅对部分位置索引应用PEFT修改，实现更精细的参数调整。

Result: 实验结果表明，对所有位置索引统一应用PEFT是冗余且可能有害的，而选择性修改可提升下游任务表现。

Conclusion: 应摒弃无差别微调策略，采用更具针对性的修改方式以优化大模型微调过程，为未来研究提供新框架。

Abstract: In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.

</details>


### [81] [SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning](https://arxiv.org/abs/2511.16198)
*Sebastian Haan*

Main category: cs.CL

TL;DR: SemanticCite 是一个基于 AI 的系统，用于通过全文分析验证引文准确性，并提供详细的推理和相关文本片段以增强上下文理解。它结合多种检索方法与四类分类体系（支持、部分支持、不支持、不确定），能够捕捉主张与来源之间的细微关系，适用于不同错误类型的纠正。实验表明，微调的轻量级语言模型在性能上可媲美大型商业系统，但计算成本显著降低，使大规模引文验证成为可能。该系统提供透明、基于证据的解释，提升用户理解和信任。研究贡献包括超过 1000 条引文的综合数据集（涵盖八个学科）、细调模型及完整的开源验证框架，为科研诚信、同行评审优化和 AI 生成内容的质量控制提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 学术文献面临引文错误、AI 生成的虚假引用以及传统引文格式无法精确定位支持段落等挑战，影响科学传播的准确性与可信度。亟需一种能自动验证引文准确性的工具，以保障研究完整性并应对日益增长的 AI 内容风险。

Method: 采用多检索策略结合四分类模型（支持、部分支持、不支持、不确定）进行引文语义分析；利用轻量级语言模型进行微调，在保证性能的同时降低计算开销；通过全文字分析提取上下文证据，生成可解释的判断结果。

Result: 微调后的轻量级模型在引文验证任务中达到与大型商业系统相当的性能，但资源消耗大幅减少；系统可有效识别各类引文错误类型，并提供透明、可追溯的解释；构建了跨八大学科的大规模标注数据集，支持后续研究与应用。

Conclusion: SemanticCite 为解决引文准确性问题提供了高效、可扩展且开源的解决方案，有助于提升科研诚信、优化同行评审流程，并为 AI 生成内容的质量控制奠定基础。

Abstract: Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.

</details>


### [82] [SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs](https://arxiv.org/abs/2511.16275)
*Xingtao Zhao,Hao Peng,Dingli Su,Xianghua Zeng,Chunyang Liu,Jinzhi Liao,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出一种名为语义结构熵（SeSE）的新型不确定性量化（UQ）框架，从语义结构信息角度提升大语言模型（LLM）在安全关键场景中的可靠性。通过构建自适应稀疏化的有向语义图以捕捉方向性语义依赖，并自动剔除冗余连接；进而利用层次抽象思想，将最优语义编码树的结构熵定义为SeSE，形式化表示语义空间压缩后的内在不确定性。更高的SeSE值意味着更高不确定性，预示着更可能产生幻觉。此外，该方法还扩展至长文本生成中的细粒度不确定性评估，通过建模断言间的随机语义交互实现理论可解释的幻觉检测。在29个模型-数据集组合上的实验表明，SeSE显著优于现有先进UQ基线方法，包括监督型强方法和近期提出的KLE。


<details>
  <summary>Details</summary>
Motivation: 当前主流的不确定性量化方法主要依赖语义概率分布或成对距离，忽视了潜在的语义结构信息，导致不确定性估计不够精确，难以有效防范大语言模型在安全关键场景下的幻觉问题。因此，亟需一种能从结构层面捕捉语义不确定性的新方法。

Method: 提出语义结构熵（SeSE）框架：1）设计自适应稀疏化有向语义图构造算法，保留关键语义依赖并去除干扰连接；2）通过层次抽象构建最优语义编码树，计算其结构熵作为不确定性度量；3）扩展至长文本生成中，基于断言间随机语义交互建模个体声明的不确定性。

Result: 在29个不同模型与数据集组合上进行的大量实验显示，SeSE在幻觉检测任务中显著优于多种先进基准方法，包括监督学习方法和近期提出的KLE方法，展现出更强的准确性与鲁棒性。

Conclusion: SeSE通过引入语义结构熵，从结构信息视角实现了对大语言模型内在不确定性的精准量化，不仅提升了幻觉检测能力，还具备良好的理论解释性，为安全部署大语言模型提供了可靠的技术支持。

Abstract: Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.

</details>


### [83] [SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning](https://arxiv.org/abs/2511.16324)
*Wei Xia,Zhi-Hong Deng*

Main category: cs.CL

TL;DR: 提出一种无需训练、模型无关的对齐框架SDA（Steering-Driven Distribution Alignment），通过动态重分配输出概率实现推理阶段的高效对齐，无需微调或大量监督。该方法轻量、资源高效，支持个性化偏好控制，在8个不同规模和来源的开源LLM上验证了其在帮助性、无害性和诚实性（3H）三个维度上的显著提升，平均提升分别为64.4%、30%和11.5%，展现出良好的通用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中的广泛部署，如何在不进行昂贵再训练或大量监督的情况下，确保模型输出与人类意图保持一致，成为关键挑战。现有方法在推理阶段缺乏高效、通用且轻量的对齐手段，亟需一种无需训练即可实现行为对齐的方案。

Method: 提出SDA框架，通过用户定义的对齐指令动态调整模型输出的概率分布，实现推理阶段的模型行为对齐。该方法不依赖模型结构或训练过程，具有模型无关性和可插拔特性，可独立使用或与训练对齐方法结合。

Result: 在8个不同规模和来源的开源大模型上评估，SDA在帮助性、诚实性和无害性三个维度上分别实现平均64.4%、30%和11.5%的性能提升，验证了其有效性、通用性和可扩展性。

Conclusion: SDA是一种高效、通用、轻量的推理阶段对齐方法，能够在不改变模型参数的前提下显著提升模型行为与人类意图的一致性，适用于多样化应用场景和个性化需求，为大语言模型的实际部署提供了重要技术支持。

Abstract: With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.

</details>


### [84] [Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement](https://arxiv.org/abs/2511.16331)
*Jiashu Yao,Heyan Huang,Shuang Zeng,Chuwei Luo,WangJie You,Jie Tang,Qingsong Liu,Yuhang Guo,Yangyang Kang*

Main category: cs.CL

TL;DR: 本文提出自重写框架（self-rewriting），通过让大推理模型自我重写推理过程，以提升内部思维质量。采用选择性重写策略，仅对模型一致性正确的简单样本进行重写，保留原有强化学习奖励信号。在实现上，将重写与生成合并于单一批次，仅引入约10%开销，保持算法可扩展性。实验表明，该方法在准确率-长度权衡上表现更优（准确率+0.6，推理长度减少46%），且在内部推理质量（LLM-as-a-judge评分）上显著提升（+7.2），有效缓解了过度思考、冗余思考等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果正确性奖励的强化学习方法仅关注最终答案正确性，缺乏对内部推理过程的细粒度监督，导致出现过思考、欠思考、冗余思考和混乱思考等内部推理质量问题。为解决此问题，提出自重写机制，旨在通过模型自我重写推理文本来改善内部思维质量。

Method: 提出选择性重写方法：仅对模型在多个推理中一致正确的‘简单’样本进行重写，以避免破坏原有的奖励信号；将重写与原始生成合并至同一训练批次，保证算法可扩展性，并引入约10%额外计算开销。模型通过重写后的推理文本学习更高质量的内部思维模式。

Result: 在多种任务和不同规模模型上验证有效；相比基线方法，准确率提升0.6%，推理长度减少46%（无需显式指令）；在LLM-as-a-judge评估下，内部推理质量评分提高7.2分，显著缓解了推理过程中的各类缺陷。

Conclusion: 自重写框架通过自我重写机制有效提升了大推理模型的内部推理质量，在不牺牲效率的前提下实现了更高的准确率与更简洁的推理路径，是增强复杂推理能力的重要进展。

Abstract: Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.

</details>


### [85] [NLP Datasets for Idiom and Figurative Language Tasks](https://arxiv.org/abs/2511.16345)
*Blake Matheny,Phuong Minh Nguyen,Minh Le Nguyen,Stephanie Reynolds*

Main category: cs.CL

TL;DR: 本文提出了一种新的大规模数据集，用于改进大语言模型（LLMs）对习语和比喻语言的理解。通过整合现有习语数据集并从大型语料库中提取上下文序列，构建了包含潜在和明确习语表达的两个新数据集，并用于评估预训练模型在习语识别任务中的表现。数据集经过后处理以支持模型无关训练，应用于槽位标注和序列标记任务，有助于提升模型对隐喻意义的处理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语料库在自然语言处理中具有优势，但习语和比喻语言仍然难以被大语言模型准确理解。现有的微调方法虽有效，但更高质量、更大规模的数据集仍能进一步缩小这一差距。因此，需要构建更全面、多样化的数据集来推动模型发展。

Method: 整合多个现有习语与比喻语言数据集，生成统一的习语列表；利用该列表从大规模语料库中抽取上下文序列；创建两个新数据集——一个包含潜在习语表达，另一个为人工标注的明确习语表达；对数据集进行后处理以实现模型无关训练兼容性；在槽位标注和序列标记任务上进行训练与评估。

Result: 所创建的数据集有效提升了预训练语言模型在习语识别任务中的表现，证明了大规模、多样化数据集对于增强模型理解隐喻语言能力的重要性。同时，这些数据集可作为未来研究的基础，支持多种模型架构的训练与评估。

Conclusion: 本研究通过构建高质量、大规模的习语与比喻语言数据集，显著改善了大语言模型对非字面语言的理解能力，为后续模型开发与评估提供了重要资源与方向。

Abstract: Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.

</details>


### [86] [Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies](https://arxiv.org/abs/2511.16353)
*Jonathan Kamp,Lisa Beinborn,Antske Fokkens*

Main category: cs.CL

TL;DR: 本文研究了人类自然语言解释（即推理理由）在评估模型是否基于正确原因学习标签中的作用。传统指标‘充分性’仅衡量推理理由的信息量，但无法揭示其对模型性能的影响。作者通过将充分性与两种建模范式关联：一是模型识别哪些词属于推理理由的能力（词元分类），二是通过注意力正则化将推理信息融入输入以提升模型性能。研究发现，信息丰富的推理理由并不一定有助于正确分类；相反，充分性更多反映非推理上下文对推理信息的干扰。此外，将推理信息引入模型输入可提升跨领域分类效果，但结果因任务和模型类型而异。最后，充分性与词元分类能力无明显关联。这些结果凸显了推理理由的复杂性，表明需要更系统化的度量方法来捕捉此类信息。


<details>
  <summary>Details</summary>
Motivation: 现有评估模型是否基于正确原因学习的指标——充分性，仅关注推理理由的信息量，无法有效揭示其对模型性能的实际影响。因此，亟需更深入理解推理理由与模型行为之间的关系，以推动更具解释性的模型设计。

Method: 将充分性与两种建模范式结合：1）通过词元分类任务评估模型识别推理词元的能力；2）通过注意力正则化将推理信息注入模型输入，观察其对跨域分类性能的影响。同时分析充分性与词元分类、模型性能之间的关系。

Result: 信息丰富的推理理由未必提升分类准确率；充分性主要反映非推理部分对推理信息的干扰；将推理信息加入输入可提升跨域性能，但效果不一致；充分性与词元分类能力无显著相关性。

Conclusion: 推理理由具有高度复杂性，当前单一指标如充分性难以全面刻画其作用机制。未来需要发展更系统、多维度的度量方法，以更好地理解推理理由如何影响模型决策过程。

Abstract: Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.

</details>


### [87] [AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser](https://arxiv.org/abs/2511.16397)
*Ren Ma,Jiantao Qiu,Chao Xu,Pei Chu,Kaiwen Liu,Pengli Ren,Yuan Qu,Jiahui Peng,Linfeng Hou,Mengjie Liu,Lindong Lu,Wenchang Ning,Jia Yu,Rui Min,Jin Shi,Haojiong Chen,Peng Zhang,Wenjian Zhang,Qian Jiang,Zengjie Hu,Guoqiang Yang,Zhenxiang Li,Fukai Shang,Zhongying Tu,Wentao Zhang,Dahua Lin,Conghui He*

Main category: cs.CL

TL;DR: 本文提出一种名为MinerU-HTML的新颖网页内容提取管道，将内容提取重构为序列标注问题，采用0.6B参数语言模型进行处理。与依赖启发式规则的现有工具（如Trafilatura）相比，MinerU-HTML通过两阶段格式化流程显式分类语义元素并转换为Markdown，显著提升结构化内容（如代码、公式、表格）的保留率。在7,887个标注网页的MainWebBench基准上，其ROUGE-N F1达81.8%，远超Trafilatura的63.6%。基于此构建的AICC（AI-ready Common Crawl）是一个7.3万亿词元的多语言语料库，在相同过滤条件下，使用AICC训练的模型在13项基准测试中平均准确率达50.8%，比使用Trafilatura提取数据的模型高出1.08个百分点，证明高质量提取对模型性能有显著影响。研究公开发布MainWebBench、MinerU-HTML和AICC，强调网页提取是语料构建中被低估但至关重要的环节。


<details>
  <summary>Details</summary>
Motivation: 现有网页数据清洗主要依赖过滤和去重，而将HTML到文本的提取视为固定预处理步骤，导致结构化内容（如公式、代码、表格）常被破坏。现有基于启发式的提取器（如Trafilatura）难以有效保留文档结构，限制了下游大模型性能。因此，作者提出改进提取质量可带来与激进过滤相当甚至更显著的性能提升，推动从“被动提取”转向“主动理解”。

Method: 将内容提取任务建模为序列标注问题，利用一个0.6B参数的语言模型进行端到端预测。采用两阶段格式化流程：第一阶段对页面中的语义元素（如标题、段落、代码块等）进行分类；第二阶段将分类结果转化为结构化的Markdown格式。该方法基于语义理解而非文本密度等启发式规则，具备良好的可扩展性。

Result: 在MainWebBench上，MinerU-HTML的ROUGE-N F1达到81.8%，显著优于Trafilatura的63.6%；对代码块和公式的保留率分别达到90.9%和94.0%。基于MinerU-HTML构建的AICC语料库（7.3万亿词元）在相同过滤策略下训练模型，平均准确率提升1.08个百分点（50.8%），优于RefinedWeb和FineWeb。

Conclusion: 高质量的HTML内容提取是影响大语言模型性能的关键因素，不应被忽视。基于语义理解的模型驱动提取方法（如MinerU-HTML）具有更强的表达能力与可扩展性，能够有效保留结构化信息，显著提升下游任务表现。本研究公开了相关数据集与工具，推动网页语料构建向更智能、更精确的方向发展。

Abstract: While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.

</details>


### [88] [Classification of worldwide news articles by perceived quality, 2018-2024](https://arxiv.org/abs/2511.16416)
*Connor McElroy,Thiago E. A. de Oliveira,Chris Brogly*

Main category: cs.CL

TL;DR: 该研究评估了监督学习和深度学习模型在区分低质量与高质量新闻文章方面的有效性。使用来自Common Crawl的140多万篇英文新闻文章，结合专家对579个网站的质量评分，构建了一个包含194个语言特征的数据集。结果显示，传统机器学习（如随机森林）表现良好（准确率0.7355，ROC AUC 0.8131），而深度学习模型中ModernBERT-large表现最佳（准确率0.8744，ROC-AUC 0.9593，F1 0.8739），DistilBERT系列也表现出色。表明基于CPU的传统与深度学习模型均可有效识别新闻内容的感知质量。


<details>
  <summary>Details</summary>
Motivation: 为了判断机器学习和深度学习模型是否能够有效区分新闻文章的感知质量，尤其是在大规模真实世界数据上的应用潜力。

Method: 采用3种传统机器学习分类器和3种深度学习模型，基于1412272篇新闻文章构建数据集，利用专家共识评级将网站分为高低质量两类，每篇文章提取194个语言特征进行训练与评估。

Result: 现代BERT-large模型在所有模型中表现最佳，准确率达0.8744，ROC-AUC达0.9593，F1为0.8739；其他BERT类模型如DistilBERT-base也有优异表现；传统机器学习模型如随机森林也具备良好区分能力。

Conclusion: 基于传统机器学习和深度学习模型，可以有效区分全球新闻文章的感知质量，且部分深度学习模型在性能上显著优于传统方法。

Abstract: This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.

</details>


### [89] [ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports](https://arxiv.org/abs/2511.16438)
*Sherine George,Nithish Saji*

Main category: cs.CL

TL;DR: ESGBench 是一个用于评估可解释 ESG 问答系统的新基准数据集和评估框架，包含多主题的领域相关问题、人工标注的答案及支持证据，旨在推动透明且负责任的 ESG 领域人工智能研究。


<details>
  <summary>Details</summary>
Motivation: 当前 ESG 问答系统缺乏可靠的评估标准，尤其在事实一致性、可追溯性和领域对齐方面存在明显缺陷，亟需一个高质量、细粒度的评估基准来推动可解释 AI 的发展。

Method: 构建涵盖多个 ESG 主题的问答对数据集，每条问题配有经人工校准的答案与支持证据，形成可验证的推理链，并利用该数据集评估主流大模型的表现。

Result: 实验表明，现有先进大模型在事实一致性、证据可追溯性以及领域知识对齐方面仍存在显著不足，暴露出当前 LLM 在处理专业可持续性报告时的关键弱点。

Conclusion: ESGBench 为 ESG 领域的可解释 AI 研究提供了有力工具，有助于提升模型的透明度与可信度，推动负责任的人工智能在可持续发展领域的应用。

Abstract: We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.

</details>


### [90] [Anatomy of an Idiom: Tracing Non-Compositionality in Language Models](https://arxiv.org/abs/2511.16467)
*Andrew Gomes*

Main category: cs.CL

TL;DR: 本文通过改进的路径修补算法发现变压器模型中处理习语表达的计算电路，揭示了'习语头'（Idiom Heads）和'增强接收'（augmented reception）等机制，展示了变压器在计算效率与鲁棒性之间的平衡，并为理解复杂语法结构的处理提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 研究变压器模型如何处理非组合性语言，特别是习语表达，以揭示其内在计算机制。

Method: 采用改进的路径修补算法进行电路发现与分析，识别关键注意力头和交互模式。

Result: 发现习语处理具有独特的计算模式，包括频繁激活的'习语头'以及早期处理导致的习语词元间增强注意力，即'增强接收'。

Conclusion: 这些发现表明变压器通过特定的电路机制在保持计算效率的同时具备处理非组合性语言的鲁棒性，为理解更复杂的语法结构提供了重要启示。

Abstract: We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.

</details>


### [91] [Arctic-Extract Technical Report](https://arxiv.org/abs/2511.16470)
*Mateusz Chiliński,Julita Ołtusek,Wojciech Jaśkowski*

Main category: cs.CL

TL;DR: Arctic-Extract is a lightweight, state-of-the-art model for extracting structural data (QA, entities, tables) from business documents. Weighing only 6.6 GiB, it runs efficiently on resource-constrained hardware like A10 GPUs (24 GB), processing up to 125 A4 pages per inference. The paper details its training protocols and evaluation results, showcasing strong performance in document understanding.


<details>
  <summary>Details</summary>
Motivation: To develop a high-performance yet deployable document understanding model suitable for real-world deployment on devices with limited computational resources, particularly for long business documents.

Method: Arctic-Extract employs an optimized architecture with efficient parameterization and training protocols tailored for structural data extraction from scanned or digital-born documents, enabling high accuracy while maintaining low memory footprint.

Result: The model achieves state-of-the-art performance in extracting structured information from business documents and can process up to 125 A4 pages on A10 GPUs, demonstrating strong scalability and efficiency on constrained hardware.

Conclusion: Arctic-Extract effectively balances high performance and low resource consumption, making it ideal for deploying document understanding systems in practical, resource-limited environments.

Abstract: Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.

</details>


### [92] [TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval](https://arxiv.org/abs/2511.16528)
*Özay Ezerceli,Mahmoud El Hussieni,Selva Taş,Reyhan Bayraktar,Fatma Betül Terzioğlu,Yusuf Çelebi,Yağız Asker*

Main category: cs.CL

TL;DR: 提出TurkColBERT，首个针对土耳其语信息检索的综合基准，系统比较密集编码器与延迟交互模型。通过两阶段适配流程，将英语和多语言编码器微调并转换为ColBERT风格检索器，在五个土耳其BEIR数据集上评估10个模型。结果显示，小型延迟交互模型（如colbert-hash-nano-tr）仅需100万参数，却保持600M密集编码器71%以上的平均mAP，且3-5倍更小；ColmmBERT-base-TR在特定任务上提升达+13.8% mAP。索引算法方面，MUVERA+Rerank比PLAID快3.33倍，且相对提升1.7% mAP，实现0.54毫秒低延迟查询。所有模型、配置和评估脚本均已开源。局限性在于依赖中等规模数据集（≤50K文档）和翻译基准，真实场景下表现需进一步验证。


<details>
  <summary>Details</summary>
Motivation: 当前神经信息检索系统在高资源语言中表现优异，但在形态丰富、资源较少的语言（如土耳其语）中研究不足。尽管密集编码器主导土耳其语检索，但延迟交互模型（保留细粒度匹配能力）尚未被系统评估。因此亟需建立针对土耳其语的全面基准，以探索不同架构的性能与效率。

Method: 提出两阶段适配管道：首先在土耳其语自然语言推理（NLI）和语义文本相似性（STS）任务上微调英文及多语言编码器；随后利用PyLate将这些模型转换为类似ColBERT的延迟交互式检索器，并在MS MARCO-TR上训练。评估涵盖10个模型，覆盖科学、金融、论辩等领域的五个土耳其BEIR数据集。同时对比不同索引算法（MUVERA vs PLAID）的效率与精度。

Result: 延迟交互模型显著优于密集编码器，即使参数量仅为后者的3-5分之一，仍能实现更高性能。例如，colbert-hash-nano-tr（100万参数）保持了600M dense encoder 71%以上的平均mAP。ColmmBERT-base-TR在领域特定任务上最高提升13.8% mAP。MUVERA+Rerank相比PLAID提速3.33倍，且相对提升1.7% mAP，使ColmmBERT-base-TR实现0.54毫秒查询延迟，具备生产部署潜力。

Conclusion: 延迟交互模型在土耳其语信息检索中展现出卓越的性能与参数效率，尤其适合资源受限环境。基于MUVERA的索引方案支持低延迟部署。该工作建立了首个土耳其语检索基准，推动该领域研究发展。未来需在更大规模数据集上验证其鲁棒性。

Abstract: Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.

</details>


### [93] [Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks](https://arxiv.org/abs/2511.16540)
*Éloïse Benito-Rodriguez,Einar Urdshals,Jasmina Nasufi,Nicky Pochinkov*

Main category: cs.CL

TL;DR: 本文提出了一种预测框架，通过分析大语言模型（LLM）的激活值来预测提示文本的文体类别。使用Mistral-7B和两个数据集，研究显示利用scikit-learn分类器可实现高达98%和71%的F1分数，且结果显著优于对照任务，证明了仅用浅层学习模型即可从LLM中提取文本文体信息的可能性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型（LLM）对于确保其安全和有益部署至关重要，但其结构的不可解释性以及无法对所有输出进行人工评估使得这一任务复杂化。因此，需要一种能够从模型内部状态推断文本特征的方法。

Method: 通过分析大语言模型在处理不同文本时的激活值，使用scikit-learn分类器训练模型以预测输入文本的文体类别，并在两个数据集上验证其性能。

Result: 在两个数据集中，该方法实现了最高98%和71%的F1分数，显著优于控制任务，表明文本文体可以从大语言模型的激活中有效推断。

Conclusion: 本研究为基于大语言模型内部表示进行文本特征预测提供了首个概念验证，展示了浅层学习模型在解析大模型行为方面的潜力，为后续可解释性和安全性研究奠定基础。

Abstract: Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.

</details>


### [94] [WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue](https://arxiv.org/abs/2511.16544)
*Zachary Ellis,Jared Joselowitz,Yash Deo,Yajie He,Anna Kalygina,Aisling Higham,Mana Rahimzadeh,Yan Jia,Ibrahim Habli,Ernest Lim*

Main category: cs.CL

TL;DR: 该研究挑战了语音识别（ASR）评估中对词错误率（WER）的依赖，发现其与临床影响关联性差。通过专家标注的金标准数据集，研究发现现有指标无法有效预测转录错误的临床风险。为此，提出基于大语言模型（Gemini-2.5-Pro）的自动化评估框架，经GEPA优化后达到人类专家水平（准确率90%，Cohen's κ=0.816），实现对临床安全性的可扩展、可验证评估。


<details>
  <summary>Details</summary>
Motivation: 当前语音识别在临床对话中的应用日益广泛，但评估仍依赖于词错误率（WER），而该指标未能反映转录错误对临床决策的实际影响。因此亟需一种能衡量错误临床后果的评估方法，以确保ASR系统的安全性与可靠性。

Method: 构建两个医生-患者对话数据集，由临床专家标注转录错误的临床影响（无、轻微或显著）。分析现有指标与专家标签的相关性，并引入大语言模型作为评判者（LLM-as-a-Judge），使用GEPA进行程序化优化，最终训练出具备人类水平判断能力的评估模型。

Result: 现有指标（包括WER）与临床风险标签相关性低；优化后的大语言模型（Gemini-2.5-Pro）在临床影响判断上达到90%准确率和0.816的Cohen's κ，表现接近人类专家。

Conclusion: 本研究提出了一个可验证、自动化的评估框架，将语音识别评价从单纯的文本准确性转向对临床安全性的考量，为高风险场景下的ASR系统部署提供了可靠评估手段。

Abstract: As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.

</details>


### [95] [Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation](https://arxiv.org/abs/2511.16577)
*Kexin Zhao,Ken Forbus*

Main category: cs.CL

TL;DR: 本文提出了一种无需人工标注训练数据的词义消歧方法，利用统计语言模型作为决策者，将符号化自然语言理解系统生成的多个候选含义转化为可区分的自然语言表达，并通过大语言模型在上下文中选择合适的解释，再反馈给符号系统。


<details>
  <summary>Details</summary>
Motivation: 现有词义消歧方法依赖于粗粒度表示（如WordNet同义词集或FrameNet框架）和人工标注的数据，难以处理更丰富的语义表示（如基于OpenCyc的表示），限制了复杂推理能力。

Method: 将符号化NLU系统生成的多候选意义转化为自然语言变体，利用大语言模型根据上下文选择最合适的解释，并将结果反馈至原系统。

Result: 在与人工标注金标准对比的评估中，该方法表现出有效性，证明其在无监督环境下实现高质量词义消歧的潜力。

Conclusion: 所提出的基于大语言模型的无监督词义消歧方法能够有效处理复杂语义表示，减少对人工标注数据的依赖，为高级自然语言理解提供了可行路径。

Abstract: Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.

</details>


### [96] [Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems](https://arxiv.org/abs/2511.16654)
*Elias Lumer,Alex Cardenas,Matt Melich,Myles Mason,Sara Dieter,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,Roberto Hernandez*

Main category: cs.CL

TL;DR: 本文对比了两种多模态RAG系统的检索方法：基于文本分块的检索（先将图像通过LLM总结为文本再嵌入）与直接多模态嵌入检索（图像原生存储于向量空间）。在新构建的金融财报电话会议基准上，实验表明直接多模态嵌入检索显著优于传统方法，mAP@5提升13%（相对提升32%），nDCG@5提升11%（相对提升20%），且生成答案更准确、事实一致。原因在于LLM摘要导致信息损失，而直接嵌入保留了视觉上下文。


<details>
  <summary>Details</summary>
Motivation: 现有多模态RAG系统依赖LLM对图像进行摘要转文本，导致视觉细节和上下文信息丢失，影响下游检索与问答性能。

Method: 提出并比较两种检索策略：(1) 先将图像用LLM总结为文本，再进行文本分块与嵌入；(2) 直接对图像进行多模态嵌入，原生保存于向量数据库中。在6个LLM和2个多模态嵌入模型上进行评估。

Result: 直接多模态嵌入检索在mAP@5和nDCG@5上分别取得13%和11%的绝对提升，相对提升达32%和20%，且生成答案更准确、事实一致。

Conclusion: LLM图像摘要会引入信息损失，而直接多模态嵌入能有效保留视觉上下文，显著提升多模态RAG系统的检索与推理性能。

Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization](https://arxiv.org/abs/2511.15714)
*Ariel Kamen,Yakov Kamen*

Main category: cs.AI

TL;DR: 该研究提出了一种基于大型语言模型（LLM）的集成框架eLLM，用于非结构化文本分类。通过整合多个模型，eLLM有效缓解了单个模型常见的不一致、幻觉、类别膨胀和误分类问题，在F1分数上相比最强单模型提升高达65%。研究通过数学建模形式化了集成决策过程，并建立了合理的聚合标准。在IAB层级分类体系下，对十种先进LLM进行零样本评估，结果显示个体模型因语义丰富文本压缩为稀疏类别表示而性能趋于饱和，而eLLM显著提升了鲁棒性和准确性，接近人类专家水平，可大幅减少对人工标注的依赖，提供一种可扩展且可靠的分类解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决单个大型语言模型在非结构化文本分类中存在的一致性差、幻觉、类别膨胀和误分类等问题，提升分类的准确性和鲁棒性。

Method: 提出集成大型语言模型（eLLM）框架，通过数学建模实现集体决策，建立原则性聚合准则；在相同零样本条件下评估十种SOTA LLMs，并利用IAB层级分类体系与人工标注数据集验证方法有效性。

Result: eLLM在F1分数上相比最强单模型提升高达65%，显著优于个体模型；在多样性模型组合下，表现接近人类专家水平，具备高鲁棒性与准确性，可大幅降低对人工标注的依赖。

Conclusion: eLLM框架通过多模型集成有效克服了单个模型的局限，实现了接近人类专家水平的文本分类性能，是一种可扩展、可靠且减少人工依赖的分类解决方案。

Abstract: This study introduces an ensemble framework for unstructured text categorization using large language models (LLMs). By integrating multiple models, the ensemble large language model (eLLM) framework addresses common weaknesses of individual systems, including inconsistency, hallucination, category inflation, and misclassification. The eLLM approach yields a substantial performance improvement of up to 65\% in F1-score over the strongest single model. We formalize the ensemble process through a mathematical model of collective decision-making and establish principled aggregation criteria. Using the Interactive Advertising Bureau (IAB) hierarchical taxonomy, we evaluate ten state-of-the-art LLMs under identical zero-shot conditions on a human-annotated corpus of 8{,}660 samples. Results show that individual models plateau in performance due to the compression of semantically rich text into sparse categorical representations, while eLLM improves both robustness and accuracy. With a diverse consortium of models, eLLM achieves near human-expert-level performance, offering a scalable and reliable solution for taxonomy-based classification that may significantly reduce dependence on human expert labeling.

</details>


### [98] [Graph-Memoized Reasoning: Foundations Structured Workflow Reuse in Intelligent Systems](https://arxiv.org/abs/2511.15715)
*Yash Raj Singh*

Main category: cs.AI

TL;DR: 本文提出Graph-Memoized Reasoning框架，通过图结构记忆存储和重用推理工作流，实现跨任务的组合式推理复用，降低计算开销并提升推理效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型推理系统在不同任务间重复计算相似推理步骤，造成资源浪费、延迟增加且难以复现，亟需持久化推理机制以实现历史计算痕迹的回忆与复用。

Method: 将过往决策图编码为图结构记忆，基于结构与语义相似性检索并重用子图，构建优化目标以平衡推理成本与存储与生成工作流之间的一致性，支持高效、一致的推理复用。

Result: 该框架实现了推理过程的高效复用，降低了推理延迟与计算成本，提升了系统的可解释性与自进化能力，为大规模智能体系统的持久记忆提供了理论基础与实践路径。

Conclusion: Graph-Memoized Reasoning为构建可解释、高效率、自我改进的推理架构奠定了基础，是实现大模型系统持久记忆的重要一步。

Abstract: Modern large language model-based reasoning systems frequently recompute similar reasoning steps across tasks, wasting computational resources, inflating inference latency, and limiting reproducibility. These inefficiencies underscore the need for persistent reasoning mechanisms that can recall and reuse prior computational traces.
  We introduce Graph-Memoized Reasoning, a formal framework for representing, storing, and reusing reasoning workflows as graph-structured memory. By encoding past decision graphs and retrieving them through structural and semantic similarity, our approach enables compositional reuse of subgraphs across new reasoning tasks.
  We formulate an optimization objective that minimizes total reasoning cost regularized by inconsistency between stored and generated workflows, providing a theoretical foundation for efficiency-consistency trade-offs in intelligent systems. We outline a conceptual evaluation protocol aligned with the proposed optimization objective.
  This framework establishes the groundwork for interpretable, cost-efficient, and self-improving reasoning architectures, offering a step toward persistent memory in large-scale agentic systems.

</details>


### [99] [MACIE: Multi-Agent Causal Intelligence Explainer for Collective Behavior Understanding](https://arxiv.org/abs/2511.15716)
*Abraham Itzhak Weinberg*

Main category: cs.AI

TL;DR: MACIE 是一个用于多智能体强化学习（MARL）系统的因果解释框架，结合结构因果模型、干预反事实分析和Shapley值，解决多智能体系统中个体贡献归因、集体行为涌现性量化及可操作解释的问题。它在四个MARL场景中表现出高精度的因果归因（平均phi_i=5.07，标准差<0.05）、有效检测合作任务中的正向涌现性（协同指数最高达0.461），且计算高效（单数据集0.79秒/次，可在CPU上实时运行）。该方法兼具因果严谨性、涌现性度量与多智能体支持，推动了可解释、可信、可问责的多智能体AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法在多智能体环境中表现不佳，无法准确将集体结果归因于个体、量化涌现行为或捕捉复杂交互关系，难以满足安全关键应用对决策透明性和责任追溯的需求。因此亟需一种能同时处理个体贡献、集体智能涌现与可读解释的新型解释框架。

Method: MACIE框架融合结构因果模型（SCM）以建模多智能体间因果关系，采用干预性反事实分析计算每个智能体的因果贡献（即干预性归因分数），利用Shapley值进行公平分配，并引入协同度量指标分离集体效应与个体贡献；最终通过自然语言生成技术整合因果洞察，生成可理解的解释叙事。

Result: 在四种多智能体强化学习场景（合作、竞争、混合动机）中，MACIE实现了精准的结果归因（平均贡献得分5.07，标准差低于0.05），成功识别出合作任务中的正向涌现行为（协同指数最高达0.461），且计算效率高，单次推理仅需0.79秒（运行于CPU），具备实时应用潜力。

Conclusion: MACIE首次系统性地将因果推断、涌现性量化与多智能体解释相结合，在保持因果严谨性的同时，提供可解释、可行动的自然语言解释，为构建可信赖、可问责的多智能体人工智能系统提供了关键技术支撑。

Abstract: As Multi Agent Reinforcement Learning systems are used in safety critical applications. Understanding why agents make decisions and how they achieve collective behavior is crucial. Existing explainable AI methods struggle in multi agent settings. They fail to attribute collective outcomes to individuals, quantify emergent behaviors, or capture complex interactions. We present MACIE Multi Agent Causal Intelligence Explainer, a framework combining structural causal models, interventional counterfactuals, and Shapley values to provide comprehensive explanations. MACIE addresses three questions. First, each agent's causal contribution using interventional attribution scores. Second, system level emergent intelligence through synergy metrics separating collective effects from individual contributions. Third, actionable explanations using natural language narratives synthesizing causal insights. We evaluate MACIE across four MARL scenarios: cooperative, competitive, and mixed motive. Results show accurate outcome attribution, mean phi_i equals 5.07, standard deviation less than 0.05, detection of positive emergence in cooperative tasks, synergy index up to 0.461, and efficient computation, 0.79 seconds per dataset on CPU. MACIE uniquely combines causal rigor, emergence quantification, and multi agent support while remaining practical for real time use. This represents a step toward interpretable, trustworthy, and accountable multi agent AI.

</details>


### [100] [How Modality Shapes Perception and Reasoning: A Study of Error Propagation in ARC-AGI](https://arxiv.org/abs/2511.15717)
*Bo Wen,Chen Wang,Erhan Bilal*

Main category: cs.AI

TL;DR: 该研究探讨了不同模态（文本与图像）在处理小色量化网格任务时对模型感知和推理的影响，发现结构化文本能精确捕捉稀疏特征坐标，图像虽能保留二维形状但受分辨率影响，结合两者可提升执行准确率（约8个感知点；中位相似度提升0.20）。通过分离感知与推理，验证了模态对系统泛化能力的瓶颈作用，并提出对齐表示与变压器归纳偏置、实现跨模态验证可提高指令准确性和执行可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有指令优先系统在处理复杂组合泛化任务时缺乏对编码方式如何影响模型感知的系统理解，且难以区分指令错误与执行错误。为推进对系统性泛化的评估，需明确不同模态在感知层面的优劣及其对整体性能的影响。

Method: 采用加权集合不一致度量方法，结合两阶段推理流程，在九种不同的文本与图像模态下隔离感知与推理过程，分析各模态在特征感知上的表现差异，并通过跨模态对比验证联合使用的效果。

Result: 结构化文本在稀疏特征定位上表现精准，图像模态能有效捕捉2D形状但对分辨率敏感，二者融合后在执行准确率上提升约8个感知点，中位相似度提高0.20，表明跨模态协同有助于提升系统可靠性。

Conclusion: 模态选择直接影响模型对输入结构的感知能力，合理对齐表示与变压器的归纳偏置，并引入文本与图像间的交叉验证机制，可在不改变基础模型的前提下显著提升指令生成准确性与执行稳定性，是实现更可靠系统性泛化的有效路径。

Abstract: ARC-AGI and ARC-AGI-2 measure generalization-through-composition on small color-quantized grids, and their prize competitions make progress on these harder held-out tasks a meaningful proxy for systematic generalization. Recent instruction-first systems translate grids into concise natural-language or DSL rules executed in generate-execute-select loops, yet we lack a principled account of how encodings shape model perception and how to separate instruction errors from execution errors. We hypothesize that modality imposes perceptual bottlenecks -- text flattens 2D structure into 1D tokens while images preserve layout but can introduce patch-size aliasing -- thereby shaping which grid features are reliably perceived. To test this, we isolate perception from reasoning across nine text and image modalities using a weighted set-disagreement metric and a two-stage reasoning pipeline, finding that structured text yields precise coordinates on sparse features, images capture 2D shapes yet are resolution-sensitive, and combining them improves execution (about 8 perception points; about 0.20 median similarity). Overall, aligning representations with transformer inductive biases and enabling cross-validation between text and image yields more accurate instructions and more reliable execution without changing the underlying model.

</details>


### [101] [Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models](https://arxiv.org/abs/2511.15720)
*Islem Sahraoui*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态AI框架，结合文本与视觉数据分析，以提升建筑工地安全监测能力。通过两个案例研究，评估了大语言模型（LLM）和视觉-语言模型（VLM）在自动化识别安全隐患方面的表现。第一项研究利用GPT 4o和GPT 4o mini分析28,000份OSHA事故报告，提取结构化信息；第二项研究采用轻量级开源模型Molmo 7B和Qwen2 VL 2B，在ConstructionSite10k数据集上测试规则级安全违规检测性能，结果显示这些小模型在特定提示配置下表现良好，证明了低资源多模态系统在规则感知安全监控中的可行性。


<details>
  <summary>Details</summary>
Motivation: 在建筑工地等高风险环境中，事故数据常以报告、检查记录和图像等多种形式存在，传统方法难以有效整合与分析，导致安全隐患识别效率低下。因此需要一种能够融合多源异构数据的智能分析框架，提升安全预警能力。

Method: 提出一个融合文本与图像分析的多模态AI框架。第一阶段使用GPT 4o和GPT 4o mini处理大量事故报告，提取关键信息；第二阶段采用轻量级开源视觉-语言模型Molmo 7B和Qwen2 VL 2B，基于自然语言提示对建筑工地图像进行规则级安全违规检测，通过公开数据集进行验证与对比。

Result: GPT系列模型成功从海量事故报告中提取出结构化安全洞察；Molmo 7B和Qwen2 VL 2B在不同提示设置下表现出与大型专有模型相当的性能，尤其在成本敏感场景中展现出良好的实用性与可扩展性，验证了轻量化多模态系统在安全监控中的潜力。

Conclusion: 多模态AI框架能有效整合文本与视觉数据，实现对建筑工地安全隐患的高效识别。即使使用小型开源模型，也能在规则感知任务中达到较高精度，为低成本、可扩展的安全监控系统提供了可行路径。

Abstract: This thesis explores a multimodal AI framework for enhancing construction safety through the combined analysis of textual and visual data. In safety-critical environments such as construction sites, accident data often exists in multiple formats, such as written reports, inspection records, and site imagery, making it challenging to synthesize hazards using traditional approaches. To address this, this thesis proposed a multimodal AI framework that combines text and image analysis to assist in identifying safety hazards on construction sites. Two case studies were consucted to evaluate the capabilities of large language models (LLMs) and vision-language models (VLMs) for automated hazard identification.The first case study introduces a hybrid pipeline that utilizes GPT 4o and GPT 4o mini to extract structured insights from a dataset of 28,000 OSHA accident reports (2000-2025). The second case study extends this investigation using Molmo 7B and Qwen2 VL 2B, lightweight, open-source VLMs. Using the public ConstructionSite10k dataset, the performance of the two models was evaluated on rule-level safety violation detection using natural language prompts. This experiment served as a cost-aware benchmark against proprietary models and allowed testing at scale with ground-truth labels. Despite their smaller size, Molmo 7B and Quen2 VL 2B showed competitive performance in certain prompt configurations, reinforcing the feasibility of low-resource multimodal systems for rule-aware safety monitoring.

</details>


### [102] [Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods](https://arxiv.org/abs/2511.15722)
*Weichen Liu,Qiyao Xue,Haoming Wang,Xiangyu Yin,Boyuan Yang,Wei Gao*

Main category: cs.AI

TL;DR: 本文提出一种基于认知视角的三维空间推理分类体系，超越传统按输入模态（如文本、图像、视频或3D）分类的方式，强调空间能力不仅依赖输入格式，更与认知功能相关。该分类体系从认知维度划分空间智能，并根据推理复杂度对任务进行组织，映射现有基准测试至该框架，分析评估指标与方法，揭示当前模型与人类水平之间的差距。同时，综述了训练和推理两类提升空间能力的方法，阐明其优势与互补机制，为新研究者提供全面理解与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态大模型空间推理的研究多按输入模态分类，但忽视了空间能力本质上与认知过程密切相关，导致跨任务比较不系统，难以准确评估模型真实空间推理水平。因此需要建立一个以认知功能为基础的统一分类框架，以实现更科学的评估与比较。

Method: 提出一个融合认知视角的任务分类体系，将空间推理任务按认知功能与推理复杂度分层；系统梳理文本、视觉语言与具身设置下的基准数据集，并分析其对应关系；评估现有评价指标与方法；总结训练型与推理型方法的策略及其协同机制。

Result: 构建了一个可支持跨任务比较的认知分类框架，揭示了当前模型在复杂空间推理上与人类能力的显著差距；明确了训练与推理方法的互补性，指出二者结合是提升空间推理能力的关键路径。

Conclusion: 该认知导向的分类体系为理解多模态大模型的空间推理能力提供了新范式，有助于推动更系统、更深入的研究，也为未来模型设计与评估提供了理论依据与实践指导。

Abstract: Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.

</details>


### [103] [Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer](https://arxiv.org/abs/2511.15741)
*Hyo-Jeong Jang*

Main category: cs.AI

TL;DR: 该论文提出了一种基于一致性引导的跨模态迁移方法，以应对多模态学习中的不确定性问题。通过将异构模态投影到共享潜在空间，实现语义一致性对齐，从而缓解模态差异、提升特征学习的稳定性与鲁棒性。实验表明，该框架在情感识别任务中显著增强了模型的稳定性、判别能力及对噪声和不完整标注的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统常因数据噪声、标签质量差和模态异质性而面临严重不确定性，尤其在人机交互场景中，数据质量、语义可靠性和标注一致性受用户和采集条件影响大，亟需一种能有效应对这些挑战的鲁棒学习方法。

Method: 提出一致性引导的跨模态迁移框架，通过将不同模态映射到共享潜在空间，利用跨模态语义一致性来指导表示学习，从而减少模态差距并揭示支持不确定性估计的结构关系。

Result: 在多模态情感识别基准测试中，该方法显著提升了模型的稳定性、判别力和对噪声及不完整监督的鲁棒性；潜在空间分析显示，即使在困难条件下也能捕捉可靠的跨模态结构。

Conclusion: 本论文为鲁棒的多模态学习提供了一个统一视角，融合了不确定性建模、语义对齐与数据高效监督，为构建可靠且自适应的脑机接口系统提供了实用洞见。

Abstract: Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.

</details>


### [104] [Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics](https://arxiv.org/abs/2511.15752)
*Hanzhi Yan,Qin Lu,Xianqiao Wang,Xiaoming Zhai,Tianming Liu,He Li*

Main category: cs.AI

TL;DR: 本文提出结合大语言模型（LLM）与AI智能体，构建双模块框架以提升在生物力学教育中的表现。通过检索增强生成（RAG）改进概念性问题的回答准确性与逻辑一致性，利用多智能体系统（MAS）解决需要多步推理和计算的任务。实验表明，RAG显著提升LLM在概念题上的性能，而MAS能有效完成方程推导、代码执行和可解释性解答，适用于工程教育中的智能辅导。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域应用中存在知识缺口，且在复杂多步推理任务中表现下降，因此需要改进其在生物力学等专业课程中的教学辅助能力。

Method: 构建双模块框架：1）使用RAG提升LLM对概念性真/假问题的回答质量；2）设计多智能体系统（MAS），由多个LLM协作完成需多步推理和代码执行的计算任务。

Result: RAG显著提升了LLM在概念性问题上的准确性和稳定性，优于原始模型；MAS成功实现多步推理、方程推导、代码执行及生成可解释解法，有效应对复杂计算任务。

Conclusion: RAG与MAS的结合为提升大语言模型在工程教育领域的专用能力提供了有效路径，具有推动智能辅导系统发展的潜力。

Abstract: While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.

</details>


### [105] [Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response](https://arxiv.org/abs/2511.15755)
*Philip Drammeh*

Main category: cs.AI

TL;DR: 多智能体协作显著提升大语言模型在生产系统故障响应中的实用性，相比单智能体方案，其可操作建议率达100%，正确性和具体性分别提升80倍和140倍，且输出质量稳定无波动，具备生产级部署可行性。研究提出新评估指标决策质量（DQ），填补现有指标空白，并证明多智能体架构是LLM应用于生产系统的必要条件。所有代码与数据开源可复现。


<details>
  <summary>Details</summary>
Motivation: 单智能体大语言模型在故障响应中生成模糊、不可用的建议，难以满足生产环境对可靠性和一致性的要求，亟需更可靠的架构改进。

Method: 通过构建容器化框架MyAntFarm.ai，在348组受控实验中对比单智能体与多智能体系统在相同故障场景下的表现，量化分析建议的可操作性、正确性与一致性，并引入新的决策质量（DQ）评估指标。

Result: 多智能体系统实现100%可操作建议率，相较单智能体的1.7%提升显著；行动具体性提高80倍，解决方案正确性提高140倍；输出质量无波动，支持服务等级协议（SLA）承诺；两者理解延迟相近（约40秒），说明优势来自质量而非速度。

Conclusion: 多智能体协同架构不再是性能优化选项，而是大语言模型实现生产级故障响应的必要条件，其核心价值在于提供确定性高质量输出，确保系统可信赖与可部署。

Abstract: Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.

</details>


### [106] [Balancing Natural Language Processing Accuracy and Normalisation in Extracting Medical Insights](https://arxiv.org/abs/2511.15778)
*Paulina Tworek,Miłosz Bargieł,Yousef Khan,Tomasz Pełech-Pilichowski,Marek Mikołajczyk,Roman Lewandowski,Jose Sousa*

Main category: cs.AI

TL;DR: 该研究比较了低计算量的基于规则的方法与大型语言模型（LLMs）在从波兰阿梅里卡地区疗养院电子健康记录中提取医疗信息的表现。结果表明，基于规则的方法在年龄和性别提取上准确率更高，而LLMs在药物名称识别方面更具适应性和可扩展性。研究还评估了文本未标准化及翻译导致的信息损失影响，发现翻译会降低LLMs性能。作者建议采用混合方法，结合规则系统的精确性与LLMs的灵活性，以实现更可靠且资源高效的临床NLP应用。


<details>
  <summary>Details</summary>
Motivation: 在非英语语境下，医疗文本的结构化信息提取仍面临挑战，尤其在资源匮乏的情况下。现有NLP技术在准确性、可扩展性与计算成本之间存在权衡，亟需探索适合真实医院环境的解决方案。

Method: 对比分析基于规则的低计算方法与大型语言模型（LLMs）在提取患者人口统计信息、临床发现和处方药物方面的表现；使用原始波兰语及翻译成英语的文本进行测试，评估文本规范化缺失与翻译带来的信息损失影响。

Result: 基于规则的方法在年龄和性别提取上表现出更高准确率；LLMs在药物名称识别上表现更优，但其性能受翻译影响较大，原文处理优于翻译后文本；整体显示精度、规范化与计算成本之间的权衡。

Conclusion: 应采用混合方法，融合规则系统的高精度与LLMs的强适应性，以推动更可靠、高效且适用于实际医疗场景的临床自然语言处理系统发展。

Abstract: Extracting structured medical insights from unstructured clinical text using Natural Language Processing (NLP) remains an open challenge in healthcare, particularly in non-English contexts where resources are scarce. This study presents a comparative analysis of NLP low-compute rule-based methods and Large Language Models (LLMs) for information extraction from electronic health records (EHR) obtained from the Voivodeship Rehabilitation Hospital for Children in Ameryka, Poland. We evaluate both approaches by extracting patient demographics, clinical findings, and prescribed medications while examining the effects of lack of text normalisation and translation-induced information loss. Results demonstrate that rule-based methods provide higher accuracy in information retrieval tasks, particularly for age and sex extraction. However, LLMs offer greater adaptability and scalability, excelling in drug name recognition. The effectiveness of the LLMs was compared with texts originally in Polish and those translated into English, assessing the impact of translation. These findings highlight the trade-offs between accuracy, normalisation, and computational cost when deploying NLP in healthcare settings. We argue for hybrid approaches that combine the precision of rule-based systems with the adaptability of LLMs, offering a practical path toward more reliable and resource-efficient clinical NLP in real-world hospitals.

</details>


### [107] [IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation](https://arxiv.org/abs/2511.15825)
*Tuan-Anh Le,Anh Mai Vu,David Yang,Akash Awasthi,Hien Van Nguyen*

Main category: cs.AI

TL;DR: IMACT-CXR is an interactive multi-agent system that enhances chest X-ray interpretation training by integrating spatial annotation, gaze analysis, knowledge retrieval, and vision-language reasoning. It uses specialized agents for coaching, evidence retrieval, case suggestion, and reasoning, guided by Bayesian Knowledge Tracing and anatomically aware segmentation. The system ensures controlled answer leakage and supports real-time deployment with promising preliminary results in localization and diagnostic reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve medical trainees' ability to interpret chest X-rays through adaptive, interactive tutoring that combines visual attention, knowledge grounding, and personalized feedback, while preventing premature exposure to ground-truth labels.

Method: IMACT-CXR employs an AutoGen-based multi-agent framework that processes learner-generated bounding boxes, gaze data, and free-text observations. Agents handle localization evaluation, Socratic questioning, PubMed evidence retrieval, REFLACX case suggestions, and vision-language reasoning via NV-Reason-CXR-3B. A TensorFlow U-Net enables lung-lobe segmentation for anatomical gaze feedback, while Bayesian Knowledge Tracing tracks mastery to guide instruction. Safety prompts prevent early label disclosure.

Result: The system demonstrates responsive tutoring with low latency, precise control over information leakage, and extensibility to live residency training. Preliminary evaluations indicate improved performance in localization accuracy and diagnostic reasoning compared to baseline approaches.

Conclusion: IMACT-CXR effectively integrates multiple AI capabilities into a cohesive, adaptive tutoring environment for chest X-ray interpretation, showing strong potential for clinical training applications and future deployment in real-world residency programs.

Abstract: IMACT-CXR is an interactive multi-agent conversational tutor that helps trainees interpret chest X-rays by unifying spatial annotation, gaze analysis, knowledge retrieval, and image-grounded reasoning in a single AutoGen-based workflow. The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations. Specialized agents evaluate localization quality, generate Socratic coaching, retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for vision-language reasoning when mastery remains low or the learner explicitly asks. Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval. A lung-lobe segmentation module derived from a TensorFlow U-Net enables anatomically aware gaze feedback, and safety prompts prevent premature disclosure of ground-truth labels. We describe the system architecture, implementation highlights, and integration with the REFLACX dataset for real DICOM cases. IMACT-CXR demonstrates responsive tutoring flows with bounded latency, precise control over answer leakage, and extensibility toward live residency deployment. Preliminary evaluation shows improved localization and diagnostic reasoning compared to baselines.

</details>


### [108] [Step-Audio-R1 Technical Report](https://arxiv.org/abs/2511.15848)
*Fei Tian,Xiangyu Tony Zhang,Yuxin Zhang,Haoyang Zhang,Yuxin Li,Daijiao Liu,Yayue Deng,Donghang Wu,Jun Chen,Liang Zhao,Chengyuan Yao,Hexin Liu,Eng Siong Chng,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.AI

TL;DR: Step-Audio-R1 是首个成功在音频领域实现推理能力的模型，通过模态锚定推理蒸馏（MGRD）框架，使推理过程基于声学特征而非幻觉，显著提升音频理解与推理性能，超越 Gemini 2.5 Pro 并接近 Gemini 3 Pro 水平，证明了推理在音频领域的可迁移性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型在推理方面表现不佳，即使有链式思考也难以提升性能，引发对音频智能是否能受益于深度思考的根本疑问。本文旨在探索并实现音频领域的有效推理机制。

Method: 提出 Modality-Grounded Reasoning Distillation (MGRD) 框架，引导模型生成基于真实声学特征的推理链，避免脱离音频内容的虚构推理。

Result: Step-Audio-R1 在语音、环境音、音乐等多类音频任务上表现优异，超越 Gemini 2.5 Pro，接近 Gemini 3 Pro 最佳水平，验证了音频推理的有效性。

Conclusion: 推理能力可跨模态迁移，只要合理锚定于具体模态特征；本研究首次实现音频领域的有效推理，为构建真正多模态深度推理系统奠定基础。

Abstract: Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.

</details>


### [109] [Decomposing Theory of Mind: How Emotional Processing Mediates ToM Abilities in LLMs](https://arxiv.org/abs/2511.15895)
*Ivan Chulo,Ananya Joshi*

Main category: cs.AI

TL;DR: 该研究通过线性探测对比了经过激活引导（CAA）的Gemma-3-4B模型与基线模型在45种认知行为上的激活差异，发现情绪感知和情绪价值处理的激活增强，而分析性思维如质疑和收敛思维被抑制，从而提升了语言模型在信念归属任务中的表现（准确率从32.5%提升至46.7%），表明大模型的理论心理能力主要依赖于情感理解而非逻辑推理。


<details>
  <summary>Details</summary>
Motivation: 当前关于激活引导能提升大语言模型理论心理能力的研究虽已取得进展，但其内部机制尚不明确。本研究旨在揭示激活变化如何影响模型在理论心理任务中的表现，特别是区分情感理解与分析推理的作用。

Method: 采用对比激活添加（CAA）方法对Gemma-3-4B进行引导，并利用在45种认知行为上训练的线性探测器分析其激活模式；在1,000个BigToM前向信念场景中评估模型表现，量化不同认知过程的激活变化。

Result: 模型在信念归属任务上的准确率从32.5%提升至46.7%，这一提升由情绪感知（+2.23）和情绪价值处理（+2.20）的激活增强驱动，同时分析性过程如质疑（-0.78）和收敛思维（-1.59）的激活被抑制。

Conclusion: 大语言模型的理论心理能力并非源于分析推理，而是由情感理解所主导，这为理解模型心智模拟机制提供了新的视角。

Abstract: Recent work shows activation steering substantially improves language models' Theory of Mind (ToM) (Bortoletto et al. 2024), yet the mechanisms of what changes occur internally that leads to different outputs remains unclear. We propose decomposing ToM in LLMs by comparing steered versus baseline LLMs' activations using linear probes trained on 45 cognitive actions. We applied Contrastive Activation Addition (CAA) steering to Gemma-3-4B and evaluated it on 1,000 BigToM forward belief scenarios (Gandhi et al. 2023), we find improved performance on belief attribution tasks (32.5\% to 46.7\% accuracy) is mediated by activations processing emotional content : emotion perception (+2.23), emotion valuing (+2.20), while suppressing analytical processes: questioning (-0.78), convergent thinking (-1.59). This suggests that successful ToM abilities in LLMs are mediated by emotional understanding, not analytical reasoning.

</details>


### [110] [Thinking, Faithful and Stable: Mitigating Hallucinations in LLMs](https://arxiv.org/abs/2511.15921)
*Chelsea Zou,Yiheng Yao,Basant Khalil*

Main category: cs.AI

TL;DR: 该研究提出了一种自校正框架，用于检测和缓解大语言模型在多步推理中的幻觉问题。通过利用细粒度的不确定性信号（如自我评估置信度一致性与词元级熵突增），实时识别不可靠且不忠实的推理过程，并设计复合奖励函数来惩罚不合理高置信度和熵突增，同时鼓励稳定准确的推理轨迹。该方法结合强化学习策略，使模型更具内省性，通过置信度感知的奖励反馈塑造生成行为，从而提升最终答案的准确性以及中间推理步骤的一致性和可信度。实验表明，该方法在提高答案正确率和推理校准方面表现优异，消融实验验证了各信号的独立贡献。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多步推理中常产生幻觉，即生成看似合理但不真实或不一致的中间推理步骤，影响最终答案的可信度。现有方法多依赖最终答案的正确性进行评估，忽视了推理过程本身的可靠性与一致性。因此，亟需一种能实时检测并纠正推理过程中不忠实行为的方法，以提升模型的可解释性与可信度。

Method: 提出一种基于细粒度不确定性信号的自校正框架：1）通过自我评估置信度与实际推理结果的一致性来衡量置信度合理性；2）检测词元级别熵的突增以识别推理中的不确定性波动。设计一个复合奖励函数，对不合理的高置信度和熵突增施加惩罚，同时奖励稳定、准确的推理路径。利用该奖励信号训练强化学习策略，引导模型调整其生成行为，增强内省能力，改善推理过程的忠实性与连贯性。

Result: 实验结果显示，所提方法在多个基准任务上显著提升了最终答案的准确率与推理校准程度。消融实验表明，两种不确定性信号均对性能提升有独立贡献，且联合使用效果最佳。模型在推理过程中表现出更强的自我监控能力，生成的中间步骤更可靠、更一致。

Conclusion: 本研究证明，通过引入细粒度的不确定性信号与置信度感知的强化学习机制，可以有效检测并缓解大语言模型在多步推理中的幻觉问题。该方法不仅提升了最终答案的准确性，更重要的是增强了推理过程的可信度与一致性，为构建更可靠、可解释的AI系统提供了新思路。

Abstract: This project develops a self correcting framework for large language models (LLMs) that detects and mitigates hallucinations during multi-step reasoning. Rather than relying solely on final answer correctness, our approach leverages fine grained uncertainty signals: 1) self-assessed confidence alignment, and 2) token-level entropy spikes to detect unreliable and unfaithful reasoning in real time. We design a composite reward function that penalizes unjustified high confidence and entropy spikes, while encouraging stable and accurate reasoning trajectories. These signals guide a reinforcement learning (RL) policy that makes the model more introspective and shapes the model's generation behavior through confidence-aware reward feedback, improving not just outcome correctness but the coherence and faithfulness of their intermediate reasoning steps. Experiments show that our method improves both final answer accuracy and reasoning calibration, with ablations validating the individual contribution of each signal.

</details>


### [111] [JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation](https://arxiv.org/abs/2511.15958)
*Zhenyu Bi,Gaurav Srivastava,Yang Li,Meng Lu,Swastik Roy,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 本文提出JudgeBoard评估框架，直接通过模型判断答案正确性，无需依赖额外答案比较。针对数学和科学常识推理任务，构建了基于准确率和Elo评分的评估排行榜。为提升小语言模型（SLMs）的判断能力，引入MAJ（多智能体判断）框架，利用多个具有不同推理特征的SLMs协作审议，逼近大语言模型（LLMs）的判断水平。实验显示，尽管单个SLMs在独立判断任务中表现逊于LLMs，但通过MAJ框架显著提升了其可靠性与一致性；在MATH数据集上，使用小型模型作为骨干的MAJ甚至优于大型模型。结果表明，多智能体SLM系统可在判断任务中达到或超越LLMs性能，具备高效、可扩展的评估潜力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-judge方法依赖于答案与标准答案或其他候选答案的间接比较，难以自动化且缺乏细粒度和可扩展性。因此亟需一种直接、高效、可扩展的评估方式，尤其适用于小语言模型（SLMs）的判断能力评估。

Method: 提出JudgeBoard评估框架，通过直接询问模型判断答案正确性实现评估；构建任务特定的评估排行榜，采用准确率与Elo评分双指标进行模型排序；设计MAJ（Multi-Agent Judging）多智能体框架，让多个具有不同推理风格的SLMs协同讨论，提升判断质量。

Result: 单个SLMs在独立判断任务中表现明显弱于LLMs；但通过MAJ框架，SLMs的判断可靠性与一致性显著提升；在MATH数据集上，基于小型模型的MAJ系统表现优于甚至超过大型模型；表明多智能体结构可有效弥补SLMs在判断能力上的不足。

Conclusion: 多智能体小语言模型系统在答案判断任务中具备媲美甚至超越大语言模型的潜力，为高效、可扩展的自动评估提供了新路径，对推动轻量化、实用化评估体系具有重要意义。

Abstract: While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.

</details>


### [112] [CARE-RAG - Clinical Assessment and Reasoning in RAG](https://arxiv.org/abs/2511.15994)
*Deepthi Potluri,Aby Mammen Mathew,Jeffrey B DeWitt,Alexander L. Rasgon,Yide Hao,Junyuan Hong,Ying Ding*

Main category: cs.AI

TL;DR: 本文研究大语言模型（LLMs）在临床场景中从检索到的证据进行正确推理的挑战，以书面暴露疗法（WET）指南为测试基准。即使提供权威文本，模型仍会出错。为此，提出一个评估框架，衡量推理的准确性、一致性和保真度。结果表明，检索增强生成（RAG）虽能约束输出，但安全部署需像评估检索一样严格评估推理过程。


<details>
  <summary>Details</summary>
Motivation: 在临床应用中，大语言模型即使获得正确的证据，也未必能正确推理，这可能导致严重后果。因此需要更严格的评估机制来确保模型输出符合结构化诊疗协议。

Method: 使用临床医生审核的问题和权威的WET指南作为输入，构建评估框架，通过准确性、一致性与保真度三个维度分析模型推理表现，并对比不同条件下模型输出的质量。

Result: 尽管提供了权威文本，模型仍存在推理错误；检索增强生成（RAG）有助于约束输出，但推理质量仍需独立且严格评估，否则存在潜在风险。

Conclusion: 在临床等高风险领域，仅依赖证据检索不足以保证可靠推理，必须将推理过程纳入与检索同等严格的评估体系，才能实现安全有效的模型部署。

Abstract: Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.

</details>


### [113] [MUSEKG: A Knowledge Graph Over Museum Collections](https://arxiv.org/abs/2511.16014)
*Jinhao Li,Jianzhong Qi,Soyeon Caren Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: MuseKG is a knowledge-graph framework that integrates structured and unstructured museum data via symbolic-neural fusion, enabling natural language querying over cultural heritage collections. It outperforms LLM-based and SPARQL methods in query accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing museum information systems fail to effectively integrate heterogeneous data like metadata, documents, and multimodal artefacts into a unified, queryable form, hindering digital cultural heritage exploration.

Method: MuseKG constructs a typed property graph linking objects, people, organizations, and labels (visual/textual), using symbolic-neural integration to unify diverse data sources and support natural language queries.

Result: Evaluations on real museum datasets show MuseKG significantly outperforms zero-shot/few-shot LLMs and SPARQL baselines in attribute, relation, and entity-based queries, demonstrating robustness and scalability.

Conclusion: Symbolic grounding in knowledge graphs is crucial for interpretable, scalable reasoning in cultural heritage; MuseKG enables web-scale integration of digital heritage knowledge.

Abstract: Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.

</details>


### [114] [SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model](https://arxiv.org/abs/2511.16018)
*Emanuel C. Silva,Emily S. M. Salum,Gabriel M. Arantes,Matheus P. Pereira,Vinicius F. Oliveira,Alessandro L. Bicho*

Main category: cs.AI

TL;DR: 本文提出SpellForger，一款玩家通过自然语言提示自定义法术的游戏，利用经过监督训练的BERT模型将文本描述映射到预设法术模板，并调整参数以保持平衡性。游戏基于Unity开发，AI后端使用Python，旨在验证AI作为核心玩法机制的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能在游戏中的应用，尤其是作为核心玩法共创工具的潜力，提升玩家个性化与创造力体验。

Method: 采用监督训练的BERT模型解析玩家输入的自然语言提示，将其映射至预设法术模板，并动态调整伤害、消耗和效果等参数以维持游戏平衡性。系统在Unity中实现，后端由Python支持。

Result: 成功开发出一个功能原型，能够实时生成符合设计逻辑的法术，支持以玩家创作为核心的互动循环，验证了AI作为直接游戏机制的有效性。

Conclusion: 该研究证明了自然语言驱动的AI可以作为游戏核心玩法的一部分，为未来基于玩家创意的动态内容生成提供了可行路径。

Abstract: Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.

</details>


### [115] [An Aligned Constraint Programming Model For Serial Batch Scheduling With Minimum Batch Size](https://arxiv.org/abs/2511.16045)
*Jorge A. Huertas,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: 本文提出了一种新的约束编程（CP）模型，用于解决带有最小批次大小要求的串行批处理（s-batch）调度问题。该模型摒弃了传统方法中依赖预定义虚拟批次集合的做法，转而使用关键对齐参数，直接在同族作业的序列上进行推理，从而获得更紧凑的公式化表达。通过利用问题结构设计定制化的搜索策略和强化约束传播器的推断水平，该模型在中小规模实例（最多100个作业）上表现出色，并在大规模实例（最多500个作业、10个家族、10台机器）上比现有方法提升达25%的求解质量。


<details>
  <summary>Details</summary>
Motivation: 现有针对串行批处理调度问题的约束编程模型依赖于预定义的虚拟批次集合，这导致维度灾难并增加问题复杂性。同时，最小批次大小是半导体制造等实际场景中的常见要求，亟需更高效、更简洁的建模方法。

Method: 提出一种新型CP模型，不依赖虚拟批次集合，而是基于关键对齐参数直接建模同一族作业在机器上的序列安排；结合问题特性设计定制化搜索阶段与增强的约束传播机制。

Result: 在近五千个实例上的实验表明，新模型在小到中等规模实例上性能优越，在大规模实例（最多500个作业）上能获得比现有方法好25%的解质量。

Conclusion: 所提出的新型CP模型有效克服了传统方法的维度瓶颈，具有更强的可扩展性和求解能力，适用于实际工业场景中的串行批处理调度问题。

Abstract: In serial batch (s-batch) scheduling, jobs from similar families are grouped into batches and processed sequentially to avoid repetitive setups that are required when processing consecutive jobs of different families. Despite its large success in scheduling, only three Constraint Programming (CP) models have been proposed for this problem considering minimum batch sizes, which is a common requirement in many practical settings, including the ion implantation area in semiconductor manufacturing. These existing CP models rely on a predefined virtual set of possible batches that suffers from the curse of dimensionality and adds complexity to the problem. This paper proposes a novel CP model that does not rely on this virtual set. Instead, it uses key alignment parameters that allow it to reason directly on the sequences of same-family jobs scheduled on the machines, resulting in a more compact formulation. This new model is further improved by exploiting the problem's structure with tailored search phases and strengthened inference levels of the constraint propagators. The extensive computational experiments on nearly five thousand instances compare the proposed models against existing methods in the literature, including mixed-integer programming formulations, tabu search meta-heuristics, and CP approaches. The results demonstrate the superiority of the proposed models on small-to-medium instances with up to 100 jobs, and their ability to find solutions up to 25\% better than the ones produces by existing methods on large-scale instances with up to 500 jobs, 10 families, and 10 machines.

</details>


### [116] [Artificial Intelligence and Accounting Research: A Framework and Agenda](https://arxiv.org/abs/2511.16055)
*Theophanis C. Stratopoulos,Victor Xiaoqi Wang*

Main category: cs.AI

TL;DR: 本文提出一个双维度框架，用于分类AI与会计研究的结合方向（研究焦点：会计导向或AI导向；方法论：基于AI或传统方法），并应用于顶级会计期刊中的AI相关研究，以识别研究机会和学者的战略定位。研究发现，尽管生成式AI降低了部分研究门槛，但同时也加剧了竞争，凸显出人类在判断、创造力和理论深度上的不可替代性，因此需改革博士教育以培养兼具AI素养与独特优势的研究人才。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和大语言模型的发展，会计研究面临变革，既带来机遇也引发竞争压力，亟需明确会计学者如何利用自身优势在新环境中保持竞争力，并重新思考研究方法与教育模式。

Method: 构建双维度分类框架（研究焦点与方法论），应用于IJAIS特刊及主流会计期刊中的人工智能相关研究，通过系统分析现有文献，揭示研究趋势与空白领域；同时比较人类研究者与AI在研究全流程中的能力差异。

Result: 生成式AI虽使研究能力更普及，但提高了对高阶贡献的要求；人类在创造性思维、理论建构与批判性判断方面仍具核心价值；当前研究存在大量低层次重复性工作，而高水平整合性研究仍稀缺。

Conclusion: 会计学者应主动战略定位，强化理论深度与跨学科协作，同时推动博士教育改革，培养兼具人工智能素养与独特学术竞争力的新一代研究者。

Abstract: Recent advances in artificial intelligence, particularly generative AI (GenAI) and large language models (LLMs), are fundamentally transforming accounting research, creating both opportunities and competitive threats for scholars. This paper proposes a framework that classifies AI-accounting research along two dimensions: research focus (accounting-centric versus AI-centric) and methodological approach (AI-based versus traditional methods). We apply this framework to papers from the IJAIS special issue and recent AI-accounting research published in leading accounting journals to map existing studies and identify research opportunities. Using this same framework, we analyze how accounting researchers can leverage their expertise through strategic positioning and collaboration, revealing where accounting scholars' strengths create the most value. We further examine how GenAI and LLMs transform the research process itself, comparing the capabilities of human researchers and AI agents across the entire research workflow. This analysis reveals that while GenAI democratizes certain research capabilities, it simultaneously intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable. These shifts call for reforming doctoral education to cultivate comparative advantages while building AI fluency.

</details>


### [117] [A Hybrid Proactive And Predictive Framework For Edge Cloud Resource Management](https://arxiv.org/abs/2511.16075)
*Hrikshesh Kumar,Anika Garg,Anshul Gupta,Yashika Agarwal*

Main category: cs.AI

TL;DR: 本文提出一种主动式云边工作负载资源管理框架，结合CNN-LSTM时间序列预测与基于多智能体深度强化学习（DRL）的编排器，将预测结果嵌入DRL智能体的状态空间，使系统具备预见性决策能力。相比传统静态阈值方法，该框架能更精准地平衡成本、性能与可靠性，在复杂多目标场景下实现长期优化，显著优于传统被动响应策略。


<details>
  <summary>Details</summary>
Motivation: 传统云边资源管理依赖静态阈值，导致资源浪费或性能下降；亟需从被动响应转向主动预测与规划，以实现成本、性能与可靠性的动态平衡。

Method: 设计一种混合架构：采用CNN-LSTM模型进行时序负荷预测，并将预测结果作为输入嵌入多智能体深度强化学习（DRL）编排器的状态空间，使智能体具备预见未来的能力，从而制定长期任务调度策略。

Result: 实验表明，该系统在复杂多目标场景下显著优于传统方法，能够有效平衡成本节约、系统健康度与用户体验，实现平滑、可持续的资源管理。

Conclusion: 通过将预测能力融入强化学习状态空间，本框架实现了真正意义上的主动资源管理，为云边协同环境提供了智能化、前瞻性的解决方案。

Abstract: Old cloud edge workload resource management is too reactive. The problem with relying on static thresholds is that we are either overspending for more resources than needed or have reduced performance because of their lack. This is why we work on proactive solutions. A framework developed for it stops reacting to the problems but starts expecting them. We design a hybrid architecture, combining two powerful tools: the CNN LSTM model for time series forecasting and an orchestrator based on multi agent Deep Reinforcement Learning In fact the novelty is in how we combine them as we embed the predictive forecast from the CNN LSTM directly into the DRL agent state space. That is what makes the AI manager smarter it sees the future, which allows it to make better decisions about a long term plan for where to run tasks That means finding that sweet spot between how much money is saved while keeping the system healthy and apps fast for users That is we have given it eyes in order to see down the road so that it does not have to lurch from one problem to another it finds a smooth path forward Our tests show our system easily beats the old methods It is great at solving tough problems like making complex decisions and juggling multiple goals at once like being cheap fast and reliable

</details>


### [118] [SkyRL-Agent: Efficient RL Training for Multi-turn LLM Agent](https://arxiv.org/abs/2511.16108)
*Shiyi Cao,Dacheng Li,Fangzhou Zhao,Shuo Yuan,Sumanth R. Hegde,Connor Chen,Charlie Ruan,Tyler Griggs,Shu Liu,Eric Tang,Richard Liaw,Philipp Moritz,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.AI

TL;DR: SkyRL-Agent is a framework for efficient, multi-turn, long-horizon agent training and evaluation. It enables fast asynchronous dispatching, lightweight tool integration, and backend interoperability. Using it, the authors train SA-SWE-32B, a 32B-parameter software engineering agent from Qwen3-32B, achieving 39.4% Pass@1 on SWE-Bench Verified with over 2x cost reduction compared to prior models. Key innovations include an optimized asynchronous pipeline dispatcher (1.55x speedup) and a tool-enhanced training recipe using AST-based code search. The agent generalizes well to other tasks like Terminal-Bench, BrowseComp-Plus, and WebArena. SkyRL-Agent’s extensibility is demonstrated via case studies in deep research, computer use, and memory agents.


<details>
  <summary>Details</summary>
Motivation: To enable efficient, scalable, and flexible training of long-horizon agentic systems, particularly for complex software engineering tasks, while reducing computational costs and improving performance through optimized dispatching and tool integration.

Method: The framework employs an optimized asynchronous pipeline dispatcher for efficient task scheduling and integrates AST-based tools to enhance code navigation during rollout. Training uses reinforcement learning exclusively on SWE tasks, with backend interoperability allowing different training backends (e.g., SkyRL-train, VeRL, Tinker). Case studies validate extensibility across diverse agent types.

Result: SA-SWE-32B achieves 39.4% Pass@1 on SWE-Bench Verified, outperforming prior models with more than 2x lower cost. The model shows strong generalization across multiple non-SWE tasks. The framework enables up to 1.55x speedup in dispatching and supports diverse agent training scenarios.

Conclusion: SkyRL-Agent provides a highly efficient, extensible, and interoperable framework for training and evaluating long-horizon agents. Its innovations in dispatching and tool integration significantly improve training efficiency and performance, enabling high-performing agents that generalize beyond their training domains.

Abstract: We introduce SkyRL-Agent, a framework for efficient, multi-turn, long-horizon agent training and evaluation. It provides efficient asynchronous dispatching, lightweight tool integration, and flexible backend interoperability, enabling seamless use with existing RL frameworks such as SkyRL-train, VeRL, and Tinker.
  Using SkyRL-Agent, we train SA-SWE-32B, a software engineering agent trained from Qwen3-32B (24.4% Pass@1) purely with reinforcement learning. We introduce two key components: an optimized asynchronous pipeline dispatcher that achieves a 1.55x speedup over naive asynchronous batching, and a tool-enhanced training recipe leveraging an AST-based search tool to facilitate code navigation, boost rollout Pass@K, and improve training efficiency. Together, these optimizations enable SA-SWE-32B to reach 39.4% Pass@1 on SWE-Bench Verified with more than 2x cost reduction compared to prior models reaching similar performance. Despite being trained solely on SWE tasks, SA-SWE-32B generalizes effectively to other agentic tasks, including Terminal-Bench, BrowseComp-Plus, and WebArena. We further demonstrate SkyRL-Agent's extensibility through case studies on deep research, computer use, and memory agents, each trained using a different training backend.

</details>


### [119] [OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe](https://arxiv.org/abs/2511.16334)
*Kaichen Zhang,Keming Wu,Zuhao Yang,Kairui Hu,Bin Wang,Ziwei Liu,Xingxuan Li,Lidong Bing*

Main category: cs.AI

TL;DR: 本文提出OpenMMReasoner，一个透明的两阶段多模态推理训练方案，包含监督微调（SFT）和强化学习（RL）阶段。SFT阶段构建了87.4万样本的冷启动数据集，通过逐步验证确保质量；RL阶段使用7.4万样本跨领域数据进一步优化推理能力。实验表明该方法在9个基准上比Qwen2.5-VL-7B-Instruct提升11.6%，凸显数据质量和训练设计的重要性。代码、流程和数据已开源。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推理研究受限于缺乏透明、可复现的数据构建与训练策略，阻碍了规模化进展。

Method: 提出两阶段训练框架：第一阶段为监督微调（SFT），构建87.4万样本冷启动数据集并进行步骤级验证；第二阶段为强化学习（RL），利用7.4万样本跨域数据增强和稳定推理能力。

Result: 在9个多模态推理基准上相比Qwen2.5-VL-7B-Instruct提升11.6%，证明了高质量数据与合理训练设计对性能的关键作用。

Conclusion: OpenMMReasoner提供了一套可复现、高效且强大的多模态推理训练范式，为未来大规模研究奠定基础，其全部资源已开源。

Abstract: Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.

</details>


### [120] [Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints](https://arxiv.org/abs/2511.16139)
*Yongnan Jin,Xurui Li,Feng Cao,Liucun Gao,Juanjuan Yao*

Main category: cs.AI

TL;DR: 提出MR-RML框架，通过GPRC实现医疗标准与语言模型的对齐，解决静态评估与动态临床需求脱节、多源标准适应难及传统奖励模型无法捕捉复杂医疗质量标准的问题。创新包括：（1）构建“维度-场景-学科”医疗标准矩阵；（2）设计独立多维奖励模型，实现评分内化以提升一致性与效率；（3）引入几何投影参考约束，将临床推理逻辑转化为数学正则化。在Healthbench上显著优于Qwen-32B和多数闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在医疗应用中受限于评估基准与真实临床认知需求不匹配、难以适应不断更新的多源医学标准，以及传统奖励模型无法有效捕捉多维度医疗质量标准。

Method: 提出MR-RML框架，结合GPRC技术，利用‘维度-场景-学科’结构化医疗标准矩阵指导数据生成与模型优化，采用多维奖励模型分解评价标准，并通过几何投影参考约束将临床推理逻辑转化为数学正则化项，实现模型训练的精准对齐。

Result: 在Healthbench基准测试中，相较于基线模型Qwen-32B，MR-RML在全集上提升45%，在困难子集上提升85%；达到开源模型中的最先进水平（62.7全集，44.7困难集），并超越多数闭源模型。

Conclusion: MR-RML通过系统性整合医疗标准与几何约束机制，有效提升了大语言模型在医疗场景下的对齐能力与实际临床价值，为构建可信赖的医疗AI系统提供了新范式。

Abstract: The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.

</details>


### [121] [TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models](https://arxiv.org/abs/2511.16423)
*Li Zhang,Zhongxuan Han,XiaoHua Feng,Jiaming Zhang,Yuyuan Li,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.AI

TL;DR: 提出一种无需训练的单轮联邦视觉语言模型适应框架TOFA，通过视觉和文本双通道提取任务相关表示，利用分层贝叶斯模型学习个性化原型分布，全局对齐本地文本提示，并引入自适应权重校准机制，在不增加客户端或服务器训练资源的前提下，有效应对数据异构性问题，显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中视觉语言模型的适应方法多为迭代训练，通信成本高且易受攻击；而现有的一次性联邦训练方法在利用多模态信息、处理数据异构性和资源消耗方面存在不足，亟需一种轻量、高效、无需训练的单轮适应方法。

Method: TOFA采用双路径设计：视觉路径使用分层贝叶斯模型学习类特定原型分布，实现个性化表征；文本路径通过评估与全局对齐本地生成的文本提示提升鲁棒性；结合自适应权重校准机制融合双模态预测结果，平衡个性化与鲁棒性。整个过程无需额外训练，完全免训练。

Result: 在9个不同联邦设置的数据集上进行的大量实验表明，TOFA在多种场景下均表现出优越性能，显著优于现有方法，同时大幅降低通信开销，具备良好的实用性和可扩展性。

Conclusion: TOFA是一种高效、轻量、无需训练的单轮联邦视觉语言模型适应框架，通过充分挖掘预训练模型的多模态能力并有效应对数据异构性，实现了在低通信成本下的高性能适应，为联邦视觉语言学习提供了新范式。

Abstract: Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.

</details>


### [122] [FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos](https://arxiv.org/abs/2511.16183)
*Jeremie Ochin,Raphael Chekroun,Bogdan Stanciulescu,Sotiris Manitsaris*

Main category: cs.AI

TL;DR: 本文提出FOOTPASS数据集，首个用于足球比赛中逐动作定位的多模态、多智能体战术上下文基准，旨在通过结合计算机视觉输出与足球战术先验知识，实现更自动化、可靠的逐动作数据提取，为数据驱动的体育分析提供关键输入。


<details>
  <summary>Details</summary>
Motivation: 当前动作识别方法在构建可靠的逐动作数据方面仍不足，难以完全自动化标注；而战术建模、轨迹预测和表现分析等研究依赖于游戏状态和逐动作数据，因此需要利用战术知识作为先验来支持视觉预测，提升数据提取的自动化与可靠性。

Method: 引入FOOTPASS数据集，整合多模态信息与多智能体战术上下文，结合计算机视觉任务（如追踪、识别）输出与足球战术规律（长期时间跨度），实现球员中心的逐动作定位。

Result: 成功构建首个支持完整比赛视频中逐动作定位的基准数据集，能够生成可靠且结构化的逐动作数据流，推动足球视频理解与数据驱动分析的发展。

Conclusion: FOOTPASS数据集为足球视频理解提供了新的多模态、多智能体框架，通过融合视觉与战术先验，显著提升了逐动作数据提取的自动化与准确性，是实现智能体育分析的重要基础。

Abstract: Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.

</details>


### [123] [From Performance to Understanding: A Vision for Explainable Automated Algorithm Design](https://arxiv.org/abs/2511.16201)
*Niki van Stein,Anna V. Kononova,Thomas Bäck*

Main category: cs.AI

TL;DR: 本文提出将自动化算法设计与可解释性结合，通过LLM驱动的算法发现、可解释的基准测试和问题类描述符构建一个闭环知识循环，推动从盲目搜索转向可解释、类特定的算法设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动化算法设计虽能生成优化启发式算法并探索复杂设计空间，但缺乏透明度，无法揭示算法为何有效、关键组件是什么或设计选择如何关联问题结构，限制了科学理解与可复用性。

Method: 提出三支柱框架：(i) LLM驱动的算法变体发现；(ii) 可解释的基准测试，用于归因性能至具体组件与超参数；(iii) 问题类描述符，连接算法行为与问题景观结构。

Result: 该集成框架形成一个自我强化的知识闭环，实现算法设计的可解释性与泛化能力，促进对优化策略成功条件的科学洞察，并加速算法创新。

Conclusion: 未来突破不在于更多自动化，而在于自动化与系统性基准测试带来的理解相结合，推动可解释、类特定的算法设计，提升效率并产生可复用的科学知识。

Abstract: Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.

</details>


### [124] [Multi-Agent Collaborative Reward Design for Enhancing Reasoning in Reinforcement Learning](https://arxiv.org/abs/2511.16202)
*Pei Yang,Ke Zhang,Ji Wang,Xiao Chen,Yuxin Tang,Eric Yang,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: CRM (Multi-Agent Collaborative Reward Model) replaces a single black-box reward model with a team of specialist evaluators to enhance robustness and interpretability in RLHF. It decomposes preference evaluation into domain-specific agents and global evaluators, fusing their signals via a centralized aggregator that balances correctness, agreement, and repetition penalties. The framework enables multi-perspective reward shaping without extra human annotations and integrates with standard RL pipelines using advantage-based updates and value modeling. rewardBench is introduced as a benchmark and training suite aligned with CRM's collaborative design.


<details>
  <summary>Details</summary>
Motivation: Conventional reward models in RLHF struggle to balance multiple conflicting preference dimensions (e.g., factuality, helpfulness, safety) and lack transparency in scoring decisions. There is a need for more interpretable and robust reward modeling approaches that support stable optimization and better insight into evaluation rationale.

Method: CRM employs a team of specialized agents (domain-specific and global evaluators) that generate partial reward signals. A centralized aggregator combines these signals using criteria such as step-wise correctness, inter-agent agreement, and repetition penalties. The aggregated reward is used for policy optimization via advantage-based methods (e.g., GAE), while a value model regresses to the aggregated reward. This approach avoids additional human annotations beyond those used to train the evaluators.

Result: CRM improves robustness and interpretability in reward modeling by enabling multi-perspective evaluation. The framework supports stable policy optimization and provides transparent insights into reward assignment. rewardBench facilitates training and assessment within the collaborative structure, demonstrating practicality and modularity for real-world deployment.

Conclusion: CRM offers a modular, transparent, and stable alternative to monolithic reward models in RLHF. By leveraging coordinated specialist agents and a unified aggregation mechanism, it enhances both performance and explainability, paving the way for more reliable and interpretable reward modeling in large language models.

Abstract: We present CRM (Multi-Agent Collaborative Reward Model), a framework that replaces a single black-box reward model with a coordinated team of specialist evaluators to improve robustness and interpretability in RLHF. Conventional reward models struggle to jointly optimize multiple, sometimes conflicting, preference dimensions (e.g., factuality, helpfulness, safety) and offer limited transparency into why a score is assigned. CRM addresses these issues by decomposing preference evaluation into domain-specific agents that each produce partial signals, alongside global evaluators such as ranker-based and embedding-similarity rewards. A centralized aggregator fuses these signals at each timestep, balancing factors like step-wise correctness, multi-agent agreement, and repetition penalties, yielding a single training reward compatible with standard RL pipelines. The policy is optimized with advantage-based updates (e.g., GAE), while a value model regresses to the aggregated reward, enabling multi-perspective reward shaping without requiring additional human annotations beyond those used to train the evaluators. To support training and assessment, we introduce rewardBench, a benchmark and training suite aligned with the collaborative structure of CRM. Together, CRM and rewardBench provide a practical, modular path to more transparent reward modeling and more stable optimization.

</details>


### [125] [FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks](https://arxiv.org/abs/2511.16216)
*Zhen Hao Wong,Jingwen Deng,Hao Liang,Runming He,Chengyu Shen,Wentao Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种自动化流水线，从教育类文档中提取高质量的问答（QA）和视觉问答（VQA）对，结合布局感知的OCR与大语言模型（LLM）语义解析，有效解决现有数据集成本高、存在幻觉和多样性不足的问题。该方法可规模化利用真实教育内容，为提升推理型大语言模型训练提供实用替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有指令微调和强化学习数据集依赖昂贵的人工标注或合成样本，存在幻觉和多样性差的问题；而教育类文档中的高质量人工撰写的问答内容尚未被充分利用，主要因难以将原始PDF转化为AI可用的监督数据。

Method: 结合布局感知的OCR技术与大语言模型进行语义解析，构建自动化流程，从教育文档中提取结构清晰、语义对齐的QA和VQA对。

Result: 在多种文档类型上的实验表明，该方法能生成准确、对齐且噪声低的QA/VQA对，显著提升教育内容的可用性，为大模型训练提供可靠的真实数据来源。

Conclusion: 该方法实现了对真实世界教育内容的高效利用，提供了一种低成本、高质量的数据生成方式，是合成数据的有效替代方案，有助于提升大语言模型的推理能力。

Abstract: The development of Large Language Models (LLMs) increasingly depends on high-quality supervised data, yet existing instruction-tuning and RL datasets remain costly to curate and often rely on synthetic samples that introduce hallucination and limited diversity. At the same time, textbooks and exercise materials contain abundant, high-quality human-authored Question-Answer(QA) content that remains underexploited due to the difficulty of transforming raw PDFs into AI-ready supervision. Although modern OCR and vision-language models can accurately parse document structure, their outputs lack the semantic alignment required for training. We propose an automated pipeline that extracts well-formed QA and visual-QA (VQA) pairs from educational documents by combining layout-aware OCR with LLM-based semantic parsing. Experiments across diverse document types show that the method produces accurate, aligned, and low-noise QA/VQA pairs. This approach enables scalable use of real-world educational content and provides a practical alternative to synthetic data generation for improving reasoning-oriented LLM training. All code and data-processing pipelines are open-sourced at https://github.com/OpenDCAI/DataFlow.

</details>


### [126] [MuISQA: Multi-Intent Retrieval-Augmented Generation for Scientific Question Answering](https://arxiv.org/abs/2511.16283)
*Zhiyuan Li,Haisheng Yu,Guangchuan Guo,Nan Zhou,Jiajun Zhang*

Main category: cs.AI

TL;DR: 本文提出MuISQA基准和一种意图感知的检索框架，以解决传统RAG系统在多意图科学问题回答中证据覆盖不全的问题。该框架利用大语言模型生成假设答案、分解为特定意图的查询，并通过RRF融合检索结果，提升跨子问题的证据覆盖率与检索准确性。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）系统通常面向单一意图，难以应对复杂科学问题中的多意图需求，导致证据覆盖不完整。

Method: 提出意图感知的检索框架：利用LLM生成假设答案，分解为意图相关的查询，检索支持片段，并通过递归排名融合（RRF）进行聚合与重排序，以平衡多意图覆盖并减少冗余。

Result: 在MuISQA基准及其他通用RAG数据集上的实验表明，该方法在检索准确性和证据覆盖方面显著优于传统方法，尤其在多意图任务中表现突出。

Conclusion: 所提出的意图感知检索框架有效提升了多意图科学问答中证据的全面性与质量，为复杂科学问题的智能回答提供了新路径。

Abstract: Complex scientific questions often entail multiple intents, such as identifying gene mutations and linking them to related diseases. These tasks require evidence from diverse sources and multi-hop reasoning, while conventional retrieval-augmented generation (RAG) systems are usually single-intent oriented, leading to incomplete evidence coverage. To assess this limitation, we introduce the Multi-Intent Scientific Question Answering (MuISQA) benchmark, which is designed to evaluate RAG systems on heterogeneous evidence coverage across sub-questions. In addition, we propose an intent-aware retrieval framework that leverages large language models (LLMs) to hypothesize potential answers, decompose them into intent-specific queries, and retrieve supporting passages for each underlying intent. The retrieved fragments are then aggregated and re-ranked via Reciprocal Rank Fusion (RRF) to balance coverage across diverse intents while reducing redundancy. Experiments on both MuISQA benchmark and other general RAG datasets demonstrate that our method consistently outperforms conventional approaches, particularly in retrieval accuracy and evidence coverage.

</details>


### [127] [Reducing Instability in Synthetic Data Evaluation with a Super-Metric in MalDataGen](https://arxiv.org/abs/2511.16373)
*Anna Luiza Gomes da Silva,Diego Kreutz,Angelo Diniz,Rodrigo Mansilha,Celso Nobre da Fonseca*

Main category: cs.AI

TL;DR: 本文提出了一种名为Super-Metric的综合评估指标，整合了八个跨四个保真度维度的指标，生成单一加权分数，用于评估合成数据质量。实验表明，该指标在十种生成模型和五个平衡数据集上比传统指标更稳定、一致，并与分类器实际性能具有更强的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有评估合成数据质量的指标在安卓恶意软件领域存在不稳定性和缺乏标准化的问题，亟需一种更可靠的评估方法。

Method: 将八个不同维度的指标整合到MalDataGen中，构建一个加权综合评分系统（Super-Metric），以全面评估合成数据的保真度。

Result: Super-Metric在多个生成模型和数据集上表现出更高的稳定性与一致性，且与分类器实际性能的相关性显著增强。

Conclusion: Super-Metric是一种有效且可靠的合成数据质量评估工具，可为安卓恶意软件领域的数据生成研究提供更统一、准确的评价标准。

Abstract: Evaluating the quality of synthetic data remains a persistent challenge in the Android malware domain due to instability and the lack of standardization among existing metrics. This work integrates into MalDataGen a Super-Metric that aggregates eight metrics across four fidelity dimensions, producing a single weighted score. Experiments involving ten generative models and five balanced datasets demonstrate that the Super-Metric is more stable and consistent than traditional metrics, exhibiting stronger correlations with the actual performance of classifiers.

</details>


### [128] [CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference](https://arxiv.org/abs/2511.16395)
*Kangwei Xu,Grace Li Zhang,Ulf Schlichtmann,Bing Li*

Main category: cs.AI

TL;DR: 提出CorrectHDL框架，利用高阶综合（HLS）结果作为功能参考，纠正大语言模型（LLM）生成的硬件描述语言（HDL）设计中的潜在错误。通过对比LLM生成电路与传统HLS工具生成的参考设计的行为，迭代优化功能正确性，同时显著提升面积和功耗效率，接近人工设计水平。


<details>
  <summary>Details</summary>
Motivation: LLM在生成HDL设计时易出现幻觉导致功能错误，现有方法难以兼顾功能正确性与设计效率，亟需一种融合生成能力与传统验证流程的解决方案。

Method: 将C/C++程序输入LLM生成HDL，使用检索增强生成（RAG）修复语法错误；通过与HLS参考设计进行仿真比对，迭代修正功能缺陷，确保正确性并提升性能。

Result: 生成的电路在面积和功耗上优于传统HLS设计，接近人工设计水平，且保持功能正确性，验证了该框架的有效性与潜力。

Conclusion: CorrectHDL框架成功实现了基于LLM的硬件设计中功能正确性与设计质量的平衡，展示了生成式AI在集成电路设计中的巨大应用前景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.

</details>


### [129] [Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report](https://arxiv.org/abs/2511.16417)
*Yan Chen,Yu Zou,Jialei Zeng,Haoran You,Xiaorui Zhou,Aixi Zhong*

Main category: cs.AI

TL;DR: Pharos-ESG提出一个统一框架，通过多模态解析、上下文叙述和层次化标注，将不规则布局的ESG报告转化为结构化表示。结合阅读顺序建模、基于目录锚点的层次分割及多模态聚合，实现视觉元素到自然语言的语义转换，并添加ESG、GRI和情感标签，提升金融研究适用性。实验表明其优于现有文档解析与多模态模型。同时发布首个大规模公开数据集Aurora-ESG，涵盖中美港市场，提供细粒度布局与语义标注，支持金融治理中的ESG整合。


<details>
  <summary>Details</summary>
Motivation: 现有ESG报告因布局混乱、内容弱结构化，难以进行大规模理解与分析，亟需高效结构化处理方法以支持金融决策与监管。

Method: 采用布局流阅读顺序建模、基于目录锚点的层次感知分割、多模态聚合管道，结合视觉元素上下文转换，生成结构化文本并标注ESG、GRI及情感信息。

Result: 在标注基准上，Pharos-ESG显著优于专用文档解析系统与通用多模态模型；Aurora-ESG数据集为首个大规模公开多模态ESG数据集，支持跨市场金融分析。

Conclusion: Pharos-ESG有效解决了ESG报告结构化难题，推动了其在金融治理与决策中的应用，Aurora-ESG数据集为后续研究提供了重要基础。

Abstract: Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.

</details>


### [130] [PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring](https://arxiv.org/abs/2511.16445)
*Joy Lai,Alex Mihailidis*

Main category: cs.AI

TL;DR: 本文提出PersonaDrift，一个用于评估机器学习与统计方法在检测痴呆患者日常沟通中渐进性变化的合成基准。该基准基于照护者访谈构建60天的模拟交互日志，涵盖情绪减弱和话题偏离两类典型变化，支持扩展至其他行为模式。实验对比了多种异常检测方法，发现针对情绪减弱可用简单统计模型（低基线变异用户），而话题偏离需依赖时序建模与个性化基线；个性化分类器普遍优于通用模型，凸显个体行为背景的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数计算工具无法有效追踪痴呆患者在日常交流中逐渐出现的细微行为变化（如情感减弱、话题漂移）。照护者虽能察觉这些变化，但缺乏系统性技术手段支持长期监测，因此亟需一种可量化、可扩展的评估框架来推动相关算法发展。

Method: 构建基于照护者访谈的合成用户画像，生成60天的数字提醒系统交互日志；通过控制不同速率注入情绪减弱与话题偏离两种渐进性变化，模拟自然认知轨迹；采用多种方法（统计模型、序列模型、监督分类）进行检测，并比较通用与个性化设置下的性能差异。

Result: 情绪减弱在基线变异较低的用户中可通过简单统计模型有效检测；话题偏离则需依赖具备时序建模能力的模型及个性化基线；所有任务中，个性化分类器表现均优于通用模型，表明个体行为上下文对检测精度至关重要。

Conclusion: PersonaDrift为研究痴呆患者沟通行为演变提供了一个可扩展、现实性强的基准平台。结果表明，个性化建模与时间序列分析是准确识别渐进式沟通变化的关键，未来工作应聚焦于构建个体化行为模型以提升检测效能。

Abstract: People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.

</details>


### [131] [Utilizing Large Language Models for Zero-Shot Medical Ontology Extension from Clinical Notes](https://arxiv.org/abs/2511.16548)
*Guanchen Wu,Yuzhang Xie,Huanwei Wu,Zhe He,Hui Shao,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: CLOZE is a zero-shot LLM-based framework that extracts and integrates medical concepts from clinical notes into ontologies without training or labeled data, preserving privacy and enabling scalable, accurate ontology extension for biomedical and clinical applications.


<details>
  <summary>Details</summary>
Motivation: Existing medical ontologies often lack coverage of novel medical concepts and relationships, limiting their utility in biomedical research and clinical applications. Clinical notes contain rich, context-specific information that could enhance ontology coverage but remain underutilized due to challenges in extraction and integration.

Method: CLOZE is a zero-shot framework leveraging large language models (LLMs) to extract medical entities and relationships from unstructured clinical notes. It automatically integrates these into hierarchical medical ontologies without requiring additional training or labeled data, while ensuring privacy by removing protected health information (PHI).

Result: Experiments show CLOZE accurately extracts and integrates new medical concepts with complex hierarchical relationships, demonstrating high accuracy, scalability, and privacy preservation. The framework is effective for extending ontologies in real-world settings.

Conclusion: CLOZE offers a cost-efficient, scalable, and privacy-preserving solution for enhancing medical ontologies using clinical notes, with strong potential to support diverse downstream applications in biomedical research and clinical informatics.

Abstract: Integrating novel medical concepts and relationships into existing ontologies can significantly enhance their coverage and utility for both biomedical research and clinical applications. Clinical notes, as unstructured documents rich with detailed patient observations, offer valuable context-specific insights and represent a promising yet underutilized source for ontology extension. Despite this potential, directly leveraging clinical notes for ontology extension remains largely unexplored. To address this gap, we propose CLOZE, a novel framework that uses large language models (LLMs) to automatically extract medical entities from clinical notes and integrate them into hierarchical medical ontologies. By capitalizing on the strong language understanding and extensive biomedical knowledge of pre-trained LLMs, CLOZE effectively identifies disease-related concepts and captures complex hierarchical relationships. The zero-shot framework requires no additional training or labeled data, making it a cost-efficient solution. Furthermore, CLOZE ensures patient privacy through automated removal of protected health information (PHI). Experimental results demonstrate that CLOZE provides an accurate, scalable, and privacy-preserving ontology extension framework, with strong potential to support a wide range of downstream applications in biomedical research and clinical informatics.

</details>


### [132] [Consciousness in Artificial Intelligence? A Framework for Classifying Objections and Constraints](https://arxiv.org/abs/2511.16582)
*Andres Campero,Derek Shiller,Jaan Aru,Jonathan Simon*

Main category: cs.AI

TL;DR: 本文提出一个分类框架，用于系统化地分析数字人工智能中意识可能性所面临的挑战。该框架借鉴Marr的认知层次理论，将挑战按粒度和力度分为三个等级：1级（支持计算功能主义但未否定数字意识）、2级（实践性挑战，认为数字意识可能性低但非不可能）、3级（严格否定数字意识的可能性）。作者应用该框架分析了14个科学与哲学文献中的代表性论点，旨在提供清晰的解析工具，而非介入争论本身。


<details>
  <summary>Details</summary>
Motivation: 当前关于数字意识是否可能的讨论存在概念模糊和论证混淆，不同挑战在力度和意图上混杂，导致理解困难。需要一个结构化框架来厘清各类挑战的本质与边界。

Method: 基于Marr的认知层次理论，构建多层级分类框架，将挑战按其作用对象（如计算功能主义或数字意识）和力度（1-3级）进行分类，并对14个经典案例逐一归类分析。

Result: 成功将14个挑战归入相应类别，揭示了现有争论中常见的误读与混淆；为后续研究提供了可操作的分类工具，有助于更精确地识别、比较和回应相关论点。

Conclusion: 本框架有效提升了对数字意识挑战的理解精度，有助于推动该领域讨论走向更清晰、更系统的方向，而无需预设立场。

Abstract: We develop a taxonomical framework for classifying challenges to the possibility of consciousness in digital artificial intelligence systems. This framework allows us to identify the level of granularity at which a given challenge is intended (the levels we propose correspond to Marr's levels) and to disambiguate its degree of force: is it a challenge to computational functionalism that leaves the possibility of digital consciousness open (degree 1), a practical challenge to digital consciousness that suggests improbability without claiming impossibility (degree 2), or an argument claiming that digital consciousness is strictly impossible (degree 3)? We apply this framework to 14 prominent examples from the scientific and philosophical literature. Our aim is not to take a side in the debate, but to provide structure and a tool for disambiguating between challenges to computational functionalism and challenges to digital consciousness, as well as between different ways of parsing such challenges.

</details>


### [133] [Formal Abductive Latent Explanations for Prototype-Based Networks](https://arxiv.org/abs/2511.16588)
*Jules Soria,Zakaria Chihani,Julien Girard-Satabin,Alban Grastien,Romain Xu-Darme,Daniela Cancila*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale

</details>


### [134] [You Only Forward Once: An Efficient Compositional Judging Paradigm](https://arxiv.org/abs/2511.16600)
*Tianlong Zhang,Hongwei Xue,Shilin Yan,Di Wu,Chen Xu,Yunyun Yang*

Main category: cs.AI

TL;DR: YOFO 是一种基于模板的多模态大语言模型（MLLM）判断方法，通过单次前向传播完成所有结构化要求的二元判断，显著提升速度并保持可解释性。它利用自回归模型在最终标记处读取对应要求的逻辑值，实现高效且细粒度的评估，支持依赖感知分析和事后思维链（CoT），在推荐系统数据集上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 判定方法面临效率与理解深度之间的权衡：单一评分输出与 MLLM 的生成特性不一致，而逐词生成判别分析又过于缓慢，难以满足高吞吐场景需求。

Method: YOFO 提出一种模板条件化的单步推理框架，将判断任务建模为对一组结构化要求的验证；利用自回归模型在一次前向传播中，通过读取每个要求对应的最终标记的逻辑值，输出二元判断结果（是/否），从而实现快速、可解释的评估。

Result: YOFO 在标准推荐数据集上达到当前最佳表现，相比传统方法实现数量级的速度提升，同时支持依赖感知的逐步判断和后验思维链（CoT）增强，进一步提升判断质量。

Conclusion: YOFO 有效解决了 MLLM 作为评判者时的效率与可解释性矛盾，通过模板驱动的单次前向传播实现快速、精细的多要求验证，适用于高吞吐、需要可解释性的实际应用。

Abstract: Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.

</details>


### [135] [Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization](https://arxiv.org/abs/2511.16602)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Yingji Zhang,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Haozhe Shan,Junbo Qi,Yan Bai,Dengjie Li,Jiachen Luo,Yidong Wang,Yong Dai,Zenglin Xu,Bin Shen,Qifan Wang,Jian Tang,Xiaozhu Ju*

Main category: cs.AI

TL;DR: 提出DPPO（刻意练习策略优化）框架，通过动态交替监督微调与强化学习，实现自动弱点识别和资源精准分配，提升稀疏数据下的学习效率。实验表明Pelican-VL 1.0模型性能较基础模型提升20.3%，优于100B参数规模的开源模型10.6%。代码与模型已开源，为构建通用具身智能体提供高效系统方案。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能系统面临真实世界数据稀缺昂贵、算法资源消耗大等问题，亟需高效学习框架以克服数据与计算瓶颈。

Method: 提出基于元认知的"Metaloop"训练框架DPPO，动态交替进行监督微调（扩展能力）与强化学习（精炼技能），实现自动弱点识别与资源定向分配，并可形式化为统一偏好学习框架。

Result: Pelican-VL 1.0在视觉-语言具身任务中性能比基线模型提升20.3%，超过100B参数规模的开源模型10.6%，验证了方法的有效性与高效性。

Conclusion: DPPO框架有效缓解了具身智能中的数据与资源瓶颈，为社区构建多功能具身智能体提供了首个系统性解决方案，具有高度可扩展性和实用性。

Abstract: Developing a universal and versatile embodied intelligence system presents two primary challenges: the critical embodied data bottleneck, where real-world data is scarce and expensive, and the algorithmic inefficiency of existing methods, which are resource-prohibitive. To address these limitations, we introduce Deliberate Practice Policy Optimization (DPPO), a metacognitive ``Metaloop'' training framework that dynamically alternates between supervised fine-tuning (competence expansion) and reinforcement learning (skill refinement). This enables automatic weakness identification and targeted resource allocation, specifically designed to maximize learning efficiency from sparse, finite data. Theoretically, DPPO can be formalised as a unified preference-learning framework. Empirically, training a vision-language embodied model with DPPO, referred to as Pelican-VL 1.0, yields a 20.3% performance improvement over the base model and surpasses open-source models at the 100B-parameter scale by 10.6%. We are open-sourcing both the models and code, providing the first systematic framework that alleviates the data and resource bottleneck and enables the community to build versatile embodied agents efficiently.

</details>


### [136] [MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support](https://arxiv.org/abs/2511.16625)
*Elias Hossain,Md Mehedi Hasan Nipu,Maleeha Sheikh,Rajib Rana,Subash Neupane,Niloofar Yousefi*

Main category: cs.AI

TL;DR: MedBayes-Lite 是一种轻量级的贝叶斯增强框架，用于改进基于Transformer的临床语言模型，使其具备不确定性感知能力。该方法无需重新训练或修改架构，参数开销低于3%，通过蒙特卡洛丢弃进行嵌入校准、不确定性加权注意力以及置信度引导决策，显著降低模型过自信程度（32%-48%），并在模拟临床场景中减少高达41%的诊断错误，提升医疗AI系统的可靠性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持系统中的Transformer模型常表现出过自信，尤其在模糊病例中缺乏可靠的不确定性估计，这可能危及患者安全，因此需要一种轻量、高效且无需重新训练的不确定性量化方法。

Method: MedBayes-Lite 采用三部分设计：(i) 基于蒙特卡洛丢弃的贝叶斯嵌入校准以捕捉认知不确定性；(ii) 不确定性加权注意力机制，对每个词元的可靠性进行边际化处理；(iii) 受临床风险最小化启发的置信度引导决策塑造。整个框架无缝集成于现有Transformer流水线中，不引入新可训练层。

Result: 在MedQA、PubMedQA和MIMIC-III等基准上，MedBayes-Lite 显著改善了模型校准性，将过自信降低32%至48%；在模拟临床环境中，可减少高达41%的诊断错误，通过标记不确定预测供人工审查。

Conclusion: MedBayes-Lite 有效实现了医疗AI系统中不确定性传播，提升了模型的可信度与可解释性，是一种无需重训练、低开销的实用解决方案，适用于真实世界临床部署。

Abstract: We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.

</details>


### [137] [Enhancing Forex Forecasting Accuracy: The Impact of Hybrid Variable Sets in Cognitive Algorithmic Trading Systems](https://arxiv.org/abs/2511.16657)
*Juan C. King,Jose M. Amigo*

Main category: cs.AI

TL;DR: 本文提出了一种基于人工智能的高级算法交易系统，专为外汇市场中的欧元/美元（EUR-USD）高频率交易设计。该系统整合了来自欧元区和美国的关键宏观经济变量（如国内生产总值、失业率）以及一系列技术指标（包括震荡指标、斐波那契水平和价格背离）。通过标准机器学习评估指标和历史数据回测，对算法性能进行量化分析，并比较基本面与技术面输入特征在生成盈利交易信号方面的预测能力。


<details>
  <summary>Details</summary>
Motivation: 在高频率外汇交易中，准确预测汇率变动是获得持续盈利的关键。传统方法通常依赖单一类型的数据（如仅技术指标或基本面数据），但实际市场行为受多重因素影响。因此，需要一种融合多源信息的智能系统以提升预测精度和交易表现。

Method: 结合宏观经济基本面数据（如GDP、失业率）与技术分析工具（如指标、震荡器、斐波那契水平、价格背离），构建多维度输入特征集；采用机器学习模型进行训练与预测，并通过回测验证其盈利能力与风险控制能力。

Result: 实证结果显示，融合型模型在预测准确性和回测收益上优于单一类型特征模型；其中，基本面与技术面特征协同作用显著提升了信号生成的稳定性和可靠性。尤其在重大经济事件前后，基本面信息增强了模型的响应能力。

Conclusion: 综合使用基本面与技术面特征的算法交易系统，在高频率外汇交易中表现出更强的预测能力和更高的盈利能力。尽管技术面特征在日常波动中更具即时性，但基本面信息在趋势转折点提供了关键补充，二者结合可实现更优的交易决策支持。

Abstract: This paper presents the implementation of an advanced artificial intelligence-based algorithmic trading system specifically designed for the EUR-USD pair within the high-frequency environment of the Forex market. The methodological approach centers on integrating a holistic set of input features: key fundamental macroeconomic variables (for example, Gross Domestic Product and Unemployment Rate) collected from both the Euro Zone and the United States, alongside a comprehensive suite of technical variables (including indicators, oscillators, Fibonacci levels, and price divergences). The performance of the resulting algorithm is evaluated using standard machine learning metrics to quantify predictive accuracy and backtesting simulations across historical data to assess trading profitability and risk. The study concludes with a comparative analysis to determine which class of input features, fundamental or technical, provides greater and more reliable predictive capacity for generating profitable trading signals.

</details>


### [138] [Cognitive Foundations for Reasoning and Their Manifestation in LLMs](https://arxiv.org/abs/2511.16660)
*Priyanka Kargupta,Shuyue Stella Li,Haocheng Wang,Jinu Lee,Shan Chen,Orevaoghene Ahia,Dean Light,Thomas L. Griffiths,Max Kleiman-Weiner,Jiawei Han,Asli Celikyilmaz,Yulia Tsvetkov*

Main category: cs.AI

TL;DR: 该研究通过整合认知科学构建了28个认知要素的分类体系，分析了17万条大模型与人类的推理轨迹，发现模型依赖浅层前向链式推理，而人类则具备层次化嵌套和元认知监控。研究揭示了模型在元认知控制上的缺失，并提出一种测试时推理引导方法，显著提升复杂任务表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能解决复杂问题，但在简单变体上失败，表明其推理机制与人类本质不同。现有研究过度关注可量化行为，忽视对元认知控制等关键认知过程的探索。

Method: 构建涵盖计算约束、元认知控制、知识表征与转换操作的28项认知要素分类；分析17万条多模态推理轨迹及54条人类思考录音；进行大规模元分析（1598篇论文）；设计测试时推理引导策略以自动构建成功推理结构。

Result: 模型在结构性推理上与人类存在系统性差异，尤其在非结构化问题上；元认知能力（如自知、评估）使用率极低；提出的引导策略使复杂任务性能最高提升60%。

Conclusion: 通过融合认知科学与大模型研究，本工作为发展基于原则性认知机制的模型提供了基础，推动模型能力提升与人类认知理论的规模化验证。

Abstract: Large language models solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. We synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning computational constraints, meta-cognitive controls, knowledge representations, and transformation operations, then analyze their behavioral manifestations in reasoning traces. We propose a fine-grained cognitive evaluation framework and conduct the first large-scale analysis of 170K traces from 17 models across text, vision, and audio modalities, alongside 54 human think-aloud traces, which we make publicly available. Our analysis reveals systematic structural differences: humans employ hierarchical nesting and meta-cognitive monitoring while models rely on shallow forward chaining, with divergence most pronounced on ill-structured problems. Meta-analysis of 1,598 LLM reasoning papers reveals the research community concentrates on easily quantifiable behaviors (sequential organization: 55%, decomposition: 60%) while neglecting meta-cognitive controls (self-awareness: 16%, evaluation: 8%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 60% on complex problems. By bridging cognitive science and LLM research, we establish a foundation for developing models that reason through principled cognitive mechanisms rather than brittle spurious reasoning shortcuts or memorization, opening new directions for both improving model capabilities and testing theories of human cognition at scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Extending Test-Time Scaling: A 3D Perspective with Context, Batch, and Turn](https://arxiv.org/abs/2511.15738)
*Chao Yu,Qixin Tan,Jiaxuan Gao,Shi Yu,Hong Lu,Xinting Yang,Zelai Xu,Yu Wang,Yi Wu,Eugene Vinitsky*

Main category: cs.LG

TL;DR: 本文研究了推理强化学习中的测试时扩展效应，提出多维测试时扩展框架，整合上下文长度、批处理和迭代轮次三个维度，显著提升复杂任务上的推理性能，并通过人类反馈优化，拓展至具身学习领域。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展受限于基础模型的上下文长度，无法充分发挥潜力；需突破单一维度扩展的局限，探索更全面的扩展机制。

Method: 提出多维测试时扩展框架（3D测试时扩展），结合上下文长度、批量采样和迭代自修正三种扩展方式，构建统一的增强体系，并引入人类反馈进行优化。

Result: 各维度均表现出测试时扩展效应但存在容量上限；三者结合显著提升在IOI、IMO、CPHO等挑战性任务上的表现；人类反馈进一步优化结果；该框架可推广至具身学习，实现人形控制行为设计。

Conclusion: 多维测试时扩展为提升推理强化学习性能提供了有效路径，结合人类反馈的闭环系统具有广泛适用性，尤其在开放域具身智能中展现出巨大潜力。

Abstract: Reasoning reinforcement learning (RL) has recently revealed a new scaling effect: test-time scaling. Thinking models such as R1 and o1 improve their reasoning accuracy at test time as the length of the reasoning context increases. However, compared with training-time scaling, test-time scaling is fundamentally limited by the limited context length of base models, which remains orders of magnitude smaller than the amount of tokens consumed during training. We revisit test-time enhancement techniques through the lens of scaling effect and introduce a unified framework of multi-dimensional test-time scaling to extend the capacity of test-time reasoning. Beyond conventional context-length scaling, we consider two additional dimensions: batch scaling, where accuracy improves with parallel sampling, and turn scaling, where iterative self-refinement enhances reasoning quality. Building on this perspective, we propose 3D test-time scaling, which integrates context, batch, and turn scaling. We show that: (1) each dimension demonstrates a test-time scaling effect, but with a bounded capacity; (2) combining all three dimensions substantially improves the reasoning performance of challenging testbeds, including IOI, IMO, and CPHO, and further benefits from human preference feedback; and (3) the human-in-the-loop framework naturally extends to a more open-ended domain, i.e., embodied learning, which enables the design of humanoid control behaviors.

</details>


### [140] [TB or Not TB: Coverage-Driven Direct Preference Optimization for Verilog Stimulus Generation](https://arxiv.org/abs/2511.15767)
*Bardia Nadimi,Khashayar Filom,Deming Chen,Hao Zheng*

Main category: cs.LG

TL;DR: TB or not TB 是一个基于微调大语言模型的自动化测试激励生成框架，采用覆盖率驱动的直接偏好优化（CD-DPO）方法。通过引入PairaNet数据集，利用仿真覆盖率指标对测试平台进行高质量标注，将量化覆盖率反馈融入优化目标，有效提升验证覆盖率。在CVDP CID12基准上，该方法相比开源和商业基线最高实现77.27%的代码覆盖率提升，验证了覆盖驱动偏好优化在LLM硬件验证中的有效性。


<details>
  <summary>Details</summary>
Motivation: 设计验证是硬件开发中最耗时、资源密集的环节，而生成有效的测试激励既关键又费力。传统方法难以高效生成高覆盖率的测试用例，亟需智能化手段提升自动化水平。

Method: 提出CD-DPO方法，结合覆盖率反馈进行偏好训练；构建PairaNet数据集，基于PyraNet并使用仿真覆盖率对测试平台进行高低质量配对标注；利用微调后的LLM生成能够最大化覆盖的测试激励。

Result: 在CVDP CID12基准测试中，TB or not TB 框架实现最高77.27%的代码覆盖率提升，显著优于现有开源与商业工具。

Conclusion: Coverage-driven direct preference optimization（CD-DPO）能有效引导LLM生成高覆盖率的测试激励，为基于LLM的硬件验证提供了高效可行的新范式。

Abstract: With the rapid advancement of Large Language Models (LLMs), there is growing interest in applying them to hardware design and verification. Among these stages, design verification remains the most time-consuming and resource-intensive phase, where generating effective stimuli for the design under test (DUT) is both critical and labor-intensive. We present {\it TB or not TB}, a framework for automated stimulus generation using LLMs fine-tuned through Coverage-Driven Direct Preference Optimization (CD-DPO). To enable preference-based training, we introduce PairaNet, a dataset derived from PyraNet that pairs high- and low-quality testbenches labeled using simulation-derived coverage metrics. The proposed CD-DPO method integrates quantitative coverage feedback directly into the optimization objective, guiding the model toward generating stimuli that maximize verification coverage. Experiments on the CVDP CID12 benchmark show that {\it TB or not TB} outperforms both open-source and commercial baselines, achieving up to 77.27\% improvement in code coverage, demonstrating the effectiveness of Coverage-driven preference optimization for LLM-based hardware verification.

</details>


### [141] [Beyond Tsybakov: Model Margin Noise and $\mathcal{H}$-Consistency Bounds](https://arxiv.org/abs/2511.15816)
*Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出了一种新的低噪声条件——模型边际噪声（MM噪声）假设，并在该条件下推导出改进的$\mathcal{H}$-一致性界。MM噪声比Tsybakov噪声条件更弱，其成立无需依赖分布的最小边际，而是基于假设与贝叶斯分类器之间的差异，因此在某些Tsybakov不成立的情况下仍可成立。该假设带来了二分类和多分类情形下更优的$\mathcal{H}$-一致性界，且保持了与Mao等（2025a）相同的理想指数，同时在中间噪声水平下平滑过渡于线性和平方根两种情形。研究还针对常见损失函数族实例化了这些界，并提供了示例表格。


<details>
  <summary>Details</summary>
Motivation: 现有理论中常用的Tsybakov噪声条件较为严格，限制了其适用范围。为获得更广泛适用且更紧致的泛化性能分析，需要一个更弱但依然能保证良好一致性的噪声假设。本文旨在通过引入依赖于具体假设的新型噪声条件（MM噪声），提升对分类算法一致性的理论刻画能力。

Method: 提出并形式化定义了模型边际噪声（MM噪声）假设；基于该假设，结合经验风险最小化框架，推导出适用于二分类与多分类问题的改进$\mathcal{H}$-一致性界；通过数学分析证明边界在不同噪声水平下的平滑过渡性质；进一步对常见损失函数（如铰链损失、平方损失等）进行实例化，并生成对比表格以展示实际效果。

Result: 在MM噪声假设下，获得了比基于Tsybakov噪声更强的$\mathcal{H}$-一致性界，具有更优的收敛速率；边界在噪声水平变化时表现出从线性到平方根的平滑过渡；对于典型损失函数，得到了具体可计算的误差上界，验证了理论的有效性与实用性。

Conclusion: MM噪声是一个比Tsybakov噪声更弱但依然有效的噪声假设，能够显著提升分类算法的一致性理论分析精度。该工作拓展了现有理论框架，为设计高效鲁棒的分类学习算法提供了新的理论支持。

Abstract: We introduce a new low-noise condition for classification, the Model Margin Noise (MM noise) assumption, and derive enhanced $\mathcal{H}$-consistency bounds under this condition. MM noise is weaker than Tsybakov noise condition: it is implied by Tsybakov noise condition but can hold even when Tsybakov fails, because it depends on the discrepancy between a given hypothesis and the Bayes-classifier rather than on the intrinsic distributional minimal margin (see Figure 1 for an illustration of an explicit example). This hypothesis-dependent assumption yields enhanced $\mathcal{H}$-consistency bounds for both binary and multi-class classification. Our results extend the enhanced $\mathcal{H}$-consistency bounds of Mao, Mohri, and Zhong (2025a) with the same favorable exponents but under a weaker assumption than the Tsybakov noise condition; they interpolate smoothly between linear and square-root regimes for intermediate noise levels. We also instantiate these bounds for common surrogate loss families and provide illustrative tables.

</details>


### [142] [discretize_distributions: Efficient Quantization of Gaussian Mixtures with Guarantees in Wasserstein Distance](https://arxiv.org/abs/2511.15854)
*Steven Adams,Elize Alwash,Luca Laurenti*

Main category: cs.LG

TL;DR: discretize_distributions 是一个高效的 Python 包，用于构建高斯混合分布的离散近似，并在 Wasserstein 距离上提供近似误差保证。它整合了先进的量化方法并扩展以提升可扩展性，支持多种量化策略（如 sigma-point 方法），具备模块化接口，便于集成到控制与验证流程中。在高维、大规模及退化高斯混合分布上的基准测试表明，该工具能以低计算成本生成高精度近似。


<details>
  <summary>Details</summary>
Motivation: 为高斯混合分布提供高效且精确的离散化方法，满足控制与验证系统对概率分布近似的严格要求，尤其在高维和复杂分布场景下需兼顾精度与计算效率。

Method: 采用先进的量化技术对高斯混合模型进行离散化，结合 sigma-point 方法等互补策略，通过模块化设计支持自定义方案，并优化算法以提升计算可扩展性。

Result: 在多种复杂场景（包括高维、大规模、退化高斯混合）下，该工具均实现了高精度近似，同时保持较低的计算开销，验证了其有效性与实用性。

Conclusion: discretize_distributions 有效解决了高斯混合分布离散化的精度与效率难题，适用于复杂系统的建模与验证任务，具有良好的可扩展性和集成能力。

Abstract: We present discretize_distributions, a Python package that efficiently constructs discrete approximations of Gaussian mixture distributions and provides guarantees on the approximation error in Wasserstein distance. The package implements state-of-the-art quantization methods for Gaussian mixture models and extends them to improve scalability. It further integrates complementary quantization strategies such as sigma-point methods and provides a modular interface that supports custom schemes and integration into control and verification pipelines for cyber-physical systems. We benchmark the package on various examples, including high-dimensional, large, and degenerate Gaussian mixtures, and demonstrate that discretize_distributions produces accurate approximations at low computational cost.

</details>


### [143] [Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Minimization](https://arxiv.org/abs/2511.15898)
*Rahul Krishna Thomas,Arka Pal*

Main category: cs.LG

TL;DR: 该论文提出了一种针对多草稿推测采样（multi-draft speculative sampling）的高效算法，通过将最优传输（OT）问题转化为凸优化问题，解决了原有线性规划在变量数量上指数级增长导致的计算瓶颈。作者证明了现有重要性采样与子集选择等重构方式仍面临指数复杂度，进而通过拟阵理论将问题重构成最大流问题，并最终实现仅需最多V个变量的凸优化，从而可在保持目标模型分布几乎不变的前提下，达到90%以上的接受率和每生成一个词元低于100毫秒的开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多草稿推测采样中虽能提升接受率和效率，但最优传输（OT）问题因涉及超过V^n个变量而难以求解，限制了其实际应用。尽管已有工作尝试通过重要性采样或子集选择重构问题，但仍无法克服指数级复杂度。因此亟需一种可高效求解的替代方法。

Method: 作者首先证明现有重构形式等价于一个指数规模的松弛化OTLP；随后逆向设计子集选择，将其建模为最大流问题；结合拟阵理论，将原问题转化为最多含V个变量的凸优化问题，从而获得可精确控制精度的高效算法。

Result: 提出的算法实现了90%以上的接受率，单个词元生成延迟低于100毫秒，且与目标模型分布偏差极小，是首个在多草稿设定下同时满足高接受率、低延迟和高保真度的方案。

Conclusion: 通过将多草稿最优传输问题转化为可解的凸优化问题，本工作首次实现了高效且精确的多草稿推测采样，为大语言模型推理加速提供了新范式。

Abstract: Speculative sampling reduces the latency of autoregressive decoding for target model LLMs without sacrificing inference quality, by using a cheap draft model to suggest a candidate token and a verification criterion to accept or resample this token. To improve acceptance and decoding efficiency, recent work has explored the multi-draft extension, where at each step $n$ draft tokens are generated, and the verification criterion is a distribution conditioned on these. When this criterion maximizes the probability of accepting some draft token, it is called the optimal transport (OT). However, finding the OT is difficult, as it is the solution of a linear program (OTLP) in over $V^n$ variables, with $V$ being the vocabulary size. Two recent theoretical works have reframed the OTLP in terms of importance sampling or subset selection. In this work, we prove that these formulations are equivalent to an exponentially large relaxed OTLP, so it remains infeasible to solve. Then, we reverse engineer subset selection to formulate the OTLP as a max-flow problem. With a novel application of polymatroid theory, we reduce the exponentially large OTLP to a convex optimization problem in at most $V$ variables. This allows us to devise an algorithm for optimal $n$-draft speculative sampling when the $n$ tokens are chosen i.i.d. from a single draft model, which can be tuned to arbitrary accuracy. Finally, we measure acceptance rates and algorithm runtimes for various $n$ and top-$k$ draft sampling settings. Our findings give the first multi-draft algorithm with 90% acceptance and under 100 ms of overhead per generated token with negligible deviation from the target model distribution.

</details>


### [144] [Unified all-atom molecule generation with neural fields](https://arxiv.org/abs/2511.15906)
*Matthieu Kirchmeyer,Pedro O. Pinheiro,Emma Willett,Karolis Martinkus,Joseph Kleinhenz,Emily K. Makowski,Andrew M. Watkins,Vladimir Gligorijevic,Richard Bonneau,Saeed Saremi*

Main category: cs.LG

TL;DR: FuncBind is a modality-agnostic, target-conditioned generative framework for structure-based drug design, leveraging neural fields and score-based models from computer vision to generate all-atom molecules across diverse atomic systems, including small molecules, macrocyclic peptides, and antibody loops. It enables de novo generation of novel antibody binders and introduces a new benchmark dataset for macrocyclic peptide generation.


<details>
  <summary>Details</summary>
Motivation: Existing generative models in structure-based drug design are often limited to specific modalities, restricting their generalization across different molecular types and sizes. There is a need for a unified, flexible framework capable of handling variable atom/residue counts and diverse chemical systems, including non-canonical amino acids.

Method: FuncBind uses neural fields to represent molecules as continuous atomic densities and applies score-based generative models with architectures adapted from computer vision. This allows the model to be trained on diverse atomic systems and generate molecules conditioned on target structures.

Result: FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops. It successfully generates novel in vitro antibody binders via de novo redesign of the H3 loop. A new dataset and benchmark for macrocyclic peptide generation are introduced.

Conclusion: FuncBind provides a unified, modality-agnostic framework for structure-conditioned molecule generation, enabling versatile applications across multiple molecular types and paving the way for broader use in drug discovery.

Abstract: Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.

</details>


### [145] [Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone](https://arxiv.org/abs/2511.15927)
*Vaibhav Singh,Oleksiy Ostapenko,Pierre-André Noël,Torsten Scholak*

Main category: cs.LG

TL;DR: DiffuApriel 是一种基于双向 Mamba 架构的掩码扩散语言模型，结合扩散目标与线性时间序列建模，相比基于 Transformer 的扩散模型，在 1.3B 模型下实现最高 4.4 倍的推理吞吐量提升。其变体 DiffuApriel-H 通过交错注意力与 Mamba 层，进一步实现最高 2.6 倍吞吐量提升，兼顾全局与局部上下文建模。结果表明，双向状态空间架构在掩码扩散语言模型中可作为高效去噪器，为更快、更节省内存的文本生成提供实用且可扩展的基础。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散语言模型依赖 Transformer 架构导致的推理效率低下问题，特别是由二次注意力复杂度和 KV 缓存开销带来的瓶颈。

Method: 提出基于双向 Mamba 架构的掩码扩散语言模型 DiffuApriel，利用 Mamba 的线性时间序列建模能力替代 Transformer，同时结合扩散目标进行训练；设计 Hybrid 变体 DiffuApriel-H，融合注意力与 Mamba 层以平衡上下文建模能力。

Result: DiffuApriel 在保持与 Transformer 基础扩散模型相当性能的同时，实现了高达 4.4 倍的长序列推理吞吐量提升；DiffuApriel-H 实现最高 2.6 倍吞吐量提升，兼具良好的上下文建模能力。

Conclusion: 双向状态空间模型（如 Mamba）可作为掩码扩散语言模型中的高效去噪器，为实现高速、低内存消耗的文本生成提供了可行且可扩展的技术路径。

Abstract: Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.

</details>


### [146] [Descend or Rewind? Stochastic Gradient Descent Unlearning](https://arxiv.org/abs/2511.15983)
*Siqiao Mu,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文研究了机器遗忘算法D2D和R2D在强凸、凸及非凸损失函数下的$(\varepsilon, \delta)$认证遗忘保证。通过分析受扰或有偏梯度系统（收缩、半收缩或扩张），提出最优耦合未学习与重新训练轨迹的方法，结合新型松弛高斯机制，实现严格的遗忘保障。结果表明，对于强凸函数，D2D因收敛至唯一全局最小值而表现更优；而R2D在凸与非凸情形下更具优势，因其能逆转累积扰动，使遗忘模型更接近重训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘算法如D2D和R2D虽易实现且具备可证明的遗忘保证，但其在非凸函数上的理论基础薄弱，尤其随机版本的D2D作为主流基线缺乏理论支撑。本文旨在为随机R2D和D2D提供统一的$(\varepsilon, \delta)$级遗忘保证，填补理论空白。

Method: 从受扰或有偏梯度系统的视角分析未学习过程，将模型演化视为动力系统，利用最优耦合技术建立未学习与重训练路径间的概率敏感性边界，并引入一种新型松弛高斯机制以实现严格$(\varepsilon, \delta)$遗忘保证。

Result: 证明了随机D2D和R2D在强凸、凸及非凸损失函数上均具有$(\varepsilon, \delta)$认证遗忘保证。在强凸情况下，D2D因系统收缩特性可获得更紧的界；在凸与非凸情况下，R2D因逆向扰动能力表现更优。

Conclusion: 本文建立了随机D2D与R2D在多种损失函数结构下的理论基础，揭示了二者在不同场景下的优势：强凸时选D2D，凸与非凸时选R2D，为实际遗忘算法设计提供了理论指导。

Abstract: Machine unlearning algorithms aim to remove the impact of selected training data from a model without the computational expenses of retraining from scratch. Two such algorithms are ``Descent-to-Delete" (D2D) and ``Rewind-to-Delete" (R2D), full-batch gradient descent algorithms that are easy to implement and satisfy provable unlearning guarantees. In particular, the stochastic version of D2D is widely implemented as the ``finetuning" unlearning baseline, despite lacking theoretical backing on nonconvex functions. In this work, we prove $(ε, δ)$ certified unlearning guarantees for stochastic R2D and D2D for strongly convex, convex, and nonconvex loss functions, by analyzing unlearning through the lens of disturbed or biased gradient systems, which may be contracting, semi-contracting, or expansive respectively. Our argument relies on optimally coupling the random behavior of the unlearning and retraining trajectories, resulting in a probabilistic sensitivity bound that can be combined with a novel relaxed Gaussian mechanism to achieve $(ε, δ)$ unlearning. We determine that D2D can yield tighter guarantees for strongly convex functions compared to R2D by relying on contraction to a unique global minimum. However, unlike D2D, R2D can achieve unlearning in the convex and nonconvex setting because it draws the unlearned model closer to the retrained model by reversing the accumulated disturbances.

</details>


### [147] [Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation](https://arxiv.org/abs/2511.16006)
*Yiling Liu,Juncheng Dong,Chen Fu,Wei Shi,Ziyang Jiang,Zhigang Hua,David Carlson*

Main category: cs.LG

TL;DR: 本文提出了一种结合子治疗组对齐（SGA）和随机时间掩码（RTM）的新框架，用于从时间序列数据中估计反事实结果。SGA通过迭代的治疗无关聚类识别细粒度的子治疗组，实现更优的分布匹配与去混杂；RTM通过训练时随机替换协变量为噪声，增强模型对历史模式的依赖，提升时间泛化能力。二者协同作用，在反事实估计上达到当前最优性能，分别优化了时间点上的去混杂与跨时间的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在时间序列中估计反事实结果对于关键决策（如何时施加救命治疗）至关重要，但面临两大挑战：反事实轨迹从未被观测到，且随时间演变的混杂因素会干扰每一步的估计。现有方法通常仅对治疗的边际分布进行粗略对齐，难以有效去混杂。

Method: 提出一种融合子治疗组对齐（SGA）与随机时间掩码（RTM）的框架。SGA采用迭代处理无关聚类识别细粒度子治疗组，实现更精确的分布匹配；RTM在训练中随机用高斯噪声替换输入协变量，促使模型依赖稳定的长期模式而非短期噪声或伪相关特征。

Result: 实验表明，单独使用SGA或RTM均能提升反事实估计效果，而两者联合使用在多个基准上实现了最先进的性能。其成功源于互补性：RTM增强时间泛化能力，SGA改善单一时点的去混杂效果。

Conclusion: 该框架通过结合细粒度去混杂（SGA）与时间鲁棒性增强（RTM），显著提升了时间序列反事实估计的准确性与可靠性，为动态决策提供了更有效的工具。

Abstract: Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point.

</details>


### [148] [CARE: Turning LLMs Into Causal Reasoning Expert](https://arxiv.org/abs/2511.16016)
*Juncheng Dong,Yiling Liu,Ahmed Aloui,Vahid Tarokh,David Carlson*

Main category: cs.LG

TL;DR: 该研究探讨了大语言模型（LLMs）在因果发现任务中的表现，发现其主要依赖变量名的语义而非观测数据，且即使使用已有因果发现算法的输出作为提示，性能反而下降。为此，作者提出CARE框架，通过监督微调提升LLMs对算法输出的理解与利用能力。实验表明，经CARE微调的Qwen2.5-1.5B模型在性能上显著优于传统算法和参数量上千倍的先进LLMs，展现出对自身知识与外部算法线索的有效整合。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在多种任务中表现优异，但在识别因果关系方面存在明显缺陷，无法有效利用观测数据进行因果推理。现有方法通过提示已有的因果发现算法输出，但效果不佳，亟需新方法提升LLMs的因果推理能力。

Method: 提出CARE框架，通过监督微调训练大语言模型，使其能够更有效地理解并利用因果发现算法的输出结果作为外部线索，从而增强其因果推理能力。

Result: 经过CARE微调的Qwen2.5-1.5B模型在因果发现任务中表现远超传统算法和参数量更大的先进LLMs，证明了该框架能有效结合模型内部知识与外部算法信息。

Conclusion: CARE框架成功提升了大语言模型在因果推理任务中的表现，展示了通过微调使其有效利用外部算法输出的可行性与优越性，为构建具备强因果智能的AI系统提供了新路径。

Abstract: Large language models (LLMs) have recently demonstrated impressive capabilities across a range of reasoning and generation tasks. However, research studies have shown that LLMs lack the ability to identify causal relationships, a fundamental cornerstone of human intelligence. We first conduct an exploratory investigation of LLMs' behavior when asked to perform a causal-discovery task and find that they mostly rely on the semantic meaning of variable names, ignoring the observation data. This is unsurprising, given that LLMs were never trained to process structural datasets. To first tackle this challenge, we prompt the LLMs with the outputs of established causal discovery algorithms designed for observational datasets. These algorithm outputs effectively serve as the sufficient statistics of the observation data. However, quite surprisingly, we find that prompting the LLMs with these sufficient statistics decreases the LLMs' performance in causal discovery. To address this current limitation, we propose CARE, a framework that enhances LLMs' causal-reasoning ability by teaching them to effectively utilize the outputs of established causal-discovery algorithms through supervised fine-tuning. Experimental results show that a finetuned Qwen2.5-1.5B model produced by CARE significantly outperforms both traditional causal-discovery algorithms and state-of-the-art LLMs with over a thousand times more parameters, demonstrating effective utilization of its own knowledge and the external algorithmic clues.

</details>


### [149] [HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming](https://arxiv.org/abs/2511.16027)
*Yang Wu,Yifan Zhang,Zhenxing Liang,Jian Cheng*

Main category: cs.LG

TL;DR: HGCN2SP is a novel hierarchical graph-based model for Two-stage Stochastic Programming (2SP) that improves scenario selection and ordering using reinforcement learning and graph convolutional networks. It outperforms traditional methods in solution quality and speed, especially on large-scale problems with unseen scenarios.


<details>
  <summary>Details</summary>
Motivation: Current scenario selection methods for 2SP problems rely on clustering or Monte Carlo sampling, which do not deeply integrate scenario information and ignore the impact of scenario order on solving time. This limits efficiency and scalability.

Method: HGCN2SP employs a hierarchical graph convolutional network to encode scenarios and model their relationships hierarchically. A reinforcement learning framework trains a policy network with an attention-based decoder to select and order scenarios optimally.

Result: HGCN2SP achieves high-quality solutions quickly and shows strong generalization to large-scale instances with many variables or scenarios not seen during training.

Conclusion: HGCN2SP effectively enhances the efficiency and scalability of solving 2SP problems by intelligently selecting and ordering scenarios through a learned hierarchical graph representation and reinforcement learning.

Abstract: Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase.

</details>


### [150] [ILoRA: Federated Learning with Low-Rank Adaptation for Heterogeneous Client Aggregation](https://arxiv.org/abs/2511.16069)
*Junchao Zhou,Junkang Liu,Fanhua Shang*

Main category: cs.LG

TL;DR: ILoRA addresses three key challenges in federated LoRA under client heterogeneity: initialization instability, rank incompatibility, and client drift. It introduces QR-based orthonormal initialization, concatenated QR aggregation, and an AdamW optimizer with rank-aware control variates, achieving superior accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Federated LoRA suffers from instability due to random initialization, aggregation errors from differing ranks, and worsened client drift under non-IID data, limiting model performance and generalization.

Method: ILoRA employs QR-based orthonormal initialization to align client subspaces, uses concatenated QR aggregation to handle heterogeneous-rank updates, and applies AdamW with rank-aware control variates to reduce client drift.

Result: ILoRA outperforms existing federated LoRA methods in accuracy and convergence stability across vision and NLP benchmarks, supported by theoretical convergence guarantees.

Conclusion: ILoRA provides a robust, unified solution for federated LoRA under client heterogeneity, enabling effective and stable model training in diverse, non-IID settings.

Abstract: Federated Learning with Low-Rank Adaptation (LoRA) faces three critical challenges under client heterogeneity: (1) Initialization-Induced Instability due to random initialization misaligning client subspaces; (2) Rank Incompatibility and Aggregation Error when averaging LoRA parameters of different ranks, which biases the global model; and (3) exacerbated Client Drift under Non-IID Data, impairing generalization. To address these challenges, we propose ILoRA, a unified framework that integrates three core innovations: a QR-based orthonormal initialization to ensure all clients start in a coherent subspace; a Concatenated QR Aggregation mechanism that fuses heterogeneous-rank updates via concatenation and decomposition, preserving information while maintaining dimension alignment; and an AdamW optimizer with rank-aware control variates to correct local updates and mitigate client drift. Supported by theoretical convergence guarantees, extensive experiments on vision and NLP benchmarks demonstrate that ILoRA consistently achieves superior accuracy and convergence stability compared to existing federated LoRA methods.

</details>


### [151] [AssayMatch: Learning to Select Data for Molecular Activity Models](https://arxiv.org/abs/2511.16087)
*Vincent Fan,Regina Barzilay*

Main category: cs.LG

TL;DR: AssayMatch 是一种用于药物发现中数据选择的框架，通过数据归因方法量化每个实验对模型性能的贡献，并利用微调的语言嵌入来捕捉实验之间的兼容性，从而构建更同质的训练集。该方法可在测试集标签未知的情况下进行数据筛选，适用于真实世界药物发现场景。实验表明，使用 AssayMatch 选择的数据训练的模型在多个任务上优于全数据集训练的模型，提升了预测能力与数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在药物发现中的表现受训练数据质量与一致性的影响较大。由于数据集规模有限，常需整合来自不同来源的生物活性数据，但这些数据因实验协议差异而引入大量噪声。因此需要一种方法来筛选出更一致、更相关的训练数据以提升模型性能。

Method: AssayMatch 利用数据归因技术评估各训练实验对模型性能的贡献，并基于此微调文本描述的语义嵌入，使其不仅反映语义相似性，还体现实验间的兼容性。在测试阶段，使用微调后的嵌入对所有可用训练数据进行排序，选择最相关数据用于训练。

Result: 在两个常见的机器学习架构上进行的实验显示，使用 AssayMatch 选择数据训练的模型在12个模型-靶点对中的9个上超越了仅使用语言特征的基线模型，且整体性能优于全数据集训练模型，证明其能有效过滤噪声实验并提高预测准确性。

Conclusion: AssayMatch 提供了一种数据驱动的高质量数据集构建机制，能够减少不兼容实验带来的噪声，显著提升药物发现中模型的预测能力和数据利用效率。该工具已开源，便于广泛应用。

Abstract: The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at https://github.com/Ozymandias314/AssayMatch.

</details>


### [152] [Pathlet Variational Auto-Encoder for Robust Trajectory Generation](https://arxiv.org/abs/2511.16105)
*Yuanbo Tang,Yan Tang,Zixuan Zhang,Zihui Zhao,Yang Li*

Main category: cs.LG

TL;DR: 本文提出一种基于路径片段（pathlet）表示的深度生成模型，用于城市轨迹生成。该模型利用城市轨迹中的规律结构，通过二进制向量编码轨迹，并结合变分自编码器（VAE）与线性解码器构建概率图模型，实现对轨迹数据分布的有效学习。即使在噪声数据下，模型仍表现出色，在两个真实数据集上分别优于强基线35.4%和26.3%。此外，模型支持基于时空约束的定制化轨迹生成，适用于轨迹预测与去噪等下游任务，且在效率上显著优于先前方法，节省64.8%时间与56.5% GPU内存。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习或生成式AI的轨迹生成方法虽表现良好，但其在噪声数据下的鲁棒性与模型可解释性不足，限制了其在真实场景中的应用与可信度。因此亟需一种既高效又可解释的轨迹生成框架。

Method: 提出基于路径片段表示的深度生成模型，采用变分自编码器（VAE）学习路径片段的潜在嵌入，结合线性解码器与可学习的路径片段字典，构建概率图模型描述轨迹生成过程；支持条件生成以满足时空约束。

Result: 模型在含噪声的真实轨迹数据上表现优异，相比强基线提升35.4%和26.3%；可有效支持轨迹预测、去噪等下游任务；计算效率显著提高，节省64.8%时间与56.5% GPU内存。

Conclusion: 所提出的路径片段表示生成模型在鲁棒性、可解释性与效率方面均优于现有方法，为隐私保护的城市移动性研究与位置服务应用提供了可靠的技术支持。

Abstract: Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.
  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.

</details>


### [153] [An Interpretability-Guided Framework for Responsible Synthetic Data Generation in Emotional Text](https://arxiv.org/abs/2511.16132)
*Paula Joy B. Martinez,Jose Marie Antonio Miñoza,Sebastian C. Ibañez*

Main category: cs.LG

TL;DR: 本文提出一种基于可解释性指导的合成数据生成框架，利用SHAP方法为大语言模型生成情感识别所需的数据。该方法在有足够种子数据的情况下，性能接近真实数据，显著优于简单生成方法，并改善了少数情绪类别的分类效果。但语言分析显示，合成文本词汇丰富度较低，缺乏个人化和时间复杂性表达。研究既提供了负责任合成数据生成的实用方案，也揭示了其局限性，强调可信AI需权衡合成数据效用与真实感之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 社交媒体情感识别对理解公众情绪至关重要，但因API成本上升和平台限制，获取训练数据变得极其昂贵。因此需要一种低成本、高效且可靠的替代方案来生成训练数据。

Method: 采用Shapley Additive Explanations（SHAP）提供可解释性指导，用于引导大语言模型生成合成社交媒体文本数据。通过分析特征重要性，确保生成内容保留关键情感语义信息。

Result: 在具备足够种子数据的前提下，该方法生成的合成数据在情感分类任务中表现接近真实数据；相比传统生成方式，显著提升性能，尤其改善了低频情绪类别的识别准确率；但合成文本在词汇多样性、个性化表达和时间复杂性方面弱于真实帖子。

Conclusion: 本研究构建了一种兼顾实用性和可解释性的合成数据生成框架，为应对数据获取难题提供新路径。然而，合成数据在语言真实性上存在固有缺陷，未来可信AI的发展必须在数据效用与真实世界表征之间做出审慎权衡。

Abstract: Emotion recognition from social media is critical for understanding public sentiment, but accessing training data has become prohibitively expensive due to escalating API costs and platform restrictions. We introduce an interpretability-guided framework where Shapley Additive Explanations (SHAP) provide principled guidance for LLM-based synthetic data generation. With sufficient seed data, SHAP-guided approach matches real data performance, significantly outperforms naïve generation, and substantially improves classification for underrepresented emotion classes. However, our linguistic analysis reveals that synthetic text exhibits reduced vocabulary richness and fewer personal or temporally complex expressions than authentic posts. This work provides both a practical framework for responsible synthetic data generation and a critical perspective on its limitations, underscoring that the future of trustworthy AI depends on navigating the trade-offs between synthetic utility and real-world authenticity.

</details>


### [154] [Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection](https://arxiv.org/abs/2511.16145)
*Zhijie Zhong,Zhiwen Yu,Kaixiang Yang,C. L. Philip Chen*

Main category: cs.LG

TL;DR: 本文挑战了时间序列异常检测（TSAD）中依赖复杂架构的主流观点，通过系统比较有监督与无监督方法，提出一个简洁的有监督基线模型STAND。实验表明，在有限标注预算下，简单有监督模型显著优于复杂的无监督方法，且少量标注带来的性能提升远超架构创新。此外，STAND在预测一致性和异常定位上表现更优，强调应将研究重心从算法复杂性转向标签利用。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测主要依赖无监督方法，因标签稀缺而忽视了实际场景中可获得的少量异常标签的价值。本文旨在探究标签的重要性是否超过模型复杂度，并推动研究范式向数据驱动转变。

Method: 通过构建并对比有监督与无监督方法，提出轻量级有监督基线模型STAND，进行多数据集上的系统性实验评估。

Result: 在五个公开数据集上，有监督方法在有限标注条件下显著优于无监督方法；少量标注带来的性能增益远大于架构改进；STAND在预测一致性与异常定位方面表现更优。

Conclusion: 标签的价值高于模型复杂度，应推动时间序列异常检测研究从算法复杂性转向数据利用，强调标签的有效使用。

Abstract: Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.

</details>


### [155] [Achieving Skilled and Reliable Daily Probabilistic Forecasts of Wind Power at Subseasonal-to-Seasonal Timescales over France](https://arxiv.org/abs/2511.16164)
*Eloi Lindas,Yannig Goude,Philippe Ciais*

Main category: cs.LG

TL;DR: 本研究提出了一种将ECMWF的次季节至季节性天气预报转化为1至46天、每日分辨率风力发电预测的框架，通过后处理校正天气预报的偏差和分散不足问题，显著优于气候基准，在连续概率评分技能和均方误差上提升50%，且在15至46天预报期内实现近乎完美的校准。


<details>
  <summary>Details</summary>
Motivation: 现有短期天气预报已广泛用于可再生能源功率预测，但更长预测时间范围（如次季节至季节性）的风力发电预测仍需深入研究。尽管次季节至季节性天气概率预报取得进展，但其在风力发电预测中的应用常受限于时间和空间聚合，难以获得高精度结果。因此，亟需一种有效方法将此类长期天气预报转化为可靠的风力发电预测。

Method: 构建了一个预测流程框架，将ECMWF的次季节至季节性天气预报转化为风力发电预测，覆盖1至46天的预报期，采用每日分辨率。该框架包含对生成的功率集合进行后处理，以修正天气预报中的系统偏差和分散不足问题。

Result: 所提方法在连续概率评分技能和均方误差指标上比气候基准提高50%；在15至46天的预报期内实现了近乎完美的校准，表明其具有良好的可靠性与准确性。

Conclusion: 本研究成功实现了从次季节至季节性天气预报到风力发电预测的有效转化，为中长期风电预测提供了可靠工具，有助于提升电网稳定性与市场风险管理能力。

Abstract: Accurate and reliable wind power forecasts are crucial for grid stability, balancing supply and demand, and market risk management. Even though short-term weather forecasts have been thoroughly used to provide short-term renewable power predictions, forecasts involving longer prediction horizons still need investigations. Despite the recent progress in subseasonal-to-seasonal weather probabilistic forecasting, their use for wind power prediction usually involves both temporal and spatial aggregation achieve reasonable skill. In this study, we present a forecasting pipeline enabling to transform ECMWF subseasonal-to-seasonal weather forecasts into wind power forecasts for lead times ranging from 1 day to 46 days at daily resolution. This framework also include post-processing of the resulting power ensembles to account for the biases and lack of dispersion of the weather forecasts. We show that our method is able to outperform a climatological baseline by 50 % in terms of both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error while also providing near perfect calibration of the forecasts for lead times ranging from 15 to 46 days.

</details>


### [156] [CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality](https://arxiv.org/abs/2511.16191)
*Xiaotong Zhan,Xi Cheng*

Main category: cs.LG

TL;DR: CausalMamba integrates Mamba-based sequence modeling, GCNs, and NOTEARS for causal discovery to detect rumors on social media. It jointly models tweet sequences and reply structures, uncovers latent causal graphs, and enables counterfactual analysis. Experiments on Twitter15 show strong performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection models lack interpretability and fail to reveal causal mechanisms behind misinformation spread.

Method: CausalMamba combines Mamba for temporal sequence modeling, GCNs for structural features, and differentiable causal discovery via NOTEARS to learn latent causal graphs and influential nodes.

Result: The model achieves competitive classification accuracy on Twitter15 and enables effective counterfactual intervention analysis. Removing top causal nodes significantly affects graph connectivity, demonstrating interpretability.

Conclusion: CausalMamba offers a unified, explainable framework for rumor detection and influence analysis, advancing toward actionable and transparent misinformation detection systems.

Abstract: Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.

</details>


### [157] [A Switching Framework for Online Interval Scheduling with Predictions](https://arxiv.org/abs/2511.16194)
*Antonios Antoniadis,Ali Shahheidar,Golnoosh Shahkarami,Abolfazl Soltani*

Main category: cs.LG

TL;DR: 本文研究了在不可撤销设置下的在线区间调度问题，引入机器学习预测来提升性能，提出SemiTrust-and-Switch框架，统一结合基于预测与经典算法，实现一致性与鲁棒性的平衡，并设计了一种随机算法，能平滑过渡于预测与鲁棒性之间，性能随预测质量渐进下降。


<details>
  <summary>Details</summary>
Motivation: 在在线区间调度中，传统方法无法利用预测信息，而直接依赖预测可能导致性能退化。因此需要一种既能利用预测提高效率，又能保证在预测错误时具备鲁棒性的机制。

Method: 提出SemiTrust-and-Switch框架，结合预测与经典调度算法；设计随机算法实现性能的平滑过渡，确保在预测误差下仍保持良好表现。

Result: 框架在特定条件下达到理论下界，证明其紧致性；所提随机算法实现了鲁棒性与平滑性的统一，性能随预测质量渐进下降。

Conclusion: SemiTrust-and-Switch框架为在线区间调度提供了高效且稳健的解决方案，能够有效融合预测信息，在保证鲁棒性的同时提升预测准确时的性能。

Abstract: We study online interval scheduling in the irrevocable setting, where each interval must be immediately accepted or rejected upon arrival. The objective is to maximize the total length of accepted intervals while ensuring that no two accepted intervals overlap. We consider this problem in a learning-augmented setting, where the algorithm has access to (machine-learned) predictions. The goal is to design algorithms that leverage these predictions to improve performance while maintaining robust guarantees in the presence of prediction errors.
  Our main contribution is the SemiTrust-and-Switch framework, which provides a unified approach for combining prediction-based and classical interval scheduling algorithms. This framework applies to both deterministic and randomized algorithms and captures the trade-off between consistency (performance under accurate predictions) and robustness (performance under adversarial inputs). Moreover, we provide lower bounds, proving the tightness of this framework in particular settings.
  We further design a randomized algorithm that smoothly interpolates between prediction-based and robust algorithms. This algorithm achieves both robustness and smoothness--its performance degrades gracefully with the quality of the prediction.

</details>


### [158] [Towards Overcoming Data Scarcity in Nuclear Energy: A Study on Critical Heat Flux with Physics-consistent Conditional Diffusion Model](https://arxiv.org/abs/2511.16207)
*Farah Alsafadi,Alexandra Akins,Xu Wu*

Main category: cs.LG

TL;DR: 本文研究了扩散模型（DM）在克服核能应用中数据稀缺问题上的有效性。利用公开的临界热流密度（CHF）数据集，作者开发了能够生成任意数量合成样本的普通和条件扩散模型，以扩充CHF数据集。普通DM随机生成数据，而条件DM可按用户指定的热工水力条件生成目标数据。通过评估特征分布、成对相关性及物理一致性，结果表明两种模型均能生成真实且符合物理规律的CHF数据。此外，不确定性量化分析验证了生成数据的可靠性，尤其条件DM在数据增强方面表现优异且不确定性可控。


<details>
  <summary>Details</summary>
Motivation: 核能应用中实验数据常受限于获取成本高、难度大或数量少，导致数据稀缺问题严重。为提升下游机器学习模型的鲁棒性，需通过生成合成数据来扩充训练集。深度生成模型如扩散模型可学习数据分布并生成高质量合成样本，是解决此问题的有效途径。

Method: 基于公开的临界热流密度（CHF）数据集，构建普通扩散模型（DM）与条件扩散模型（CDM）。普通DM用于生成随机但统计上合理的合成样本；条件DM则根据用户指定的热工水力条件生成目标数据。模型性能通过特征分布匹配度、成对相关性保持能力及物理一致性进行评估，并结合不确定性量化方法验证生成数据的可信度。

Result: 两种模型均成功生成了符合实际分布且物理一致的CHF数据。条件扩散模型在数据增强方面表现尤为出色，可在保持较低不确定性水平的前提下有效扩充数据集，显著提升下游预测模型的训练质量与鲁棒性。

Conclusion: 扩散模型，特别是条件扩散模型，是克服核能领域数据稀缺问题的有力工具。其生成的合成数据不仅具备高保真度和物理合理性，且可通过不确定性量化确保可靠性，具有广泛的应用前景。

Abstract: Deep generative modeling provides a powerful pathway to overcome data scarcity in energy-related applications where experimental data are often limited, costly, or difficult to obtain. By learning the underlying probability distribution of the training dataset, deep generative models, such as the diffusion model (DM), can generate high-fidelity synthetic samples that statistically resemble the training data. Such synthetic data generation can significantly enrich the size and diversity of the available training data, and more importantly, improve the robustness of downstream machine learning models in predictive tasks. The objective of this paper is to investigate the effectiveness of DM for overcoming data scarcity in nuclear energy applications. By leveraging a public dataset on critical heat flux (CHF) that cover a wide range of commercial nuclear reactor operational conditions, we developed a DM that can generate an arbitrary amount of synthetic samples for augmenting of the CHF dataset. Since a vanilla DM can only generate samples randomly, we also developed a conditional DM capable of generating targeted CHF data under user-specified thermal-hydraulic conditions. The performance of the DM was evaluated based on their ability to capture empirical feature distributions and pair-wise correlations, as well as to maintain physical consistency. The results showed that both the DM and conditional DM can successfully generate realistic and physics-consistent CHF data. Furthermore, uncertainty quantification was performed to establish confidence in the generated data. The results demonstrated that the conditional DM is highly effective in augmenting CHF data while maintaining acceptable levels of uncertainty.

</details>


### [159] [Mind the Gap: Bridging Prior Shift in Realistic Few-Shot Crop-Type Classification](https://arxiv.org/abs/2511.16218)
*Joana Reuss,Ekaterina Gikalo,Marco Körner*

Main category: cs.LG

TL;DR: 本文提出了一种名为Dirichlet Prior Augmentation (DirPA)的新方法，旨在解决农业作物分类中因类别不平衡导致的模型泛化问题。该方法在少样本学习过程中主动模拟目标域中未知的标签分布偏斜，通过将真实世界分布建模为狄利克雷分布的随机变量，实现先验增强。实验表明，DirPA能有效调整决策边界并稳定训练过程，起到动态特征正则化的作用。


<details>
  <summary>Details</summary>
Motivation: 真实世界的农业分布通常呈现严重的类别不平衡，且作物类型分类的标注数据稀缺且获取成本高。当前多数研究在训练时人为平衡数据集，与真实场景不一致，导致训练与测试阶段的标签分布存在偏差，影响模型在真实环境中的泛化能力。

Method: 将真实世界的标签分布建模为狄利克雷分布的随机变量，在少样本学习过程中进行先验增强，从而模拟实际中的类别分布偏斜，提升模型对真实分布的适应性。

Result: 实验结果表明，DirPA能够有效调整模型的决策边界，稳定训练过程，并作为动态特征正则化器，显著提升模型在真实世界条件下的泛化性能。

Conclusion: DirPA通过在训练阶段主动模拟真实世界的类别分布偏斜，有效缓解了训练与测试分布不一致的问题，提升了少样本学习在真实农业场景中的适用性和鲁棒性。

Abstract: Real-world agricultural distributions often suffer from severe class imbalance, typically following a long-tailed distribution. Labeled datasets for crop-type classification are inherently scarce and remain costly to obtain. When working with such limited data, training sets are frequently constructed to be artificially balanced -- in particular in the case of few-shot learning -- failing to reflect real-world conditions. This mismatch induces a shift between training and test label distributions, degrading real-world generalization. To address this, we propose Dirichlet Prior Augmentation (DirPA), a novel method that simulates an unknown label distribution skew of the target domain proactively during model training. Specifically, we model the real-world distribution as Dirichlet-distributed random variables, effectively performing a prior augmentation during few-shot learning. Our experiments show that DirPA successfully shifts the decision boundary and stabilizes the training process by acting as a dynamic feature regularizer.

</details>


### [160] [Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective](https://arxiv.org/abs/2511.16231)
*Yang Yu*

Main category: cs.LG

TL;DR: 该论文分析了pass@k作为大语言模型多步推理能力评估和优化目标的适用性，发现其在探索关键阶段提供渐弱的学习信号，并可能导致‘探索崩溃’。研究指出pass@k本质上是对pass@1的正样本重加权，虽适合作为诊断工具，但不适合作为直接优化目标，建议采用显式鼓励高效探索的机制来改进强化学习中的推理任务。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在复杂多步推理任务中的表现依赖于有效的评估与优化方法。pass@k因其直观性被广泛用于评估和强化学习优化，但其潜在缺陷尚未充分揭示，因此需要深入分析其有效性与局限性。

Method: 通过推导pass@k的梯度，分析其与pass@1的关系，研究在不同策略分布下的学习信号强度，并模拟探索崩溃现象以揭示其动态行为。

Result: pass@k在高探索需求场景中学习信号趋近于零，且随着策略集中，pass@k与pass@1的差异缩小；表明其在优化中可能失效。

Conclusion: pass@k适合作为诊断工具，但不适合作为直接优化目标；应引入显式探索机制以提升强化学习在推理任务中的表现。

Abstract: The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.

</details>


### [161] [GeoPTH: A Lightweight Approach to Category-Based Trajectory Retrieval via Geometric Prototype Trajectory Hashing](https://arxiv.org/abs/2511.16258)
*Yang Xu,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出GeoPTH，一种轻量级、非学习的轨迹相似性检索框架，通过几何原型点集作为锚点构建数据依赖的哈希函数，利用鲁棒的Hausdorff度量高效映射轨迹至最近原型，实现高精度与高效率的类别化轨迹检索。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹相似性检索方法存在计算成本高或训练成本大且不稳定的缺陷，亟需一种高效、轻量且稳定的替代方案。

Method: 采用代表性轨迹原型（保留几何特征的小点集）作为锚点，构建数据依赖的哈希函数；通过鲁棒的Hausdorff度量将新轨迹映射到最近原型，完成高效哈希与检索。

Result: GeoPTH在检索精度上媲美传统度量与先进学习方法，显著优于简单嵌入二值化生成的二进制码，在效率方面全面超越所有对比方法。

Conclusion: 轻量级、以原型为中心的方法为轨迹检索提供了高效且强大的实用替代方案，实现了卓越的检索性能与计算效率。

Abstract: Trajectory similarity retrieval is an important part of spatiotemporal data mining, however, existing methods have the following limitations: traditional metrics are computationally expensive, while learning-based methods suffer from substantial training costs and potential instability. This paper addresses these problems by proposing \textbf{Geo}metric \textbf{P}rototype \textbf{T}rajectory \textbf{H}ashing (GeoPTH), a novel, lightweight, and non-learning framework for efficient category-based trajectory retrieval. GeoPTH constructs data-dependent hash functions by using representative trajectory prototypes, i.e., small point sets preserving geometric characteristics, as anchors. The hashing process is efficient, which involves mapping a new trajectory to its closest prototype via a robust, \textit{Hausdorff} metric. Extensive experiments show that GeoPTH's retrieval accuracy is highly competitive with both traditional metrics and state-of-the-art learning methods, and it significantly outperforms binary codes generated through simple binarization of the learned embeddings. Critically, GeoPTH consistently outperforms all competitors in terms of efficiency. Our work demonstrates that a lightweight, prototype-centric approach offers a practical and powerful alternative, achieving an exceptional retrieval performance and computational efficiency.

</details>


### [162] [Graph Diffusion Counterfactual Explanation](https://arxiv.org/abs/2511.16287)
*David Bechtoldt,Sidney Bender*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型和无分类器引导的图反事实解释框架，用于生成结构上最小变化且分布一致的反事实样本，解决图数据中解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在图数据上预测准确但缺乏可解释性，而图结构的离散性和非欧几里得特性使得反事实解释难以构建。

Method: 结合离散扩散模型与分类器自由引导技术，生成图结构上的反事实样本。

Result: 方法能可靠生成分布内、结构差异最小的反事实解释，适用于离散分类和连续属性任务。

Conclusion: 所提框架为图数据提供了有效的反事实解释手段，提升了模型决策的透明度与可信度。

Abstract: Machine learning models that operate on graph-structured data, such as molecular graphs or social networks, often make accurate predictions but offer little insight into why certain predictions are made. Counterfactual explanations address this challenge by seeking the closest alternative scenario where the model's prediction would change. Although counterfactual explanations are extensively studied in tabular data and computer vision, the graph domain remains comparatively underexplored. Constructing graph counterfactuals is intrinsically difficult because graphs are discrete and non-euclidean objects. We introduce Graph Diffusion Counterfactual Explanation, a novel framework for generating counterfactual explanations on graph data, combining discrete diffusion models and classifier-free guidance. We empirically demonstrate that our method reliably generates in-distribution as well as minimally structurally different counterfactuals for both discrete classification targets and continuous properties.

</details>


### [163] [Optimizing Operation Recipes with Reinforcement Learning for Safe and Interpretable Control of Chemical Processes](https://arxiv.org/abs/2511.16297)
*Dean Brandner,Sergio Lucia*

Main category: cs.LG

TL;DR: 本文提出一种基于操作配方中嵌入的专家知识的新方法，利用强化学习优化配方及其底层线性控制器的参数，以实现更优的化学过程操作。该方法数据需求少、约束处理能力强且可解释性高，在工业级间歇聚合反应器仿真中表现出接近最优控制器的性能，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在化学过程最优操作中面临硬约束（如质量和安全）难以满足、训练数据不足等问题；而详细动态模型虽可替代数据但计算复杂度高，模型预测控制也因模型复杂度而难以应用。因此，现有化工过程多依赖人工制定的操作配方和简单线性控制器，导致性能不佳且灵活性差。

Method: 将专家知识融入操作配方，使用强化学习优化配方参数及配套线性控制器，构建可优化的结构化操作策略，从而降低数据需求、增强约束处理能力并提升可解释性。

Result: 在工业级间歇聚合反应器的仿真实验中，所提方法能够接近最优控制器的性能，同时显著减少数据需求、更好地处理约束，并提供更具可解释性的操作方案。

Conclusion: 通过结合专家知识与强化学习优化，该方法为化学过程的最优操作提供了一种高效、可靠且可解释的解决方案，具有良好的应用前景。

Abstract: Optimal operation of chemical processes is vital for energy, resource, and cost savings in chemical engineering. The problem of optimal operation can be tackled with reinforcement learning, but traditional reinforcement learning methods face challenges due to hard constraints related to quality and safety that must be strictly satisfied, and the large amount of required training data. Chemical processes often cannot provide sufficient experimental data, and while detailed dynamic models can be an alternative, their complexity makes it computationally intractable to generate the needed data. Optimal control methods, such as model predictive control, also struggle with the complexity of the underlying dynamic models. Consequently, many chemical processes rely on manually defined operation recipes combined with simple linear controllers, leading to suboptimal performance and limited flexibility.
  In this work, we propose a novel approach that leverages expert knowledge embedded in operation recipes. By using reinforcement learning to optimize the parameters of these recipes and their underlying linear controllers, we achieve an optimized operation recipe. This method requires significantly less data, handles constraints more effectively, and is more interpretable than traditional reinforcement learning methods due to the structured nature of the recipes. We demonstrate the potential of our approach through simulation results of an industrial batch polymerization reactor, showing that it can approach the performance of optimal controllers while addressing the limitations of existing methods.

</details>


### [164] [Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning](https://arxiv.org/abs/2511.16333)
*Mohammad Areeb Qazi,Maryam Nadeem,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 该论文综述了用于医疗系统的世界模型，强调其在预测动态、多步模拟、反事实评估和规划方面的潜力。重点涵盖医学影像、疾病进展建模和机器人手术三个领域，并提出一个从L1到L4的能力层级框架，当前多数系统仅达到L1-L2，L3和L4罕见。指出关键挑战包括动作空间不明确、安全约束缺失、干预验证薄弱、多模态状态构建不完整及轨迹级不确定性校准不足。最后提出将生成式骨干（如Transformer、扩散模型）与因果/机械基础结合的研究方向，以实现临床可靠的预测优先型世界模型。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型缺乏物理基础和时间推理能力，难以满足临床决策支持对预测性、可靠性和数据效率的要求；随着语言模型规模扩展收益递减，具备多模态、时序一致性与动作条件表示的世界模型成为更具前景的方向。

Method: 系统综述近期在医疗影像、电子健康记录疾病进展建模、机器人手术与手术规划中的世界模型研究，采用能力层级框架（L1-L4）评估模型性能，并分析共性技术缺口。

Result: 多数现有系统仅实现L1（时间预测）和L2（动作条件预测），极少达到L3（反事实多步模拟）和L4（规划/控制）；存在动作空间模糊、安全约束不足、干预验证弱、多模态状态不完整及不确定性校准差等核心缺陷。

Conclusion: 未来应发展融合生成式架构与因果/机械原理的预测优先型世界模型，推动临床可信、安全可控的智能医疗系统建设。

Abstract: Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.

</details>


### [165] [Improving Iterative Gaussian Processes via Warm Starting Sequential Posteriors](https://arxiv.org/abs/2511.16340)
*Alan Yufei Dong,Jihao Andreas Lin,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过利用包含在大型线性系统中的较小系统已知解来改进迭代高斯过程（GP）中线性求解器的收敛速度。该方法特别适用于增量数据添加的任务，在达到容差时实现加速，并在固定计算预算下提升贝叶斯优化性能。


<details>
  <summary>Details</summary>
Motivation: 提高高斯过程的可扩展性对于序列决策任务至关重要，而现有方法在解决大规模问题时仍存在收敛慢、计算成本高等挑战，尤其是在增量数据场景下。因此需要更高效的求解策略。

Method: 利用嵌套在大系统中的小系统已知解，指导或初始化大系统的迭代求解过程，从而加速共轭梯度等迭代线性求解器的收敛。

Result: 在达到指定容差时实现了显著的速度提升；在固定计算预算下，贝叶斯优化的性能得到改善。

Conclusion: 所提方法有效提升了迭代高斯过程在增量学习场景下的求解效率与优化性能，为大规模高斯过程应用提供了实用且可扩展的解决方案。

Abstract: Scalable Gaussian process (GP) inference is essential for sequential decision-making tasks, yet improving GP scalability remains a challenging problem with many open avenues of research. This paper focuses on iterative GPs, where iterative linear solvers, such as conjugate gradients, stochastic gradient descent or alternative projections, are used to approximate the GP posterior. We propose a new method which improves solver convergence of a large linear system by leveraging the known solution to a smaller system contained within. This is significant for tasks with incremental data additions, and we show that our technique achieves speed-ups when solving to tolerance, as well as improved Bayesian optimisation performance under a fixed compute budget.

</details>


### [166] [Are Foundation Models Useful for Bankruptcy Prediction?](https://arxiv.org/abs/2511.16375)
*Marcin Kostrzewa,Oleksii Furman,Roman Furman,Sebastian Tomczak,Maciej Zięba*

Main category: cs.LG

TL;DR: 本文首次系统比较了Llama-3.3-70B-Instruct等基础模型与传统机器学习方法（如XGBoost、CatBoost）在企业破产预测中的表现。基于超过一百万条来自维斯格拉德集团公司的数据，研究发现经典机器学习模型在所有预测时间范围内均优于基础模型，且大语言模型存在概率估计不可靠的问题，不适用于风险敏感的金融场景。尽管TabPFN表现接近简单基线，但其计算成本过高，性能提升不足以证明投入合理性。结论表明，当前基础模型虽具通用性，但在破产预测任务中仍不如专用方法有效。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型在企业破产预测中的实际有效性，填补其在金融领域尤其是高不平衡数据下系统性评估的空白。

Method: 采用Llama-3.3-70B-Instruct和TabPFN等基础模型，与XGBoost、CatBoost等经典机器学习方法进行对比实验，使用涵盖超百万公司记录的维斯格拉德集团数据集，在多个预测时间窗口下评估模型性能。

Result: XGBoost和CatBoost在所有预测时间窗口上均显著优于基础模型；大语言模型的概率估计不稳定，影响其在金融风控中的应用；TabPFN虽有竞争力但计算开销过大，性价比低。

Conclusion: 尽管基础模型具备广泛适用性，但在企业破产预测这一特定任务中，其表现仍逊于经过专门优化的传统机器学习方法，提示应谨慎将其用于高风险金融决策场景。

Abstract: Foundation models have shown promise across various financial applications, yet their effectiveness for corporate bankruptcy prediction remains systematically unevaluated against established methods. We study bankruptcy forecasting using Llama-3.3-70B-Instruct and TabPFN, evaluated on large, highly imbalanced datasets of over one million company records from the Visegrád Group. We provide the first systematic comparison of foundation models against classical machine learning baselines for this task. Our results show that models such as XGBoost and CatBoost consistently outperform foundation models across all prediction horizons. LLM-based approaches suffer from unreliable probability estimates, undermining their use in risk-sensitive financial settings. TabPFN, while competitive with simpler baselines, requires substantial computational resources with costs not justified by performance gains. These findings suggest that, despite their generality, current foundation models remain less effective than specialized methods for bankruptcy forecasting.

</details>


### [167] [Optimal Fairness under Local Differential Privacy](https://arxiv.org/abs/2511.16377)
*Hrad Ghoukasian,Shahab Asoodeh*

Main category: cs.LG

TL;DR: 本文研究如何最优设计局部差分隐私（LDP）机制，以减少数据不公平性并提升下游分类的公平性。针对二值敏感属性，推导出闭式最优机制；针对多值属性，提出可计算的优化框架。理论证明：在歧视-准确率最优分类器下，降低数据不公平性必然导致分类不公平性下降，建立了隐私感知预处理与分类公平性的直接联系。实验表明，所提方法在多种数据集和公平性度量下持续优于现有LDP机制，同时保持接近非私有模型的准确性。相比主流预处理和后处理公平方法，该机制在准确率-公平性权衡上表现更优，并有效保护敏感属性隐私。整体表明LDP是一种原则性强且有效的预处理公平干预技术。


<details>
  <summary>Details</summary>
Motivation: 现有LDP机制在保障隐私的同时可能引入或加剧数据不公平性，影响下游分类的公平性。因此，亟需设计能主动减少数据不公平性的隐私机制，实现隐私保护与分类公平性的协同优化。

Method: 针对二值敏感属性，推导闭式最优LDP机制；针对多值属性，构建可计算的优化框架。通过理论分析建立数据不公平性与分类不公平性之间的因果关系，并基于此设计兼顾隐私、公平与准确性的机制。

Result: 所提方法在多个数据集和公平性指标下显著降低数据不公平性，且保持高分类准确率；优于现有LDP机制及主流公平预处理/后处理方法，在准确率-公平性权衡上表现更优，同时确保敏感属性隐私安全。

Conclusion: 局部差分隐私不仅能够有效保护隐私，还可作为一项原则性且高效的预处理公平干预手段，通过优化机制设计实现数据公平性与分类公平性的同步提升。

Abstract: We investigate how to optimally design local differential privacy (LDP) mechanisms that reduce data unfairness and thereby improve fairness in downstream classification. We first derive a closed-form optimal mechanism for binary sensitive attributes and then develop a tractable optimization framework that yields the corresponding optimal mechanism for multi-valued attributes. As a theoretical contribution, we establish that for discrimination-accuracy optimal classifiers, reducing data unfairness necessarily leads to lower classification unfairness, thus providing a direct link between privacy-aware pre-processing and classification fairness. Empirically, we demonstrate that our approach consistently outperforms existing LDP mechanisms in reducing data unfairness across diverse datasets and fairness metrics, while maintaining accuracy close to that of non-private models. Moreover, compared with leading pre-processing and post-processing fairness methods, our mechanism achieves a more favorable accuracy-fairness trade-off while simultaneously preserving the privacy of sensitive attributes. Taken together, these results highlight LDP as a principled and effective pre-processing fairness intervention technique.

</details>


### [168] [Generative Modeling of Clinical Time Series via Latent Stochastic Differential Equations](https://arxiv.org/abs/2511.16427)
*Muhammad Aslanimoghanloo,Ahmed ElGazzar,Marcel van Gerven*

Main category: cs.LG

TL;DR: 本文提出了一种基于潜在神经随机微分方程（SDE）的生成建模框架，用于处理电子健康记录和医疗注册表中的临床时间序列数据。该框架将临床时间序列视为潜在受控随机动力系统的离散观测，通过模态相关的发射模型和变分推断实现状态估计与参数学习，能够有效应对不规则采样、复杂生理机制及测量与疾病进展中的不确定性。在模拟肺癌药物动力学-药效学模型和12,000名患者的重症监护室真实数据上验证，其在个体治疗效应估计和生理信号概率预测任务中均优于常微分方程和LSTM基线模型，在准确性和不确定性估计方面表现更优，展现出支持临床决策的精准、不确定性感知预测潜力。


<details>
  <summary>Details</summary>
Motivation: 临床时间序列数据虽蕴含丰富患者轨迹信息，但面临不规则采样、复杂生理动态及测量与疾病进展中的不确定性等挑战，现有方法难以统一建模这些复杂性。

Method: 提出基于潜在神经随机微分方程（SDE）的生成建模框架，将临床时间序列视为潜在受控随机动力系统的离散观测；采用模态依赖的发射模型与神经SDE结合，通过变分推断进行状态估计与参数学习。

Result: 在模拟肺癌PKPD模型和真实ICU数据集上的实验表明，该框架在个体治疗效应估计和生理信号概率预测任务中，均优于传统常微分方程和LSTM模型，具有更高的预测精度和更优的不确定性建模能力。

Conclusion: 所提出的基于潜在神经SDE的框架能统一处理不规则采样、非线性动态与随机性，具备良好的可扩展性与鲁棒性，为实现精准、不确定性感知的临床预测提供了有力工具，具有广泛应用于临床决策支持的潜力。

Abstract: Clinical time series data from electronic health records and medical registries offer unprecedented opportunities to understand patient trajectories and inform medical decision-making. However, leveraging such data presents significant challenges due to irregular sampling, complex latent physiology, and inherent uncertainties in both measurements and disease progression. To address these challenges, we propose a generative modeling framework based on latent neural stochastic differential equations (SDEs) that views clinical time series as discrete-time partial observations of an underlying controlled stochastic dynamical system. Our approach models latent dynamics via neural SDEs with modality-dependent emission models, while performing state estimation and parameter learning through variational inference. This formulation naturally handles irregularly sampled observations, learns complex non-linear interactions, and captures the stochasticity of disease progression and measurement noise within a unified scalable probabilistic framework. We validate the framework on two complementary tasks: (i) individual treatment effect estimation using a simulated pharmacokinetic-pharmacodynamic (PKPD) model of lung cancer, and (ii) probabilistic forecasting of physiological signals using real-world intensive care unit (ICU) data from 12,000 patients. Results show that our framework outperforms ordinary differential equation and long short-term memory baseline models in accuracy and uncertainty estimation. These results highlight its potential for enabling precise, uncertainty-aware predictions to support clinical decision-making.

</details>


### [169] [A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms](https://arxiv.org/abs/2511.16475)
*Ali Murtaza Caunhye,Asad Jeewa*

Main category: cs.LG

TL;DR: 本文比较了决策变换器（DT）与传统离线强化学习算法（如IQL、CQL）在ANT连续控制环境中的表现，重点分析其在密集和稀疏奖励设置下的性能差异。研究发现，DT对奖励密度变化不敏感，在稀疏奖励场景中尤其擅长，特别是在中等质量数据下表现优异；而传统基于值的方法在高质密集奖励数据中表现更好，CQL则表现出更均衡的性能。此外，DT性能方差小但计算开销大。结果表明，序列建模方法更适合奖励结构不确定或数据质量混合的场景，而基于值的方法在密集奖励且高质量数据条件下仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习算法在处理不同奖励密度时面临探索与利用的平衡难题，尤其在稀疏奖励环境下表现不佳。尽管决策变换器（DT）通过序列建模方式在多个基准上取得优异成果，但其在不同奖励结构下的适应性尚未系统评估。因此，本文旨在深入比较DT与传统方法在多样奖励设置下的性能，揭示其各自优势与适用场景。

Method: 在ANT连续控制环境中，采用多种数据集（包括高质、中等质量和低质数据），分别在密集和稀疏奖励设置下对比DT、IQL和CQL三种算法的性能。通过重复实验评估策略学习效果、泛化能力及性能稳定性，并记录计算资源消耗。

Result: DT在稀疏奖励场景中表现显著优于传统方法，尤其是在中等质量数据下；在密集奖励下，IQL表现最佳，CQL表现稳定；总体而言，DT具有更低的性能方差，但计算成本远高于其他方法。

Conclusion: 序列建模方法（如DT）在奖励结构不确定或数据质量不一的场景中更具鲁棒性，适合复杂现实应用；而传统价值基方法在密集奖励和高质量数据条件下依然具有优势。选择算法应根据任务的奖励密度与数据质量进行权衡。

Abstract: The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.

</details>


### [170] [Limitations of Scalarisation in MORL: A Comparative Study in Discrete Environments](https://arxiv.org/abs/2511.16476)
*Muhammad Sa'ood Shah,Asad Jeewa*

Main category: cs.LG

TL;DR: 该研究评估了多种多目标强化学习（MORL）算法在离散动作和观测空间环境中的表现，重点分析了线性与切比雪夫标量化函数在逼近帕累托前沿时的局限性。结果表明，标量化方法的表现高度依赖于环境及帕累托前沿形状，难以覆盖全部解空间，且权重配置复杂。相比之下，基于内层多策略的算法（如Pareto Q-Learning）更具鲁棒性和通用性，更适合动态不确定环境中的智能决策。


<details>
  <summary>Details</summary>
Motivation: 现有标量化方法在复杂不确定环境中难以准确逼近帕累托前沿，导致决策性能受限，亟需评估其局限性并探索更优替代方案。

Method: 采用外层多策略方法评估单策略算法MO Q-Learning（使用线性与切比雪夫标量化），同时研究内层多策略算法Pareto Q-Learning，以对比不同方法在多目标决策中的表现。

Result: 标量化方法性能受环境和帕累托前沿形状影响大，易丢失学习过程中发现的解，偏好特定区域解，权重调整困难；而内层多策略方法表现出更强的鲁棒性与泛化能力。

Conclusion: 在复杂、不确定的多目标环境中，内层多策略算法相较于标量化方法更具可持续性和通用性，有望支持更有效的智能决策。

Abstract: Scalarisation functions are widely employed in MORL algorithms to enable intelligent decision-making. However, these functions often struggle to approximate the Pareto front accurately, rendering them unideal in complex, uncertain environments. This study examines selected Multi-Objective Reinforcement Learning (MORL) algorithms across MORL environments with discrete action and observation spaces. We aim to investigate further the limitations associated with scalarisation approaches for decision-making in multi-objective settings. Specifically, we use an outer-loop multi-policy methodology to assess the performance of a seminal single-policy MORL algorithm, MO Q-Learning implemented with linear scalarisation and Chebyshev scalarisation functions. In addition, we explore a pioneering inner-loop multi-policy algorithm, Pareto Q-Learning, which offers a more robust alternative. Our findings reveal that the performance of the scalarisation functions is highly dependent on the environment and the shape of the Pareto front. These functions often fail to retain the solutions uncovered during learning and favour finding solutions in certain regions of the solution space. Moreover, finding the appropriate weight configurations to sample the entire Pareto front is complex, limiting their applicability in uncertain settings. In contrast, inner-loop multi-policy algorithms may provide a more sustainable and generalizable approach and potentially facilitate intelligent decision-making in dynamic and uncertain environments.

</details>


### [171] [Correlation-Aware Feature Attribution Based Explainable AI](https://arxiv.org/abs/2511.16482)
*Poushali Sengupta,Yan Zhang,Frank Eliassen,Sabita Maharjan*

Main category: cs.LG

TL;DR: ExCIR是一种基于相关性的可解释性方法，通过轻量级传输协议仅用少量数据即可复现完整模型的特征重要性排序。它通过鲁棒中心化（如中位数或中值）处理特征与输出的共变关系，量化特征与模型输出之间的符号一致共变。进一步提出的BlockCIR是ExCIR的分组扩展，将相关特征集作为一个整体评分，避免共线性区域（如同义词或重复传感器）的重复计算，提升稳定性与平滑性。在文本、表格、信号和图像等多种数据集上，ExCIR表现出与主流基线和全模型高度一致的可解释性，且显著降低计算时间，具备高效、稳定、可扩展的特性，适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有全局归因方法存在计算成本高、在相关输入下不稳定、难以扩展至大规模或异构数据集的问题，亟需一种高效、稳定且可扩展的可解释性方法以满足高风险应用中的透明度、信任和合规需求。

Method: 提出ExCIR方法，基于鲁棒中心化（如中位数或中值）对特征和输出进行预处理，计算其符号一致的共变关系；引入轻量级传输协议，仅使用部分数据即可还原完整模型的特征重要性排名；进一步提出BlockCIR，将相关特征聚类并作为整体评分单元，减少共线性带来的重复计数问题。

Result: ExCIR在多种数据类型（文本、表格、信号、图像）上均展现出与主流基线及全模型高度一致的解释结果，保持了稳定的top-k特征排序，同时大幅降低运行时间，证明其在计算效率、一致性与可扩展性方面的优势。

Conclusion: ExCIR及其分组扩展BlockCIR提供了一种计算高效、结果稳定、可扩展的可解释性框架，适用于复杂模型在真实场景中的部署，有效解决了现有方法在计算成本与稳定性上的瓶颈。

Abstract: Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.

</details>


### [172] [Loss Functions Robust to the Presence of Label Errors](https://arxiv.org/abs/2511.16512)
*Nicholas Pellegrino,David Szczecina,Paul Fieguth*

Main category: cs.LG

TL;DR: 本文提出两种新的简单损失函数，通过降低或忽略难以分类的样本（即可能含有标签错误的样本）的权重，以提升标签错误检测性能。实验表明，在人工污染的数据上，该方法在错误检测的F1分数上优于传统的交叉熵和焦点损失基线。


<details>
  <summary>Details</summary>
Motivation: 现有标签错误检测方法依赖于对标签错误具有鲁棒性的模型，但获取此类模型通常需在污染数据上训练，带来挑战。通过调整损失函数，尤其是借鉴焦点损失的思想，旨在改进模型对错误标签的容忍度。

Method: 提出两种新的损失函数，分别通过降权或忽略难以分类的样本（可能包含标签错误的样本），以减少模型对错误标签的过拟合。

Result: 在人工污染的数据集上，所提方法显著提升了标签错误检测的F1分数，优于传统交叉熵损失和焦点损失的基线表现。

Conclusion: 通过设计针对困难样本的去权重或忽略策略，新提出的损失函数有效提升了模型在标签错误检测任务中的性能，为构建更鲁棒的训练过程提供了可行方案。

Abstract: Methods for detecting label errors in training data require models that are robust to label errors (i.e., not fit to erroneously labelled data points). However, acquiring such models often involves training on corrupted data, which presents a challenge. Adjustments to the loss function present an opportunity for improvement. Motivated by Focal Loss (which emphasizes difficult-to-classify samples), two novel, yet simple, loss functions are proposed that de-weight or ignore these difficult samples (i.e., those likely to have label errors). Results on artificially corrupted data show promise, such that F1 scores for detecting errors are improved from the baselines of conventional categorical Cross Entropy and Focal Loss.

</details>


### [173] [Saving Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2511.16520)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: cs.LG

TL;DR: FMPlug 提出一种插件式框架，通过实例引导的时间依赖预热策略和锐利高斯性正则化，显著提升基础流匹配模型在图像修复和科学逆问题中的性能，使其成为可复用的实用先验。


<details>
  <summary>Details</summary>
Motivation: 基础流匹配模型在逆问题求解中表现不如特定领域或未训练的先验，亟需方法来释放其潜力。

Method: 引入 FMPlug 框架，结合实例引导的时间依赖预热策略与尖锐高斯性正则化，实现问题特异性引导并保持高斯结构。

Result: 在图像恢复和科学逆问题上均取得显著性能提升，验证了基础流匹配模型作为通用先验的可行性与实用性。

Conclusion: FMPlug 为使基础流匹配模型成为实际、可复用的逆问题求解先验提供了可行路径。

Abstract: Foundation flow-matching (FM) models promise a universal prior for solving inverse problems (IPs), yet today they trail behind domain-specific or even untrained priors. How can we unlock their potential? We introduce FMPlug, a plug-in framework that redefines how foundation FMs are used in IPs. FMPlug combines an instance-guided, time-dependent warm-start strategy with a sharp Gaussianity regularization, adding problem-specific guidance while preserving the Gaussian structures. This leads to a significant performance boost across image restoration and scientific IPs. Our results point to a path for making foundation FM models practical, reusable priors for IP solving.

</details>


### [174] [Dynamic Participation in Federated Learning: Benchmarks and a Knowledge Pool Plugin](https://arxiv.org/abs/2511.16523)
*Ming-Lun Lee,Fu-Shiang Yang,Cheng-Kuan Lin,Yan-Ann Chen,Chih-Yu Lin,Yu-Chee Tseng*

Main category: cs.LG

TL;DR: 本文提出了首个专为动态客户端参与联邦学习（DPFL）设计的开源基准框架，系统性地支持对DPFL挑战的研究。该框架可配置数据分布、参与模式和评估指标，用于评测四类主流联邦学习模型在动态环境下的表现，发现其性能显著下降。为此，作者提出KPFL——一种通用插件式方法，通过共享知识池、双龄权重与数据偏置加权及生成式知识蒸馏，缓解训练不稳定性并防止知识丢失。实验表明，动态参与对联邦学习性能有显著影响，而KPFL能有效提升模型鲁棒性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多假设客户端持续参与，忽视了实际中客户端可能间歇性加入或退出的动态参与场景；同时缺乏系统性的基准测试框架来支持对动态参与问题的研究。

Method: 构建一个可配置的数据分布、参与模式和评估指标的开源框架，用于模拟和评测动态客户端参与下的联邦学习表现；提出KPFL方法，利用共享知识池、双龄权重、数据偏置加权以及生成式知识蒸馏来应对动态参与带来的模型不稳定和知识流失问题。

Result: 实验结果显示，动态参与会导致联邦学习模型性能显著下降；而所提出的KPFL方法在多个基准设置下均有效提升了模型的鲁棒性和泛化能力，证明其在应对动态参与场景中的优越性。

Conclusion: 动态客户端参与是联邦学习中不可忽视的实际挑战，当前主流模型在此场景下表现不佳；提出的框架和KPFL方法为解决这一问题提供了有效的工具与解决方案，具有重要的实践价值和研究意义。

Abstract: Federated learning (FL) enables clients to collaboratively train a shared model in a distributed manner, setting it apart from traditional deep learning paradigms. However, most existing FL research assumes consistent client participation, overlooking the practical scenario of dynamic participation (DPFL), where clients may intermittently join or leave during training. Moreover, no existing benchmarking framework systematically supports the study of DPFL-specific challenges. In this work, we present the first open-source framework explicitly designed for benchmarking FL models under dynamic client participation. Our framework provides configurable data distributions, participation patterns, and evaluation metrics tailored to DPFL scenarios. Using this platform, we benchmark four major categories of widely adopted FL models and uncover substantial performance degradation under dynamic participation. To address these challenges, we further propose Knowledge-Pool Federated Learning (KPFL), a generic plugin that maintains a shared knowledge pool across both active and idle clients. KPFL leverages dual-age and data-bias weighting, combined with generative knowledge distillation, to mitigate instability and prevent knowledge loss. Extensive experiments demonstrate the significant impact of dynamic participation on FL performance and the effectiveness of KPFL in improving model robustness and generalization.

</details>


### [175] [FairLRF: Achieving Fairness through Sparse Low Rank Factorization](https://arxiv.org/abs/2511.16549)
*Yuanbo Guo,Jun Xia,Yiyu Shi*

Main category: cs.LG

TL;DR: 本文提出了一种面向公平性的低秩分解（FairLRF）框架，利用奇异值分解（SVD）在不牺牲模型性能的前提下提升深度学习模型的公平性。不同于传统SVD用于模型压缩，该工作首次将SVD应用于公平性增强，通过分析单位矩阵中各元素对不同群体偏见的贡献差异，选择性地移除偏见诱导成分，有效降低群体间差异。实验表明，该方法优于传统LRF及当前主流公平性优化技术，并进行了超参数影响的消融研究。


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解方法大多依赖计算开销大或导致模型准确率显著下降的策略，在资源受限的实际场景中实用性有限。因此亟需一种既能提升公平性又不影响性能的高效方法。

Method: 提出FairLRF方法，基于SVD分解权重矩阵，分析单位矩阵中各元素对敏感属性下群体偏见的贡献度，选择性移除高偏见贡献成分，从而在保持模型性能的同时提升公平性。

Result: 实验结果显示，FairLRF在多个数据集上均显著优于传统LRF和现有先进公平性方法，在公平性指标上表现更优，同时保持了较高的模型精度；消融实验验证了关键超参数对结果的影响。

Conclusion: 这是首个将SVD用于公平性增强而非压缩的开创性工作，证明了SVD在公平性优化中的潜力，为实际部署中兼顾公平与性能提供了新思路。

Abstract: As deep learning (DL) techniques become integral to various applications, ensuring model fairness while maintaining high performance has become increasingly critical, particularly in sensitive fields such as medical diagnosis. Although a variety of bias-mitigation methods have been proposed, many rely on computationally expensive debiasing strategies or suffer substantial drops in model accuracy, which limits their practicality in real-world, resource-constrained settings. To address this issue, we propose a fairness-oriented low rank factorization (LRF) framework that leverages singular value decomposition (SVD) to improve DL model fairness. Unlike traditional SVD, which is mainly used for model compression by decomposing and reducing weight matrices, our work shows that SVD can also serve as an effective tool for fairness enhancement. Specifically, we observed that elements in the unitary matrices obtained from SVD contribute unequally to model bias across groups defined by sensitive attributes. Motivated by this observation, we propose a method, named FairLRF, that selectively removes bias-inducing elements from unitary matrices to reduce group disparities, thus enhancing model fairness. Extensive experiments show that our method outperforms conventional LRF methods as well as state-of-the-art fairness-enhancing techniques. Additionally, an ablation study examines how major hyper-parameters may influence the performance of processed models. To the best of our knowledge, this is the first work utilizing SVD not primarily for compression but for fairness enhancement.

</details>


### [176] [Toward Valid Generative Clinical Trial Data with Survival Endpoints](https://arxiv.org/abs/2511.16551)
*Perrine Chassat,Van Tuan Nguyen,Lucas Ducrot,Emilie Lanoy,Agathe Guilloux*

Main category: cs.LG

TL;DR: 本文提出一种基于变分自编码器（VAE）的生成模型，用于在不假设独立删失的情况下联合生成混合类型协变量和生存结局，以构建合成对照组。该方法在隐私约束下的数据共享和对照组增强场景中表现优于基于GAN的基线模型，在保真度、效用和隐私保护方面均取得进展，但揭示了类型I错误率和检验效能的系统性校准问题，并提出后生成选择程序以改善校准效果。


<details>
  <summary>Details</summary>
Motivation: 临床试验面临患者群体分散、入组缓慢和成本不可持续等问题，尤其在肿瘤学和罕见病的后期试验中更为突出。现有外部对照组虽利用真实世界数据，但生成合成对照组使用生成式AI是一种有前景的替代方案。然而，时间至事件结果作为主要终点，其建模在删失和小样本条件下极具挑战性。现有生成方法多基于GAN，依赖大量数据、稳定性差且需强假设（如独立删失），因此亟需更稳健、灵活的生成框架。

Method: 提出一种新型变分自编码器（VAE）模型，将混合类型协变量与生存结局统一建模于潜变量框架下，无需假设独立删失。通过联合建模实现对复杂生存数据的生成，适用于合成控制臂构建。在两种实际场景中评估：(i) 隐私约束下的数据共享，以合成数据替代原始数据；(ii) 对照组增强，缓解治疗组与对照组间的不平衡。采用多种指标进行评估，包括生成质量、统计效用和隐私保护水平。

Result: 所提方法在合成和真实试验数据集上均显著优于现有GAN基线模型，在保真度、实用性和隐私保护方面表现更优。同时发现现有生成模型存在系统性校准偏差，即类型I错误率和检验效能不准确。提出一种后生成选择策略有效提升了校准性能，表明生成式生存建模已取得重要进展，但仍面临开放挑战。

Conclusion: 本研究展示了基于VAE的生成模型在构建合成控制臂方面的有效性与潜力，特别是在处理生存数据和隐私保护方面具有优势。尽管仍存在校准问题，但通过后处理改进可提升实用性。该工作为未来生成式AI在临床试验设计中的应用提供了新路径，也指出了进一步研究的方向。

Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.

</details>


### [177] [ECPv2: Fast, Efficient, and Scalable Global Optimization of Lipschitz Functions](https://arxiv.org/abs/2511.16575)
*Fares Fourati,Mohamed-Slim Alouini,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: ECPv2 是一种可扩展且理论基础扎实的全局优化算法，适用于未知 Lipschitz 常数的 Lipschitz 连续函数。它在 ECP 框架基础上引入三项创新：自适应下界避免无效接受区域、基于 Worst-m 内存机制限制历史评估比较范围、固定随机投影加速高维距离计算。理论证明 ECPv2 保持无遗憾保证并具有最优有限时间界，同时显著扩大接受区域概率。实验验证表明，其在多种高维非凸问题上性能优于或匹配现有最优方法，且大幅减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 解决 ECP 算法在计算成本高和早期行为过于保守方面的局限性，提升全局优化效率与实用性，尤其在高维、非凸、未知常数场景下。

Method: 提出 ECPv2 算法，包含三项关键技术：自适应下界机制以避免无效接受区域；Worst-m 内存机制，仅保留最近 m 次评估进行比较；固定随机投影以降低高维空间中距离计算复杂度。结合理论分析与实证实验，确保算法在理论上具备无遗憾性质，并在实践中高效可靠。

Result: ECPv2 在多个高维非凸优化基准测试中表现优异，性能稳定超越或媲美当前最先进优化器，同时显著缩短实际运行时间。理论分析证实其具备最优有限时间边界和高概率接受区域扩展能力。

Conclusion: ECPv2 通过结构化改进，在保持理论优势的同时有效提升了算法的实用性和可扩展性，是高维非凸全局优化中一个强有力的新工具。

Abstract: We propose ECPv2, a scalable and theoretically grounded algorithm for global optimization of Lipschitz-continuous functions with unknown Lipschitz constants. Building on the Every Call is Precious (ECP) framework, which ensures that each accepted function evaluation is potentially informative, ECPv2 addresses key limitations of ECP, including high computational cost and overly conservative early behavior. ECPv2 introduces three innovations: (i) an adaptive lower bound to avoid vacuous acceptance regions, (ii) a Worst-m memory mechanism that restricts comparisons to a fixed-size subset of past evaluations, and (iii) a fixed random projection to accelerate distance computations in high dimensions. We theoretically show that ECPv2 retains ECP's no-regret guarantees with optimal finite-time bounds and expands the acceptance region with high probability. We further empirically validate these findings through extensive experiments and ablation studies. Using principled hyperparameter settings, we evaluate ECPv2 across a wide range of high-dimensional, non-convex optimization problems. Across benchmarks, ECPv2 consistently matches or outperforms state-of-the-art optimizers, while significantly reducing wall-clock time.

</details>


### [178] [Almost Sure Convergence Analysis of Differentially Private Stochastic Gradient Methods](https://arxiv.org/abs/2511.16587)
*Amartya Mukherjee,Jun Liu*

Main category: cs.LG

TL;DR: 本文证明了在标准光滑性假设下，差分隐私随机梯度下降（DP-SGD）在非凸和强凸设置中均几乎必然收敛，前提是步长满足标准衰减条件。该分析还扩展到动量变体如DP-SHB和DP-NAG，通过精心设计的能量函数实现了类似保证。这些结果为差分隐私优化提供了更强的理论基础，表明尽管存在隐私扰动，算法在凸与非凸情形下仍具有路径稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有对DP-SGD的理论分析主要关注期望收敛或高概率收敛，但缺乏对单条轨迹几乎必然收敛的探讨。本文旨在填补这一理论空白，增强对DP-SGD长期行为的理解。

Method: 基于标准光滑性假设，采用渐进分析方法，结合步长衰减条件，构建能量函数以证明几乎必然收敛性，并将方法推广至动量型算法如DP-SHB和DP-NAG。

Result: 证明了DP-SGD在非凸和强凸环境下均几乎必然收敛；其动量变体（DP-SHB、DP-NAG）也具备类似的几乎必然收敛性质。

Conclusion: 本研究为差分隐私优化提供了更坚实的理论支持，表明即使在引入隐私噪声后，算法仍保持路径上的稳定性，适用于广泛的优化场景。

Abstract: Differentially private stochastic gradient descent (DP-SGD) has become the standard algorithm for training machine learning models with rigorous privacy guarantees. Despite its widespread use, the theoretical understanding of its long-run behavior remains limited: existing analyses typically establish convergence in expectation or with high probability, but do not address the almost sure convergence of single trajectories. In this work, we prove that DP-SGD converges almost surely under standard smoothness assumptions, both in nonconvex and strongly convex settings, provided the step sizes satisfy some standard decaying conditions. Our analysis extends to momentum variants such as the stochastic heavy ball (DP-SHB) and Nesterov's accelerated gradient (DP-NAG), where we show that careful energy constructions yield similar guarantees. These results provide stronger theoretical foundations for differentially private optimization and suggest that, despite privacy-induced distortions, the algorithm remains pathwise stable in both convex and nonconvex regimes.

</details>


### [179] [gfnx: Fast and Scalable Library for Generative Flow Networks in JAX](https://arxiv.org/abs/2511.16592)
*Daniil Tiapkin,Artem Agarkov,Nikita Morozov,Ian Maksimov,Askar Tsyganov,Timofei Gritsaev,Sergey Samsonov*

Main category: cs.LG

TL;DR: gfnx is a fast, scalable JAX-based package for training and evaluating Generative Flow Networks (GFlowNets), offering extensive benchmarks, diverse environments (e.g., sequence generation, molecular design, Bayesian learning), and significant speedups over PyTorch-based implementations—up to 55× on CPU and 80× on GPU. It aims to standardize evaluation and accelerate GFlowNet research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of efficient, standardized tools for training and benchmarking GFlowNets, enabling faster experimentation and reproducible evaluation across diverse applications.

Method: gfnx leverages JAX for high-performance computation, provides single-file implementations of core GFlowNet objectives, and includes synthetic hypergrids, sequence generation tasks, molecular generation with reward designs, phylogenetic tree construction, Bayesian structure learning, and Ising model sampling.

Result: gfnx achieves up to 55× speedup on CPU-based sequence generation and up to 80× speedup on GPU-based Bayesian network learning compared to PyTorch-based baselines, demonstrating strong scalability and efficiency.

Conclusion: gfnx establishes a new standard for empirical evaluation of GFlowNets through its speed, modularity, and rich benchmark suite, significantly accelerating research and application development in the field.

Abstract: In this paper, we present gfnx, a fast and scalable package for training and evaluating Generative Flow Networks (GFlowNets) written in JAX. gfnx provides an extensive set of environments and metrics for benchmarking, accompanied with single-file implementations of core objectives for training GFlowNets. We include synthetic hypergrids, multiple sequence generation environments with various editing regimes and particular reward designs for molecular generation, phylogenetic tree construction, Bayesian structure learning, and sampling from the Ising model energy. Across different tasks, gfnx achieves significant wall-clock speedups compared to Pytorch-based benchmarks (such as torchgfn library) and author implementations. For example, gfnx achieves up to 55 times speedup on CPU-based sequence generation environments, and up to 80 times speedup with the GPU-based Bayesian network structure learning setup. Our package provides a diverse set of benchmarks and aims to standardize empirical evaluation and accelerate research and applications of GFlowNets. The library is available on GitHub (https://github.com/d-tiapkin/gfnx) and on pypi (https://pypi.org/project/gfnx/). Documentation is available on https://gfnx.readthedocs.io.

</details>


### [180] [Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies](https://arxiv.org/abs/2511.16596)
*Zohar Rimon,Elisei Shafer,Tal Tepper,Efrat Shimron,Aviv Tamar*

Main category: cs.LG

TL;DR: 本文提出了一种基于自监督学习的机器触诊方法，通过编码器-解码器框架从触觉测量序列中学习物体的表征，用于触觉成像和变化检测等下游任务。研究构建了仿真环境与真实世界数据集（结合机器人触觉传感器与MRI真值图像），验证了该方法在捕捉复杂触觉模式方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统触诊依赖人工，缺乏自动化和量化分析手段；现有触觉系统仅能提供力的简单映射，无法充分捕捉物体内部结构信息。因此需要一种能够从触觉数据中学习深层表征的新方法，以实现更精确的触觉成像与异常检测。

Method: 采用编码器-解码器架构进行自监督学习，利用机器人采集的触觉序列数据训练模型，使其预测不同位置的感官读数；通过学习得到的表征用于触觉成像与变化检测任务。

Result: 实验表明，所学表征能够有效还原软组织的内部结构，并成功识别出物体形态的变化，在仿真与真实数据上均表现良好。

Conclusion: 本研究验证了基于自监督学习的机器触诊方法的可行性，其学习到的表征具有丰富的语义信息，可广泛应用于医疗触觉感知的自动化与智能化。

Abstract: Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a $\textit{representation}$ from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection.

</details>


### [181] [Stabilizing Policy Gradient Methods via Reward Profiling](https://arxiv.org/abs/2511.16629)
*Shihab Ahmed,El Houcine Bergou,Aritra Dutta,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出一种通用的奖励轮廓框架，可无缝集成到任何策略梯度算法中，通过基于高置信度性能估计选择性更新策略，理论上保证不减慢基线算法收敛速度，且以高概率实现稳定、单调的性能提升。实验表明，在8个连续控制基准测试中，该方法可使收敛速度提高最多1.5倍，返回值方差减少最多1.75倍，显著提升策略学习的可靠性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有策略梯度方法因梯度估计方差过高，常面临奖励改进不可靠和收敛缓慢的问题，亟需一种能提升稳定性与效率的通用改进框架。

Method: 提出一种通用奖励轮廓框架，通过高置信度性能估计选择性更新策略，可与任意策略梯度算法结合，确保收敛性不受影响的同时提升性能改善的稳定性。

Result: 在Box2D和MuJoCo/PyBullet的8个连续控制任务中，该方法实现最高1.5倍的收敛加速和最高1.75倍的返回值方差降低，验证了其有效性与鲁棒性。

Conclusion: 所提出的奖励轮廓框架为复杂环境中策略学习提供了一条通用、理论支持充分且高效的路径，显著提升了策略梯度方法的可靠性与效率。

Abstract: Policy gradient methods, which have been extensively studied in the last decade, offer an effective and efficient framework for reinforcement learning problems. However, their performances can often be unsatisfactory, suffering from unreliable reward improvements and slow convergence, due to high variance in gradient estimations. In this paper, we propose a universal reward profiling framework that can be seamlessly integrated with any policy gradient algorithm, where we selectively update the policy based on high-confidence performance estimations. We theoretically justify that our technique will not slow down the convergence of the baseline policy gradient methods, but with high probability, will result in stable and monotonic improvements of their performance. Empirically, on eight continuous-control benchmarks (Box2D and MuJoCo/PyBullet), our profiling yields up to 1.5x faster convergence to near-optimal returns, up to 1.75x reduction in return variance on some setups. Our profiling approach offers a general, theoretically grounded path to more reliable and efficient policy learning in complex environments.

</details>


### [182] [Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter](https://arxiv.org/abs/2511.16665)
*Qinghao Hu,Shang Yang,Junxian Guo,Xiaozhe Yao,Yujun Lin,Yuxian Gu,Han Cai,Chuang Gan,Ana Klimovic,Song Han*

Main category: cs.LG

TL;DR: TLT提出一种无损加速推理强化学习训练的系统，通过集成自适应推测解码解决长尾响应导致的效率瓶颈。其核心包括：(1) 自适应草稿模型（Adaptive Drafter），在空闲GPU上持续训练以保持与目标模型对齐；(2) 自适应回放引擎（Adaptive Rollout Engine），通过内存高效预捕获CUDAGraph并动态选择最优推测解码策略。实验表明，TLT实现超过1.7倍的端到端训练加速，保持模型精度，并生成可用于高效部署的高质量草稿模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的强化学习训练因响应生成存在长尾分布，导致少数极长响应严重拖慢训练效率，造成资源浪费和成本上升。现有方法难以有效应对动态工作负载、目标模型演化及草稿模型训练开销等挑战。

Method: 提出TLT系统，结合自适应推测解码技术，包含两个关键组件：自适应草稿模型（Adaptive Drafter）利用空闲GPU持续训练，无需额外成本维持与目标模型的一致性；自适应回放引擎（Adaptive Rollout Engine）维护预捕获的CUDAGraph池，根据输入批次动态选择最优推测解码策略，实现高效且灵活的推理加速。

Result: TLT在真实场景中实现了超过1.7倍的端到端强化学习训练速度提升，同时保持了与基线相当的模型准确率，并生成了一个可直接用于高效部署的高质量草稿模型，为后续推理服务提供额外价值。

Conclusion: TLT成功解决了推理强化学习训练中的长尾效率瓶颈问题，通过轻量级自适应机制实现无损加速，兼具性能提升与实用价值，是大规模语言模型训练优化的重要进展。

Abstract: The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.

</details>
